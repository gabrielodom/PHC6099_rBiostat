---
title: "Generalized Linear Models: Negative Binomial"
author: "Natalie Goulett"
toc: true
number-sections: true
format: html
embed-resources: false
---

## What is Negative Binomial Regression?

Poisson regression is commonly used when modeling count data. In this type of GLM, the counts outcome is required to follow a Poisson distribution wherein its mean is equal to its variance. A Poisson distribution, as we have learned, describes the probability of a given number of events occurring within a fixed interval. Poisson regression models are accurate but inefficient for data that are under-dispersed or over-dispersed, that is, the variance is smaller or larger than the mean. Standard error estimates are thus underestimated by Poisson regression, which results in an inflated chance of type 1 error (incorrectly rejecting a true null hypothesis). In real count data, the variance is often greater than the mean. 

Instead, negative binomial regression is used to model count data when the data are over-dispersed. A binomial distribution is a distribution of the number of successes (x) in a fixed number (n) of independent Bernoulli trials. The negative binomial distribution, on the other hand, is the distribution of the number of trials (x) needed to get a fixed number of successes (x). The negative binomial regression model has a dispersion parameter, $\theta$, to represent random error. This allows it more flexibility to model distributions that cannot follow a Poisson distribution.

Furthermore, a distribution of count data that have an attainable maximum value can not follow a Poisson distribution, whose support ranges from zero to infinity. The negative binomial distribution, with its extra $\theta$ parameter, provides a better fit for these types of count data. If the largest actual observed values are relatively close to the largest possible value of the outcome, a negative binomial distribution should be used. 
<!--For a distribution with a very large number of observations and a small probability of success (or, the probability that the event occurs), a Poisson distribution may be used. -->

For an in-depth explanation of the theory behind the negative binomial distribution and regression model, along with examples, please read [this chapter](https://www.ncss.com/wp-content/themes/ncss/pdf/Procedures/NCSS/Negative_Binomial_Regression.pdf) by NCSS Statistical Software.


### Negative Binomial Regression Formula

The negative binomial regression model formula is as follows: 

$$
\log(\mu_i) = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_k X_{ik}
$$

where:

- $\mu_i$ is the mean of the response variable for the $i$-th observation,
- $\beta_0$ is the intercept,
- $\beta_1, \beta_2, \ldots, \beta_k$ are the regression coefficients, and
- $X_{i1}, X_{i2}, \ldots, X_{ik}$ are the predictor variables.

The variance of the response variable $Y_i$ is given by:

$$
\text{Var}(Y_i) = \mu_i + \frac{\mu_i^2}{\theta}
$$

where:
$\theta$ is the **dispersion parameter**. It measures the extra variation, or dispersion, in the data that is not explained by the Poisson distribution. A small theta indicates the variance is much larger than $\mu$. As theta increases, the variance approaches $\mu$.


### Assumptions of Negative Binomial GLM

- **Independence:** Observations should be independent of one another.
- **Linearity:** the relationship between the dependent variable and independent variables should be linear.
- **No Multicolinearity:** Independent variables should not be highly correlated with one another. 
<!-- - **Over-Dispersion:** Variance should be significantly greater than the mean, indicated by a large dispersion parameter... Except for this example -->


### Other GLMs for Count Data

[**A zero-inflated Poisson regression**](https://stats.oarc.ucla.edu/r/dae/zip/) is used to model discrete (count) data that has an excess amount of zero outcomes. This model may be used to model data that are over-dispersed due to the presence of excessive zeros that are generated by a separate process from the count values.

[**Zero-truncated Poisson regression**](https://stats.oarc.ucla.edu/r/dae/zip/) is used to model count data that can not contain a zero value. This model should be used if the data generating process does not allow for a zero value to occur - for example, when measuring the number of days patients spent at a hospital.

**Censored regression models**, such as the [Tobit model](https://stats.oarc.ucla.edu/r/dae/tobit-models/), are used to model data with right- or left-censoring in the dependent variable. Censoring from the right occurs when the actual values at or above the maximum threshold are recorded as the value of that threshold. In left-censoring, values below a threshold take on the value of that threshold.

<!-- discuss offset option or other models? see https://www.rpubs.com/Shaunson26/offsetglm -->



## Mental Illness Example

To explore a possible relationship between mental illness and illicit drug use, we will create a negative binomial GLM using participant data from a clinical trial to predict the number of comorbid mental illnesses that clinical trial participants have been diagnosed with in addition to opioid use disorder (OUD). The data were collected during the clinical trial ["CTN-0094: Individual Level Predictive Modeling of Opioid Use Disorder Treatment Outcome"](https://ctnlibrary.org/protocol/ctn0094/) and provided by the [`public.ctn0094data`](https://cran.r-project.org/web/packages/public.ctn0094data/index.html) and [`public.ctn0094extra`](https://cran.r-project.org/web/packages/public.ctn0094extra/index.html) packages. These packages contain demographic, clinical, and drug use data on trial participants seeking treatment for OUD.


### Install Packages
```{r}
#| label: install-packages
#| warning: false
#| echo: false

 # install.packages("tidyverse")
 # install.packages("skimr")
 # install.packages("MASS")
 # install.packages("public.ctn0094data") # loads with ""extra package
 # install.packages("public.ctn0094extra")
 # install.packages("GGally")

library(skimr)
library(MASS)
library(tidyverse)
library(public.ctn0094data)
library(public.ctn0094extra)
library(GGally)
```


### Data Cleaning and Exploration

```{r}
#| label: create-psych-dataset

psychDx_df <- 
  psychiatric %>% 
  # select Identifier column `who` and disease state columns
  dplyr::select("who", starts_with("has_")) %>% 
  dplyr::select(!ends_with("_dx")) %>% 
  drop_na()

head(psychDx_df)

countComorbidDx_int <- 
  psychDx_df %>% 
  dplyr::select(-who) %>% 
  # count number of psychiatric disorders per ID
  mutate(
    across(
      # create data frame containing `TRUE` if the value in each cell of the 
      #   columns equals "Yes" and `FALSE` otherwise
      .cols = has_schizophrenia:has_epilepsy,
      .fns = ~ `==`(.x, "Yes")
    )
  ) %>% 
  # `rowSums(.)` sums the number of `TRUE` values in this data frame for each
  #   row. Result is converted to integers for our new column, `nComorbidDx`.
  mutate(nComorbidDx = as.integer(rowSums(.))) %>% 
  # extract column as vector
  pull(nComorbidDx)

# add vector of count of psychiatric disorders to dataset as a column
psychDx_df$nComorbidDx <- countComorbidDx_int

# visualize distribution of psych disorder counts (correctly, using a bar chart)
ggplot(psychDx_df) +
  aes(nComorbidDx) +
  labs(
    title = "Bar Chart of Comorbid Dx Distribution",
    x = "Number of Comorbid Diagnoses",
    y = "Count"
  ) +
  geom_bar()

# visualize with a histogram (inappropriate for our count data)
hist(
  countComorbidDx_int, 
  main = "Histogram (WRONG) of Comorbid Dx Distribution",
  xlab = "Number of Comorbid Diagnoses",
  ylab = "Count"
)
  
```

::: {.callout-note appearance="simple"}

## Bar Chart vs. Histogram

When graphing count data, use a bar chart, not a histogram. Histograms treat data as continuous and will graph count data as if it may contain decimals.

:::

This distribution supports using negative binomial regression because our count cannot go past 6. A normal (Poisson) binomial regression support, however, ranges from 0 to infinity, and would therefore be an inappropriate distribution to model the data with. The negative binomial model has more flexibility to model count data that has a maximum value.

We will now join our dataset with the `demographics` and `rbs` datasets from the [Clinical Trials Network study CTN-0094](https://ctnlibrary.org/protocol/ctn0094/) by the subject ID, `"who"`, to get demographic and substance use data for each subject.
<!-- what is a promise?-->

```{r}
#| label: add-predictors

# inspect demographics, qol, and rbs datasets from CTN and join to PsychDx
data("demographics")
data("rbs")

# `rbs` is in long format, so pivot wider
rbs_wide <- rbs %>% 
   pivot_wider(
    id_cols = who,
    names_from = what,
    values_from = days
  )

# left join by `who` to retain rows from `demographics` for all matching `who`s
#   in psychDx_df
dxPred_df <- left_join(psychDx_df, demographics, by = "who") %>% 
  left_join(rbs_wide, by = "who")

```

```{r}
#| label: remove-obsolete-columns

# the diagnosis columns for each disease and `who` column are no longer needed
#   after the join and thus are removed
dxPred2_df <- dxPred_df %>% 
  dplyr::select(
    -(has_schizophrenia:has_epilepsy),
    -who
  )

print(dxPred2_df)
```


### Predictor Selection

We will now check to see how much data are missing. A negative binomial GLM can only be built using complete rows, meaning rows with no missing values. Predictors with a high proportion of data should not be used as they would negatively impact the GLM's fit to the data. If we use all features, we will be left with 567 out of 3080 rows with complete cases. The number of usable rows can be increased by removing incomplete predictor columns.

```{r}
#| label: check-missing-data
# check for features with lots of missing data and remove them.

skim(dxPred2_df)
# count rows missing any data on substance use
sum(is.na(dxPred2_df$cocaine) | is.na(dxPred2_df$heroin) | is.na(dxPred2_df$speedball) | is.na(dxPred2_df$opioid) | is.na(dxPred2_df$speed))
```

If I use all features, I will be left with 567 out of 3080 rows with complete cases. I can increase the number of complete rows by removing incomplete predictor columns. The features `job`, `is_living_stable`, `education`, and `marital` from the `demographics ` dataset are all missing data on about half of the participants (N = 3080) and should therefore be removed from the model. Furthermore, about one-third of participants are missing data on their drug usage from the `rbs` dataset. I will also remove `speedball` and `speed` as they are 27% missing.

We will use `cor()` to create a correlation matrix of the continuous independent variables to investigate possible multicolinearity.

```{r}
#| label: pairs-plot
#| warning: false

# remove features missing > 20% of observations
dxPred3_df <- dxPred2_df %>% 
  dplyr::select(
    -(job:marital),
    - speed,
    - speedball
  )

print(dxPred3_df)

# calculate Spearman correlation matrix for the continuous predictors
round(
  cor(
    dxPred3_df[, c("age", "cocaine", "heroin", "opioid")],
    # use only pairwise complete observations for calculating correlations
    use = "pairwise.complete.obs",
    method = "spearman"
  ),
  # round to 2 digits for easy interpretation
  digits = 2
)
```

I will also remove `opioid` because it is highly correlated with `heroin`, which I chose to retain over `opioid` simply because it contains more data.

The next step is to create a pairs plot to investigate the remaining predictors and determine whether any pairs are highly correlated.

```{r}
#| label: pairs-plot-2
#| warning: false
#| echo: false

# remove features missing data on > 20% of observations
dxPred4_df <- 
  dxPred3_df %>% 
  dplyr::select(-opioid)

# use ggpairs to explore pairwise relationships of remaining features
pairs_plot <- ggpairs(
  data = dxPred4_df,
  aes(alpha = 0.5), 
  lower = list(
    continuous = "smooth",
    combo = "facetdensity",
    discrete = "facetbar"
  ),
  upper = list(
    continuous = "cor", combo = "box_no_facet", discrete = "ratio"
  ),
  title = "Pairs plot of all comorbidity predictors",
  labeller = label_wrap_gen(width = 10)
) +
  theme_bw(base_size = 10) + 
  theme(
    strip.text = element_text(size = 8), 
    axis.text = element_text(size = 8), 
    plot.title = element_text(size = 14, face = "bold"),
    legend.position = "bottom"
  )

print(pairs_plot)
```

The remaining variables exhibit no strong correlations or unexpected behavior, so they will all be used in the model.


### Negative Binomial GLM

```{r}
#| label: nb-glm

# create negative binomial regression to predict number of comorbid diagnoses
nb_mod <- MASS::glm.nb(
  nComorbidDx ~
    age +
    is_hispanic +
    relevel(race, ref = "White") +
    is_male +
    cocaine * heroin,
  data = dxPred4_df
) 

summary(nb_mod)
```

The beta coefficients indicate the log change in the expected count of comorbid diagnoses for a one-unit change in the predictor variable. For continuous predictors like `age`, `cocaine`, `heroin`, and the `cocaine:heroin` interaction, a positive coefficient means that as the predictor increases, the expected count increases. For categorical predictors like `is_hispanic`, `is_male`, and `race`, the coefficient indicates the change in the log count relative to the reference category.

We can compare the null deviance to the residual deviance to determine whether our predictors are useful for predicting the dependent variable. The **null deviance** is a measure of how well `nComorbidDx` is predicted by the null model, which consists of the intercept with no predictors, whereas the **residual deviance** quantifies how well the `nComorbidDx` can be predicted by the model with the predictors. A residual deviance that is much higher than the degrees of freedom indicates data are over-dispersed. The residual deviance is smaller than the null deviance of this model, meaning the predictors are predicting.

After removing insignificant predictors, we are left with this model, where cocaine and heroin are "the number of days in the past 30 days where the substance was used":

$$
n_{\text{Dx}} = e^{\left( 0.252 - 0.362 \cdot \text{race}_{\text{Black}} - 0.517 \cdot \text{gender}_{\text{Male}} + 0.023 \cdot \text{cocaine} - 0.001 \cdot \text{cocaine} \cdot \text{heroin} \right)}
$$

### Interpretation

We must exponentiate the beta coefficients to calculate the multiplicative effect they have on an individual's expected number of comorbid diagnoses.

```{r}
#| label: exponentiate-beta-coefficients

# exponentiate intercept
exp(coef(nb_mod)[1])
# exponentiate beta for race = Black group
exp(coef(nb_mod)[4])
# exponentiate beta for male = Yes group
exp(coef(nb_mod)[7])
# exponentiate beta for cocaine use
exp(coef(nb_mod)[8])
# exponentiate beta for cocaine:heroin interaction
exp(coef(nb_mod)[10])
```

Based on this model, the expected number of comorbid mental health diagnoses for a white, non-hispanic female who does not use cocaine alone and does not use both cocaine and heroin together is 1.29 (on average, at least one other diagnosis in addition to OUD). We expect black individuals to have 69.7% fewer comorbid diagnoses than individuals of other racial groups when all other predictors are zero (p < 0.0001); that is, the expected number of comorbid mental health diagnoses for a black, non-hispanic female who does not use cocaine alone and does not use both cocaine and heroin together is 0.896. Men are also expected to have 59.6% fewer comorbid diagnoses compared to women when all other predictors are zero (p < 0.0001); that is, the expected number of comorbid mental health diagnoses for a white, non-hispanic male who does not use cocaine alone and does not use both cocaine and heroin together is 0.768.

Furthermore, an individual's number of comorbid diagnoses increased by 2.37% for every 1-day increase in days of cocaine use per month (p = 0.001). Interestingly, heroin moderated this relationship: for every 1-day increase in both heroin and cocaine usage per month, an individual's number of comorbid diagnoses was expected to decrease by 0.060% (p = 0.031).


### Goodness-of-Fit Testing

To test whether the model fits the data well, I will overlay our distribution of comorbid diagnoses over a simulated negative binomial distribution that was generated using the parameters of the empirical distribution following [this example](https://stackoverflow.com/questions/77651778/how-to-fit-negative-binomial-distribution-on-a-histogram-using-ggplot2/77651920#77651920).

```{r}
#| label: Dx-distribution-vs-negative-binomial-distribution

set.seed(20220625)

# generate a true negative binomial distribution with the same, n, number of 
#   successes, and probability of success

# create df of the Dx distribution containing only values for which data are 
#   complete (and thus used in the regression, nb_mod)
xx <- dxPred4_df[complete.cases(dxPred4_df), "nComorbidDx"]

# xx must be a numeric vector for fitdistr; put column data in vector
xx_fit <- as.numeric(xx$nComorbidDx)

# fit parameters of Dx distribution
fit <- MASS::fitdistr(xx_fit, densfun = "negative binomial")

# create example negative binomial data frame (is this supposed to replace 
#   the simulated dist, or should I name it something else?)
yy <- data.frame(negative_binomial = 0:45)

# calculate density for each observation given mu and size from simulated
#   negative binomial distribution
yy$density <- dnbinom(
  yy$negative_binomial,
  mu = fit$estimate["mu"],
  size = fit$estimate["size"]
)

# convert to factor to make x-axis discrete
xx$nComorbidDx <- as.factor(xx$nComorbidDx)

# graph Dx distribution with blue bars
ggplot(data = xx, aes(x = nComorbidDx)) +
  geom_bar(
    # set y = the density (frequency) of each Dx. `..density..` is a variable
    #  created by `geom_histogram()`.
    aes(y = after_stat(count/sum(count))),
    color = "black",
    fill = "lightblue"
  ) +
  # graph red lines to represent the simulated neg binomial distribution with 
  #   the same parameters as the Dx distribution
  labs(
    title = "Comorbid Mental Illness Dx Distribution vs. A Negative Binomial Distribution",
    x = "Number of Comorbid Diagnoses",
    y = "Density"
  ) +
  # Add red lines to represent theoretical negative binomial distribution
  geom_linerange(
    data = yy,
    aes(x = as.factor(negative_binomial), ymin = 0, ymax = density),
    color = "red") +
  scale_x_discrete(limits = factor(0:10))
```

The theoretical negative binomial distribution fits the empirical data well. It returns small probabilities of greater than six comorbid diagnoses occurring, but is a better fit to the data overall than a Poisson distribution. To illustrate this point, I will also compare the empirical distribution to a simulated Poisson distribution using the parameters of the empirical distribution.

```{r}
#| label: Dx-distribution-vs-Poisson-distribution

# Fit parameters of Dx distribution to a Poisson distribution
fit2 <- MASS::fitdistr(xx_fit, densfun = "Poisson")

# Create example Poisson data frame
yy2 <- data.frame(poisson = 0:45)

# Calculate density for each observation given lambda from the Poisson
#   distribution
yy2$density <- dpois(yy2$poisson, lambda = fit2$estimate["lambda"])

# Graph Dx distribution with blue bars
ggplot(data = xx, aes(x = nComorbidDx)) +
  geom_bar(
    # Set y = the density (frequency) of each Dx. `..density..` is a variable 
    #   created by `geom_histogram()`.
    aes(y = after_stat(count/sum(count))),
    color = "black",
    fill = "lightblue"
  ) +
  # Add labels
  labs(
    title = "Comorbid Mental Illness Dx Distribution vs. A Poisson Distribution",
    x = "Number of Comorbid Diagnoses",
    y = "Density"
  ) +
  # Add red lines to represent the theoretical Poisson distribution
  geom_linerange(
    data = yy2,
    aes(x = as.factor(poisson), ymin = 0, ymax = density),
    color = "red"
  ) +
  scale_x_discrete(limits = factor(0:10))
```

Notice the Poisson distribution models the density of zero and one values as roughly equal, thereby underestimating the density of zero values and overestimating the density of one values in the empirical dataset. Although the differences in fit between the two examples may seem insignificant, remember that precision is key when making decisions that impact people's health. (pro tip: if the graphs don't support my point, just keep re-rendering them until they do!)
