[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "An R Cookbook for Public Health",
    "section": "",
    "text": "0.1 Source Code for PHC6099 Course Notes\nThis material is for the course “R Computing for Health Sciences”. The course notes are published here: https://gabrielodom.github.io/PHC6099_rBiostat/",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course Materials for PHC 6099: 'R Computing for Health Sciences'</span>"
    ]
  },
  {
    "objectID": "index.html#source-code-for-phc6099-course-notes",
    "href": "index.html#source-code-for-phc6099-course-notes",
    "title": "An R Cookbook for Public Health",
    "section": "",
    "text": "0.1.1 Topics\nThe chapters are:\n\nExploring Data\n\nggplot2:: mosaic plots, histograms, and violin plots\nggplot2:: scatterplots and facets \nskimr::\ntable1::\ngtsummary::\n\nOne-Sample Tests\n\n\\(Z\\)-test\nPaired \\(t\\)-test\nPaired Wilcoxon test\nTransformations to Normality\nMcNemar’s Test\nFisher’s Exact Test\nChi-Square Goodness of Fit\nBootstrapped Confidence Intervals\n\nTwo-Sample Tests\n\n\\(t\\)-test\nWelch’s \\(t\\)-test\nMann-Whitney \\(U\\) test\nCochran’s \\(Q\\) test\n\\(\\chi^2\\) Test for Independence\n\nANOVA and Linear Regression\n\nOne-Way ANOVA\nTwo-way ANOVA\nWelch’s ANOVA\nKruskal-Wallace Test\nTukey HSD Post-Hoc Test\nRepeated Measures ANOVA\nRandom Intercept Models\nCorrelation Matrices and Covariances\nMultiple Regression (linear)\nPolynomial regression\n\nGeneralized Linear Models\n\nGeneralized Linear Models: Binary\nGeneralized Linear Models: Ordered\nGeneralized Linear Models: Count (Poisson)\nGeneralized Linear Models: Count (Negative Binomial)\n\nSpecial Topics\n\nLinear Mixed Effects Models\nStructural Equation Models\nCox Proportional Hazards Regression\n(TBD) Multivariate Methods for Genetics/Genomics\nRidge, LASSO, and Elastic Net Regression\n\nPower Calculations (in progress)\n\n\n\n0.1.2 Lesson Outline\nThis is a shell of a lesson that can be copied and pasted for new lessons (or to edit and clean up existing lessons). If you copy this shell, then change all the headings from level 4 to 2. Replace &lt;the method&gt; with the name of your method, or its abbreviation. The file lessons/00_lesson_template.qmd has a .qmd template with these sections.\n\n0.1.2.1 Introduction to &lt;the method&gt;\n\n\n0.1.2.2 Mathematical definition of &lt;the method&gt;\n\n\n0.1.2.3 Data source and description\n\n\n0.1.2.4 Cleaning the data to create a model data frame\n\n\n0.1.2.5 Assumptions of &lt;the method&gt;\n\n\n0.1.2.6 Checking the assumptions with plots\n\n\n0.1.2.7 Code to run &lt;the method&gt;\n\n\n0.1.2.8 Code output\n\n\n0.1.2.9 Brief interpretation of the output",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course Materials for PHC 6099: 'R Computing for Health Sciences'</span>"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "2  About this Book",
    "section": "",
    "text": "2.1 About these Chapters\nThese are the written lecture materials for the class PHC 6099 at Florida International University’s Stempel College of Public Health. Each of the lessons were written by students, so we don’t guarantee that they are always mathematically/statistically accurate. You should use this material as a simple place to start to use these methods, but always read more about these methods when you use them to give yourself a better understanding of their theoretical foundations. This material should not be used to replace a traditional textbook in applied biostatistics. Here are some rather standard books on applied biostatistics (there are free/cheap versions on the internet for most of these texts, but I trust you to find them yourself):",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>About this Book</span>"
    ]
  },
  {
    "objectID": "about.html#about-these-chapters",
    "href": "about.html#about-these-chapters",
    "title": "2  About this Book",
    "section": "",
    "text": "Biostatistical Analysis. Jerrold H. Zar. https://www.pearson.com/en-us/subject-catalog/p/biostatistical-analysis/P200000006419/9780134995441.\nRegression Modelling Strategies. Frank E. Harrell, Jr. https://link.springer.com/book/10.1007/978-3-319-19425-7\nCategorical Data Analysis. Alan Agresti. https://onlinelibrary.wiley.com/doi/book/10.1002/0471249688",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>About this Book</span>"
    ]
  },
  {
    "objectID": "about.html#getting-help-in-r",
    "href": "about.html#getting-help-in-r",
    "title": "2  About this Book",
    "section": "2.2 Getting Help in R",
    "text": "2.2 Getting Help in R\nThis is the second semester of the “R” course sequence at FIU, so we spend very little time explaining the basics of the R language (the first semester is PHC6701; the text for that class is available here: https://gabrielodom.github.io/PHC6701_r4ds/). If you are new to R, please go back to the previous semester’s material and work through that first. If you want to see how we made this book, the source code and data sets for this book are available here: https://github.com/gabrielodom/PHC6099_rBiostat.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>About this Book</span>"
    ]
  },
  {
    "objectID": "01_header_EDA.html",
    "href": "01_header_EDA.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Text Outline\nThis text covers the basics of:",
    "crumbs": [
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "01_header_EDA.html#text-outline",
    "href": "01_header_EDA.html#text-outline",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Exploring data\nOne-sample tests\nTwo-sample tests\nLinear regression and ANOVA\nGeneralized linear models\nSome special topics\nMaybe a bit about power calculations…",
    "crumbs": [
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "01_header_EDA.html#part-outline",
    "href": "01_header_EDA.html#part-outline",
    "title": "Exploratory Data Analysis",
    "section": "Part Outline",
    "text": "Part Outline\nThis part of the text contains the chapters on the following packages which are useful for exploratory data analysis:\n\nggplot2:: mosaic plots, histograms, and violin plots\nggplot2:: scatterplots and facets\nskimr::\ntable1::\ngtsummary::",
    "crumbs": [
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "lessons/01_mosaic_violin.html",
    "href": "lessons/01_mosaic_violin.html",
    "title": "\n3  Mosaic & Box/Violin Plots\n",
    "section": "",
    "text": "3.1 Packages for this Lesson\n# Installing Required Packages\n# install.packages(\"public.ctn0094data\")\n# install.packages(\"tidyverse\")\n# install.packages(\"ggmosaic\")\n\n# Loading Required Packages\nlibrary(public.ctn0094data)\nlibrary(tidyverse)\nlibrary(ggmosaic)",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Mosaic & Box/Violin Plots</span>"
    ]
  },
  {
    "objectID": "lessons/01_mosaic_violin.html#introduction-to-mosaic-and-boxviolin-plots",
    "href": "lessons/01_mosaic_violin.html#introduction-to-mosaic-and-boxviolin-plots",
    "title": "\n3  Mosaic & Box/Violin Plots\n",
    "section": "\n3.2 Introduction to Mosaic and Box/Violin Plots",
    "text": "3.2 Introduction to Mosaic and Box/Violin Plots\nMosaic, box, and violin plots are useful for visualizing summary statistics.\nA mosaic plot is a special type of stacked bar chart used for two or more categorical variables. The width of the columns is proportional to the number of observations in each level of the variable plotted on the horizontal, or x-axis. The vertical length of the bars is proportional to the number of observations in the second variable within each level of the first variable.\nBox and violin plots are used for continuous variables by group. Box plots display six summary measures (the minimum, first quartile (Q1), median, third quartile (Q3), the interquartile range, and maximum). A violin plot illustrates the distribution of numerical data for one or more level of a categorical variable by combining summary statistics and density of each variable. Each curve corresponds to the respective frequency of data points within each region. A box plot is typically overlaid to provide additional information.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Mosaic & Box/Violin Plots</span>"
    ]
  },
  {
    "objectID": "lessons/01_mosaic_violin.html#data-source-and-description",
    "href": "lessons/01_mosaic_violin.html#data-source-and-description",
    "title": "\n3  Mosaic & Box/Violin Plots\n",
    "section": "\n3.3 Data Source and Description",
    "text": "3.3 Data Source and Description\nThe National Drug Abuse Treatment Clinical Trials Network (CTN) is a means by which medical and specialty treatment providers, treatment researchers, participating patients, and the National Institute on Drug Abuse cooperatively develop, validate, refine, and deliver new treatment options to patients. The CTN 094 demographics and everybody data sets from the public.ctn0094data package were utilized for the following visualizations. CTN 094 is a comprehensive, harmonized and normalized database of treatment data from CTN_0027, CTN_0030, and CTN_0051, where experiences of individuals with opioid use disorder (OUD) who seek care are described.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Mosaic & Box/Violin Plots</span>"
    ]
  },
  {
    "objectID": "lessons/01_mosaic_violin.html#cleaning-the-data-to-create-a-model-data-frame",
    "href": "lessons/01_mosaic_violin.html#cleaning-the-data-to-create-a-model-data-frame",
    "title": "\n3  Mosaic & Box/Violin Plots\n",
    "section": "\n3.4 Cleaning the Data to Create a Model Data Frame",
    "text": "3.4 Cleaning the Data to Create a Model Data Frame\nThe demographics and everybody data sets within the public.ctn0094data package were joined by ID (who variable). Race, age, is_male (gender), and project were selected features for the following visualizations.\n\n# Creating model data frame to include age, race, project, and is_male\n# from demographics and everybody data sets. Joined by subject ID (who)\ndemographics_df &lt;- demographics %&gt;% \n  left_join(everybody, by = \"who\")  %&gt;%\n  select(age, race, project, is_male)",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Mosaic & Box/Violin Plots</span>"
    ]
  },
  {
    "objectID": "lessons/01_mosaic_violin.html#assumptions-with-mosaic-boxviolin-plots",
    "href": "lessons/01_mosaic_violin.html#assumptions-with-mosaic-boxviolin-plots",
    "title": "\n3  Mosaic & Box/Violin Plots\n",
    "section": "\n3.5 Assumptions with Mosaic & Box/Violin Plots",
    "text": "3.5 Assumptions with Mosaic & Box/Violin Plots\nIn mosaic plots, two categorical variables are plotted along the horizontal (x) and vertical (y) axis. Each combination of categories forms a rectangle or tile within the plot.\nIn box and violin plots, a categorical variable is plotted along the horizontal or x-axis, while a continuous variable is plotted along the vertical or y-axis. Violin plots can be limiting if symmetry, skew, or other shape and variability characteristics are different between groups because precise comparison cannot be easily interpreted between density curves. For this reason, violin plots are typically rendered with another overlaid chart type, like box plot quartiles.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Mosaic & Box/Violin Plots</span>"
    ]
  },
  {
    "objectID": "lessons/01_mosaic_violin.html#code-to-run-mosaic-boxviolin-plots-output",
    "href": "lessons/01_mosaic_violin.html#code-to-run-mosaic-boxviolin-plots-output",
    "title": "\n3  Mosaic & Box/Violin Plots\n",
    "section": "\n3.6 Code to Run Mosaic & Box/Violin Plots & output",
    "text": "3.6 Code to Run Mosaic & Box/Violin Plots & output\n\n3.6.1 Mosaic Plots\nIn order to create a Mosaic plot, you must specify what data object you will be using within the ggplot() function. Then you will set aesthetic mapping options within the following geometric object layer: geom_mosaic().\nIn geom_mosaic(), the following aesthetics can be specified:\n\n\nweight: a weighting variable.\n\nx: categorical variable for the x-axis.\n\nSpecified as x = product(var1, var2, ...)\n\nThe product() function is used to extract the values from the categorical variable specified.\n\n\n\nalpha: a variable specifying transparency.\n\nIf the variable is not called in x:, then alpha: will be added in the first position.\n\n\n\nfill: a variable specifying fill color.\n\nIf the variable is not called in x:, then fill: will be added after the optional alpha: variable.\n\n\n\nconds: a variable specifying conditions.\n\nSpecified as conds = product(var1, var2, ...)\n\n\n\n\nThe ordering of the variables is vital as the product plot is created hierarchically.\n\n3.6.1.1 Basic Mosaic Plot\nIn the following example of a basic mosaic plot, we visualize the distribution of Race among CTN Projects 27, 30, and 51.\n\n# Basic Mosaic Plot\nmosaic_basic &lt;- demographics_df %&gt;% \n  ggplot() +\n  geom_mosaic(\n    aes(\n      # geom_mosaic() does not have one-to-one mapping between a variable and the x- \n      # or y-axis. So you must use the product() function when assigning a variable\n      # to the x-axis to account for the variable number of variables.\n      x = product(project),\n      fill = race\n    )\n  ) +\n  labs(\n    y = \"Race\",\n    x = \"Project\",\n    title = \"Mosaic Plot of Race by CTN Project\") +\n  # Specifies default `geom_mosaic` aesthetics, e.g white panel background, \n  # removes grid lines, adjusts widths and heights of rows and columns to \n  # reflect frequencies\n  theme_mosaic() +\n  # Removes legend illustrating Race and respective fill colors\n  theme(legend.position = \"None\")\n  \nmosaic_basic\n\n\n\n\n\n\nFigure 3.1\n\n\n\n\n\n3.6.1.2 More Advanced Mosaic Plot\nIn a more advanced version of a mosaic plot, we can visualize more than 2 categorical variables. The following example utilizes race, project, and ethnicity among CTN Projects 27, 30, and 51.\n\n# Advanced Mosaic Plot\nmosaic_advanced &lt;- demographics_df %&gt;% \n  ggplot() +\n  geom_mosaic(\n    aes(\n      x = product(race, project),\n      fill = is_male\n    )\n  ) +\n  labs(\n    y = \"Race\",\n    x = \"Project\",\n    title = \"Mosaic Plot of Race by Gender and CTN Project\",\n    fill = \"Gender\"\n  ) +\n  scale_fill_manual(\n    labels = c(\"No\" = \"Female\", \"Yes\" = \"Male\"),\n    values = c(\"darkseagreen2\", \"darkslategray3\", \"grey\")\n  ) +\n  theme_mosaic() +\n  # Adjust axis tick labels to 60 degrees and justification to the right\n  # with hjust (horizontal justification) and vjust (vertical justification)\n  theme(axis.text.x = element_text(angle = 60, hjust = 1, vjust = 1))\n  \nmosaic_advanced\n\n\n\n\n\n\nFigure 3.2\n\n\n\n\n\n3.6.2 Box Plots\nIn order to create a box plot, you must specify what data object you will be using within the ggplot() function. Then you will set aesthetic mapping options within the aes() or aesthetic layer. The geom_boxplot() layer specifies the box plot.\nThe following aesthetics are understood by geom_boxplot():\n\n\nx or y: Specifies the categorical variable along the x- or y-axis.\n\nlower or xlower: Specifies the 25th percentile/first quartile.\n\nupper or xupper: Specifies the 75th percentile/third quartile.\n\nmiddle or xmiddle: Specifies the 50th percentile/second quartile/median.\n\nymin or xmin: Specifies the y or x minimum for the plot.\n\nymax or xmax: Specifies the y or x maximum for the plot.\n\nalpha: Specifies a variable to determine transparency.\n\ncolor: Assigns an outline color to respective levels of a specified categorical variable.\n\nfill: Assigns a fill color to respective levels of a specified categorical variable.\n\ngroup: Partitions data by a discrete variable when no other grouping variable is specified, or grouping is incorrectly defaulted by R.\n\nlinetype: Specifies line type of box plot.\n\nlinewidth: Specifies line width of box plot.\n\nshape: Specifies the shape of the (outlier) points.\n\nsize: Specifies the size of the points and text.\n\nweight: Specifies a weight variable.\n\n\n3.6.2.1 Basic Box Plot\nThe following is a basic box plot showing the relationship between one continuous and one categorical variable.\n\n# Box Plot\nbox_basic &lt;- demographics_df %&gt;% \n  ggplot() +\n  aes(x = race, y = age, color = race) +\n  labs(\n    x = \"Race\",\n    y = \"Age\",\n    title = \"Box Plot of Race and Age\",\n    color = \"Race\"\n  ) +\n  # Using width to adjust the width of the boxes\n  geom_boxplot(width = 0.5) +\n  theme(legend.position = \"None\")\n\nbox_basic\n\n\n\n\n\n\nFigure 3.3\n\n\n\n\n\n3.6.2.2 More Advanced Box Plot\nWith geom_box(), you can also specify a additional categorical variable (different from your x and y variables) to break up your plot by that variable. For example, the following plot takes the previous plot of race and age and adds information side-by-side by gender (is_male).\n\n# Box Plot\nbox_advanced &lt;- demographics_df %&gt;% \n  ggplot() +\n  aes(x = race, y = age, color = is_male) +\n  # changing the labels for is_male, and specifying the colors we want\n  scale_color_manual(\n    labels = c(\"No\" = \"Female\", \"Yes\" = \"Male\"),\n    values = c(\"darkorchid4\", \"darkolivegreen4\")\n  ) +\n  labs(\n    x = \"Race\",\n    y = \"Age\",\n    title = \"Box Plot of Race and Age\",\n    color = \"Gender\"\n  ) +\n  # Using width to adjust the width of the boxes\n  geom_boxplot(width = 0.5)\n\nbox_advanced\n\n\n\n\n\n\nFigure 3.4\n\n\n\n\n\n3.6.3 Violin Plot\nIn order to create a Violin plot, you must specify what data object you will be using within the ggplot() function. Then you will set aesthetic mapping options within the aes() or aesthetic layer. The geom_violin() layer specifies the violin plot. An additional call for geom_boxplot() will overlay box quartiles on the violin plot to display summary statistics.\nThe following aesthetics are understood by geom_violin():\n\n\nx: Specifies the categorical variable along the x-axis.\n\ny: Specifies the continuous variable along the y-axis.\n\nalpha: Specifies a variable to determine transparency.\n\ncolor: Assigns an outline color to respective levels of a specified categorical variable.\n\nfill: Assigns a fill color to respective levels of a specified categorical variable.\n\ngroup: Partitions data by a discrete variable when no other grouping variable is specified, or grouping is incorrectly defaulted by R.\n\nlinetype: Specifies line type of violin plot.\n\nlinewidth: Specifies line width of violin plot.\n\nweight: Specifies a weight variable.\n\n\n# Violin Plot\nviolin_basic &lt;- demographics_df %&gt;% \n  ggplot() +\n  aes(x = race, y = age, color = race) +\n  scale_color_manual(\n    values = c(\"coral1\", \"darkgreen\", \"deepskyblue2\", \"darkorchid2\")\n  ) +\n  labs(\n    x = \"Race\",\n    y = \"Age\",\n    title = \"Violin Plot of Race and Age\",\n    subtitle = \"With Summary Information\",\n    color = \"Race\"\n  ) +\n  geom_violin() +\n  geom_boxplot(width = 0.1) +\n  theme(legend.position = \"None\")\n\nviolin_basic                        \n\n\n\n\n\n\nFigure 3.5",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Mosaic & Box/Violin Plots</span>"
    ]
  },
  {
    "objectID": "lessons/01_mosaic_violin.html#brief-interpretation",
    "href": "lessons/01_mosaic_violin.html#brief-interpretation",
    "title": "\n3  Mosaic & Box/Violin Plots\n",
    "section": "\n3.7 Brief Interpretation",
    "text": "3.7 Brief Interpretation\n\n3.7.1 Mosaic Plot\n\nCompared to Project 27 and Project 51, Project 30 had the highest proportion of participants who indicated that their race is ‘White’.\nCompared to Project 30 and Project 51, Project 27 had the highest proportion of participants who indicated that their race is ‘Other’.\nCompared to Project 27 and Project 51, Project 30 has the lowest proportion of participants who indicated that their race is ‘Other’.\n\n3.7.2 Box Plot\n\nParticipants who indicated that their race is ‘Black’ exhibited the highest median age of around 45 years old\nParticipants who indicated that their race is ‘White’ exhibited the lowest median age at approximately 31 years old.\n\n3.7.3 Violin Plot\nThis plot more clearly shows the bimodality of age by race among Black and ‘Other’ participants in CTN. It also shows the skewness of age in the White participants. Specifically:\n\nParticipants who indicated that their race is ‘White’ exhibited peak density around mid-20s compared to those who indicated that their race is ‘Black’, where peak density is exhibited around late-40s.\nParticipants who indicated that their race is ‘White’ had the lowest median age at approximately 31 years old, where participants who indicated that their race is ‘Black’ had the highest median age at approximately 45 years old.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Mosaic & Box/Violin Plots</span>"
    ]
  },
  {
    "objectID": "lessons/01_mosaic_violin.html#conclusion",
    "href": "lessons/01_mosaic_violin.html#conclusion",
    "title": "\n3  Mosaic & Box/Violin Plots\n",
    "section": "\n3.8 Conclusion",
    "text": "3.8 Conclusion\nThis lesson discusses three different plots for one-dimensional data: the Mosaic, Box, and Violin plots. Figure 3.1 is a basic mosaic plots that shows race by CTN project. In Figure 3.2, we added a third variable to the visualization: gender. The box plots, Figure 3.3 and Figure 3.4 we plotted age (continuous) by race and age by race and gender, respectively. Finally, Figure 3.5 shows a violin plot with an overlaid box plot for age by race in the CTN projects.\nMosaic plots are useful for proportionally visualizing the observations of two or more categorical variables. Box and violin plots can be used to visualize a continuous variable by one, or two in the case of box plots, categorical variables. Violin plots build on box plots in that they are able to provide quick information on the potential multimodal distribution(s) and skewness of a continuous variable across categories, as we saw in Figure 3.5.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Mosaic & Box/Violin Plots</span>"
    ]
  },
  {
    "objectID": "lessons/01_scatterplots.html",
    "href": "lessons/01_scatterplots.html",
    "title": "\n4  How to Create a Scatterplot\n",
    "section": "",
    "text": "4.1 Introduction to Scatterplots\nScatterplots display the relationship between two variables using dots to represent the values for each numeric variable. This presentation will examine the relationship between GDP per capita and Fertility over time using ggplot with facets.\nHypothesis: A negative relationship exists between GDP per capita and fertility i.e. as GDP per capita increases, fertility decreases.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>How to Create a Scatterplot</span>"
    ]
  },
  {
    "objectID": "lessons/01_scatterplots.html#gapminder-data-description",
    "href": "lessons/01_scatterplots.html#gapminder-data-description",
    "title": "\n4  How to Create a Scatterplot\n",
    "section": "\n4.2 Gapminder data description",
    "text": "4.2 Gapminder data description\nData was obtained from the dslabs package and comes from Gapminder a Swedish non-profit organization. The Gapmidner data set has health and income outcomes for 184 countries from 1960 to 2016. Gapminder aims to promote a fact-based worldview by providing accessible and understandable global development data. The dataset covers a wide range of variables, including economic, social, and health-related indicators like GDP, infant mortality, life expectancy, fertility, as well as population, making it a valuable resource for understanding global trends and patterns over time. Countries and territories with missing information were not excluded from the data set as the lack of information can also be looked into and shed light on why data was not collected or provided. To determine whether a country’s health and income outcomes are influenced by population sizes and GDP per capita, the data will be used to create a series of graphs to view different trends.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>How to Create a Scatterplot</span>"
    ]
  },
  {
    "objectID": "lessons/01_scatterplots.html#cleaning-the-data-to-create-a-model-data-frame",
    "href": "lessons/01_scatterplots.html#cleaning-the-data-to-create-a-model-data-frame",
    "title": "\n4  How to Create a Scatterplot\n",
    "section": "\n4.3 Cleaning the data to create a model data frame",
    "text": "4.3 Cleaning the data to create a model data frame\nA tibble was created from the gapminder dataset, and a new column was created to measure GDP per capita. Overall, using tibbles enhances the readability, usability, and compatibility of your code within the tidyverse ecosystem.\n\n# Creating gapminder dataset\\tibble\ngapminder_df &lt;-\n  as_tibble(gapminder) %&gt;%\n  mutate(gdp_per_capita = gdp / population)",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>How to Create a Scatterplot</span>"
    ]
  },
  {
    "objectID": "lessons/01_scatterplots.html#components-of-ggplot2",
    "href": "lessons/01_scatterplots.html#components-of-ggplot2",
    "title": "\n4  How to Create a Scatterplot\n",
    "section": "\n4.4 Components of ggplot2\n",
    "text": "4.4 Components of ggplot2\n\nggplot2 is a package used to create graphs and visualize data. The main three components of ggplot2 are the data, aesthetics and geom layers.\n\nThe data layer - states what data will be used to graph\nThe aesthetics layer - specifies the variables that are being mapped\nThe geom layer - specifies the type of graph to be produced",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>How to Create a Scatterplot</span>"
    ]
  },
  {
    "objectID": "lessons/01_scatterplots.html#code-to-run-interpretable-scatterplots-and-create-facets",
    "href": "lessons/01_scatterplots.html#code-to-run-interpretable-scatterplots-and-create-facets",
    "title": "\n4  How to Create a Scatterplot\n",
    "section": "\n4.5 Code to run interpretable scatterplots and create facets",
    "text": "4.5 Code to run interpretable scatterplots and create facets\nIn order to create a scatter-plot using ggplot, you must specify what data you will be using, state which variables will be mapped and how under aesthetics. What differentiates the scatter-plot from any other type of graph will be specified under the geom layer. For the scatter-plot, geom_point will be used.\nIn this example, we will analyze the relationship between fertility rates and gdp per capita for each country in 2011.\n\nfig_bubble_2011 &lt;-\n  ggplot(data = filter(gapminder_df, year == 2011)) +\n  aes(x = gdp_per_capita, y = fertility) +\n  geom_point()\n\nfig_bubble_2011 \n\n\n\n\n\n\nFigure 4.1: Association between fertility rates and gdp per capita for each country in 2011\n\n\n\n\nIn the example above, we have mapped out fertility as our y-axis and gdp per capita as our x-axis. However, at it’s very basic level, there is not enough information provided to accurately analyze the relationship between the two. For this reason, we can add additional layers that will provide more information to properly analyze the scatter-plot.\n\nfig_bubble_pretty_2011 &lt;-\n  ggplot(data = filter(gapminder_df, year == 2011)) +\n  aes(\n    x = gdp_per_capita,\n    y = fertility,\n    # will change the size of the point based on population size \n    size = population, \n    # will assign colors based on the continent the country is in \n    color = continent\n  ) +\n  # gives a range as to how big or small the points of population should be\n  scale_size(range = c(1, 20)) + \n  # removes N/A from the legend and titles it Continent \n  scale_colour_discrete(na.translate = F, name = \"Continent\") +\n  # removes population size from the legend \n  guides(size = \"none\") +\n  scale_x_continuous(\n    name = \"GDP per Capita\",\n    trans = \"log10\",\n    # transforms numbers from scientific notation to regular number \n    labels = scales::comma\n  ) +\n  labs(\n    title = \"Fertility rate descreases as GDP per capita increases in 2011\",\n    y = \"Fertility rates\",\n    caption = \"Source: Gapminder\"\n  ) +\n  # the ylim was set based on the fertility, lowest was near 1 & highest was above 7\n  ylim(1.2, 8.0) +\n  # alpha increases transparency of the points to ensure they can all be seen\n  geom_point(alpha = 0.5) \n\nfig_bubble_pretty_2011\n\n\n\n\n\n\nFigure 4.2: Association between fertility rates and gdp per capita for each country, grouped by continent, in 2011\n\n\n\n\nFigure 4.2 builds on the previous scatterplot of Fertility Rates (y axis) against GDP per capita (x axis) for 2011. The bubble size depicts respective country populations, and continents are coded by colors according to the key. This figure displays a negative relationship between GDP per capita and Fertility Rates. It supports the Hypothesis which states that as GDP per capita increases, Fertility Rates decreases. This trend can be confirmed for all continents, however, the degree to which fertility rates drop between continents varies. Most European country appear below a fertility rate of 2 babies per woman. The Americas appear to follow closely behind (under 4), followed by Oceania and Asia. A significant number of African countries still maintained higher fertility rates with lower GDP per capita for 2011.\nThis is an example of wanting to create four separate graphs to see the relationship between fertility rates and GDP per capita based on the years 1960, 1975, 1990 and 2005. In this example we omitted the facet argument.\n\nfig_bubble_multiple &lt;-\n  ggplot(data = filter(gapminder_df, year %in% c(1960, 1975, 1990, 2005))) +\n  aes(\n    x = gdp_per_capita,\n    y = fertility,\n    size = population,\n    color = continent\n  ) +\n  scale_x_continuous(\n    name = \"GDP per Capita\",\n    trans = \"log10\",\n    labels = scales::comma\n  ) +\n  scale_size(range = c(0, 20)) +\n  guides(size = \"none\") +\n  scale_colour_discrete(na.translate = FALSE, name = \"Continent\") +\n  labs(\n    title = \"Fertility continues to decrease as GDP per capita increases\",\n    caption = \"Source: Gapminder\",\n    y = \"Fertility rates\"\n  ) +\n  geom_point(alpha = 0.3) \n\nfig_bubble_multiple\n\n\n\n\n\n\nFigure 4.3: Association between fertility rates and GDP per capita based on the years 1960, 1975, 1990 and 2005\n\n\n\n\nWithout having used the facet argument, all points of all four years have been included into one graph. This graph does not provide us with the information we were looking for.\n\nfig_bubble_multiple_facet &lt;-\n  ggplot(data = filter(gapminder_df, year %in% c(1960, 1975, 1990, 2005))) +\n  aes(\n    x = gdp_per_capita,\n    y = fertility,\n    size = population,\n    color = continent\n  ) +\n  scale_x_continuous(\n    name = \"GDP per Capita\",\n    trans = \"log10\",\n    labels = scales::comma\n  ) +\n  scale_size(range = c(0, 20)) +\n  guides(size = \"none\") +\n  scale_colour_discrete(na.translate = FALSE , name = \"Continent\") +\n  labs(\n    title = \"Fertility continues to decrease as GDP per capita increases\",\n    caption = \"Source: Gapminder\",\n    y = \"Fertility rates\"\n  ) +\n  geom_point(alpha = 0.3) +\n  # specifiying we want the graphs split based on year\n  facet_wrap(~ year)\n\nfig_bubble_multiple_facet\n\n\n\n\n\n\nFigure 4.4: Association between fertility rates and GDP per capita based on the years 1960, 1975, 1990 and 2005, using facet\n\n\n\n\nNow that we’ve specified the facet argument, we now have four separate graphs that can be properly analysed. In Figure 4.4 we see an increasingly negative relationship between the two variables over time. This observation is congruent with the hypothesis that as GDP per capita increases, fertility decreases.\nThis global trend can be attributed to the increasing proportion of women in the workforce in the mid to late 20th century. As a result of World War II (1939-1945), women took on roles outside the home to compensate for men at war. Despite increased GDP per capita, this may have contributed to reduced fertility (babies per woman) over time. In 1960, a clear disparity among continents is seen. Most European countries’ fertility rates fell below 5, while their GDP per capita increased. Most African countries maintained high fertility rates above 5, but little change is seen in GDP per capita. The Asian continent shows the most variation among countries during that year. Some smaller Asian countries continued to maintain high fertility rates as GDP per capita increased in 1960. However, others displayed a drastic decrease in fertility rates by 1960. The Americas followed a steady decline over the years. By 2005, an overall negative relationship can be seen with most countries’ fertility rates below 5 babies per woman.\n\nfig_bubble_row_2011 &lt;-\n  ggplot(data = filter(gapminder_df, year == 2011)) +\n  aes(\n    x = gdp_per_capita,\n    y = fertility\n  ) +\n  geom_point(alpha = 0.5) +\n  facet_wrap(~ continent, nrow = 1)\n\nfig_bubble_row_2011 \n\n\n\n\n\n\nFigure 4.5: Association between fertility rates and GDP per capita based on continent\n\n\n\n\nIn the graph above, we see an example of separating the single graph into graphs based on continent. It has also been specified to have all graphs appear in one single row through the nrow argument. Very importantly however, this graph is unclear and cannot be used to compare the relationship between fertility and gdp per capita.\n\nfig_bubble_facet_2011 &lt;-\n  ggplot(data = filter(gapminder_df, year == 2011)) +\n  aes(\n    x = gdp_per_capita,\n    y = fertility\n  ) +\n  scale_x_continuous(\n    name = \"GDP per Capita\",\n    trans = \"log10\",\n    labels = scales::comma\n  )  +\n  geom_point(alpha = 0.5) +\n  facet_wrap(~ continent)\n\nfig_bubble_facet_2011 \n\n\n\n\n\n\nFigure 4.6: Association between fertility rates and GDP per capita based on continent not using nrow\n\n\n\n\nIn the next example above, we removed the nrow argument and the system automatically separated the graphs into three columns with two rows. Additionally, we changed the x-axis to a log scale to better interpret gdp per capita. There is a way to determine a relationship between fertility and gdp per capita by continent.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>How to Create a Scatterplot</span>"
    ]
  },
  {
    "objectID": "lessons/01_scatterplots.html#public-health-interpretation",
    "href": "lessons/01_scatterplots.html#public-health-interpretation",
    "title": "\n4  How to Create a Scatterplot\n",
    "section": "\n4.6 Public Health Interpretation",
    "text": "4.6 Public Health Interpretation\nA global negative trend is depicted between GDP per capita and fertility over time. Such changes were due to wars as well as social, cultural and economic changes that incentivize smaller families especially in Asian countries. Most European, American and Asian countries depicted significant decreases in fertility rates over time as GDP per capita increased. On the other hand, African countries remain in the top rank for fertility over the years. These differences are depicted in the population pyramid changes of developed vs developing countries. Public health policies can be tailored to incentivizing increased fertility in developed countries to ensure generation continuity, and effective family planning strategies in developing countries.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>How to Create a Scatterplot</span>"
    ]
  },
  {
    "objectID": "lessons/01_scatterplots.html#conclusion",
    "href": "lessons/01_scatterplots.html#conclusion",
    "title": "\n4  How to Create a Scatterplot\n",
    "section": "\n4.7 Conclusion",
    "text": "4.7 Conclusion\nIn this lesson, the basic functions of ggplot2 package were shown, which can create a scatterplot. There are three layers to the code to make a plot in R: data, aesthetic, and geometric. Within the aesthetic layer, functions can be added such as size and color to analyze more variables. Additionally, facets can split up graphs over a categorical variable, adding another potential variable to analyze in the plot.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>How to Create a Scatterplot</span>"
    ]
  },
  {
    "objectID": "lessons_original/01_skimr.html",
    "href": "lessons_original/01_skimr.html",
    "title": "\n5  Skimr Package\n",
    "section": "",
    "text": "5.1 Introduction\nSkimr is an R package designed to provide summary statistics about variables in data frames, tibbles, data tables and vectors. The function is modifiable where you can add additional variables, which are not a part of default summary function within R. Skimr allows us to quickly assess data quality by feature and type in a quick report. This is a critical step in Data Exploration, where Understanding our data helps us to generate a hypothesis and determine what data analysis are appropriate.\nThis presentation will cover the simplest and most effective ways to explore data in R.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Skimr Package</span>"
    ]
  },
  {
    "objectID": "lessons_original/01_skimr.html#introduction",
    "href": "lessons_original/01_skimr.html#introduction",
    "title": "\n5  Skimr Package\n",
    "section": "",
    "text": "5.1.1 Packages\nTo begin we will upload the packages necessary for the lesson, this includes the following:\n\n\nreadr() to import our data file\n\nknitr() that houses the kable() feature that allows us to construct and customize tables.\n\ntidyverse houses the dyplyrpackage that assists with data manipulation and visualization.\nTheskimrpackage provides a compact summary of the variables in a dataset.\n\n\n# install.packages(\"skimr\")\n# install.packages(\"knitr\")\n# install.packages(\"tidyverse\")\n\n# load all the packages we will need to analyze the data and use the skim\n#   function\nlibrary(skimr)\nlibrary(knitr)\nlibrary(readxl)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n5.1.2 Census Data\nFor this assignment we will be using the Census_2010 dataset. There is no code book associated with the data, making it difficult to provide an accurate description of the variables. The information recorded shows the United States population estimates from the years 2010-2015, as well as relevant variables like net population change, number of births, number of deaths, international and domestic migration. Within the dataframe, there are 3,193 observations and 100 variables.\nThe data can be imported into R from the following link: https://fiudit-my.sharepoint.com/:x:/g/personal/ssinc013_fiu_edu/ESK1A13PstVGtf7HUwNNt68Bnh1YPfH8L-hnvMUxjBuCVw?e=CCwQU9\n\n# import the data\n# census_2010 &lt;- read_csv(\"Data/census_2010.csv\")\ncensus_2010 &lt;- readxl::read_xlsx(\"../data/01_census_2010.xlsx\")\n\n# what are the variables\ncolnames(census_2010) %&gt;% \n  head(n = 10)\n\n [1] \"SUMLEV\"            \"REGION\"            \"DIVISION\"         \n [4] \"STATE\"             \"COUNTY\"            \"STNAME\"           \n [7] \"CTYNAME\"           \"CENSUS2010POP\"     \"ESTIMATESBASE2010\"\n[10] \"POPESTIMATE2010\"",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Skimr Package</span>"
    ]
  },
  {
    "objectID": "lessons_original/01_skimr.html#the-summary-function",
    "href": "lessons_original/01_skimr.html#the-summary-function",
    "title": "\n5  Skimr Package\n",
    "section": "\n5.2 The Summary() Function",
    "text": "5.2 The Summary() Function\nIn R, the most similar function is summary(). The summary() function in R can be used to quickly summarize the values in a data frame or vector.\nThis syntax shows examples of the summary function using both our data set, and a vector:\n\n#| label: Summary-syntax-with-data\n\n# Example using summary function with data\nsummary(census_2010$CENSUS2010POP)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n      82    11299    26424   193387    71404 37253956 \n\n# Example using summary function with vector\n# Define vector\nx &lt;- c(3, 4, 23, 5, 7, 8, 9, 12, 26, 15, 20, 21, NA)\n\n# Summarize values in vector\nsummary(x)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   3.00    6.50   10.50   12.75   20.25   26.00       1 \n\n\nThe summary() function automatically calculates: The minimum value, The value of the 1st quartile (25th percentile), The median value, The value of the 3rd quartile (75th percentile) and The maximum value. Any missing values (NA) in the vector, the summary() function will automatically exclude them when calculating the summary statistics.\nNow, let’s see how skim() compares.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Skimr Package</span>"
    ]
  },
  {
    "objectID": "lessons_original/01_skimr.html#skimr-package",
    "href": "lessons_original/01_skimr.html#skimr-package",
    "title": "\n5  Skimr Package\n",
    "section": "\n5.3 Skimr Package",
    "text": "5.3 Skimr Package\nThe skim() function will generate a summary of the variables in your dataset, including their data type, number of non-missing values, minimum and maximum values, median, mean, standard deviation, and more (Waring et al. 2022).\nThe following syntax ensures that the data is compatible with Skimr functions.\n\nCode# is the summary data a skimr dataframe\nskim(census_2010) %&gt;% \n  is_skim_df() # TRUE\n\n[1] TRUE\nattr(,\"message\")\ncharacter(0)\n\n\nWe can explore the data as a tibble:\n\nCode# use skim to get descriptive statistics of the data\nskim(census_2010) %&gt;% \n  head(n = 10)\n\n\nData summary\n\n\nName\ncensus_2010\n\n\nNumber of rows\n3193\n\n\nNumber of columns\n100\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n8\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\nSTNAME\n0\n1\n4\n20\n0\n51\n0\n\n\nCTYNAME\n0\n1\n4\n33\n0\n1927\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nSUMLEV\n0\n1\n49.84\n1.25\n40\n50\n50\n50\n50\n▁▁▁▁▇\n\n\nREGION\n0\n1\n2.67\n0.81\n1\n2\n3\n3\n4\n▁▆▁▇▂\n\n\nDIVISION\n0\n1\n5.19\n1.97\n1\n4\n5\n7\n9\n▂▇▅▆▃\n\n\nSTATE\n0\n1\n30.26\n15.15\n1\n18\n29\n45\n56\n▃▇▆▆▇\n\n\nCOUNTY\n0\n1\n101.92\n107.63\n0\n33\n77\n133\n840\n▇▁▁▁▁\n\n\nCENSUS2010POP\n0\n1\n193387.05\n1176201.45\n82\n11299\n26424\n71404\n37253956\n▇▁▁▁▁\n\n\nESTIMATESBASE2010\n0\n1\n193396.87\n1176244.25\n82\n11299\n26446\n71491\n37254503\n▇▁▁▁▁\n\n\nPOPESTIMATE2010\n0\n1\n193765.65\n1178710.28\n83\n11275\n26467\n71721\n37334079\n▇▁▁▁▁\n\n\n\n\n\nUsing skimr functions provides a cleaner and more detailed display of the results compared to the summary() function. In this example we are showing the first ten variables in our data set. The data summary tab shows the number of rows and columns, column type frequency and group variables. There is also additional descriptive information like missing values, unique characters.\nThis will be relevant for data cleaning as well as understanding the distribution. Both are critical to determine which statistical analysis would be most appropriate to use for a project.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Skimr Package</span>"
    ]
  },
  {
    "objectID": "lessons_original/01_skimr.html#other-skimr-features",
    "href": "lessons_original/01_skimr.html#other-skimr-features",
    "title": "\n5  Skimr Package\n",
    "section": "\n5.4 Other Skimr Features",
    "text": "5.4 Other Skimr Features\n\n5.4.1 Separate dataframes by type\nThe data frames produced by skim() are wide and sparse, filled with columns that are mostly NA. For that reason, it can be convenient to work with “by type” subsets of the original data frame. These smaller subsets have their NA columns removed.\nFeatures:\n\n\npartition() - Creates a list of smaller data frames. Each entry in the list is a data type from the original dataframe\n\nbind() - Takes the list and rebuilds the original dataframe.\n\nyank() - Extract a subtable from a dataframe with a particular type.\n\nThe following syntax is using partition() to separate the large census_df.\n\nCode# split the character and numeric data\nseparate_df &lt;- partition(skim(census_2010))\n# check only the character data\nseparate_df$character\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\nSTNAME\n0\n1\n4\n20\n0\n51\n0\n\n\nCTYNAME\n0\n1\n4\n33\n0\n1927\n0\n\n\n\n\nCode# create summary statistics for only numeric variables\nnumeric_separate_df &lt;- separate_df[2]\n# pull out the desired summary statistics in the nested list\nhead(numeric_separate_df$numeric[\"mean\"]) %&gt;% \n  kable(digits = 1) \n\n\n\nmean\n\n\n\n49.8\n\n\n2.7\n\n\n5.2\n\n\n30.3\n\n\n101.9\n\n\n193387.1\n\n\n\n\n\nThe following syntax is using bind() to combine the smaller character and numeric lists into the desired df.\n\nCode# combine the character and numeric data\nhead(bind(separate_df))\n\n\nData summary\n\n\nName\ncensus_2010\n\n\nNumber of rows\n3193\n\n\nNumber of columns\n100\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\nSTNAME\n0\n1\n4\n20\n0\n51\n0\n\n\nCTYNAME\n0\n1\n4\n33\n0\n1927\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nSUMLEV\n0\n1\n49.84\n1.25\n40\n50\n50\n50\n50\n▁▁▁▁▇\n\n\nREGION\n0\n1\n2.67\n0.81\n1\n2\n3\n3\n4\n▁▆▁▇▂\n\n\nDIVISION\n0\n1\n5.19\n1.97\n1\n4\n5\n7\n9\n▂▇▅▆▃\n\n\nSTATE\n0\n1\n30.26\n15.15\n1\n18\n29\n45\n56\n▃▇▆▆▇\n\n\n\n\nCode# confirm that the bound table is the same as the original skimmed table\nidentical(bind(separate_df), skim(census_2010)) \n\n[1] TRUE\n\n\nThe following syntax is using yank() to extract a specific table eg.character to examine.\n\nCode# Extract character data\nyank(skim(census_2010), \"character\")\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\nSTNAME\n0\n1\n4\n20\n0\n51\n0\n\n\nCTYNAME\n0\n1\n4\n33\n0\n1927\n0\n\n\n\n\n\n\n5.4.2 Skimr with Dplyr\nSkimr functions can be used in combination with Dplyr functions to examine specific variables within the census dataset.\nThe following example used skim() with filter() to display the variable CENSUS2010POP. The dataframe was further customized to display variable name and data type using select().\n\nCode# use dplyr functions on the statistics summary table\ncensus_filter &lt;- skim(census_2010) %&gt;% \n  filter(skim_variable == \"CENSUS2010POP\")\ncensus_filter\n\n\nData summary\n\n\nName\ncensus_2010\n\n\nNumber of rows\n3193\n\n\nNumber of columns\n100\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\nCENSUS2010POP\n0\n1\n193387\n1176201\n82\n11299\n26424\n71404\n37253956\n▇▁▁▁▁\n\n\n\nCodecensus_select &lt;- skim(census_2010) %&gt;% \n  select(skim_type, skim_variable)\nhead(census_select)\n\n# A tibble: 6 × 2\n  skim_type skim_variable\n  &lt;chr&gt;     &lt;chr&gt;        \n1 character STNAME       \n2 character CTYNAME      \n3 numeric   SUMLEV       \n4 numeric   REGION       \n5 numeric   DIVISION     \n6 numeric   STATE        \n\n\nYou can also customize the output of the skim() function by using various arguments. For example, you can use the numeric argument to specify which variables should be treated as numeric variables, or use the ranges argument to specify custom ranges for variables.\nUsing skim() in combination with mutate() we will compute a new variable to add to our skim dataframe.\n\nCode# create a new variable calculate the change in birth rate from 2010 to 2011\ncensus_2010 %&gt;% \n  # new variable\n  mutate(net_birth = BIRTHS2011 - BIRTHS2010) %&gt;% \n  # move the variable to the beginning of the dataset\n  relocate(net_birth, .after = CENSUS2010POP) %&gt;% \n  # summary statistics table\n  skim() %&gt;% \n  # only the first fifteen variables\n  head(n = 15) %&gt;% \n  # change the formatting \n  kable(digit = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_type\nskim_variable\nn_missing\ncomplete_rate\ncharacter.min\ncharacter.max\ncharacter.empty\ncharacter.n_unique\ncharacter.whitespace\nnumeric.mean\nnumeric.sd\nnumeric.p0\nnumeric.p25\nnumeric.p50\nnumeric.p75\nnumeric.p100\nnumeric.hist\n\n\n\ncharacter\nSTNAME\n0\n1\n4\n20\n0\n51\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\ncharacter\nCTYNAME\n0\n1\n4\n33\n0\n1927\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nnumeric\nSUMLEV\n0\n1\nNA\nNA\nNA\nNA\nNA\n49.84\n1.25\n40\n50\n50\n50\n50\n▁▁▁▁▇\n\n\nnumeric\nREGION\n0\n1\nNA\nNA\nNA\nNA\nNA\n2.67\n0.81\n1\n2\n3\n3\n4\n▁▆▁▇▂\n\n\nnumeric\nDIVISION\n0\n1\nNA\nNA\nNA\nNA\nNA\n5.19\n1.97\n1\n4\n5\n7\n9\n▂▇▅▆▃\n\n\nnumeric\nSTATE\n0\n1\nNA\nNA\nNA\nNA\nNA\n30.26\n15.15\n1\n18\n29\n45\n56\n▃▇▆▆▇\n\n\nnumeric\nCOUNTY\n0\n1\nNA\nNA\nNA\nNA\nNA\n101.92\n107.63\n0\n33\n77\n133\n840\n▇▁▁▁▁\n\n\nnumeric\nCENSUS2010POP\n0\n1\nNA\nNA\nNA\nNA\nNA\n193387.05\n1176201.45\n82\n11299\n26424\n71404\n37253956\n▇▁▁▁▁\n\n\nnumeric\nnet_birth\n0\n1\nNA\nNA\nNA\nNA\nNA\n1870.12\n11792.85\n-3\n96\n232\n639\n386443\n▇▁▁▁▁\n\n\nnumeric\nESTIMATESBASE2010\n0\n1\nNA\nNA\nNA\nNA\nNA\n193396.87\n1176244.25\n82\n11299\n26446\n71491\n37254503\n▇▁▁▁▁\n\n\nnumeric\nPOPESTIMATE2010\n0\n1\nNA\nNA\nNA\nNA\nNA\n193765.65\n1178710.28\n83\n11275\n26467\n71721\n37334079\n▇▁▁▁▁\n\n\nnumeric\nPOPESTIMATE2011\n0\n1\nNA\nNA\nNA\nNA\nNA\n195251.40\n1189647.76\n90\n11277\n26417\n72387\n37700034\n▇▁▁▁▁\n\n\nnumeric\nPOPESTIMATE2012\n0\n1\nNA\nNA\nNA\nNA\nNA\n196744.52\n1200508.37\n81\n11195\n26362\n72496\n38056055\n▇▁▁▁▁\n\n\nnumeric\nPOPESTIMATE2013\n0\n1\nNA\nNA\nNA\nNA\nNA\n198200.69\n1211123.45\n89\n11180\n26519\n72222\n38414128\n▇▁▁▁▁\n\n\nnumeric\nPOPESTIMATE2014\n0\n1\nNA\nNA\nNA\nNA\nNA\n199754.09\n1222669.36\n87\n11121\n26483\n72257\n38792291\n▇▁▁▁▁\n\n\n\n\n\n\n5.4.3 Adding Variables\n\nbase - An sfl that sets skimmers for all column types.\nappend - Whether the provided options should be in addition to the defaults already in skim. Default is TRUE.\n\nAs mentioned, skim() is designed to display default statistics, however you can use this function to change the summary statistics that it returns.\nskim_with() is type closure: a function that returns adds a new variable to the table. This lets you have several skimming functions in a single R session, but it also means that you need to assign the return of skim_with() before you can use it.\nYou assign values within skim_with() by using the sfl() helper (skimr function list). It identifies which skimming functions you want to remove, by setting them to NULL. Assign an sfl to each column type that you wish to modify.\nFor example, we will add the following variables to the dataframe: median, min, max, IQR, length.\n\nCodemy_skim &lt;- skim_with(\n  numeric = sfl(median, min, max, IQR),\n  character = sfl(length), \n  append = TRUE\n)\n\n# add new variables into the summary table\ncensus_2010 %&gt;% \n  my_skim() %&gt;% \n  head(n = 10)\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n3193\n\n\nNumber of columns\n100\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n8\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\nlength\n\n\n\nSTNAME\n0\n1\n4\n20\n0\n51\n0\n3193\n\n\nCTYNAME\n0\n1\n4\n33\n0\n1927\n0\n3193\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\nmedian\nmin\nmax\nIQR\n\n\n\nSUMLEV\n0\n1\n49.84\n1.25\n40\n50\n50\n50\n50\n▁▁▁▁▇\n50\n40\n50\n0\n\n\nREGION\n0\n1\n2.67\n0.81\n1\n2\n3\n3\n4\n▁▆▁▇▂\n3\n1\n4\n1\n\n\nDIVISION\n0\n1\n5.19\n1.97\n1\n4\n5\n7\n9\n▂▇▅▆▃\n5\n1\n9\n3\n\n\nSTATE\n0\n1\n30.26\n15.15\n1\n18\n29\n45\n56\n▃▇▆▆▇\n29\n1\n56\n27\n\n\nCOUNTY\n0\n1\n101.92\n107.63\n0\n33\n77\n133\n840\n▇▁▁▁▁\n77\n0\n840\n100\n\n\nCENSUS2010POP\n0\n1\n193387.05\n1176201.45\n82\n11299\n26424\n71404\n37253956\n▇▁▁▁▁\n26424\n82\n37253956\n60105\n\n\nESTIMATESBASE2010\n0\n1\n193396.87\n1176244.25\n82\n11299\n26446\n71491\n37254503\n▇▁▁▁▁\n26446\n82\n37254503\n60192\n\n\nPOPESTIMATE2010\n0\n1\n193765.65\n1178710.28\n83\n11275\n26467\n71721\n37334079\n▇▁▁▁▁\n26467\n83\n37334079\n60446",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Skimr Package</span>"
    ]
  },
  {
    "objectID": "lessons_original/01_skimr.html#conclusion",
    "href": "lessons_original/01_skimr.html#conclusion",
    "title": "\n5  Skimr Package\n",
    "section": "\n5.5 Conclusion",
    "text": "5.5 Conclusion\nOverall, Skimr is a useful package for quickly summarizing the variables in a dataset and gaining insights into its structure and content.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Skimr Package</span>"
    ]
  },
  {
    "objectID": "lessons_original/01_skimr.html#references",
    "href": "lessons_original/01_skimr.html#references",
    "title": "\n5  Skimr Package\n",
    "section": "\n5.6 References",
    "text": "5.6 References\n\n\n\n\nWaring, Elin, Michael Quinn, Amelia McNamara, Eduardo Arino de la Rubia, Hao Zhu, and Shannon Ellis. 2022. Skimr: Compact and Flexible Summaries of Data. https://docs.ropensci.org/skimr/.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Skimr Package</span>"
    ]
  },
  {
    "objectID": "lessons/01_table1.html",
    "href": "lessons/01_table1.html",
    "title": "\n6  Demographics Table With Table1\n",
    "section": "",
    "text": "6.1 Introduction\nIn most scientific research journals, the first included table is often referred to as Table1. It is a table that presents descriptive statistics of baseline characteristics of the study population stratified by exposure. This package makes it fairly straightforward to produce such a table using R. Table1 includes descriptive statistics for the total study sample, with the rows (explanatory variables) consisting of the key study variables that are often included in the final analysis1. Then within the columns (outcome of interest/response variable), you will find cells given as an (%) for categorical variables, whereas a mean, SD, or the median will be provided for continuous variables. Additionally, there will be a total column provided which can help in the assessment of the overall sample.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Demographics Table With Table1</span>"
    ]
  },
  {
    "objectID": "lessons/01_table1.html#necessary-packages",
    "href": "lessons/01_table1.html#necessary-packages",
    "title": "\n6  Demographics Table With Table1\n",
    "section": "\n6.2 Necessary Packages",
    "text": "6.2 Necessary Packages\nThe htmlTable package allows for the usage of the table1() function to create a table 1, while also making life easy when attempting to copy this table into a Word document.\nThe boot package was created to aid in performing bootstrapping analysis. With it comes numerous data sets, specifically clinical trial data sets to make this possible. However, there is no code book provided within the package when the data is downloaded as a csv file. This is a link on Github that explains and elaborates on every data within the package itself2.\n\n#install.packages(\"htmlTable\")\n#install.packages(\"boot\")\n\n# Load libraries\nlibrary(htmlTable)\nlibrary(table1)\nlibrary(boot)",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Demographics Table With Table1</span>"
    ]
  },
  {
    "objectID": "lessons/01_table1.html#data-source-and-description",
    "href": "lessons/01_table1.html#data-source-and-description",
    "title": "\n6  Demographics Table With Table1\n",
    "section": "\n6.3 Data source and description",
    "text": "6.3 Data source and description\nToday, we will be using the melanoma data set which consists of malignant melanoma measurements of patients. Each patient had their tumor surgically removed between the years of 1962 and 1977 at the Department of Plastic Surgery, University Hospital of Odense located in Denamrk. Each surgery consisted of the complete removal of the tumor with an additional removal of about 2.5cm of the surrounding skin. When this was completed, the thickness of the tumor was recorded along with the physical appearance of ulceration vs no ulceration, as it is an important prognostic indication of those with a thick/ulcerated tumor to have an increased chance of death as a consequence of melanoma.\n\ndata(melanoma, package = \"boot\")\nmelanoma_data &lt;- melanoma\n\n#Now that we loaded the raw data set, we will conduct a visual exploration before wrangling #the data and applying any functions, while also considering the requirements involved in #the construction of a table1.\n\nsummary(melanoma_data)\n\n      time          status          sex              age             year     \n Min.   :  10   Min.   :1.00   Min.   :0.0000   Min.   : 4.00   Min.   :1962  \n 1st Qu.:1525   1st Qu.:1.00   1st Qu.:0.0000   1st Qu.:42.00   1st Qu.:1968  \n Median :2005   Median :2.00   Median :0.0000   Median :54.00   Median :1970  \n Mean   :2153   Mean   :1.79   Mean   :0.3854   Mean   :52.46   Mean   :1970  \n 3rd Qu.:3042   3rd Qu.:2.00   3rd Qu.:1.0000   3rd Qu.:65.00   3rd Qu.:1972  \n Max.   :5565   Max.   :3.00   Max.   :1.0000   Max.   :95.00   Max.   :1977  \n   thickness         ulcer      \n Min.   : 0.10   Min.   :0.000  \n 1st Qu.: 0.97   1st Qu.:0.000  \n Median : 1.94   Median :0.000  \n Mean   : 2.92   Mean   :0.439  \n 3rd Qu.: 3.56   3rd Qu.:1.000  \n Max.   :17.42   Max.   :1.000",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Demographics Table With Table1</span>"
    ]
  },
  {
    "objectID": "lessons/01_table1.html#cleaning-the-data-to-create-a-model-data-frame",
    "href": "lessons/01_table1.html#cleaning-the-data-to-create-a-model-data-frame",
    "title": "\n6  Demographics Table With Table1\n",
    "section": "\n6.4 Cleaning the data to create a model data frame",
    "text": "6.4 Cleaning the data to create a model data frame\nLet us now explore the type of variables within the data set.\n\ntypeof(melanoma_data$status) \n\n[1] \"double\"\n\n\nWe will first provide a basic table1 to illustrate how the function works. Currently, all the variables are in numeric/double formats, however for the creation of a basic table1, it is of importance to convert the dependent/response variable of interest to reflect categories (factor).\nOur main variable of interest (dependent/response) is the status. According to the code book found in Github, status is coded into three levels that indicate the patients status at the end of the study. Level 1 indicates that they had died from melanoma, Level 2 indicates that they were still alive at the conclusion of the study, and Level 3 indicates that they had died from causes unrelated to their melanoma. As such, we will factor the “status” variable into three levels. With this in mind, let us go ahead and convert melanoma into a factor variable with three levels. For ease of analysis we will use 2 = “Alive” as the reference level. This can be done in two ways:\n\nAlthough more time consuming, it is highly recommended that beginners utilize the function as.factor() and then utilize the recode_factor() function to minimize the errors.\nWhen you become more skilled and are able to understand how the factor function works, it is possible to do everything in one step with the factor() function. In this function you can put levels and labels all in one function instead of having to break it up into more than one function.\n\nFor our example we will use as.factor then recode_factor() using 2 = “Alive” as our reference group.\n\nmelanoma_data$status &lt;-\n  as.factor(melanoma_data$status)\n\n# print the first six observations\nhead(melanoma_data$status)\n\n[1] 3 3 2 3 1 1\nLevels: 1 2 3\n\n# Recode\nmelanoma_data$status &lt;- recode_factor(\n  melanoma_data$status, \n  \"2\" = \"Alive\", # this is the reference group\n  \"1\" = \"Died from melanoma\",\n  \"3\" = \"Non-Melanoma death\"\n)\n\n# Print the first six observations\nhead(melanoma_data$status)\n\n[1] Non-Melanoma death Non-Melanoma death Alive              Non-Melanoma death\n[5] Died from melanoma Died from melanoma\nLevels: Alive Died from melanoma Non-Melanoma death\n\n\nAs you can see in the variable levels, “Alive” is the reference level. It is extremely important to pick a reference level to lay the foundation of the table along with highlighting the outcome of interest of your hypothesis. In summary, this lays the foundation of a well organized table.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Demographics Table With Table1</span>"
    ]
  },
  {
    "objectID": "lessons/01_table1.html#creation-of-basic-table-1",
    "href": "lessons/01_table1.html#creation-of-basic-table-1",
    "title": "\n6  Demographics Table With Table1\n",
    "section": "\n6.5 Creation of basic table 1",
    "text": "6.5 Creation of basic table 1\nNow that our main variable of interest is a factor with three levels, we will run a basic table1 with the independent/explanatory variables of interest: sex, age, ulcer, and thickness.\nRecall that the explanatory variables of interest are still in “double” formats. Conveniently, to analyze data before the independent variables are converted to factors and labeled, the table1 provides the ability to highlight level results. This only applies for independent variables that are in numeric/double formats in which each number represents a group. For instance 0 although is a number format we know it has a group meaning such as male.\nFor the independent variables, if they have factors in the front, it provides the number of cases (aka observations). If they are a continuous variable, we will get the mean, the SD, the minimum and the maximum amounts.\n\nbasic_table1 &lt;- table1( \n  ~ factor(sex) + age + factor(ulcer) + thickness | status, \n  data = melanoma_data\n)\n\nbasic_table1\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlive(N=134)\nDied from melanoma(N=57)\nNon-Melanoma death(N=14)\nOverall(N=205)\n\n\n\nfactor(sex)\n\n\n\n\n\n\n0\n91 (67.9%)\n28 (49.1%)\n7 (50.0%)\n126 (61.5%)\n\n\n1\n43 (32.1%)\n29 (50.9%)\n7 (50.0%)\n79 (38.5%)\n\n\nage\n\n\n\n\n\n\nMean (SD)\n50.0 (15.9)\n55.1 (17.9)\n65.3 (10.9)\n52.5 (16.7)\n\n\nMedian [Min, Max]\n52.0 [4.00, 84.0]\n56.0 [14.0, 95.0]\n65.0 [49.0, 86.0]\n54.0 [4.00, 95.0]\n\n\nfactor(ulcer)\n\n\n\n\n\n\n0\n92 (68.7%)\n16 (28.1%)\n7 (50.0%)\n115 (56.1%)\n\n\n1\n42 (31.3%)\n41 (71.9%)\n7 (50.0%)\n90 (43.9%)\n\n\nthickness\n\n\n\n\n\n\nMean (SD)\n2.24 (2.33)\n4.31 (3.57)\n3.72 (3.63)\n2.92 (2.96)\n\n\nMedian [Min, Max]\n1.36 [0.100, 12.9]\n3.54 [0.320, 17.4]\n2.26 [0.160, 12.6]\n1.94 [0.100, 17.4]\n\n\n\n\n\n\n\nNote that the table1 package uses a familiar formula interface, where the variables to include in the table are separated by ‘+’ symbols, the “stratification” variable (which creates the columns) appears to the right of a “conditioning” symbol ‘|’, and the data argument specifies a data.frame that contains the variables in the formula.\nIf we do not put factor for a grouped variable then the following will happen:\n\nwrong_table1 &lt;- table1(\n  ~ sex + age + ulcer + thickness | status, \n  data = melanoma_data\n)\n\nwrong_table1\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlive(N=134)\nDied from melanoma(N=57)\nNon-Melanoma death(N=14)\nOverall(N=205)\n\n\n\nsex\n\n\n\n\n\n\nMean (SD)\n0.321 (0.469)\n0.509 (0.504)\n0.500 (0.519)\n0.385 (0.488)\n\n\nMedian [Min, Max]\n0 [0, 1.00]\n1.00 [0, 1.00]\n0.500 [0, 1.00]\n0 [0, 1.00]\n\n\nage\n\n\n\n\n\n\nMean (SD)\n50.0 (15.9)\n55.1 (17.9)\n65.3 (10.9)\n52.5 (16.7)\n\n\nMedian [Min, Max]\n52.0 [4.00, 84.0]\n56.0 [14.0, 95.0]\n65.0 [49.0, 86.0]\n54.0 [4.00, 95.0]\n\n\nulcer\n\n\n\n\n\n\nMean (SD)\n0.313 (0.466)\n0.719 (0.453)\n0.500 (0.519)\n0.439 (0.497)\n\n\nMedian [Min, Max]\n0 [0, 1.00]\n1.00 [0, 1.00]\n0.500 [0, 1.00]\n0 [0, 1.00]\n\n\nthickness\n\n\n\n\n\n\nMean (SD)\n2.24 (2.33)\n4.31 (3.57)\n3.72 (3.63)\n2.92 (2.96)\n\n\nMedian [Min, Max]\n1.36 [0.100, 12.9]\n3.54 [0.320, 17.4]\n2.26 [0.160, 12.6]\n1.94 [0.100, 17.4]\n\n\n\n\n\n\n\nAs you can see above, we have the incorrect values provided of the explanatory variables. For example, in the variable of sex, we expect to see the number of individuals who identify as male or female, but instead we observe the mean, which is not a proper descriptive statistic as sex is a categorical variable.\nTo avoid this issue as well as problems in other procedures (like logistic regressions), it is crucial that we remember to factor the variables before we run any function. But because we don’t have nice labels for the variables and categories, it doesn’t look great. To improve things, we can create factors with descriptive labels for the categorical variables (sex and ulcer), label each variable the way we want, and specify units for the continuous variables (age and thickness). According to the code book, the patient’s sex: 1 = male, 0 = female, and ulcer is an indicator of ulceration : 1 = present, 0 = absent. We also specify that the overall column to be labeled “Total” and be positioned on the left, and add a caption and footnote:\n\nmelanoma_data$sex &lt;- as.factor(melanoma_data$sex)\n\n# print the first six observations\nhead(melanoma_data$sex)\n\n[1] 1 1 1 0 1 1\nLevels: 0 1\n\n# Recode\nmelanoma_data$sex &lt;- recode_factor(\n  melanoma_data$sex, \n  \"0\" = \"Female\",\n  \"1\" = \"Male\"\n)\n\n# Print the first six observations\nhead(melanoma_data$sex)\n\n[1] Male   Male   Male   Female Male   Male  \nLevels: Female Male\n\n\n\ntypeof(melanoma_data$ulcer)\n\n[1] \"double\"\n\nmelanoma_data$ulcer &lt;- as.factor(melanoma_data$ulcer)\n\n# print the first six observations\nhead(melanoma_data$ulcer)\n\n[1] 1 0 0 0 1 1\nLevels: 0 1\n\n# Recode\nmelanoma_data$ulcer &lt;- recode_factor(\n  melanoma_data$ulcer, \n  \"0\" = \"Absent\",\n  \"1\" = \"Present\"\n)\n\n# Print the first six observations\nhead(melanoma_data$ulcer)\n\n[1] Present Absent  Absent  Absent  Present Present\nLevels: Absent Present\n\n\nIn addition, we need to add units to the two continuous variables age and thickness. According to the code book, age is the patient’s age measured in years and thickness corresponds to the tumor’s thickness in millimeters (mm). The package table1 provides an easy way to demonstrate measurement information:\n\nunits(melanoma_data$age) &lt;- \"years\"\nunits(melanoma_data$thickness) &lt;- \"mm\"\n\nAdditionally, for visual and descriptive purposes, the function table1 is able to easily provide labels for the variables that will be shown in the final table using the label() function. Also, (caption \\&lt;-) provides a title for the table and (footnote \\&lt;-) provides any footnote information.\n\nlabel(melanoma_data$sex) &lt;- \"Sex\"\nlabel(melanoma_data$age) &lt;- \"Age\"\nlabel(melanoma_data$ulcer) &lt;- \"Ulceration\"\nlabel(melanoma_data$thickness) &lt;-\"Thickness*\"\n\ncaption_char &lt;- \"Table 1. Melanoma Dataset Descriptive Statistics\"\nfootnote_char &lt;- \"*Also known as Breslow thickness\"\n\nBelow, we can demonstrate the final table1 layout. As you can see, you no longer use factor() in front of the variable as we already factorized it in the previous steps.\n\ntable1(\n  ~ sex + age + ulcer + thickness | status, \n  data = melanoma_data,\n  overall = c(left = \"Total\"), \n  caption = caption_char, \n  footnote = footnote_char\n)\n\n\nTable 1. Melanoma Dataset Descriptive Statistics\n\n\n\nTotal(N=205)\nAlive(N=134)\nDied from melanoma(N=57)\nNon-Melanoma death(N=14)\n\n\n*Also known as Breslow thickness\n\n\n\nSex\n\n\n\n\n\n\nFemale\n126 (61.5%)\n91 (67.9%)\n28 (49.1%)\n7 (50.0%)\n\n\nMale\n79 (38.5%)\n43 (32.1%)\n29 (50.9%)\n7 (50.0%)\n\n\nAge (years)\n\n\n\n\n\n\nMean (SD)\n52.5 (16.7)\n50.0 (15.9)\n55.1 (17.9)\n65.3 (10.9)\n\n\nMedian [Min, Max]\n54.0 [4.00, 95.0]\n52.0 [4.00, 84.0]\n56.0 [14.0, 95.0]\n65.0 [49.0, 86.0]\n\n\nUlceration\n\n\n\n\n\n\nAbsent\n115 (56.1%)\n92 (68.7%)\n16 (28.1%)\n7 (50.0%)\n\n\nPresent\n90 (43.9%)\n42 (31.3%)\n41 (71.9%)\n7 (50.0%)\n\n\nThickness* (mm)\n\n\n\n\n\n\nMean (SD)\n2.92 (2.96)\n2.24 (2.33)\n4.31 (3.57)\n3.72 (3.63)\n\n\nMedian [Min, Max]\n1.94 [0.100, 17.4]\n1.36 [0.100, 12.9]\n3.54 [0.320, 17.4]\n2.26 [0.160, 12.6]",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Demographics Table With Table1</span>"
    ]
  },
  {
    "objectID": "lessons/01_table1.html#changing-the-tables-appearance",
    "href": "lessons/01_table1.html#changing-the-tables-appearance",
    "title": "\n6  Demographics Table With Table1\n",
    "section": "\n6.6 Changing the table’s appearance",
    "text": "6.6 Changing the table’s appearance\nThe default style of table1 uses an Arial font, and resembles the booktabs style commonly used in LaTeX. While this default style is not ugly, inevitably there will be a desire to customize the visual appearance of the table (fonts, colors, gridlines, etc). The package provides a limited number of built-in options for changing the style, while further customization can be achieved in R Markdown documents using CSS.3\n\n6.6.1 Using built-in styles\nThe package includes a limited number of built-in styles including:\n\nzebra: alternating shaded and unshaded rows (zebra stripes)\ngrid: show all grid lines\nshade: shade the header row(s) in gray\ntimes: use a serif font\n\nThese styles can be selected using the topclass argument of table1. Some examples follow:\n\ntable1(~ sex + age + ulcer + thickness | status, \n       data = melanoma_data,\n       overall = c(left = \"Total\"), \n       caption = caption_char, \n       footnote = footnote_char, \n       topclass=\"Rtable1-zebra\"\n)\n\n\nTable 1. Melanoma Dataset Descriptive Statistics\n\n\n\nTotal(N=205)\nAlive(N=134)\nDied from melanoma(N=57)\nNon-Melanoma death(N=14)\n\n\n*Also known as Breslow thickness\n\n\n\nSex\n\n\n\n\n\n\nFemale\n126 (61.5%)\n91 (67.9%)\n28 (49.1%)\n7 (50.0%)\n\n\nMale\n79 (38.5%)\n43 (32.1%)\n29 (50.9%)\n7 (50.0%)\n\n\nAge (years)\n\n\n\n\n\n\nMean (SD)\n52.5 (16.7)\n50.0 (15.9)\n55.1 (17.9)\n65.3 (10.9)\n\n\nMedian [Min, Max]\n54.0 [4.00, 95.0]\n52.0 [4.00, 84.0]\n56.0 [14.0, 95.0]\n65.0 [49.0, 86.0]\n\n\nUlceration\n\n\n\n\n\n\nAbsent\n115 (56.1%)\n92 (68.7%)\n16 (28.1%)\n7 (50.0%)\n\n\nPresent\n90 (43.9%)\n42 (31.3%)\n41 (71.9%)\n7 (50.0%)\n\n\nThickness* (mm)\n\n\n\n\n\n\nMean (SD)\n2.92 (2.96)\n2.24 (2.33)\n4.31 (3.57)\n3.72 (3.63)\n\n\nMedian [Min, Max]\n1.94 [0.100, 17.4]\n1.36 [0.100, 12.9]\n3.54 [0.320, 17.4]\n2.26 [0.160, 12.6]\n\n\n\n\n\n\ntable1(~ sex + age + ulcer + thickness | status, \n       data = melanoma_data,\n       overall = c(left = \"Total\"), \n       caption = caption_char, \n       footnote = footnote_char, \n       topclass=\"Rtable1-grid\"\n)\n\n\nTable 1. Melanoma Dataset Descriptive Statistics\n\n\n\nTotal(N=205)\nAlive(N=134)\nDied from melanoma(N=57)\nNon-Melanoma death(N=14)\n\n\n*Also known as Breslow thickness\n\n\n\nSex\n\n\n\n\n\n\nFemale\n126 (61.5%)\n91 (67.9%)\n28 (49.1%)\n7 (50.0%)\n\n\nMale\n79 (38.5%)\n43 (32.1%)\n29 (50.9%)\n7 (50.0%)\n\n\nAge (years)\n\n\n\n\n\n\nMean (SD)\n52.5 (16.7)\n50.0 (15.9)\n55.1 (17.9)\n65.3 (10.9)\n\n\nMedian [Min, Max]\n54.0 [4.00, 95.0]\n52.0 [4.00, 84.0]\n56.0 [14.0, 95.0]\n65.0 [49.0, 86.0]\n\n\nUlceration\n\n\n\n\n\n\nAbsent\n115 (56.1%)\n92 (68.7%)\n16 (28.1%)\n7 (50.0%)\n\n\nPresent\n90 (43.9%)\n42 (31.3%)\n41 (71.9%)\n7 (50.0%)\n\n\nThickness* (mm)\n\n\n\n\n\n\nMean (SD)\n2.92 (2.96)\n2.24 (2.33)\n4.31 (3.57)\n3.72 (3.63)\n\n\nMedian [Min, Max]\n1.94 [0.100, 17.4]\n1.36 [0.100, 12.9]\n3.54 [0.320, 17.4]\n2.26 [0.160, 12.6]\n\n\n\n\n\n\ntable1(~ sex + age + ulcer + thickness | status, \n       data = melanoma_data,\n       overall = c(left = \"Total\"), \n       caption = caption_char, \n       footnote = footnote_char, \n       topclass=\"Rtable1-grid Rtable1-shade Rtable1-times\"\n)\n\n\nTable 1. Melanoma Dataset Descriptive Statistics\n\n\n\nTotal(N=205)\nAlive(N=134)\nDied from melanoma(N=57)\nNon-Melanoma death(N=14)\n\n\n*Also known as Breslow thickness\n\n\n\nSex\n\n\n\n\n\n\nFemale\n126 (61.5%)\n91 (67.9%)\n28 (49.1%)\n7 (50.0%)\n\n\nMale\n79 (38.5%)\n43 (32.1%)\n29 (50.9%)\n7 (50.0%)\n\n\nAge (years)\n\n\n\n\n\n\nMean (SD)\n52.5 (16.7)\n50.0 (15.9)\n55.1 (17.9)\n65.3 (10.9)\n\n\nMedian [Min, Max]\n54.0 [4.00, 95.0]\n52.0 [4.00, 84.0]\n56.0 [14.0, 95.0]\n65.0 [49.0, 86.0]\n\n\nUlceration\n\n\n\n\n\n\nAbsent\n115 (56.1%)\n92 (68.7%)\n16 (28.1%)\n7 (50.0%)\n\n\nPresent\n90 (43.9%)\n42 (31.3%)\n41 (71.9%)\n7 (50.0%)\n\n\nThickness* (mm)\n\n\n\n\n\n\nMean (SD)\n2.92 (2.96)\n2.24 (2.33)\n4.31 (3.57)\n3.72 (3.63)\n\n\nMedian [Min, Max]\n1.94 [0.100, 17.4]\n1.36 [0.100, 12.9]\n3.54 [0.320, 17.4]\n2.26 [0.160, 12.6]\n\n\n\n\n\nNote that the style name needs to be preceded by the prefix Rtable1-. Multiple styles can be applied in combination by separating them with a space.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Demographics Table With Table1</span>"
    ]
  },
  {
    "objectID": "lessons/01_table1.html#conclusion",
    "href": "lessons/01_table1.html#conclusion",
    "title": "\n6  Demographics Table With Table1\n",
    "section": "\n6.7 Conclusion",
    "text": "6.7 Conclusion\nIn conclusion, table1 is one of the most utilized tools in the scientific research field. Understanding how to use the table1 package in R can be of benefit to many. It is important to note that this presentation is just a brief summary with what is possible with this package. For example, you can add extra columns to the table, other than descriptive statistics. This can be accomplished using the extra.col option. In addition, you can also stratify the response variable to highlight two of the responses, like dead or alive in our example.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Demographics Table With Table1</span>"
    ]
  },
  {
    "objectID": "lessons/01_table1.html#references",
    "href": "lessons/01_table1.html#references",
    "title": "\n6  Demographics Table With Table1\n",
    "section": "References",
    "text": "References\n\n\n\n\n1. \nHayes-Larson E, Kezios KL, Mooney SJ, Lovasi G. Who is in this study, anyway? Guidelines for a useful Table 1. Journal of Clinical Epidemiology [Internet] 2019;114:125–32. Available from: http://dx.doi.org/10.1016/j.jclinepi.2019.06.011\n\n\n\n2. \nA. C. Davison, D. V. Hinkley. Bootstrap methods and their applications [Internet]. Cambridge: Cambridge University Press; 1997. Available from: doi:10.1017/CBO9780511802843\n\n\n\n3. \nCohen J. Statistical power analysis for the behavioral sciences. 2nd ed. Hillsdale, N.J: L. Erlbaum Associates; 1988.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Demographics Table With Table1</span>"
    ]
  },
  {
    "objectID": "lessons/01_gtsummary.html",
    "href": "lessons/01_gtsummary.html",
    "title": "\n7  Table by gtsummary\n",
    "section": "",
    "text": "7.1 Packages for this Lesson\n# Installing Required Packages\n# install.packages(\"public.ctn0094data\")\n# install.packages(\"parameters\")\n# install.packages(\"tidyverse\")\n# install.packages(\"gtsummary\")\n\n# Loading Required Packages\nlibrary(public.ctn0094data)\nlibrary(parameters)\nlibrary(tidyverse)\nlibrary(gtsummary)",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Table by gtsummary</span>"
    ]
  },
  {
    "objectID": "lessons/01_gtsummary.html#introduction-to-gtsummary",
    "href": "lessons/01_gtsummary.html#introduction-to-gtsummary",
    "title": "\n7  Table by gtsummary\n",
    "section": "\n7.2 Introduction to ‘gtsummary’",
    "text": "7.2 Introduction to ‘gtsummary’\nThe gtsummary package is useful mainly for creating publication-ready tables (i.e.demographic table, simple summary table, contingency-table, regression table, etc.). The best feature of this package is it can automatically detect if the data is continuous, dichotomous or categorical, and which descriptive statistics needs to apply.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Table by gtsummary</span>"
    ]
  },
  {
    "objectID": "lessons/01_gtsummary.html#data-source-and-description",
    "href": "lessons/01_gtsummary.html#data-source-and-description",
    "title": "\n7  Table by gtsummary\n",
    "section": "\n7.3 Data Source and Description",
    "text": "7.3 Data Source and Description\nThe public.ctn0094data package provides harmonized and normalized data sets from the CTN-0094 clinical trial. These data sets describe the experiences of care-seeking individuals suffering from opioid use disorder (OUD). The trial is part of the Clinical Trials Network (CTN) protocol number 0094, funded by the US National Institute of Drug Abuse (NIDA). It is used by the NIDA to develop, validate, refine, and deliver new treatment options to patients.\nIn this lesson, I used the demographics, and fagerstrom data sets from the public.ctn0094data package to demonstrate the gtsummary function. The demographics part contains the demographic variables such as age, sex, race, marital status etc. The fagerstrom part contains data on smoking habit (smoker/non-smoker, Fagerstrom Test for Nicotine Dependence Score (ranging from 0 to 10) ~ FTND, Number of cigarettes smoked per day.). The FTND is a questionnaire that assesses the physical dependence of adults on nicotine. The test uses yes/no questions scored from 0 to 1 and multiple-choice questions scored from 0 to 3, and the total score ranges from 0 to 10. The higher the score, the more intense the patient’s nicotine dependence is. The score categories are: 8+: High dependence, 7–5: Moderate dependence, 4–3: Low to moderate dependence and 0–2: Low dependence.\n\n# Searching suitable data sets: You can skip \ndata(package = \"public.ctn0094data\")",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Table by gtsummary</span>"
    ]
  },
  {
    "objectID": "lessons/01_gtsummary.html#creating-model-data-frames",
    "href": "lessons/01_gtsummary.html#creating-model-data-frames",
    "title": "\n7  Table by gtsummary\n",
    "section": "\n7.4 Creating Model Data Frames",
    "text": "7.4 Creating Model Data Frames\nThe demographics and fagerstrom data sets within the public.ctn0094data package were joined by ID (who variable) and a new dta frame smoking_df is created.\n\n# Joining data sets: \nsmoking_df &lt;- demographics %&gt;% \n  left_join(fagerstrom, by = \"who\")",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Table by gtsummary</span>"
    ]
  },
  {
    "objectID": "lessons/01_gtsummary.html#demographic-table-with-tbl_summary-function",
    "href": "lessons/01_gtsummary.html#demographic-table-with-tbl_summary-function",
    "title": "\n7  Table by gtsummary\n",
    "section": "\n7.5 Demographic Table with tbl_summary Function",
    "text": "7.5 Demographic Table with tbl_summary Function\n\n7.5.1 Creating Table 1: Demographic Characteristic\nIn order to create a basic demographic table, I will now select which variables I want to show in the table and then use the tbl_summary function to create the table. I am also adding the description of the variables I included in my table.\n\n\nage: an integer variable that indicates the Age of the patient.\n\nrace: a factor variable with levels ‘Black’, ‘Other Refused/missing’, and ‘White’, which represents the Self-reported race of the patient.\n\neducation: a factor variable denotes the Education level at intake, with levels ‘HS/GED’ for high school graduate or equivalent, ‘Less than HS’ for less than high school education, ‘More than HS’ for some education beyond high school, and ‘Missing’ if the information is not provided.\n\nis_male: a factor variable with levels ‘No’ and ‘Yes’, describing the Sex (not gender) of the patient, where ‘Yes’ indicates male.\n\nmarital: a factor variable indicating the Marital status at intake, with levels ‘Married or Partnered’, ‘Never married’, ‘Separated/Divorced/Widowed’, and ‘Not answered’ if the question was not asked during intake.\n\nis_smoker: a factor indicating whether the patient is a smoker or not. Levels include “No” (not a smoker) and “Yes” (a smoker).\n\n\n# Selecting variables in a new data frame `table_1df` for table 1\ntable_1df &lt;- smoking_df %&gt;% \n  select(age, race, education, is_male, marital, is_smoker)\n\n# Table 1\ntable_1 &lt;- table_1df  %&gt;% tbl_summary()\n\ntable_1\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nN = 3,5601\n\n\n\n\nage\n34 (27, 45)\n\n\n    Unknown\n208\n\n\nrace\n\n\n\n    Black\n365 (10%)\n\n\n    Other\n506 (14%)\n\n\n    Refused/missing\n58 (1.6%)\n\n\n    White\n2,631 (74%)\n\n\neducation\n\n\n\n    HS/GED\n691 (39%)\n\n\n    Less than HS\n352 (20%)\n\n\n    More than HS\n724 (41%)\n\n\n    Unknown\n1,793\n\n\nis_male\n2,351 (66%)\n\n\n    Unknown\n4\n\n\nmarital\n\n\n\n    Married or Partnered\n329 (19%)\n\n\n    Never married\n1,028 (59%)\n\n\n    Separated/Divorced/Widowed\n394 (23%)\n\n\n    Unknown\n1,809\n\n\nis_smoker\n2,631 (85%)\n\n\n    Unknown\n460\n\n\n\n\n1 Median (IQR); n (%)\n\n\n\n\n\n\n\n7.5.2 Customizing Table 1: Changing the Label\nI am using label function to change the label of all variables. Other customization will be shown in the next contingency table.\n\n# Changing the Label\n\ntable_1 &lt;-\n  table_1df %&gt;% \n  tbl_summary(\n    label = list(\n      age = \"Age\",\n      race = \"Race\",\n      education = \"Education level\",\n      is_male = \"Male\",\n      marital = \"Marital status\",\n      is_smoker = \"Smoker\"\n    )\n  )\n\ntable_1\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nN = 3,5601\n\n\n\n\nAge\n34 (27, 45)\n\n\n    Unknown\n208\n\n\nRace\n\n\n\n    Black\n365 (10%)\n\n\n    Other\n506 (14%)\n\n\n    Refused/missing\n58 (1.6%)\n\n\n    White\n2,631 (74%)\n\n\nEducation level\n\n\n\n    HS/GED\n691 (39%)\n\n\n    Less than HS\n352 (20%)\n\n\n    More than HS\n724 (41%)\n\n\n    Unknown\n1,793\n\n\nMale\n2,351 (66%)\n\n\n    Unknown\n4\n\n\nMarital status\n\n\n\n    Married or Partnered\n329 (19%)\n\n\n    Never married\n1,028 (59%)\n\n\n    Separated/Divorced/Widowed\n394 (23%)\n\n\n    Unknown\n1,809\n\n\nSmoker\n2,631 (85%)\n\n\n    Unknown\n460\n\n\n\n\n1 Median (IQR); n (%)",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Table by gtsummary</span>"
    ]
  },
  {
    "objectID": "lessons/01_gtsummary.html#contingency-table-with-tbl_summary-function",
    "href": "lessons/01_gtsummary.html#contingency-table-with-tbl_summary-function",
    "title": "\n7  Table by gtsummary\n",
    "section": "\n7.6 Contingency Table with tbl_summary Function",
    "text": "7.6 Contingency Table with tbl_summary Function\n\n7.6.1 Creating Table 2: Demographic Variables by Smoking Status\nI will now show the table 1 demographic variables by smoking habit status (is_smoker, Yes = smoker and No = non-smokers)\n\n# Contingency table \ntable_2 &lt;- table_1df %&gt;% tbl_summary(by = is_smoker) \n\ntable_2\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nNo, N = 4691\n\n\nYes, N = 2,6311\n\n\n\n\nage\n36 (28, 47)\n33 (26, 44)\n\n\n    Unknown\n7\n79\n\n\nrace\n\n\n\n\n    Black\n46 (9.8%)\n259 (9.8%)\n\n\n    Other\n68 (14%)\n376 (14%)\n\n\n    Refused/missing\n1 (0.2%)\n18 (0.7%)\n\n\n    White\n354 (75%)\n1,978 (75%)\n\n\neducation\n\n\n\n\n    HS/GED\n104 (34%)\n531 (41%)\n\n\n    Less than HS\n23 (7.5%)\n290 (22%)\n\n\n    More than HS\n179 (58%)\n488 (37%)\n\n\n    Unknown\n163\n1,322\n\n\nis_male\n336 (72%)\n1,724 (66%)\n\n\nmarital\n\n\n\n\n    Married or Partnered\n75 (25%)\n236 (18%)\n\n\n    Never married\n152 (50%)\n775 (59%)\n\n\n    Separated/Divorced/Widowed\n77 (25%)\n293 (22%)\n\n\n    Unknown\n165\n1,327\n\n\n\n\n1 Median (IQR); n (%)\n\n\n\n\n\n\n\n7.6.2 Removing Missing Data\nIf I do not want to show the missing data in my table, I will use missing = \"no\".\n\n# Removing Missing Data\ntable_2nm &lt;- table_1df %&gt;% tbl_summary(by = is_smoker,\n                                   missing = \"no\") \ntable_2nm\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nNo, N = 4691\n\n\nYes, N = 2,6311\n\n\n\n\nage\n36 (28, 47)\n33 (26, 44)\n\n\nrace\n\n\n\n\n    Black\n46 (9.8%)\n259 (9.8%)\n\n\n    Other\n68 (14%)\n376 (14%)\n\n\n    Refused/missing\n1 (0.2%)\n18 (0.7%)\n\n\n    White\n354 (75%)\n1,978 (75%)\n\n\neducation\n\n\n\n\n    HS/GED\n104 (34%)\n531 (41%)\n\n\n    Less than HS\n23 (7.5%)\n290 (22%)\n\n\n    More than HS\n179 (58%)\n488 (37%)\n\n\nis_male\n336 (72%)\n1,724 (66%)\n\n\nmarital\n\n\n\n\n    Married or Partnered\n75 (25%)\n236 (18%)\n\n\n    Never married\n152 (50%)\n775 (59%)\n\n\n    Separated/Divorced/Widowed\n77 (25%)\n293 (22%)\n\n\n\n\n1 Median (IQR); n (%)\n\n\n\n\n\n\n\n7.6.3 Applying Statistical Tests\nI will use add_p function to show the statistical analysis. This will automatically detect if data in each variable is continuous, dichotomous or categorical, and apply the appropriate descriptive statistics accordingly.\n\n# Adding p-value\ntable_2 &lt;- table_1df %&gt;% tbl_summary(by = is_smoker,\n                                   missing = \"no\") %&gt;% \n  add_p()\n\ntable_2\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nNo, N = 4691\n\n\nYes, N = 2,6311\n\n\np-value2\n\n\n\n\nage\n36 (28, 47)\n33 (26, 44)\n&lt;0.001\n\n\nrace\n\n\n0.8\n\n\n    Black\n46 (9.8%)\n259 (9.8%)\n\n\n\n    Other\n68 (14%)\n376 (14%)\n\n\n\n    Refused/missing\n1 (0.2%)\n18 (0.7%)\n\n\n\n    White\n354 (75%)\n1,978 (75%)\n\n\n\neducation\n\n\n&lt;0.001\n\n\n    HS/GED\n104 (34%)\n531 (41%)\n\n\n\n    Less than HS\n23 (7.5%)\n290 (22%)\n\n\n\n    More than HS\n179 (58%)\n488 (37%)\n\n\n\nis_male\n336 (72%)\n1,724 (66%)\n0.010\n\n\nmarital\n\n\n0.006\n\n\n    Married or Partnered\n75 (25%)\n236 (18%)\n\n\n\n    Never married\n152 (50%)\n775 (59%)\n\n\n\n    Separated/Divorced/Widowed\n77 (25%)\n293 (22%)\n\n\n\n\n\n\n1 Median (IQR); n (%)\n\n\n\n2 Wilcoxon rank sum test; Fisher’s exact test; Pearson’s Chi-squared test\n\n\n\n\n\n\n\nNote: The footnote 2 shows all the statistical tests applied to this table. It can be understandable from the table that for categorical variable it applied Pearson’s Chi-squared test, for continuous non-normal distributed variable it applied Wilcoxon rank sum test; and for small sample data, it applied Fisher’s exact test. It would be great to see different footnotes for each of the test next to each p-value, however, I did not find a way to do that.\n\n7.6.4 Customizing Table 2(a)\nI will now customize the table 2 to show total number and overall number and show missing values by using the following functions:\n\n# Adding total and overall number \ntable_2 &lt;- table_1df %&gt;% tbl_summary(by = is_smoker,\n                                   label = list(\n                                     age = \"Age\",\n                                     race = \"Race\",\n                                     education = \"Education level\",\n                                     is_male = \"Male\",\n                                     marital = \"Marital status\"\n                                   ),\n                                   missing_text = \"(Missing)\"\n                                  ) %&gt;% \n  add_p() %&gt;%\n  add_n() %&gt;%\n  add_overall() \n\ntable_2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nN\n\nOverall, N = 3,1001\n\n\nNo, N = 4691\n\n\nYes, N = 2,6311\n\n\np-value2\n\n\n\n\nAge\n3,014\n34 (27, 45)\n36 (28, 47)\n33 (26, 44)\n&lt;0.001\n\n\n    (Missing)\n\n86\n7\n79\n\n\n\nRace\n3,100\n\n\n\n0.8\n\n\n    Black\n\n305 (9.8%)\n46 (9.8%)\n259 (9.8%)\n\n\n\n    Other\n\n444 (14%)\n68 (14%)\n376 (14%)\n\n\n\n    Refused/missing\n\n19 (0.6%)\n1 (0.2%)\n18 (0.7%)\n\n\n\n    White\n\n2,332 (75%)\n354 (75%)\n1,978 (75%)\n\n\n\nEducation level\n1,615\n\n\n\n&lt;0.001\n\n\n    HS/GED\n\n635 (39%)\n104 (34%)\n531 (41%)\n\n\n\n    Less than HS\n\n313 (19%)\n23 (7.5%)\n290 (22%)\n\n\n\n    More than HS\n\n667 (41%)\n179 (58%)\n488 (37%)\n\n\n\n    (Missing)\n\n1,485\n163\n1,322\n\n\n\nMale\n3,100\n2,060 (66%)\n336 (72%)\n1,724 (66%)\n0.010\n\n\nMarital status\n1,608\n\n\n\n0.006\n\n\n    Married or Partnered\n\n311 (19%)\n75 (25%)\n236 (18%)\n\n\n\n    Never married\n\n927 (58%)\n152 (50%)\n775 (59%)\n\n\n\n    Separated/Divorced/Widowed\n\n370 (23%)\n77 (25%)\n293 (22%)\n\n\n\n    (Missing)\n\n1,492\n165\n1,327\n\n\n\n\n\n\n1 Median (IQR); n (%)\n\n\n\n2 Wilcoxon rank sum test; Fisher’s exact test; Pearson’s Chi-squared test\n\n\n\n\n\n\n\n\n7.6.5 Customizing Table 2(b)\nI will now customize the title, caption and header and made the variable names bold of table 2 by using the following functions:\n\n# Adding title, caption and header \ntable_2 &lt;- table_1df %&gt;% tbl_summary(by = is_smoker,\n                                   label = list(\n                                     age = \"Age\",\n                                     race = \"Race\",\n                                     education = \"Education level\",\n                                     is_male = \"Male\",\n                                     marital = \"Marital status\"\n                                   ),\n                                   missing_text = \"(Missing)\"\n                                  ) %&gt;% \n  add_p() %&gt;%\n  add_n() %&gt;%\n  add_overall() %&gt;%\n  bold_labels() %&gt;%\n  modify_caption(\"Table 2. Demographic characteristics according to smoking status\") %&gt;%\n  modify_header(label ~ \"**Demographic characteristics**\") %&gt;%\n  modify_spanning_header(c(\"stat_1\", \"stat_2\") ~ \"**Smoking status**\") \n  \ntable_2\n\n\n\n\n\nTable 2. Demographic characteristics according to smoking status\n\n\n\n\n\n\n\n\n\n\nDemographic characteristics\nN\n\nOverall, N = 3,1001\n\nSmoking status\n\np-value2\n\n\n\n\nNo, N = 4691\n\n\nYes, N = 2,6311\n\n\n\n\n\nAge\n3,014\n34 (27, 45)\n36 (28, 47)\n33 (26, 44)\n&lt;0.001\n\n\n    (Missing)\n\n86\n7\n79\n\n\n\nRace\n3,100\n\n\n\n0.8\n\n\n    Black\n\n305 (9.8%)\n46 (9.8%)\n259 (9.8%)\n\n\n\n    Other\n\n444 (14%)\n68 (14%)\n376 (14%)\n\n\n\n    Refused/missing\n\n19 (0.6%)\n1 (0.2%)\n18 (0.7%)\n\n\n\n    White\n\n2,332 (75%)\n354 (75%)\n1,978 (75%)\n\n\n\nEducation level\n1,615\n\n\n\n&lt;0.001\n\n\n    HS/GED\n\n635 (39%)\n104 (34%)\n531 (41%)\n\n\n\n    Less than HS\n\n313 (19%)\n23 (7.5%)\n290 (22%)\n\n\n\n    More than HS\n\n667 (41%)\n179 (58%)\n488 (37%)\n\n\n\n    (Missing)\n\n1,485\n163\n1,322\n\n\n\nMale\n3,100\n2,060 (66%)\n336 (72%)\n1,724 (66%)\n0.010\n\n\nMarital status\n1,608\n\n\n\n0.006\n\n\n    Married or Partnered\n\n311 (19%)\n75 (25%)\n236 (18%)\n\n\n\n    Never married\n\n927 (58%)\n152 (50%)\n775 (59%)\n\n\n\n    Separated/Divorced/Widowed\n\n370 (23%)\n77 (25%)\n293 (22%)\n\n\n\n    (Missing)\n\n1,492\n165\n1,327\n\n\n\n\n\n\n1 Median (IQR); n (%)\n\n\n\n2 Wilcoxon rank sum test; Fisher’s exact test; Pearson’s Chi-squared test\n\n\n\n\n\n\n\n\n7.6.6 Customizing Table 2(c)\nHere, I am keeping only those customization that I prefer to have in my final table 2.\n\n# Final table\ntable_2 &lt;- table_1df %&gt;% tbl_summary(by = is_smoker,\n                                   label = list(\n                                     age = \"Age\",\n                                     race = \"Race\",\n                                     education = \"Education level\",\n                                     is_male = \"Male\",\n                                     marital = \"Marital status\"\n                                   ),\n                                   missing = \"no\"\n                                  ) %&gt;% \n  add_p() %&gt;%\n  #add_n() %&gt;%\n  #add_overall() %&gt;%\n  bold_labels() %&gt;%\n  modify_caption(\"Table 2. Demographic characteristics according to smoking status\") %&gt;%\n  modify_header(label ~ \"**Characteristics**\") %&gt;%\n  modify_spanning_header(c(\"stat_1\", \"stat_2\") ~ \"**Smoking Status**\") %&gt;%\n  modify_footnote(all_stat_cols() ~ \"Median (IQR) for Age; n (%) for all other variables\") \n  \ntable_2\n\n\n\n\n\nTable 2. Demographic characteristics according to smoking status\n\n\n\n\n\n\n\n\nCharacteristics\nSmoking Status\n\np-value2\n\n\n\n\nNo, N = 4691\n\n\nYes, N = 2,6311\n\n\n\n\n\nAge\n36 (28, 47)\n33 (26, 44)\n&lt;0.001\n\n\nRace\n\n\n0.8\n\n\n    Black\n46 (9.8%)\n259 (9.8%)\n\n\n\n    Other\n68 (14%)\n376 (14%)\n\n\n\n    Refused/missing\n1 (0.2%)\n18 (0.7%)\n\n\n\n    White\n354 (75%)\n1,978 (75%)\n\n\n\nEducation level\n\n\n&lt;0.001\n\n\n    HS/GED\n104 (34%)\n531 (41%)\n\n\n\n    Less than HS\n23 (7.5%)\n290 (22%)\n\n\n\n    More than HS\n179 (58%)\n488 (37%)\n\n\n\nMale\n336 (72%)\n1,724 (66%)\n0.010\n\n\nMarital status\n\n\n0.006\n\n\n    Married or Partnered\n75 (25%)\n236 (18%)\n\n\n\n    Never married\n152 (50%)\n775 (59%)\n\n\n\n    Separated/Divorced/Widowed\n77 (25%)\n293 (22%)\n\n\n\n\n\n\n1 Median (IQR) for Age; n (%) for all other variables\n\n\n\n2 Wilcoxon rank sum test; Fisher’s exact test; Pearson’s Chi-squared test\n\n\n\n\n\n\n\n\n7.6.7 Interpretation of Table 2\nInterpreting the variable Education level:\nNull Hypothesis (H₀): There is no association between education level and smoking status.\nAlternative Hypothesis (H₁): There is an association between education level and smoking status.\nSince the p-value is less than 0.001, we reject the null hypothesis. This indicates that there is a statistically significant association between education level and smoking status. However, to understand the nature of this association (whether education level affects smoking status or vice versa), further analysis would be needed.\n\n7.6.8 Missing value distribution in Table 2\nWe often want to see the missing value distribution among the the demographic variables. For example, we want to see the missing value distribution for the smoking status variable. First, we need to re-code the NA into a new category for is_smoker variable and recreate the table.\n\n7.6.8.1 Missing value data creation\n\n# Recoding `is_smoker` variable into `is_smoker_new`\ntable_1df &lt;- table_1df %&gt;% \n  mutate(is_smoker_new = ifelse(is.na(is_smoker), 99, is_smoker))  # converting all NA to 99\n\n# Convert into factor\ntable_1df$is_smoker_new &lt;- factor(table_1df$is_smoker_new,\n                                  levels = c(1, 2, 99),\n                                  labels = c(\"No\", \"Yes\", \"Missing\"))\n\n# New data frame \ntable_1df_new &lt;- table_1df %&gt;% \n  select(age, race, education, is_male, marital, is_smoker_new)\n\n\n7.6.8.2 Missing value table creation\n\n# Final table\ntable_2miss &lt;- table_1df_new %&gt;% tbl_summary(by = is_smoker_new,\n                                   label = list(\n                                     age = \"Age\",\n                                     race = \"Race\",\n                                     education = \"Education level\",\n                                     is_male = \"Male\",\n                                     marital = \"Marital status\"\n                                   ),\n                                   missing = \"no\"\n                                  ) %&gt;% \n  add_p() %&gt;%\n  #add_n() %&gt;%\n  #add_overall() %&gt;%\n  bold_labels() %&gt;%\n  modify_caption(\"Table 2. Demographic characteristics according to smoking status\") %&gt;%\n  modify_header(label ~ \"**Characteristics**\") %&gt;%\n  modify_spanning_header(c(\"stat_1\", \"stat_2\", \"stat_3\") ~ \"**Smoking Status**\") %&gt;%\n  modify_footnote(all_stat_cols() ~ \"Median (IQR) for Age; n (%) for all other variables\") \n  \ntable_2miss\n\n\n\n\n\nTable 2. Demographic characteristics according to smoking status\n\n\n\n\n\n\n\n\n\nCharacteristics\nSmoking Status\n\np-value2\n\n\n\n\nNo, N = 4691\n\n\nYes, N = 2,6311\n\n\nMissing, N = 4601\n\n\n\n\n\nAge\n36 (28, 47)\n33 (26, 44)\n39 (29, 47)\n&lt;0.001\n\n\nRace\n\n\n\n&lt;0.001\n\n\n    Black\n46 (9.8%)\n259 (9.8%)\n60 (13%)\n\n\n\n    Other\n68 (14%)\n376 (14%)\n62 (13%)\n\n\n\n    Refused/missing\n1 (0.2%)\n18 (0.7%)\n39 (8.5%)\n\n\n\n    White\n354 (75%)\n1,978 (75%)\n299 (65%)\n\n\n\nEducation level\n\n\n\n&lt;0.001\n\n\n    HS/GED\n104 (34%)\n531 (41%)\n56 (37%)\n\n\n\n    Less than HS\n23 (7.5%)\n290 (22%)\n39 (26%)\n\n\n\n    More than HS\n179 (58%)\n488 (37%)\n57 (38%)\n\n\n\nMale\n336 (72%)\n1,724 (66%)\n291 (64%)\n0.019\n\n\nMarital status\n\n\n\n&lt;0.001\n\n\n    Married or Partnered\n75 (25%)\n236 (18%)\n18 (13%)\n\n\n\n    Never married\n152 (50%)\n775 (59%)\n101 (71%)\n\n\n\n    Separated/Divorced/Widowed\n77 (25%)\n293 (22%)\n24 (17%)\n\n\n\n\n\n\n1 Median (IQR) for Age; n (%) for all other variables\n\n\n\n2 Kruskal-Wallis rank sum test; Pearson’s Chi-squared test",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Table by gtsummary</span>"
    ]
  },
  {
    "objectID": "lessons/01_gtsummary.html#regression-table-with-tbl_regression-function",
    "href": "lessons/01_gtsummary.html#regression-table-with-tbl_regression-function",
    "title": "\n7  Table by gtsummary\n",
    "section": "\n7.7 Regression Table with tbl_regression() Function",
    "text": "7.7 Regression Table with tbl_regression() Function\n\n7.7.1 Creating Regression Model\nHere, we are creating a logistic regression model where smoking status is the response variable, education is exploratory variable and age, race and sex are considered as confounders.\n\n# Building the Multivariable logistic model\nm1 &lt;- glm(is_smoker ~  education + age + race + is_male, \n          table_1df, \n          family = binomial)\n\n# View raw model results\nsummary(m1)$coefficients\n\n                         Estimate  Std. Error     z value     Pr(&gt;|z|)\n(Intercept)            3.50434562 0.389242905  9.00297879 2.196748e-19\neducationLess than HS  1.01143171 0.252724965  4.00210447 6.278157e-05\neducationMore than HS -0.60886151 0.144410039 -4.21619932 2.484542e-05\nage                   -0.04564764 0.006417912 -7.11253757 1.139285e-12\nraceOther             -0.24842217 0.315210858 -0.78811425 4.306299e-01\nraceRefused/missing    0.39602359 1.124629178  0.35213704 7.247355e-01\nraceWhite             -0.01922531 0.251971208 -0.07629961 9.391807e-01\nis_maleYes            -0.39712021 0.147363550 -2.69483338 7.042384e-03\n\n\n\n7.7.2 Creating Table 3: Regression Table\nHere, I am using tbl_regression function to see the regression results in the table. The exponentiate = TRUE shows the data as Odds Ratio after exponentiation of the beta values.\n\n# Creating Regression Table \ntable_3 &lt;- tbl_regression(m1, exponentiate = TRUE)\n\ntable_3\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nOR1\n\n\n95% CI1\n\np-value\n\n\n\neducation\n\n\n\n\n\n    HS/GED\n—\n—\n\n\n\n    Less than HS\n2.75\n1.71, 4.61\n&lt;0.001\n\n\n    More than HS\n0.54\n0.41, 0.72\n&lt;0.001\n\n\nage\n0.96\n0.94, 0.97\n&lt;0.001\n\n\nrace\n\n\n\n\n\n    Black\n—\n—\n\n\n\n    Other\n0.78\n0.42, 1.44\n0.4\n\n\n    Refused/missing\n1.49\n0.23, 29.4\n0.7\n\n\n    White\n0.98\n0.59, 1.59\n&gt;0.9\n\n\nis_male\n\n\n\n\n\n    No\n—\n—\n\n\n\n    Yes\n0.67\n0.50, 0.89\n0.007\n\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n7.7.3 Customizing Table 3\nHere, I have customized the table 3 by using functions I applied in table 1.\n\n# Customizing Regression Table \ntable_3 &lt;- tbl_regression(m1, exponentiate = TRUE,\n                           label = list(\n                             age = \"Age\",\n                             race = \"Race\",\n                             education = \"Education level\",\n                             is_male = \"Male\"\n                             ),\n                          missing = \"no\"\n                          ) %&gt;% \n  bold_labels() %&gt;%\n  bold_p(t = 0.10) %&gt;%  \n  italicize_levels() %&gt;%\n  modify_caption(\"Table 3. Logistic Regression for smoking status as response varialbe (n=3014)\")\n\ntable_3\n\n\n\n\n\nTable 3. Logistic Regression for smoking status as response varialbe (n=3014)\n\n\n\n\n\n\n\nCharacteristic\n\nOR1\n\n\n95% CI1\n\np-value\n\n\n\nEducation level\n\n\n\n\n\n    HS/GED\n—\n—\n\n\n\n    Less than HS\n2.75\n1.71, 4.61\n&lt;0.001\n\n\n    More than HS\n0.54\n0.41, 0.72\n&lt;0.001\n\n\nAge\n0.96\n0.94, 0.97\n&lt;0.001\n\n\nRace\n\n\n\n\n\n    Black\n—\n—\n\n\n\n    Other\n0.78\n0.42, 1.44\n0.4\n\n\n    Refused/missing\n1.49\n0.23, 29.4\n0.7\n\n\n    White\n0.98\n0.59, 1.59\n&gt;0.9\n\n\nMale\n\n\n\n\n\n    No\n—\n—\n\n\n\n    Yes\n0.67\n0.50, 0.89\n0.007\n\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n7.7.4 Interpreting Table 3\nInterpreting the variable Education level:\nFor individuals with less than high school education, the odds of being a smoker are 2.75 times higher compared to those with HS/GED, after adjusting for age, race, and sex.\nConversely, for individuals with more than high school education, the odds of being a smoker are 0.54 times lower compared to those with HS/GED, after adjusting for age, race, and sex.\nInterpreting the variable Age:\nFor each unit increase in age, the odds of being a smoker decrease by a factor of 0.96 (or 4%), after adjusting for education, race, and sex.\nIn R, for interpreting categorical variables, reference level is selected by alphabetic order, therefore, the HS/GED is selected as reference level (H), next one is Less than HS (L) and then More than HS (M).\n\n7.7.5 Changing the Reference Level in Table 3\nOften, we need to change the reference level as per our analysis need or aim of the study. We can select the specific reference level and run the table 3. First step is to check if the variable is in factor format. If it is not in factor format, we need to convert it into factor. Next, we can use the following codes to refer and use in table 3.\n\n7.7.5.1 New Model with New Reference Level\nHere I am creating model 2 (m2) wit the new reference as Less than HS for the education variable.\n\n# Check factor format\nstr(table_1df$education) # It shows that it is in factor format.\n\n Factor w/ 3 levels \"HS/GED\",\"Less than HS\",..: 3 3 3 3 NA 1 3 NA 1 3 ...\n\n# Building the glm model with specific reference level for education  = \"Less than HS\".\nm2 &lt;- glm(is_smoker ~  relevel(factor(education), ref = \"Less than HS\")  + age + race + is_male, \n          table_1df, \n          family = binomial)\n\n# View raw model results\nsummary(m2)$coefficients\n\n                                                                Estimate\n(Intercept)                                                   4.51577733\nrelevel(factor(education), ref = \"Less than HS\")HS/GED       -1.01143171\nrelevel(factor(education), ref = \"Less than HS\")More than HS -1.62029322\nage                                                          -0.04564764\nraceOther                                                    -0.24842217\nraceRefused/missing                                           0.39602359\nraceWhite                                                    -0.01922531\nis_maleYes                                                   -0.39712021\n                                                              Std. Error\n(Intercept)                                                  0.436459823\nrelevel(factor(education), ref = \"Less than HS\")HS/GED       0.252724965\nrelevel(factor(education), ref = \"Less than HS\")More than HS 0.244106320\nage                                                          0.006417912\nraceOther                                                    0.315210858\nraceRefused/missing                                          1.124629178\nraceWhite                                                    0.251971208\nis_maleYes                                                   0.147363550\n                                                                 z value\n(Intercept)                                                  10.34637575\nrelevel(factor(education), ref = \"Less than HS\")HS/GED       -4.00210447\nrelevel(factor(education), ref = \"Less than HS\")More than HS -6.63765370\nage                                                          -7.11253757\nraceOther                                                    -0.78811425\nraceRefused/missing                                           0.35213704\nraceWhite                                                    -0.07629961\nis_maleYes                                                   -2.69483338\n                                                                 Pr(&gt;|z|)\n(Intercept)                                                  4.346284e-25\nrelevel(factor(education), ref = \"Less than HS\")HS/GED       6.278157e-05\nrelevel(factor(education), ref = \"Less than HS\")More than HS 3.187156e-11\nage                                                          1.139285e-12\nraceOther                                                    4.306299e-01\nraceRefused/missing                                          7.247355e-01\nraceWhite                                                    9.391807e-01\nis_maleYes                                                   7.042384e-03\n\n\n\n7.7.5.2 Creating and Customizing New Table 3 with New Reference Level\nHere, I have created the new table 3 for m2 model and customized it accordingly.\n\n# Customizing Regression Table \ntable_3n &lt;- tbl_regression(m2, exponentiate = TRUE,  # Creating the table\n                           label = list(\n                             age = \"Age\",\n                             race = \"Race\",\n                             education = \"Education level\",\n                             is_male = \"Male\"\n                             ),\n                          missing = \"no\"\n                          ) %&gt;% \n  bold_labels() %&gt;%\n  bold_p(t = 0.10) %&gt;%  \n  italicize_levels() %&gt;%\n  modify_caption(\"Table 3. Logistic Regression for smoking status as response varialbe (n=3014)\")\n\ntable_3n\n\n\n\n\n\nTable 3. Logistic Regression for smoking status as response varialbe (n=3014)\n\n\n\n\n\n\n\nCharacteristic\n\nOR1\n\n\n95% CI1\n\np-value\n\n\n\nrelevel(factor(education), ref = \"Less than HS\")\n\n\n\n\n\n    Less than HS\n—\n—\n\n\n\n    HS/GED\n0.36\n0.22, 0.59\n&lt;0.001\n\n\n    More than HS\n0.20\n0.12, 0.31\n&lt;0.001\n\n\nAge\n0.96\n0.94, 0.97\n&lt;0.001\n\n\nRace\n\n\n\n\n\n    Black\n—\n—\n\n\n\n    Other\n0.78\n0.42, 1.44\n0.4\n\n\n    Refused/missing\n1.49\n0.23, 29.4\n0.7\n\n\n    White\n0.98\n0.59, 1.59\n&gt;0.9\n\n\nMale\n\n\n\n\n\n    No\n—\n—\n\n\n\n    Yes\n0.67\n0.50, 0.89\n0.007\n\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Table by gtsummary</span>"
    ]
  },
  {
    "objectID": "lessons/01_gtsummary.html#conclusion-take-home-message",
    "href": "lessons/01_gtsummary.html#conclusion-take-home-message",
    "title": "\n7  Table by gtsummary\n",
    "section": "\n7.8 Conclusion (Take Home Message)",
    "text": "7.8 Conclusion (Take Home Message)\n\nWe can use gtsummary package for creating publication-ready tables.\nThe tbl_summary() and the tbl_regression() are the frequently used functions in this package.\nMultiple other functions can be used to customize the table and can address the journal requirements.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Table by gtsummary</span>"
    ]
  },
  {
    "objectID": "02_header_one-sample.html",
    "href": "02_header_one-sample.html",
    "title": "One-Sample Tests",
    "section": "",
    "text": "Text Outline\nThis text covers the basics of:",
    "crumbs": [
      "One-Sample Tests"
    ]
  },
  {
    "objectID": "02_header_one-sample.html#text-outline",
    "href": "02_header_one-sample.html#text-outline",
    "title": "One-Sample Tests",
    "section": "",
    "text": "Exploring data\nOne-sample tests\nTwo-sample tests\nLinear regression and ANOVA\nGeneralized linear models\nSome special topics\nMaybe a bit about power calculations…",
    "crumbs": [
      "One-Sample Tests"
    ]
  },
  {
    "objectID": "02_header_one-sample.html#part-outline",
    "href": "02_header_one-sample.html#part-outline",
    "title": "One-Sample Tests",
    "section": "Part Outline",
    "text": "Part Outline\nThis part of the text contains the chapters on various one-sample statistical tests:\n\n\\(Z\\)-test\nPaired \\(t\\)-test\nPaired Wilcoxon test\nTransformations to Normality\nMcNemar’s Test\nFisher’s Exact Test\nChi-Square Goodness of Fit\nBootstrapped Confidence Intervals",
    "crumbs": [
      "One-Sample Tests"
    ]
  },
  {
    "objectID": "lessons/02_z-test_one_prop.html",
    "href": "lessons/02_z-test_one_prop.html",
    "title": "\n8  Z-Test for One Proportion\n",
    "section": "",
    "text": "8.1 Introduction to One-Sample \\(Z\\)-Tests\nThe one-sample \\(Z\\)-test is used to compare a sample proportion to a population proportion.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Z-Test for One Proportion</span>"
    ]
  },
  {
    "objectID": "lessons/02_z-test_one_prop.html#mathematical-definition-of-the-one-sample-z-test",
    "href": "lessons/02_z-test_one_prop.html#mathematical-definition-of-the-one-sample-z-test",
    "title": "\n8  Z-Test for One Proportion\n",
    "section": "\n8.2 Mathematical definition of the One-Sample \\(Z\\)-Test",
    "text": "8.2 Mathematical definition of the One-Sample \\(Z\\)-Test\nConsider a sample of size \\(n\\) with binary values (such as “true” or “false”). Let \\(p_{s}\\) and \\(p_{E}\\) be the observed sample and expected (population) proportions, respectively. The formula to calculate the \\(z\\) statistic is\n\\[\nz \\equiv \\frac{\n  p_s - p_E\n}{\n  \\sqrt{\n    \\frac{1}{n}p_s(1 - p_s)\n  }\n}.\n\\]",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Z-Test for One Proportion</span>"
    ]
  },
  {
    "objectID": "lessons/02_z-test_one_prop.html#data-source-and-description",
    "href": "lessons/02_z-test_one_prop.html#data-source-and-description",
    "title": "\n8  Z-Test for One Proportion\n",
    "section": "\n8.3 Data source and description",
    "text": "8.3 Data source and description\nWe will use the CTN-0094 data set, a data set of harmonized clinical trials for opioid use disorder. The full database is in public.ctn0094data::, engineered features are in public.ctn0094extra::, and clinical trial outcomes (wrangled dependent variables) are in CTNote::. We will install all three packages, but only use CTNote:: for now.\n\n# install.packages(\"public.ctn0094data\")\n# install.packages(\"public.ctn0094extra\")\n# install.packages(\"CTNote\")\n\nlibrary(CTNote)\nlibrary(tidyverse)",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Z-Test for One Proportion</span>"
    ]
  },
  {
    "objectID": "lessons/02_z-test_one_prop.html#cleaning-the-data-to-create-a-model-data-frame",
    "href": "lessons/02_z-test_one_prop.html#cleaning-the-data-to-create-a-model-data-frame",
    "title": "\n8  Z-Test for One Proportion\n",
    "section": "\n8.4 Cleaning the data to create a model data frame",
    "text": "8.4 Cleaning the data to create a model data frame\nBecause our method requires only one sample, we have very little work to do. We will use the Kosten et al. (1993) definition of opioid abstinence, provided in the data set outcomesCTN0094 as the column kosten1993_isAbs.\n\n# What do the values look like?\nsummary(outcomesCTN0094$kosten1993_isAbs)\n\n   Mode   FALSE    TRUE \nlogical    2158    1402 \n\n# How many samples are there?\nnrow(outcomesCTN0094)\n\n[1] 3560\n\n\nThere are 3560 logical values, and TRUE indicates that the trial participant achieved abstinence according to the definition used in Kosten et al. (1993).",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Z-Test for One Proportion</span>"
    ]
  },
  {
    "objectID": "lessons/02_z-test_one_prop.html#assumptions-of-the-one-sample-z-test",
    "href": "lessons/02_z-test_one_prop.html#assumptions-of-the-one-sample-z-test",
    "title": "\n8  Z-Test for One Proportion\n",
    "section": "\n8.5 Assumptions of the One-Sample \\(Z\\)-Test",
    "text": "8.5 Assumptions of the One-Sample \\(Z\\)-Test\nTo use a one-sample \\(Z\\)-test, we make the following assumptions:\n\nThe data are from a random sample\nEach observation in the data are independent\nNeither the sample proportion nor population proportions are “extreme”; usually we apply this method if these proportions are between 5% and 95%.\nThe data can be described as “successes” and “failures”, and there are at least 10 samples in each category.\n\nIf these assumptions hold, then \\[\nz \\sim N(0, 1).\n\\]",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Z-Test for One Proportion</span>"
    ]
  },
  {
    "objectID": "lessons/02_z-test_one_prop.html#checking-the-assumptions-with-plots",
    "href": "lessons/02_z-test_one_prop.html#checking-the-assumptions-with-plots",
    "title": "\n8  Z-Test for One Proportion\n",
    "section": "\n8.6 Checking the assumptions with plots",
    "text": "8.6 Checking the assumptions with plots\n\n8.6.1 Independence and Randomness\nBecause the samples were collected at random via an FDA approved clinical trial protocol, we assume that all the participants were randomly selected and are independent of each other.\n\n8.6.2 “Extreme” Proportions\nAccording to Ling et al. (2020), the 12-month abstinence proportion of all 533 participants in their study was 40.5 percent. As we can see here, our abstinence rates are 39.4. Neither these proportions are smaller than 5% or greater than 95%.\n\n(pExpected &lt;- 0.508 * (425/533))\n\n[1] 0.4050657\n\n# Count the number of TRUE values\n(nAbstinent &lt;- sum(outcomesCTN0094$kosten1993_isAbs))\n\n[1] 1402\n\n\n\n8.6.3 Type and Counts of Data\nWe observe binary data, and we see at least 10 successes and at least 10 failures.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Z-Test for One Proportion</span>"
    ]
  },
  {
    "objectID": "lessons/02_z-test_one_prop.html#code-to-run-a-one-sample-z-test",
    "href": "lessons/02_z-test_one_prop.html#code-to-run-a-one-sample-z-test",
    "title": "\n8  Z-Test for One Proportion\n",
    "section": "\n8.7 Code to run a One-Sample \\(Z\\)-Test",
    "text": "8.7 Code to run a One-Sample \\(Z\\)-Test\nNow that we have checked our assumptions, we can perform the one-sample \\(Z\\)-test for proportions.\n\nprop.test(\n  x = nAbstinent,\n  n = nrow(outcomesCTN0094),\n  p = pExpected\n)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  nAbstinent out of nrow(outcomesCTN0094), null probability pExpected\nX-squared = 1.8218, df = 1, p-value = 0.1771\nalternative hypothesis: true p is not equal to 0.4050657\n95 percent confidence interval:\n 0.3777537 0.4101176\nsample estimates:\n        p \n0.3938202",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Z-Test for One Proportion</span>"
    ]
  },
  {
    "objectID": "lessons/02_z-test_one_prop.html#brief-interpretation-of-the-output",
    "href": "lessons/02_z-test_one_prop.html#brief-interpretation-of-the-output",
    "title": "\n8  Z-Test for One Proportion\n",
    "section": "\n8.8 Brief interpretation of the output",
    "text": "8.8 Brief interpretation of the output\nThe 95% confidence interval contains the population proportion, so we fail to reject the hypothesis that the patients from these clinical trials achieve different abstinence rates than the general population.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Z-Test for One Proportion</span>"
    ]
  },
  {
    "objectID": "lessons/02_ttest_one_samp.html",
    "href": "lessons/02_ttest_one_samp.html",
    "title": "\n9  One-sample/paired t-test\n",
    "section": "",
    "text": "10 Introduction to t-test\nA t-test is a statistical tool used to evaluate the means of one or two populations through hypothesis testing.It assesses whether:\nIt is important to make a note of that if we have two independent groups that differ from each other, we will use an Independent two-sample t-test explained at xxxxxx",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>One-sample/paired t-test</span>"
    ]
  },
  {
    "objectID": "lessons/02_ttest_one_samp.html#assumptions-for-a-one-sample-t-test",
    "href": "lessons/02_ttest_one_samp.html#assumptions-for-a-one-sample-t-test",
    "title": "\n9  One-sample/paired t-test\n",
    "section": "\n11.1 Assumptions for a One-sample T-test",
    "text": "11.1 Assumptions for a One-sample T-test\n\n\nRandom Sampling: The data must come from a random sample.\n\nMetric Scale of Measurement: The variable should be measured on an interval or ratio scale.\n\nNormal Distribution: The data should be approximately normally distributed.\n\nTo check these assumptions, we can use various visualization techniques:\n\n\nQ-Q Plot (Quantile-Quantile Plot): To check the normality of the data.\n\nHistogram: To observe the shape of the data distribution.\n\nBoxplot: To check the distribution characteristics and for outliers.\n\nResidual Plot: To check independence and homogeneity of variances.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>One-sample/paired t-test</span>"
    ]
  },
  {
    "objectID": "lessons/02_ttest_one_samp.html#one-tailed-and-two-tailed-t-tests",
    "href": "lessons/02_ttest_one_samp.html#one-tailed-and-two-tailed-t-tests",
    "title": "\n9  One-sample/paired t-test\n",
    "section": "\n11.2 One-tailed and Two-tailed T-tests",
    "text": "11.2 One-tailed and Two-tailed T-tests\nOne-tailed T-test: Used when the hypothesis predicts a specific direction of the difference (e.g., the sample mean is greater than the population mean).\nTwo-tailed T-test: Used when the hypothesis does not predict a specific direction, only that there is a difference (e.g., the sample mean is different from the population mean).",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>One-sample/paired t-test</span>"
    ]
  },
  {
    "objectID": "lessons/02_ttest_one_samp.html#defining-the-question-and-hypotheses",
    "href": "lessons/02_ttest_one_samp.html#defining-the-question-and-hypotheses",
    "title": "\n9  One-sample/paired t-test\n",
    "section": "\n11.3 Defining the Question and Hypotheses",
    "text": "11.3 Defining the Question and Hypotheses\nBefore conducting a one-sample t-test, you need to:\nDefine the Research Question: Clarify what you are investigating.\nFormulate Hypotheses:\nNull Hypothesis (H0): There is no difference between the sample mean and the population mean. Alternative Hypothesis (H1): There is a difference (or a specific directional difference).\nExample Question for a One-sample T-test: To illustrate, in the case of evaluating managers’ health perceptions:\nQuestion: Does the health perception of managers in Canada significantly differ from the general population’s health perception?\nThis question sets the stage for determining the appropriate hypotheses and whether to use a one-tailed or two-tailed t-test.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>One-sample/paired t-test</span>"
    ]
  },
  {
    "objectID": "lessons/02_ttest_one_samp.html#statistical-background",
    "href": "lessons/02_ttest_one_samp.html#statistical-background",
    "title": "\n9  One-sample/paired t-test\n",
    "section": "\n11.4 Statistical background",
    "text": "11.4 Statistical background\nTo determine if the sample mean significantly differs from the population mean, follow these steps: 1. Calculate the test statistic 𝑡using the formula: \n2. Calculate Degrees of Freedom (df): Degrees of freedom are determined by subtracting one from the number of samples (df = n - 1).\n3. Estimate Standard Deviation: Use the sample to estimate the population standard deviation.\n4. Determine Critical T-value: With the degrees of freedom known, refer to a t-distribution table to find the critical t-value. For example, with 12 samples (df = 11) and a 5% significance level:\nOne-tailed test: Read the t-value at 0.95. Two-tailed test: Read the t-value at 0.975.\n\nFor a two-tailed test at a 5% significance level, the critical t-value is 2.201.\n5. Compare Calculated T-value to Critical T-value: If the calculated t-value is less than the critical t-value, the difference is not significant. If the calculated t-value exceeds the critical t-value, the difference is significant.\nThis process allows you to decide if there is a statistically significant difference between the sample mean and the population mean.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>One-sample/paired t-test</span>"
    ]
  },
  {
    "objectID": "lessons/02_ttest_one_samp.html#data-source-and-description",
    "href": "lessons/02_ttest_one_samp.html#data-source-and-description",
    "title": "\n9  One-sample/paired t-test\n",
    "section": "\n11.5 Data source and description",
    "text": "11.5 Data source and description\nWe’ll use the sample dataset to demonstrate one-sample t-test.\n\n# Load the dataset\nsample &lt;- read_excel(\"../data/02_Sample_Dataset_2014.xlsx\")",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>One-sample/paired t-test</span>"
    ]
  },
  {
    "objectID": "lessons/02_ttest_one_samp.html#code-to-run-one-sample",
    "href": "lessons/02_ttest_one_samp.html#code-to-run-one-sample",
    "title": "\n9  One-sample/paired t-test\n",
    "section": "\n11.6 Code to run One-sample",
    "text": "11.6 Code to run One-sample\nWe want to hypothesize that the mean English scores in the sample dataset is significantly different from 80. To determine whether we can make this claim or not, we will use a one-sample t-test. We will use a significance level of 0.05, which represents the percentage of error that we can tolerate.\n\n11.6.1 First, we need to check Normality Assumption\n\n# Check normality for English in sample\n\n# Histogram\nggplot(\n  sample, \n  aes(x = English)\n) + \n  geom_histogram(\n    bins = 30, \n    fill = \"blue\", \n    color = \"black\"\n  ) +\n  ggtitle(\"Histogram of English\") +\n  theme_minimal()\n\nWarning: Removed 26 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\nHistogram of English Scores:  The histogram shows the distribution of ‘English’ scores. The x-axis represents the ‘English’ scores, while the y-axis represents the frequency of each score range.  From the histogram, the distribution appears to be roughly symmetrical, resembling a bell-shaped curve. This indicates that the scores are approximately normally distributed. \n\n# Q-Q plot\nqqPlot(sample$English, main = \"Q-Q Plot of English\")\n\n\n\n\n\n\n\n[1] 105  68\n\n\nQ-Q Plot of English Scores:  The x-axis represents the theoretical quantiles, while the y-axis represents the sample quantiles.  In the provided Q-Q plot, most of the points lie close to the reference line, indicating that the English scores are approximately normally distributed.\n\n11.6.2 Perform one-sample t-test\n\n# Perform one-sample t-test\none_sample_result &lt;- t.test(\n  sample$English, \n  mu = 80\n)\n\n# Print the result\nprint(one_sample_result)\n\n\n    One Sample t-test\n\ndata:  sample$English\nt = 8.2422, df = 408, p-value = 2.334e-15\nalternative hypothesis: true mean is not equal to 80\n95 percent confidence interval:\n 82.12271 83.45240\nsample estimates:\nmean of x \n 82.78756 \n\n\nAbove code is the example of One Sample t-test where mu = 80 is our hypothesized mean. Looking over different parameters:\nHypothesis Null Hypothesis (\\(H_{0}\\)): The true mean of the English scores is equal to 80. Alternative Hypothesis (\\(H_{1}\\)): The true mean of the English scores is not equal to 80\n\nt-value (8.2422): The t-value indicates how many standard deviations the sample mean (82.78756) is away from the hypothesized population mean (80). A t-value of 8.2422 is quite large, suggesting that the sample mean is significantly different from 80.\nDegrees of Freedom (df = 408): The degrees of freedom for this test is 408, which corresponds to the sample size minus one (n - 1).\np-value (2.334e-15): The p-value represents the probability of observing a test statistic as extreme as, or more extreme than, the one observed under the null hypothesis. A p-value of 2.334e-15 is extremely small, far below common significance levels such as 0.05 or 0.01. This indicates strong evidence against the null hypothesis.\n95% Confidence Interval [82.12271, 83.45240]: This interval provides a range within which we are 95% confident that the true population mean lies. Since 80 is not within this interval, it further supports the conclusion that the true mean is different from 80.\nSample Mean (82.78756): The mean of the sample data is 82.78756, which is higher than the hypothesized mean of 80.\n\n\nIn summary, based on this analysis, we conclude that the true mean ‘English’ score in the sample is significantly different from, and in this case higher than, 80.\n\n\nNote that:\nif you want to test whether the mean English score is less than 80 (one-tailed test), type this:\n\n\nt.test(\n  sample$English, \n  mu = 80,\n  alternative = \"less\"\n)\n\n\n    One Sample t-test\n\ndata:  sample$English\nt = 8.2422, df = 408, p-value = 1\nalternative hypothesis: true mean is less than 80\n95 percent confidence interval:\n     -Inf 83.34512\nsample estimates:\nmean of x \n 82.78756 \n\n\n\nOr, if you want to test whether the mean English score is greater than 80 (one-tailed test), type this:\n\n\nt.test(\n sample$English, \n  mu = 80,\n  alternative = \"greater\"\n)\n\n\n    One Sample t-test\n\ndata:  sample$English\nt = 8.2422, df = 408, p-value = 1.167e-15\nalternative hypothesis: true mean is greater than 80\n95 percent confidence interval:\n 82.22999      Inf\nsample estimates:\nmean of x \n 82.78756",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>One-sample/paired t-test</span>"
    ]
  },
  {
    "objectID": "lessons/02_ttest_one_samp.html#assumptions-for-paired-t-test",
    "href": "lessons/02_ttest_one_samp.html#assumptions-for-paired-t-test",
    "title": "\n9  One-sample/paired t-test\n",
    "section": "\n12.1 Assumptions for Paired T-Test",
    "text": "12.1 Assumptions for Paired T-Test\nBefore performing the paired t-test, check these assumptions.\n\n\nNormally Distributed Differences: The differences between paired values must be normally distributed. Examples:\n\nThe difference in weight of a person at two different times.\nThe difference in the number of points after two dice rolls.\n\n\n\nInterval-scaled Variables: The variables should be measured on an interval scale, allowing for meaningful calculation of differences. Examples:\n\nSalary of a person (in Euros)\nEducational level of a person\n\n\n\n\nIf assumptions 1 and 2 are not met, use the Wilcoxon test, the non-parametric alternative.\n\n\n\nDependent Groups or Samples: The groups must be dependent, meaning each value in one group is related to a value in the other group. Example: Measuring the weight of the same person before and after a diet.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>One-sample/paired t-test</span>"
    ]
  },
  {
    "objectID": "lessons/02_ttest_one_samp.html#defining-the-question-and-hypotheses-1",
    "href": "lessons/02_ttest_one_samp.html#defining-the-question-and-hypotheses-1",
    "title": "\n9  One-sample/paired t-test\n",
    "section": "\n12.2 Defining the Question and Hypotheses",
    "text": "12.2 Defining the Question and Hypotheses\nBefore conducting a paired sample t-test, you need to:\n\nDefine the Research Question:\n\nClarify what you are investigating. The general question is: Is there a statistically significant difference between the mean values of two dependent groups?\nExample questions include: a) Does the new drug improve memory performance? b) Does the newly introduced lubricant affect downtime?\n\nHypotheses\n\nFormulate Hypotheses,\nNull Hypothesis (\\(H_{0}\\)): The mean values of the two dependent groups are equal.\nAlternative Hypothesis (\\(H_{1}\\)): The mean values of the two dependent groups are different.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>One-sample/paired t-test</span>"
    ]
  },
  {
    "objectID": "lessons/02_ttest_one_samp.html#statistical-background-1",
    "href": "lessons/02_ttest_one_samp.html#statistical-background-1",
    "title": "\n9  One-sample/paired t-test\n",
    "section": "\n12.3 Statistical background",
    "text": "12.3 Statistical background\nTo perform a paired t-test, we follow these steps:\n\nCalculate Differences: Compute the difference for each pair of observations from the two groups. Difference(i) = Value(i-after) - Value(i-before)\nCalculate the Mean of Differences (\\(d\\)): Find the average of these differences\nCalculate the Test Statistic (\\(t\\)): The test statistic𝑡is calculated similar to a one-sample t-test, where the mean difference (d) is compared to zero. The formula for (\\(t\\)) is:\n\n\\[\nt := \\frac{\\bar{d} - 0}{SE_{\\bar{d}}},\n\\]\nwhere (\\(SE_{\\bar{d}}\\)) is the standard error of the mean difference; that is\n\\[\nSE_{\\bar{d}} := \\frac{s_{\\bar{d}}}{\\sqrt n},\n\\] where \\(s_{\\bar{d}}\\) is the standard deviation of the differences, and n is the sample size.\n\n12.3.1 Effect Size for Paired T-test\nEffect size is crucial for interpreting the practical significance of the results. For a paired t-test, effect size \\(r\\) can be calculated using:\n\\[\nr := \\frac{t}{\\sqrt{t^2 + df}}.\n\\]\nEffect Size Interpretation:\nr=0.2: Small effect r=0.5: Medium effect r=0.8: Large effect",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>One-sample/paired t-test</span>"
    ]
  },
  {
    "objectID": "lessons/02_ttest_one_samp.html#we-will-now-demonstrate-a-paired-sample-t-test",
    "href": "lessons/02_ttest_one_samp.html#we-will-now-demonstrate-a-paired-sample-t-test",
    "title": "\n9  One-sample/paired t-test\n",
    "section": "\n12.4 We will now demonstrate a Paired sample T-Test",
    "text": "12.4 We will now demonstrate a Paired sample T-Test\nWe still use the sample dataset. want to check whether the socres for English and Writing is significant different.\nHypothese: Null hypothesis(\\(H_{0}\\)):The mean difference between English and Writing scores is zero. \\(H_{0}:μ_{D}=0\\) Alternative hypothesis(\\(H_{1}\\)): The mean difference between English and Writing scores is not zero. \\(H_{0}:μ_{D}≠0\\)\n\n12.4.1 Check the Normality Assumption\n\n# Calculate the differences\ndifferences &lt;- sample$English - sample$Writing\n\n\n# Visualize the differences using a Q-Q plot and histogram\n# Histogram\nhist(differences, main=\"Histogram of Differences\", xlab=\"Differences\", col=\"blue\", breaks=20)\n\n\n\n\n\n\n\nHistogram of Differences: The x-axis represents the differences, and the y-axis represents the frequency of each difference range.  The histogram appears to be roughly bell-shaped and symmetrical, centered around zero. This suggests that the differences are approximately normally distributed.\n\n# Q-Q plot\nqqnorm(differences, main=\"Q-Q Plot of Differences\")\nqqline(differences, col=\"blue\")\n\n\n\n\n\n\n\nQ-Q Plot of Differences: The x-axis represents the theoretical quantiles of a normal distribution, while the y-axis represents the sample quantiles.  In the Q-Q plot, the points mostly lie along the reference line (45-degree line), indicating that the sample differences follow a normal distribution. \nNow we Perform paired sample t-test\n\n# Perform paired sample t-test\npaired_sample_result &lt;- t.test(\n  sample$English, \n  sample$Writing, \n  paired = TRUE\n)\nprint(paired_sample_result)\n\n\n    Paired t-test\n\ndata:  sample$English and sample$Writing\nt = 9.4512, df = 378, p-value &lt; 2.2e-16\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 2.704479 4.125389\nsample estimates:\nmean difference \n       3.414934 \n\n\nAbove code is the example of paired t-test. Looking over different parameters:\n\nt-statistic (9.4512): The t-statistic indicates how many standard deviations the sample mean difference is away from the hypothesized mean difference (which is 0 under the null hypothesis). A t-statistic of 9.4512 is very large, suggesting a significant difference between the ‘English’ and ‘Writing’ scores.\ndf = 378: The degrees of freedom for this test is 378, which corresponds to the number of paired observations minus one (n - 1). p-value (&lt; 2.2e-16):\np-value less than 2.2e-16: A p-value less than 2.2e-16 is extremely small, far below common significance levels such as 0.05 or 0.01. This indicates strong evidence against the null hypothesis.\n95% Confidence Interval [2.704479, 4.125389]: This interval provides a range within which we are 95% confident that the true mean difference between the English and Writing scores lies. Since the interval does not include 0, it further supports the conclusion that there is a significant difference between the scores.\nSample Estimate (Mean Difference = 3.414934): The mean difference between the English and Writing scores is 3.414934, meaning that, on average, English scores are higher than Writing scores by approximately 3.41 points.\n\n\nWe conclude that there is a statistically significant difference between English and Writing scores, with English scores being higher on average.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>One-sample/paired t-test</span>"
    ]
  },
  {
    "objectID": "lessons/02_wilcoxon_one_samp.html",
    "href": "lessons/02_wilcoxon_one_samp.html",
    "title": "\n10  Wilcoxon Signed Rank Test for One Sample\n",
    "section": "",
    "text": "10.1 Introduction to Wilcoxson Signed Rank Test\nThe one-sample Wilcoxson Signed Rank Test is used to compare a sample proportion to a population proportion.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Wilcoxon Signed Rank Test for One Sample</span>"
    ]
  },
  {
    "objectID": "lessons/02_wilcoxon_one_samp.html#mathematical-definition-of-the-wilcoxson-signed-rank-test",
    "href": "lessons/02_wilcoxon_one_samp.html#mathematical-definition-of-the-wilcoxson-signed-rank-test",
    "title": "\n10  Wilcoxon Signed Rank Test for One Sample\n",
    "section": "\n10.2 Mathematical definition of the Wilcoxson Signed Rank Test",
    "text": "10.2 Mathematical definition of the Wilcoxson Signed Rank Test\nLet’s assume that we have one sample of size \\(n\\), \\(x_1, x_2, \\ldots, x_n\\), which cannot be approximated by a normal distribution. Because of this, we are no longer comparing \\(\\bar{x}\\) to \\(\\mu\\), but we are instead asking if the sample median is equal to a population median, \\(M\\). For more detail, see the maths here: https://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test.\nHere are the steps to calculate this test statistic manually:\n\nSubtract the population median from each sample: \\(x^*_i := x_1 - M\\)\n\nTake the absolute value of the shifted samples, \\(|x^*_i|\\).\nRank these absolute values.\nMultiply the signs of the shifted samples by the ranks of the absolute values.\nSum these products and compare them to a normal distribution with mean 0 and \\(\\sigma^2 = \\frac{1}{6}(2n+1)(n+1)n\\). (We will not explain the maths here to show why this can be approximately normal, or why this is the estimated variance.)\n\nConsider a simple example: we want to ask if the number of people visiting a local clinic per hour is different from the county median of 2.9 visits per hour. Here is a small sample of simulated (non-normal) data:\n\nset.seed(123)\n\nN &lt;- 15\nnClinicVisits &lt;- rpois(n = N, lambda = 4)\n\n# Plot the data and visually compare to the county median.\nhist(nClinicVisits)\nabline(v = 2.9, lwd = 2, col = \"red\")\n\n\n\n\n\n\n\nNow let’s go through our steps:\n\nsteps_df &lt;- tibble::tibble(x = nClinicVisits)\n\n# 1. shift the sample by the population median\nsteps_df$xStar &lt;- steps_df$x - 2.9\n\n# 2. absolute value\nsteps_df$absXStar &lt;- abs(steps_df$xStar)\n\n# 3. ranks\nsteps_df$xRank &lt;- rank(steps_df$absXStar)\n\n# 4. signs x ranks\nsteps_df$signRank &lt;- sign(steps_df$xStar) * steps_df$xRank\n\n# Inspect our steps\nsteps_df\n\n# A tibble: 15 × 5\n       x  xStar absXStar xRank signRank\n   &lt;int&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1     3  0.100    0.100   1.5      1.5\n 2     6  3.1      3.1    11.5     11.5\n 3     3  0.100    0.100   1.5      1.5\n 4     6  3.1      3.1    11.5     11.5\n 5     7  4.1      4.1    13.5     13.5\n 6     1 -1.9      1.9     9       -9  \n 7     4  1.1      1.1     6        6  \n 8     7  4.1      4.1    13.5     13.5\n 9     4  1.1      1.1     6        6  \n10     4  1.1      1.1     6        6  \n11     8  5.1      5.1    15       15  \n12     4  1.1      1.1     6        6  \n13     5  2.1      2.1    10       10  \n14     4  1.1      1.1     6        6  \n15     2 -0.9      0.9     3       -3  \n\n\nNow we can calculate the Wilcoxon Signed Rank test statisic and compare it to its asymptotic \\(p\\)-value.\n\n# 5. Compare sum to normal distribution and calculate the p-value\noneTailP &lt;- pnorm(\n  q = sum(steps_df$signRank),\n  mean = 0,\n  sd = sqrt((2 * N + 1) * (N + 1) * N / 6)\n)\n(1 - oneTailP) / 2\n\n[1] 0.001601623\n\n\nHow does this compare to the exact distribution \\(p\\)-value?\n\nwilcox.test(x = nClinicVisits, mu = 2.9)\n\nWarning in wilcox.test.default(x = nClinicVisits, mu = 2.9): cannot compute\nexact p-value with ties\n\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  nClinicVisits\nV = 108, p-value = 0.00672\nalternative hypothesis: true location is not equal to 2.9",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Wilcoxon Signed Rank Test for One Sample</span>"
    ]
  },
  {
    "objectID": "lessons/02_wilcoxon_one_samp.html#data-source-and-description",
    "href": "lessons/02_wilcoxon_one_samp.html#data-source-and-description",
    "title": "\n10  Wilcoxon Signed Rank Test for One Sample\n",
    "section": "\n10.3 Data source and description",
    "text": "10.3 Data source and description\nNow that we have seen how the test works, we will apply it to a real data scenario. We will use gene-level \\(p\\)-values from the Golub and Van Loan (1999) data set from the R package multtest:: (https://rdrr.io/bioc/multtest/man/golub.html); the original is a data set of data set of gene expression values for leukemia, but we have gene-specific \\(p\\)-values from a gene-level hypothesis test. We created these \\(p\\)-values in the script R/create_golub_data_20240523.R, but they do not represent any real analysis results.\n\nlibrary(tidyverse)",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Wilcoxon Signed Rank Test for One Sample</span>"
    ]
  },
  {
    "objectID": "lessons/02_wilcoxon_one_samp.html#cleaning-the-data-to-create-a-model-data-frame",
    "href": "lessons/02_wilcoxon_one_samp.html#cleaning-the-data-to-create-a-model-data-frame",
    "title": "\n10  Wilcoxon Signed Rank Test for One Sample\n",
    "section": "\n10.4 Cleaning the data to create a model data frame",
    "text": "10.4 Cleaning the data to create a model data frame\nBecause our method requires only one sample, we have very little work to do. We import the data set of \\(p\\)-values.\n\ngolub_pVals_num &lt;- readRDS(file = \"../data/02_golub_pVals_20240523.rds\")\n\nThere are 3051 \\(p\\)-values. The null hypothesis would be that there is no statistically significant effects in the data, so the distribution of these \\(p\\)-values should be a Uniform distribution. Our hypothesis is that the population mean is then 0.5 (the average value of a Uniform distribution).",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Wilcoxon Signed Rank Test for One Sample</span>"
    ]
  },
  {
    "objectID": "lessons/02_wilcoxon_one_samp.html#assumptions-of-the-wilcoxson-signed-rank-test",
    "href": "lessons/02_wilcoxon_one_samp.html#assumptions-of-the-wilcoxson-signed-rank-test",
    "title": "\n10  Wilcoxon Signed Rank Test for One Sample\n",
    "section": "\n10.5 Assumptions of the Wilcoxson Signed Rank Test",
    "text": "10.5 Assumptions of the Wilcoxson Signed Rank Test\nTo use a one-sample Wilcoxson Signed Rank Test, we make the following assumptions:\n\nThe data are from a random sample\nEach observation in the data are independent\nThe values can be “ranked” (this assumption gets fuzzy when you have discrete data, because it’s possible to get ties or values that are exactly 0 in those cases)\n\nIf these assumptions hold, then the test statistic is asymptotically normal. If your data has lots of zeros or equal values (which would result in tied ranks), then use this method with caution.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Wilcoxon Signed Rank Test for One Sample</span>"
    ]
  },
  {
    "objectID": "lessons/02_wilcoxon_one_samp.html#checking-the-assumptions",
    "href": "lessons/02_wilcoxon_one_samp.html#checking-the-assumptions",
    "title": "\n10  Wilcoxon Signed Rank Test for One Sample\n",
    "section": "\n10.6 Checking the assumptions",
    "text": "10.6 Checking the assumptions\n\n10.6.1 Independence and Randomness\nThese are gene-level \\(p\\)-values, so we do not have “independence”. However, because this is a pedagogical example, we will take a random sample of these genes to test (and this random sample should be independent enough, but we have no guarantee of this).\n\n# Create random sample of genes to test\nset.seed(20150516)\ngene_sample &lt;- sample(\n  x = golub_pVals_num,\n  size = 200,\n  replace = FALSE\n)\n\nWhat does the data distribution look like?\n\nhist(gene_sample)\n\n\n\n\n\n\n\nRemember, this is a “fake” analysis (all 38 samples in this data are leukemia cases, and I tested one half against the other—there should absolutely NOT be any real biological signal in this data).\n\n10.6.2 Type of Data\nThese values are \\(p\\)-values, so they can be ranked.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Wilcoxon Signed Rank Test for One Sample</span>"
    ]
  },
  {
    "objectID": "lessons/02_wilcoxon_one_samp.html#code-to-run-a-wilcoxson-signed-rank-test",
    "href": "lessons/02_wilcoxon_one_samp.html#code-to-run-a-wilcoxson-signed-rank-test",
    "title": "\n10  Wilcoxon Signed Rank Test for One Sample\n",
    "section": "\n10.7 Code to run a Wilcoxson Signed Rank Test",
    "text": "10.7 Code to run a Wilcoxson Signed Rank Test\nNow that we have checked our assumptions, we can perform the Wilcoxson Signed Rank Test on random samples of the genes to test if they have an average value of 0.5.\n\nwilcox.test(\n  x = gene_sample,\n  mu = 0.5, # average from all theoretical p-values under H0\n  alternative = \"less\" # H1: random p-values &lt; 0.5\n)\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  gene_sample\nV = 5859, p-value = 1.584e-07\nalternative hypothesis: true location is less than 0.5",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Wilcoxon Signed Rank Test for One Sample</span>"
    ]
  },
  {
    "objectID": "lessons/02_wilcoxon_one_samp.html#brief-interpretation-of-the-output",
    "href": "lessons/02_wilcoxon_one_samp.html#brief-interpretation-of-the-output",
    "title": "\n10  Wilcoxon Signed Rank Test for One Sample\n",
    "section": "\n10.8 Brief interpretation of the output",
    "text": "10.8 Brief interpretation of the output\nThe \\(p\\)-value for this test is less than 0.05, so we reject the hypothesis that the average gene-specific \\(p\\)-value for this set of results is greater than or equal to 0.5 (the theoretical average of \\(p\\)-values under the null hypothesis).",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Wilcoxon Signed Rank Test for One Sample</span>"
    ]
  },
  {
    "objectID": "lessons/02_mcnemar_paired_samp.html",
    "href": "lessons/02_mcnemar_paired_samp.html",
    "title": "\n11  McNemar’s Test\n",
    "section": "",
    "text": "11.1 Introduction to McNemar’s Test\nMcNemar’s Test is a non-parametric test used to analyze dichotomous data (in a 2 x 2) for paired samples. It is similar to a paired \\(t\\)-test, but for dichotomous rather than continuous variables. It is also akin to a Fisher’s exact test, but for paired data rather than un-paired data. The test requires one nominal dependent variable with 2 categories, and one independent variable with 2 dependent, mutually exclusive, groups. It is important to note that a “pair” can also represent a single individual’s pre- and post-test/intervention results.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>McNemar's Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_mcnemar_paired_samp.html#mathematical-definition-of-mcnemars-test",
    "href": "lessons/02_mcnemar_paired_samp.html#mathematical-definition-of-mcnemars-test",
    "title": "\n11  McNemar’s Test\n",
    "section": "\n11.2 Mathematical definition of McNemar’s Test",
    "text": "11.2 Mathematical definition of McNemar’s Test\nConsider binary repeated measured data for \\(n\\) observations; the observations come from one group under two conditions (e.g. “time period 1” vs “time period 2”, 0 vs. 1), the samples are treated with some treatment within equally across each condition, and the outcome of interest is dichotomous (such as “success” or “failure”, 1 vs. 0). If these restrictive assumptions are met, then the data can be compactly represented as a contingency table that looks like this:\n\n\n\nCondition 2 + (1)\nCondition 2 - (0)\n\n\n\nCondition 1 + (1)\n(a)\n(b)\n\n\nCondition 1 - (0)\n(c)\n(d)\n\n\n\nWhere:\n\n\na: is the number of pairs where both conditions are positive. E.g., the count of participants for whom the treatment was effective at time points 1 and 2.\n\nb: is the number of pairs where the first condition is positive and the second condition is negative. E.g., the count of participants for whom the treatment was effective at time 1 but not effective at time 2.\n\nc: is the number of pairs where the first condition is negative and the second condition is positive. E.g., the count of participants for whom the treatment was not effective at time 1 but was effective at time 2.\n\nd: is the number of pairs where both conditions are negative. E.g., the count of participants for whom the treatment was neither effective at time 1 nor at time 2.\n\nThe test focuses on the discordant pairs, b and c, which are pairs that change from one condition to the other. With a sufficiently large number of discordant pairs, McNemar’s Test follows a Chi-squared distribution with 1 degree of freedom. The formula for the test statistic, \\(\\chi^{2}_{\\text{Obs}}\\), is as follows:\n\\(\\chi^{2}_{\\text{Obs}} := \\frac{(b - c)^2}{b + c};\\ \\chi^{2}_{\\text{Obs}} \\sim \\chi^{2}_{\\nu = 1}.\\)",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>McNemar's Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_mcnemar_paired_samp.html#data-description",
    "href": "lessons/02_mcnemar_paired_samp.html#data-description",
    "title": "\n11  McNemar’s Test\n",
    "section": "\n11.3 Data Description",
    "text": "11.3 Data Description\nFor this lesson, we will look at patients in treatment for opioid use disorder, We will measure abstinence at the start and end of treatment via weekly Urine Drug Screen (UDS). The abstinence measurements are in the public.ctn0094extra::derived_weeklyOpioidPattern data set. Our definition of abstinence will be “3 consecutive negative urine screens” from weeks 2-4 (start of treatment) and weeks 10-12 (end of treatment).\n\n11.3.1 Cleaning UDS Data\n\ntxSuccess_df &lt;- \n  public.ctn0094extra::derived_weeklyOpioidPattern %&gt;% \n  # Combine UDS across treatment phases\n  mutate(udsPattern = paste0(Phase_1, Phase_2)) %&gt;% \n  # Extract the UDS patterns for the start and end of treatment\n  mutate(\n    startTxPattern = str_sub(udsPattern, start = 2, end = 4),\n    endTxPattern = str_sub(udsPattern, start = 10, end = 12)\n  ) %&gt;% \n  # Check for abstinence during the start and end of treatment\n  mutate(\n    startAbs = startTxPattern == \"---\",\n    endAbs = endTxPattern == \"---\"\n  ) %&gt;% \n  select(who, udsPattern, startAbs, endAbs) \n\ntxSuccess_df\n\n# A tibble: 3,560 × 4\n     who udsPattern                startAbs endAbs\n   &lt;int&gt; &lt;chr&gt;                     &lt;lgl&gt;    &lt;lgl&gt; \n 1     1 ooooooooooooooo           FALSE    FALSE \n 2     2 ----oo-o-o-o+o            TRUE     FALSE \n 3     3 o-ooo-ooooooooooooooooo   FALSE    FALSE \n 4     4 --------------------o-oo  TRUE     TRUE  \n 5     5 ooooooooooooooo           FALSE    FALSE \n 6     6 -ooooooooooooo            FALSE    FALSE \n 7     7 ----oooooooooooooooooooo  TRUE     FALSE \n 8     8 ooooooooooooooooooooooooo FALSE    FALSE \n 9     9 oooooooooooooooooooooo    FALSE    FALSE \n10    10 --o--*++o-++++++++o+-o    FALSE    FALSE \n# ℹ 3,550 more rows\n\n\n\n11.3.2 Creating Comparison Table from this Dataset\nNow that we have a binary measure of success at two time points (start and end of treatment), we can create a 2 x 2 contingency table:\n\ntxAbs_tbl &lt;- table(\n  # Rows of the table\n  txSuccess_df$startAbs,\n  # Columns of the table\n  txSuccess_df$endAbs\n)\n\ntxAbs_tbl\n\n       \n        FALSE TRUE\n  FALSE  2676  268\n  TRUE    395  221\n\n\nHere are the categories for each of the states of the patients:\n\n\na (FALSE & FALSE) means that the subject was abstinent from opioids neither at the start nor end of the trial,\n\nb (FALSE & TRUE) means that the subject was not abstinent from opioids at the start of the trial but abstinent at the end,\n\nc (TRUE & FALSE) means that the subject was abstinent from opioids at the start of the trial but not abstinent at the end, and\n\nd (TRUE & TRUE) means that the subject was abstinent from opioids both at the start and end of the trial.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>McNemar's Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_mcnemar_paired_samp.html#assumptions-of-mcnemars-test",
    "href": "lessons/02_mcnemar_paired_samp.html#assumptions-of-mcnemars-test",
    "title": "\n11  McNemar’s Test\n",
    "section": "\n11.4 Assumptions of McNemar’s Test",
    "text": "11.4 Assumptions of McNemar’s Test\nThe assumptions of McNemar’s Test are as follows:\n\n\nAssumption 1: You have one categorical dependent variable with two categories (i.e., a dichotomous variable) and one categorical independent variable with two related groups.\n\nAssumption 2: The two groups of the dependent variable are mutually exclusive, which means that the groups do not overlap—a participant can only be in one of the two groups.\n\nAssumption 3: The cases are a random sample from the population of interest.\n\nAssumption 4: At least 25 discordant pairs (\\(c + b \\geq 25\\))",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>McNemar's Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_mcnemar_paired_samp.html#checking-the-assumptions-of-mcnemars-test",
    "href": "lessons/02_mcnemar_paired_samp.html#checking-the-assumptions-of-mcnemars-test",
    "title": "\n11  McNemar’s Test\n",
    "section": "\n11.5 Checking the Assumptions of McNemar’s Test",
    "text": "11.5 Checking the Assumptions of McNemar’s Test\nTo check our assumptions, we will review and inspect the contingency table with the two variables of interest.\n\n# Printing the contigency table with margin totals and overal totals\naddmargins(txAbs_tbl)\n\n       \n        FALSE TRUE  Sum\n  FALSE  2676  268 2944\n  TRUE    395  221  616\n  Sum    3071  489 3560\n\n\nThe categorical dependent variable is abstinence of opioids in urine samples (e.g positive or negative for the substance); we see that the dependent variable at the start of treatment is related to the dependent variable at the end of treatment because we are detecting opioids within the same person. The categorical independent variable is the two time periods (start and end of treatment). The two groups are mutually exclusive, as urine cannot be simultaneously positive and negative and the participant cannot simultaneously be at the start and end of treatment. Finally, we note that the sum of the off-diagonal cells is at least 25.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>McNemar's Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_mcnemar_paired_samp.html#code-to-run-mcnemars-test",
    "href": "lessons/02_mcnemar_paired_samp.html#code-to-run-mcnemars-test",
    "title": "\n11  McNemar’s Test\n",
    "section": "\n11.6 Code to run McNemar’s Test",
    "text": "11.6 Code to run McNemar’s Test\nRecall that we created a 2x2 contingency table with the table() function above. This table object is one of the the data structures which can be supplied to the function mcnemar.test(). The other is the two columns of binary values (which can be coercible to binary factors).\n\n# Table Input Syntax\nmcnemar.test(txAbs_tbl)\n\n\n    McNemar's Chi-squared test with continuity correction\n\ndata:  txAbs_tbl\nMcNemar's chi-squared = 23.946, df = 1, p-value = 9.909e-07\n\n# Factor Vector Input Syntax\nmcnemar.test(\n  x = txSuccess_df$startAbs,\n  y = txSuccess_df$endAbs\n)\n\n\n    McNemar's Chi-squared test with continuity correction\n\ndata:  txSuccess_df$startAbs and txSuccess_df$endAbs\nMcNemar's chi-squared = 23.946, df = 1, p-value = 9.909e-07\n\n\nThe output from the McNemar’s test will show the Chi-Squared value, degrees of freedom (expected to be 1 as both categories only have 2 possible values), and the \\(p\\)-value.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>McNemar's Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_mcnemar_paired_samp.html#brief-interpretation-of-the-output",
    "href": "lessons/02_mcnemar_paired_samp.html#brief-interpretation-of-the-output",
    "title": "\n11  McNemar’s Test\n",
    "section": "\n11.7 Brief Interpretation of the Output",
    "text": "11.7 Brief Interpretation of the Output\nThe resulting \\(p\\)-value in this case is below 0.05, which indicates that the marginal probabilities between startAbs and endAbs are different. What is curious is that the count for start of treatment abstinence is higher than the count for end of treatment abstinence.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>McNemar's Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_mcnemar_paired_samp.html#conclusion",
    "href": "lessons/02_mcnemar_paired_samp.html#conclusion",
    "title": "\n11  McNemar’s Test\n",
    "section": "\n11.8 Conclusion",
    "text": "11.8 Conclusion\nIf you have paired samples, and the variable of interest is binary, then use McNemar’s test. The pairing could be within subject but across time or space, or it could represent different measures of success or failure on the same subject (of use in psychometrics). Regardless, it’s often that you’d like to include some covariate or additional factor, which this technique does not allow. In those cases, you may want Survival analysis (an event may occur or not within a time interval) or some other technique.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>McNemar's Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_cochrans_q_paired_samp.html",
    "href": "lessons/02_cochrans_q_paired_samp.html",
    "title": "\n12  Cochran’s Q Test\n",
    "section": "",
    "text": "12.1 Introduction to the Cochran’s \\(Q\\) Test\nAn extension of McNemar’s Test (a non-parametric test used to analyze dichotomous data (in a 2 x 2) for paired samples) is Cochran’s \\(Q\\) Test, which is a non-parametric test to analyze dichotomous data in a repeated measures design. Whereas McNemar’s Test would look at a binary outcome for the same individuals measured at only two time points, Cochran’s \\(Q\\) Test deals with binary outcomes for the same individuals at three or more time points. The test requires one nominal dependent variable with 2 categories, and one independent variable with 3 or more dependent, mutually exclusive, groups. Common data examples include presence/absence of a condition for a single individual at various time points in treatment, or positive/negative results for a single individual from various potentially-competing assessment instruments. It can also be used to evaluate “inter-observer variability” (where different judges of the same event/phenomenon measure success or failure) or “inter-instrument/concurrent validity” (where multiple instruments applied to the same subject measure success or failure).",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Cochran's Q Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_cochrans_q_paired_samp.html#mathematical-definition-of-cochrans-q-test",
    "href": "lessons/02_cochrans_q_paired_samp.html#mathematical-definition-of-cochrans-q-test",
    "title": "\n12  Cochran’s Q Test\n",
    "section": "\n12.2 Mathematical definition of Cochran’s \\(Q\\) Test",
    "text": "12.2 Mathematical definition of Cochran’s \\(Q\\) Test\nConsider binary repeated measured data for \\(n\\) observations; the observations come from one group under three or more conditions (e.g. “time period 1”, “time period 2”, “time period 3”, or “instrument 1” vs. “instrument 2” vs. “instrument 3”), the samples are treated with some treatment within equally across each condition, and the outcome of interest is dichotomous (such as “success” or “failure”, 1 vs. 0). The data then can be “wrangled” into a form that looks like this:\n\neg_df &lt;- tibble(\n  SubjectID = c(\"Bob\", \"Larry\", \"Junior\", \"Archibald\", \"Lunt\"),\n  test1 = c(0L, 1L, 1L, 0L, 0L),\n  test2 = c(0L, 1L, 1L, 0L, 1L),\n  test3 = c(1L, 1L, 1L, 0L, 1L)\n)\n\nknitr::kable(eg_df)\n\n\n\nSubjectID\ntest1\ntest2\ntest3\n\n\n\nBob\n0\n0\n1\n\n\nLarry\n1\n1\n1\n\n\nJunior\n1\n1\n1\n\n\nArchibald\n0\n0\n0\n\n\nLunt\n0\n1\n1\n\n\n\n\n\nThis is a simple example, but notice the structure of the data:\n\nThere are \\(B = 5\\) “blocks” (subjects, organisms, experimental units, etc.)\nThere are \\(K = 3\\) “experiments” (treatments, assessments, time points, exams, interviews, instruments, etc.)\nThe outcome of each “experiment” for each “block” is only ever binary (True/False, success/failure, presence/absence, etc.)\nThis data is in a \\(B \\times K\\) matrix.\n\nBefore we define the test statistic, we need some mathematical notation. Let \\(S(X_b)\\) be the row sum for row \\(b \\in 1, \\ldots, B\\) of the binary data matrix \\(X\\). Similarly let \\(S(X_k)\\) be the column sum for column \\(k \\in 1, \\ldots, K\\) of the binary data matrix \\(X\\). Finally, we define \\(N := S(X)\\) be the sum of all rows and columns. Using this notation, the formula for the test statistic is as follows:\n\\[\n\\chi^{2}_{\\text{Obs}} := k(k - 1) \\times\n\\frac{\n  \\sum\\limits_{k = 1}^K \\left[ S(X_k) -N/k \\right]^2\n}{\n  \\sum\\limits_{b = 1}^B S(X_b) \\left( k - S(X_b) \\right)\n},\n\\]\nwhere If the assumptions are met, then Cochran’s \\(Q\\) Test follows a Chi-squared distribution with \\(k - 1\\) degrees of freedom; that is, \\(\\chi^{2}_{\\text{Obs}} \\sim \\chi^{2}_{\\nu = k - 1}\\).",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Cochran's Q Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_cochrans_q_paired_samp.html#data-description",
    "href": "lessons/02_cochrans_q_paired_samp.html#data-description",
    "title": "\n12  Cochran’s Q Test\n",
    "section": "\n12.3 Data Description",
    "text": "12.3 Data Description\nFor this lesson, we will look at patients in treatment for opioid use disorder, We will measure abstinence within each month (4-week period) via weekly Urine Drug Screen (UDS). The abstinence measurements are in the public.ctn0094extra::derived_weeklyOpioidPattern data set. Our definition of abstinence will be “75% negative urine screens” from weeks 1-4 (start of treatment), weeks 5-8 (mid-treatment), and weeks 9-12 (end of treatment).\n\n12.3.1 Cleaning UDS Data\n\ntxSuccess_df &lt;- \n  public.ctn0094extra::derived_weeklyOpioidPattern %&gt;% \n  # Combine UDS across treatment phases\n  mutate(udsPattern = paste0(Phase_1, Phase_2)) %&gt;% \n  # The CTNote outcome builder functions are not vectorized, so we need to\n  #   use rowwise() to call them\n  rowwise() %&gt;% \n  # Count negative UDS in each phase of treatment\n  mutate(\n    nStartTxNeg = CTNote::count_matches(\n      udsPattern, match_is = \"-\", start = 1, end = 4\n    ),\n    nMidTxNeg = CTNote::count_matches(\n      udsPattern, match_is = \"-\", start = 5, end = 8\n    ),\n    nEndTxNeg = CTNote::count_matches(\n      udsPattern, match_is = \"-\", start = 9, end = 12\n    )\n  ) %&gt;% \n  # Check for abstinence during the start and end of treatment\n  mutate(\n    startAbs = nStartTxNeg &gt;= 3,\n    midAbs = nMidTxNeg &gt;= 3,\n    endAbs = nEndTxNeg &gt;= 3\n  ) %&gt;% \n  select(who, udsPattern, startAbs, midAbs, endAbs) %&gt;% \n  ungroup()\n\ntxSuccess_df\n\n# A tibble: 3,560 × 5\n     who udsPattern                startAbs midAbs endAbs\n   &lt;int&gt; &lt;chr&gt;                     &lt;lgl&gt;    &lt;lgl&gt;  &lt;lgl&gt; \n 1     1 ooooooooooooooo           FALSE    FALSE  FALSE \n 2     2 ----oo-o-o-o+o            TRUE     FALSE  FALSE \n 3     3 o-ooo-ooooooooooooooooo   FALSE    FALSE  FALSE \n 4     4 --------------------o-oo  TRUE     TRUE   TRUE  \n 5     5 ooooooooooooooo           FALSE    FALSE  FALSE \n 6     6 -ooooooooooooo            FALSE    FALSE  FALSE \n 7     7 ----oooooooooooooooooooo  TRUE     FALSE  FALSE \n 8     8 ooooooooooooooooooooooooo FALSE    FALSE  FALSE \n 9     9 oooooooooooooooooooooo    FALSE    FALSE  FALSE \n10    10 --o--*++o-++++++++o+-o    TRUE     FALSE  FALSE \n# ℹ 3,550 more rows\n\n\n\n12.3.2 Creating Comparison Table from this Dataset\nWe now have a binary measure of success at three time points (start, middle, and end of treatment). Our first step is to pivot the data so that we have 3 columns: who, when (start, middle, end), and value (“treatment success” measured as abstinence or not). Note that we also need to save the when column as an ordered factor so that the levels aren’t sorted alphabetically.\n\ntxSuccessLong_df &lt;- \n  txSuccess_df %&gt;% \n  select(-udsPattern) %&gt;% \n  pivot_longer(\n    cols = startAbs:endAbs,\n    names_to = \"when\",\n    values_to = \"abstinent\"\n  ) %&gt;% \n  mutate(\n    when = str_remove(when, pattern = \"Abs\")\n  ) %&gt;% \n  mutate(\n    when = factor(when, levels = c(\"start\", \"mid\", \"end\"), ordered = TRUE)\n  )\n\ntxSuccessLong_df\n\n# A tibble: 10,680 × 3\n     who when  abstinent\n   &lt;int&gt; &lt;ord&gt; &lt;lgl&gt;    \n 1     1 start FALSE    \n 2     1 mid   FALSE    \n 3     1 end   FALSE    \n 4     2 start TRUE     \n 5     2 mid   FALSE    \n 6     2 end   FALSE    \n 7     3 start FALSE    \n 8     3 mid   FALSE    \n 9     3 end   FALSE    \n10     4 start TRUE     \n# ℹ 10,670 more rows\n\n\nNow, we can create a 2 x 3 contingency table:\n\ntxAbs_tbl &lt;- stats::xtabs(\n  ~abstinent + when, data = txSuccessLong_df\n)\n\ntxAbs_tbl\n\n         when\nabstinent start  mid  end\n    FALSE  2599 2825 2765\n    TRUE    961  735  795",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Cochran's Q Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_cochrans_q_paired_samp.html#assumptions-of-cochrans-q-test",
    "href": "lessons/02_cochrans_q_paired_samp.html#assumptions-of-cochrans-q-test",
    "title": "\n12  Cochran’s Q Test\n",
    "section": "\n12.4 Assumptions of Cochran’s \\(Q\\) Test",
    "text": "12.4 Assumptions of Cochran’s \\(Q\\) Test\nThe assumptions of Cochran’s \\(Q\\) Test are as follows:\n\n\nAssumption 1: You have one categorical dependent variable with two categories (i.e., a dichotomous variable) and one categorical independent variable with \\(K \\ge 3\\) related groups.\n\nAssumption 2: The two groups of the dependent variable are mutually exclusive, which means that the groups do not overlap—a participant can only be in one of the two groups.\n\nAssumption 3: The cases are a random sample from the population of interest.\n\nAssumption 4: There are a sufficiently large number of independent subjects, \\(B\\) (also called “blocks”); note that there is no definition of exactly how many samples are needed to be considered “large”.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Cochran's Q Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_cochrans_q_paired_samp.html#checking-the-assumptions-of-cochrans-q-test",
    "href": "lessons/02_cochrans_q_paired_samp.html#checking-the-assumptions-of-cochrans-q-test",
    "title": "\n12  Cochran’s Q Test\n",
    "section": "\n12.5 Checking the Assumptions of Cochran’s \\(Q\\) Test",
    "text": "12.5 Checking the Assumptions of Cochran’s \\(Q\\) Test\nTo check our assumptions, we will review and inspect the contingency table with the two variables of interest. The categorical dependent variable is abstinence of opioids in urine samples (e.g positive or negative for the substance); we see that the dependent variable at the start of treatment is related to the dependent variable in the middle and at the end of treatment because we are detecting opioids within the same person. The categorical independent variable is the three time periods (start, middle, and end of treatment). The two groups are mutually exclusive, as urine cannot be simultaneously positive and negative and the participant cannot simultaneously be at the start and end of treatment. Finally, we note that there are 3560 samples, which counts as \\(B\\) being “large”.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Cochran's Q Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_cochrans_q_paired_samp.html#code-to-run-cochrans-q-test",
    "href": "lessons/02_cochrans_q_paired_samp.html#code-to-run-cochrans-q-test",
    "title": "\n12  Cochran’s Q Test\n",
    "section": "\n12.6 Code to run Cochran’s \\(Q\\) Test",
    "text": "12.6 Code to run Cochran’s \\(Q\\) Test\nRecall that we created a 2x3 contingency table with the xtab() function (from the stats:: package) above. This table object is helpful for us to check assumptions, but it is not a required data structure for the cochran_qtest() function (from the rstatix:: package). We can supply our pivoted “long” data to this function, but we have to use a creative formula object to define the experimental design. From the help documentation, this formula looks like a ~ b | c, where a is the outcome variable name (abstinent for us); b is the within-subjects factor variables (when); and c is the column name containing individuals/subjects identifier (who).\n\nrstatix::cochran_qtest(\n  data = txSuccessLong_df,\n  formula = abstinent ~ when | who\n)\n\n# A tibble: 1 × 6\n  .y.           n statistic    df        p method          \n* &lt;chr&gt;     &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;           \n1 abstinent  3560      84.4     2 4.64e-19 Cochran's Q test\n\n\nThe output from the Cochran’s \\(Q\\) Test will show the Chi-Squared test statistic, degrees of freedom (expected to be \\(3 - 1 = 2\\) as there are only 3 time points), and the \\(p\\)-value.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Cochran's Q Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_cochrans_q_paired_samp.html#brief-interpretation-of-the-output",
    "href": "lessons/02_cochrans_q_paired_samp.html#brief-interpretation-of-the-output",
    "title": "\n12  Cochran’s Q Test\n",
    "section": "\n12.7 Brief Interpretation of the Output",
    "text": "12.7 Brief Interpretation of the Output\nThe resulting \\(p\\)-value in this case is below 0.05, which indicates that the marginal probabilities between start, mid, and end measures of abstinent are different. Going back to review txAbs_tbl, we see that the count for start of treatment abstinence is higher than the counts for middle and end of treatment abstinence. Note that there is no “post-hoc” test for Cochran’s \\(Q\\) test, so if you really need to know which groups are different, then you have to use \\(k(k - 1)\\) pairwise McNemar tests (which is not a great idea).",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Cochran's Q Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_cochrans_q_paired_samp.html#conclusion",
    "href": "lessons/02_cochrans_q_paired_samp.html#conclusion",
    "title": "\n12  Cochran’s Q Test\n",
    "section": "\n12.8 Conclusion",
    "text": "12.8 Conclusion\nIf you have repeated paired samples, and the variable of interest is binary, then use Cochran’s \\(Q\\) Test. The pairing could be within subject but across time or space, or it could represent different measures of success or failure on the same subject (of use in psychometrics). Regardless, it’s often that you’d like to include some covariate or additional factor, which this technique does not allow. Also, you probably want to know which time points or which interventions are different. In those cases, you may want Survival analysis (an event may occur or not within a time interval) or some other more sophisticated technique.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Cochran's Q Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_chiSq_GoF.html",
    "href": "lessons/02_chiSq_GoF.html",
    "title": "\n13  The Chi Squared Goodness-of-Fit Test\n",
    "section": "",
    "text": "13.1 Introduction to the \\(\\chi^2\\) Goodness of Fit (GoF) Test\nNon-parametric tests and robust tests often have lower statistical power than their traditional counterparts. These more traditional hypothesis tests make distributional assumptions. For instance, a test may assume that the original observations are approximately normal or that counts follow a Poisson distribution. While we most commonly assess distributional assumptions visually, for instance with a Q-Q plot or histogram, there are some instances where we need to test adherence of data to a specified distribution with a proper statistical hypothesis test. One such test is the \\(\\chi^2\\) Goodness of Fit (GoF) test, published in the textbook Statistical Methods (Snedecor and Cochran, 1989). We draw some of our formulation and examples from this US NIST handbook: https://www.itl.nist.gov/div898/handbook/eda/section3/eda35f.htm.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>The Chi Squared Goodness-of-Fit Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_chiSq_GoF.html#mathematical-definition-of-the-chi2-goodness-of-fit-gof-test",
    "href": "lessons/02_chiSq_GoF.html#mathematical-definition-of-the-chi2-goodness-of-fit-gof-test",
    "title": "\n13  The Chi Squared Goodness-of-Fit Test\n",
    "section": "\n13.2 Mathematical definition of the \\(\\chi^2\\) Goodness of Fit (GoF) Test",
    "text": "13.2 Mathematical definition of the \\(\\chi^2\\) Goodness of Fit (GoF) Test\nThe idea of this test is to “bin” all the observations (think histogram) and then test the counts of the observations in each bin (observed) against the number of counts in the same bins from the target distribution (expected).\nConsider a sample of \\(n\\) observations, \\(x_1, x_2, \\ldots, x_n\\), from an unknown distribution with cumulative distribution function (CDF) \\(\\mathcal{F}_0\\) with support \\((-\\infty, \\infty)\\). Let \\(\\mathcal{F}_A\\) denote the CDF distribution to compare against (such as a Normal distribution, Gamma distribution, Negative Binomial distribution, or whatever distribution you think best describes the population data). Partition the \\(n\\) observations into \\(k+1\\) bins using \\(k\\) cut points \\(T_1, T_2, \\ldots, T_k\\), creating the non-intersecting intervals \\(\\{ {(-\\infty, T_1]},\\ (T_1, T_2],\\ \\ldots,\\ (T_k, \\infty)\\}\\) which span the support of \\(\\mathcal{F}_0\\). For the sake of notation, let \\(T_0 = -\\infty\\) and \\(T_{k+1} = \\infty\\).\nOnce we have the data partitions, we then tally the observed count in each bin, \\(O_1, O_2, \\ldots, O_{k+1}\\). We also find the probabilities of being in each bin \\((T_{i-1}, T_i],\\ i \\in 1, 2, \\ldots, k+1\\) from the target distribution \\(\\mathcal{F}_A\\). That is, calculate \\(p_i \\equiv \\mathcal{F}_A(T_i) - \\mathcal{F}_A(T_{i-1})\\), noting that \\(\\sum_{i = 1}^{k+1} p_i = 1\\). Multiply the probabilities for each bin, \\(p_i\\), by the total sample size, \\(n\\), to generate the expected count for each bin, \\(E_1, E_2, \\ldots, E_{k+1}\\).\nNow that we have the observed counts, \\(O_i\\), and the expected counts, \\(E_i\\), we calculate the \\(\\chi^2\\) Goodness of Fit Test statistic as \\[\n\\chi^2_{\\text{Obs}} \\equiv \\sum\\limits_{i = 1}^{k+1} \\frac{(O_i - E_i)^2}{E_i}.\n\\]\nThe distribution of this test statistic is approximately \\(\\chi^2\\) with \\(k^* + (p + 1)\\) degrees of freedom, where \\(k^*\\) is the number of non-empty bins and \\(p\\) is the number of parameters of \\(\\mathcal{F}_A\\). For example, let’s assume that we are comparing the data against a Beta distribution with two parameters \\(\\{\\alpha,\\beta\\}\\). Further, assume we choose to bin the observed data into deciles so that we have at least one observation in each bin. Thus, we would have \\(k^* = 10\\) bins and \\(p = 2\\) parameters. So, we would compare the test statistic \\(\\chi^2_{\\text{Obs}}\\) against a \\(\\chi^2\\) distribution with \\(k^* + (p + 1) = (10) + (2 + 1) = 13\\) degrees of freedom.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>The Chi Squared Goodness-of-Fit Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_chiSq_GoF.html#data-source-and-description",
    "href": "lessons/02_chiSq_GoF.html#data-source-and-description",
    "title": "\n13  The Chi Squared Goodness-of-Fit Test\n",
    "section": "\n13.3 Data source and description",
    "text": "13.3 Data source and description\nWhen we treat people who are addicted to licit and/or illicit substances, one potential predictor of recovery is baseline “risky health behaviours”; the Addictions, Drug, and Alcohol Institute hosts a copy of the Risk Behavior Survey questionnaire: https://adai.uw.edu/instruments/pdf/Risk_Behavior_Survey_209.pdf. While exploring the results of this questionnaire among patients in treatment, we might want to know if the total number of sexual partners (column txx_frq in the data set sex) follow a Poisson distribution.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>The Chi Squared Goodness-of-Fit Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_chiSq_GoF.html#cleaning-the-data-to-create-a-model-data-frame",
    "href": "lessons/02_chiSq_GoF.html#cleaning-the-data-to-create-a-model-data-frame",
    "title": "\n13  The Chi Squared Goodness-of-Fit Test\n",
    "section": "\n13.4 Cleaning the data to create a model data frame",
    "text": "13.4 Cleaning the data to create a model data frame\nThis data is already relatively clean, other than the missing values. Let’s remove them.\n\nsummary(public.ctn0094data::sex$txx_frq)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n    1.0     3.0     6.0    10.4    16.0    44.0    1327 \n\nnPartners_int &lt;- \n  public.ctn0094data::sex %&gt;% \n  drop_na(txx_frq) %&gt;% \n  pull(txx_frq) %&gt;% \n  as.integer()\n\nHere is a histogram of the total number of sexual partners for participants in the CTN-0094 data warehouse:\n\nhist(\n  nPartners_int,\n  main = \"Number of Sexual Partners per Patient\",\n  xlab = \"Count, using 'Sturges' Bins\"\n)\n\n\n\n\n\n\nFigure 13.1\n\n\n\n\nIt does not appear that this data follows a Poisson distribution. That said, a journal reviewer may still ask for a \\(p\\)-value as evidence.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>The Chi Squared Goodness-of-Fit Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_chiSq_GoF.html#assumptions-of-the-chi2-goodness-of-fit-gof-test",
    "href": "lessons/02_chiSq_GoF.html#assumptions-of-the-chi2-goodness-of-fit-gof-test",
    "title": "\n13  The Chi Squared Goodness-of-Fit Test\n",
    "section": "\n13.5 Assumptions of the \\(\\chi^2\\) Goodness of Fit (GoF) Test",
    "text": "13.5 Assumptions of the \\(\\chi^2\\) Goodness of Fit (GoF) Test\nRecall that the \\(k^*\\) parameter counts only the number of non-empty bins, so we should choose a small enough number of bins to ensure that all bins have at least a couple data points. I don’t actually know for sure what the minimum should be, but I’d say at least 5 per bin, but the traditional \\(\\chi^2\\) Test of Independence assumes at least 10 values per bin. The more bins you use (up to a point), the better the empirical CDF can approximate the shape of the distributional CDF. However, two things work against you: 1) more bins require more samples, and 2) more bins mean that you’re more likely to reject the null hypothesis even when you shouldn’t.\nSummary of Assumptions:\n\nYour samples are independent\nYour sample was taken at random\nYour samples are (or can be represented as) count data\nYou have enough samples to have a few per bin (at least 5-10)",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>The Chi Squared Goodness-of-Fit Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_chiSq_GoF.html#choosing-the-correct-number-of-bins",
    "href": "lessons/02_chiSq_GoF.html#choosing-the-correct-number-of-bins",
    "title": "\n13  The Chi Squared Goodness-of-Fit Test\n",
    "section": "\n13.6 Choosing the Correct Number of Bins",
    "text": "13.6 Choosing the Correct Number of Bins\nFor a test that depends on the number of bins, we would expect there to be some “correct” number. This is a reasonably important question. Unfortunately, as best as these authors are aware, there isn’t one best rule for selecting the correct number of bins. Thankfully, there has been some research done on the optimal number of bins for histograms (which is very similar). We can apply these various “rules” to the \\(\\chi^2\\) GoF test as well.\nThe standard method to accomplish this is to set breaks = k+1 in the hist() call. However, this number of bins doesn’t always change when you want it to: the help file says that if you give a number of bins for the breaks argument, that “the number is a suggestion only”. In order to “force” the number of bins to be what you want, you have to create a sequence of cut points yourself, using the seq() function. But note that the cut points include the minimum and the maximum of the data (\\(T_0\\) and \\(T_{k+1}\\)); so if you want 10 bins, you’ll need a vector of “cuts” with length 11. An example of this code is:\n\nnBins &lt;- 10\nseq(\n  from = min(nPartners_int),\n  to = max(nPartners_int),\n  length.out = nBins + 1\n)\n\n [1]  1.0  5.3  9.6 13.9 18.2 22.5 26.8 31.1 35.4 39.7 44.0\n\n\n\n13.6.1 Bins by Decile\nWe could divide the observations by into bins at every 10th percentile of the data; this would yield 197.2 participants per bin, on average. It would work pretty well for this data set, but that’s purely a coincidence (but see the odd trivia below the Sturge’s Rule section). This would yield the same default histogram as above.\n\nhist(\n  nPartners_int,\n  breaks = seq(\n    from = min(nPartners_int),\n    to = max(nPartners_int),\n    length.out = 10 + 1\n  ),\n  main = \"Number of Sexual Partners per Patient\",\n  xlab = \"Count, using Decile Bins\"\n)\n\n\n\n\n\n\nFigure 13.2\n\n\n\n\n\n13.6.2 A Bin for Each Unique Count\nA simple rule for discrete data would be a bin for each unique value. In our example, the most number of bins which make sense are all the observed counts (in our case, 44 bins). Let’s rebuild the above histogram with a bin for each unique count value.\n\nhist(\n  nPartners_int,\n  breaks = seq(\n    from = min(nPartners_int),\n    to = max(nPartners_int),\n    length.out = length(unique(nPartners_int)) + 1\n  ),\n  main = \"Number of Sexual Partners per Patient\",\n  xlab = \"Count, using Unique Value Bins\"\n)\n\n\n\n\n\n\nFigure 13.3\n\n\n\n\nThis retains some of the shape of the data we saw with 10 bins, but it’s very “noisy”. This is probably not a good option to use.\n\n13.6.3 Sturges Breaks\nWe should recall that in the first histogram, we changed the number of breaks from the default. The default number of histogram breaks in R is an option called \"Sturges\". Sturge’s Rule states that an optimal number of bins for a histogram (and subsequently, a \\(\\chi^2\\) GoF test) is \\(\\log_2(n) + 1\\), rounded up. For our example, this would be 12 breaks:\n\n# Sample size\nn_int &lt;- length(nPartners_int)\n\n# Sturge's rule: number of breaks (round up)\nlog2(n_int) + 1\n\n[1] 11.94544\n\n# Histogram\nhist(\n  nPartners_int,\n  breaks = seq(\n    from = min(nPartners_int),\n    to = max(nPartners_int),\n    length.out = 12 + 1\n  ),\n  main = \"Number of Sexual Partners per Patient\",\n  xlab = \"Count, using Sturge's Bins\"\n)\n\n\n\n\n\n\nFigure 13.4\n\n\n\n\n\nAside: why does using decile bins work here? According to Sturge’s rule, for any data set with number of samples between 70 and 2000, 10 bins is reasonably close to the optimal number calculated by Sturge’s rule (for 70 samples, the optimal Sturge’s bins is 8; for 2000, it’s 12). So while it’s purely coincidence for our data set, using decile-based bins will work for many of the data sets that students will encounter in their statistics and methods classes.\n\n\n13.6.4 Other Rules to Calculate the Breaks\nThere are a few other rules to calculate the number of bins, all rounded up:\n\nThe Square Root rule: \\(\\sqrt{n}\\)\n\nThe Rice rule: \\(\\sqrt[3]{2n}\\)\n\nScott’s rule: \\(R / \\frac{3.49s}{\\sqrt[3]{n}}\\), where \\(R\\) is the range of the data and \\(s\\) is the sample standard deviation (this rule assumes the data can be approximated by a Normal distribution)\nFreedman-Diaconis’s rule: \\(R / \\frac{3.49I}{\\sqrt[3]{n}}\\), where \\(R\\) is the range of the data and \\(I\\) is the sample interquartile range (this rule is an extension of Scott’s rule, and therefore also assumes the data can be approximated by a Normal distribution)\n\n\nNote: from what I’ve found online, there are a few different formulations to these rules, but I’m using those in the link above, as corroborated by page 26 of these notes from Prof. Fawcett at the University of Newcastle: http://www.mas.ncl.ac.uk/~nlf8/teaching/mas1343/notes/chap4-5.pdf\n\n\n# Range (for Scott's and F-D rules)\nrange_int &lt;- max(nPartners_int) - min(nPartners_int)\n\n# Square Root rule\nsqrt(n_int)\n\n[1] 44.40721\n\n# The Rice rule\n(2 * n_int) ^ (1/3)\n\n[1] 15.79958\n\n# Scott's rule\nrange_int / ( 3.49 * sd(nPartners_int) * n_int ^ (-1/3) )\n\n[1] 18.55387\n\n# Freedman-Diaconis's rule (two versions)\nrange_int / ( 2 * IQR(nPartners_int) / (n_int ^ (1/3)) )\n\n[1] 20.73946\n\nrange_int / ( 3.49 * IQR(nPartners_int) / (n_int ^ (1/3)) )\n\n[1] 11.88508\n\n\nSo here are the histograms using these rules:\n\npar(mfrow = c(2, 2))\n\nhist(\n  nPartners_int,\n  breaks = seq(\n    from = min(nPartners_int),\n    to = max(nPartners_int),\n    length.out = 45 + 1\n  ),\n  main = \"Number of Sexual Partners per Patient\",\n  xlab = \"Count, using Square Root rule for Bins\"\n)\n\nhist(\n  nPartners_int,\n  breaks = seq(\n    from = min(nPartners_int),\n    to = max(nPartners_int),\n    length.out = 16 + 1\n  ),\n  main = \"Number of Sexual Partners per Patient\",\n  xlab = \"Count, using The Rice rule for Bins\"\n)\n\nhist(\n  nPartners_int,\n  breaks = seq(\n    from = min(nPartners_int),\n    to = max(nPartners_int),\n    length.out = 21 + 1\n  ),\n  main = \"Number of Sexual Partners per Patient\",\n  xlab = \"Count, using Scott's rule for Bins\"\n)\n\nhist(\n  nPartners_int,\n  breaks = seq(\n    from = min(nPartners_int),\n    to = max(nPartners_int),\n    length.out = 12 + 1\n  ),\n  main = \"Number of Sexual Partners per Patient\",\n  xlab = \"Count, using Freedman-Diaconis's rule for Bins\"\n)\n\n\npar(mfrow = c(1, 1))\n\n\n\n\n\n\nFigure 13.5",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>The Chi Squared Goodness-of-Fit Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_chiSq_GoF.html#code-to-run-the-chi2-goodness-of-fit-gof-test",
    "href": "lessons/02_chiSq_GoF.html#code-to-run-the-chi2-goodness-of-fit-gof-test",
    "title": "\n13  The Chi Squared Goodness-of-Fit Test\n",
    "section": "\n13.7 Code to run the \\(\\chi^2\\) Goodness of Fit (GoF) Test",
    "text": "13.7 Code to run the \\(\\chi^2\\) Goodness of Fit (GoF) Test\nThe Sturge and Freedman-Diaconis rules both yielded 12 bins, so we will use this number. Here is a list of the information we need:\n\nthe original observed tally of sexual partners per patient\na choice for the number of bins (12, in this case), and\na target distribution (Poisson).\n\nRecall that from our summary statement, the minimum and maximum number of sexual partners were 1 and 44, respectively. The cut() function will return the number of intervals requested which divide these observed values. These would correspond to \\(x\\)-axis values of the each left and right column of our histogram. Here are our steps to assign each observed tally to its appropriate bin:\n\nbins_df &lt;- \n  tibble(original = nPartners_int) %&gt;% \n  # Create 12 bins\n  mutate(partition = cut(original, breaks = 12)) %&gt;% \n  # Extract lower and upper limit of the bins; retain the original partition\n  #   column to compare our work\n  separate(partition, into = c(\"lower\", \"upper\"), sep = \",\", remove = FALSE) %&gt;%\n  # Remove the leading \"(\" and trailing \"]\", then transform to numeric\n  mutate(\n    lower = as.numeric(str_sub(lower, start = 2L)),\n    upper = as.numeric(str_sub(upper, end = -2L))\n  ) %&gt;% \n  # Replace the smallest and largest limits with the support of the Poisson\n  #   distribution. Note that this will depend on the target distribution you \n  #   choose.\n  mutate(\n    lower = case_when(\n      lower == min(lower) ~ 0,\n      lower &gt; min(lower) ~ lower\n    ),\n    upper = case_when(\n      upper == max(upper) ~ Inf,\n      upper &lt; max(upper) ~ upper\n    )\n  )\n\nbins_df\n\n# A tibble: 1,972 × 4\n   original partition    lower upper\n      &lt;int&gt; &lt;fct&gt;        &lt;dbl&gt; &lt;dbl&gt;\n 1       16 (15.3,18.9]  15.3  18.9 \n 2        6 (4.58,8.17]   4.58  8.17\n 3       16 (15.3,18.9]  15.3  18.9 \n 4        9 (8.17,11.8]   8.17 11.8 \n 5       16 (15.3,18.9]  15.3  18.9 \n 6       20 (18.9,22.5]  18.9  22.5 \n 7        5 (4.58,8.17]   4.58  8.17\n 8        3 (0.957,4.58]  0     4.58\n 9        2 (0.957,4.58]  0     4.58\n10        5 (4.58,8.17]   4.58  8.17\n# ℹ 1,962 more rows\n\n\nNow, we can calculate the observed counts, the bin expected probabilities according to the specified Poisson distribution, and the expected counts\n\n# Poisson parameter\nestLambda_num &lt;- mean(nPartners_int)\n\nsteps_df &lt;- \n  bins_df %&gt;% \n  group_by(partition) %&gt;% \n  summarise(\n    lower = unique(lower),\n    upper = unique(upper),\n    observed = n()\n  ) %&gt;% \n  # Now calculate the probabilities of a random value from the target Poisson\n  #   distribution falling into these bins\n  mutate(\n    p = ppois(q = upper, lambda = estLambda_num) - \n      ppois(q = lower, lambda = estLambda_num)\n  ) %&gt;% \n  # And now the expected counts (I'm rounding here just for readability, so I \n  #   also include the non-rounded version for computation)\n  mutate(expected = p * n_int) %&gt;% \n  mutate(expected_rd = round(p * n_int, 1)) %&gt;% \n  # (O - E)^2 / E\n  mutate(summand = (observed - expected)^2 / expected)\n\n# Do our probabilities sum to 1?\nsum(steps_df$p)\n\n[1] 0.9999697\n\nsteps_df\n\n# A tibble: 12 × 8\n   partition    lower  upper observed        p expected expected_rd      summand\n   &lt;fct&gt;        &lt;dbl&gt;  &lt;dbl&gt;    &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;\n 1 (0.957,4.58]  0      4.58      610 2.24e- 2  4.42e+1        44.2      7.24e 3\n 2 (4.58,8.17]   4.58   8.17      440 2.67e- 1  5.26e+2       526.       1.40e 1\n 3 (8.17,11.8]   8.17  11.8        84 3.61e- 1  7.12e+2       712.       5.53e 2\n 4 (11.8,15.3]  11.8   15.3        31 2.86e- 1  5.64e+2       564.       5.03e 2\n 5 (15.3,18.9]  15.3   18.9       556 5.37e- 2  1.06e+2       106.       1.91e 3\n 6 (18.9,22.5]  18.9   22.5        47 1.01e- 2  1.98e+1        19.8      3.73e 1\n 7 (22.5,26.1]  22.5   26.1       130 4.92e- 4  9.70e-1         1        1.72e 4\n 8 (26.1,29.7]  26.1   29.7        32 1.22e- 5  2.41e-2         0        4.24e 4\n 9 (29.7,33.2]  29.7   33.2         5 5.55e- 7  1.10e-3         0        2.28e 4\n10 (33.2,36.8]  33.2   36.8        27 5.47e- 9  1.08e-5         0        6.76e 7\n11 (36.8,40.4]  36.8   40.4         4 1.30e-10  2.57e-7         0        6.22e 7\n12 (40.4,44]    40.4  Inf           6 6.11e-13  1.20e-9         0        2.99e10\n\n\nWe can now calculate the test statistic and \\(p\\) value.\n\n# Test statistic\n(chiSq_ts &lt;- sum(steps_df$summand))\n\n[1] 30026566644\n\n# critical value (k* = 12, p = 1)\n(chiSq_cv &lt;- qchisq(p = 1 - 0.025, df = 12 + 1 + 1))\n\n[1] 26.11895\n\n# The test statistic &gt; the critical value, so reject F_A as the distribution\n\n# p-value\n1 - pchisq(q = chiSq_ts, df = 12 + 1 + 1)\n\n[1] 0",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>The Chi Squared Goodness-of-Fit Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_chiSq_GoF.html#brief-interpretation-of-the-output",
    "href": "lessons/02_chiSq_GoF.html#brief-interpretation-of-the-output",
    "title": "\n13  The Chi Squared Goodness-of-Fit Test\n",
    "section": "\n13.8 Brief interpretation of the output",
    "text": "13.8 Brief interpretation of the output\nShockingly (this is sarcasm), the data that had two distinct peaks and looked nothing like a Poisson distribution can not actually be approximated by a Poisson distribution. Go figure. Regardless, this process can be replicated for your own data as well. We are showing off the \\(\\chi^2\\) GoF version because it works for ANY distribution you can think of.\n\n13.8.1 Brief Aside: Simple Normality Tests\nHowever, this all gets much simpler if you only care about testing for normality (which is usually a dumb thing to do anyway, but I digress). If your data are continuous and “normal-ish”, just use the Kolmogorov-Smirnov test, Shapiro-Wilk test, or the Anderson-Darling test instead. Our data are NOT “normal-ish”, so these results are invalid (but I’m showing you the code anyway).\n\n# Kolmogorov-Smirnov test\nks.test(nPartners_int, \"pnorm\")\n\nWarning in ks.test.default(nPartners_int, \"pnorm\"): ties should not be present\nfor the one-sample Kolmogorov-Smirnov test\n\n\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  nPartners_int\nD = 0.9311, p-value &lt; 2.2e-16\nalternative hypothesis: two-sided\n\n# Shapiro-Wilk test\nshapiro.test(nPartners_int)\n\n\n    Shapiro-Wilk normality test\n\ndata:  nPartners_int\nW = 0.86427, p-value &lt; 2.2e-16\n\n# Anderson-Darling test (requires the nortest:: package)\nnortest::ad.test(nPartners_int)\n\n\n    Anderson-Darling normality test\n\ndata:  nPartners_int\nA = 102.73, p-value &lt; 2.2e-16\n\n\nLook at those magical tiny \\(p\\)-values! So we’ve learned that non-normal data isn’t normal. How surprising (again, sarcasm).",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>The Chi Squared Goodness-of-Fit Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_chiSq_GoF.html#wrapping-up",
    "href": "lessons/02_chiSq_GoF.html#wrapping-up",
    "title": "\n13  The Chi Squared Goodness-of-Fit Test\n",
    "section": "\n13.9 Wrapping Up",
    "text": "13.9 Wrapping Up\nIf you need to check if your data are approximately normal, or approximately any other distribution, just plot the data. Use a density, Q-Q plot, or even a histogram. The material in this lesson is to help you when you run into a pesky reviewer / boss / collaborator (who just happens to be addicted to \\(p\\)-values) and they want to perform a test for distributional assumptions.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>The Chi Squared Goodness-of-Fit Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_transformations.html",
    "href": "lessons/02_transformations.html",
    "title": "\n14  Transformations to Normality\n",
    "section": "",
    "text": "14.1 Introduction\nThe pattern of values obtained when a variable is measured in a large number of individuals is called a distribution. Distributions can be broadly classified as normal and non-normal. The normal distribution is also called ‘Gaussian distribution’ as it was first described by K.F. Gauss. This chapter outlines the process of transforming data to achieve a normal distribution in R. Parametric methods, such as t-tests and ANOVA, require that the dependent (outcome) variable is approximately normally distributed within each group being compared. When the normality assumption is not satisfied, transforming the data can correct the non-normal distributions. For t-tests and ANOVA, it is sufficient to transform the dependent variable. However, for linear regression, transformations may be applied to the independent variable, the dependent variable, or both to achieve a linear relationship between variables and ensure homoscedasticity.\nHere are the libraries we will use for this material:\nlibrary(ggplot2)\nlibrary(gridExtra)\nlibrary(GGally)\nlibrary(moments)\nlibrary(knitr)\nlibrary(MASS)",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Transformations to Normality</span>"
    ]
  },
  {
    "objectID": "lessons/02_transformations.html#when-to-apply-transformations-to-normality",
    "href": "lessons/02_transformations.html#when-to-apply-transformations-to-normality",
    "title": "\n14  Transformations to Normality\n",
    "section": "\n14.2 When to Apply Transformations to Normality",
    "text": "14.2 When to Apply Transformations to Normality\nOne of the critical assumptions of statistical hypothesis testing is that the data are samples from a normal distribution. Therefore, it is essential to identify whether distributions are skewed or normal. There are several straightforward methods to detect skewness. Firstly, if the mean is less than twice the standard deviation, the distribution is likely skewed. Additionally, in a normally distributed population, the mean and standard deviation of the samples are independent. This characteristic can be used to detect skewness; if the standard deviation increases as the mean increases across groups from a population, the distribution is skewed. Beyond these simple methods, normality can be verified using statistical tests such as the Shapiro-Wilk test, the Kolmogorov-Smirnov test, and the Anderson-Darling test. Additionally, the moments package in R can be used to calculate skewness quantitatively. The skewness is determined using the third standardized moment, providing a measure of the asymmetry of the data distribution. If skewness is identified, efforts should be made to transform the data to achieve a normal distribution. This transformation is crucial for applying robust parametric tests in the analysis.\nTransformations can also be employed to facilitate comparison and interpretation. A classical example of a variable commonly reported after logarithmic transformation is the hydrogen ion concentration (pH). Another instance where transformation aids in data comparison is the logarithmic transformation of a dose-response curve. When plotted, the dose-response relationship is curvilinear; however, plotting the response against the logarithm of the dose (log dose-response plot) results in an elongated S-shaped curve. The middle portion of this curve forms a straight line, making it easier to compare two straight lines by measuring their slopes than to compare two curves. Thus, transformation can significantly enhance data comparison.\nIn summary, transformations can be applied to normalize data distribution or to simplify interpretation and comparison.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Transformations to Normality</span>"
    ]
  },
  {
    "objectID": "lessons/02_transformations.html#types-of-transformations-to-normality",
    "href": "lessons/02_transformations.html#types-of-transformations-to-normality",
    "title": "\n14  Transformations to Normality\n",
    "section": "\n14.3 Types of Transformations to Normality",
    "text": "14.3 Types of Transformations to Normality\nOften, the transformation that normalizes the distribution also equalizes the variance. While there are several types of transformations available, such as logarithmic, square root, reciprocal, cube root, and Box-Cox, the first three are the most commonly used. Among the transformations discussed in this section, the logarithmic transformation is the most often used. The following guidelines can help in selecting the appropriate method of transformation:\n\n14.3.1 Logarithmic Transformation\nIf the standard deviation is proportional to the mean, the distribution is positively skewed, making logarithmic transformation ideal. Note that when using a log transformation, a constant should be added to all values to ensure they are positive before transformation. The log tranformation is \\[\ny' = \\log(y + c),\n\\] where \\(y\\) is the original value and \\(c\\) is a constant added to ensure all values are positive.\n\n\n\n\n\n\n\n\n\n\n\n14.3.2 Square Root Transformation\nWhen the variance is proportional to the mean, square root transformation is preferred. This is particularly applicable to variables measured as counts, such as the number of malignant cells in a microscopic field or the number of deaths from swine flu. The square root transformation is: \\[\ny' = \\sqrt{y}.\n\\]\n\n\n\n\n\n\n\n\n\n\n\n14.3.3 Arithmetic Reciprocal Transformation\nIf the observations are truncated on the right (such as often the case for academic grade distributions), then one preliminary transformation is to “reverse” the values by subtracting each value from the maximum of all observed values (or from the maximum possible value for observations on a defined scale). This operation “flips” the data distribution from having a heavy left tail to having a heavy right tail, which allows us to perform a secondary transformation (such as a log or square root). This transformation is: \\[\ny' = \\max(y) - y.\n\\]\n\n\n\n\n\n\n\n\nNOTE: now that the data are right-skewed, other transformations can be applied as usual.\n\n14.3.4 Geometric Reciprocal Transformation\nIf the standard deviation is proportional to the mean squared, a reciprocal transformation is appropriate. This is typically used for highly variable quantities, such as serum creatinine levels. Note that this transformation requires all values to be positive or all values to be negative before applying it. \\[\ny' = \\frac{1}{y \\pm c},\n\\] where \\(y\\) is the original value and \\(c\\) is a constant added (subtracted) to ensure all values are positive (negative).\n\n\n\n\n\n\n\n\n\n\n\n14.3.5 Box-Cox Transformation\nThe Box-Cox transformation is a family of power transformations that can be used to stabilize variance and make the data more closely conform to a normal distribution, especially when the best power transformation (e.g., square root, logarithmic) is uncertain. By estimating an optimal parameter \\(\\lambda\\) from the data, the Box-Cox transformation tailors the transformation to the specific dataset’s needs. The transformation is defined as:\n\\[\ny(\\lambda) = \\begin{cases}\n  \\frac{y^\\lambda - 1}{\\lambda} & \\text{if } \\lambda \\neq 0 \\\\\n  \\log(y) & \\text{if } \\lambda = 0\n\\end{cases}\n\\] Here, \\(\\lambda\\) is a parameter that is estimated from the data. The Box-Cox transformation is particularly useful because it includes many of the other transformations (such as the logarithmic and square root transformations) as special cases.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Transformations to Normality</span>"
    ]
  },
  {
    "objectID": "lessons/02_transformations.html#examples-of-transformations-to-normality",
    "href": "lessons/02_transformations.html#examples-of-transformations-to-normality",
    "title": "\n14  Transformations to Normality\n",
    "section": "\n14.4 Examples of Transformations to Normality",
    "text": "14.4 Examples of Transformations to Normality\n\n14.4.1 Data Source and Description\nThe USJudgeRatings dataset is a built-in dataset in R that contains ratings of 43 judges in the US Superior Court. The ratings are based on the evaluations from lawyers who have had cases before these judges. The dataset includes multiple variables that represent different aspects of the judges’ performance.\n\n14.4.1.1 Variables in the Dataset\n\n\nCONT: Judicial “controlling” or authoritative nature.\n\nINTG: Judicial integrity.\n\nDMNR: Judicial demeanor.\n\nDILG: Judicial diligence.\n\nCFMG: Case flow management.\n\nDECI: Judicial decision-making.\n\nPREP: Judicial preparation.\n\nFAMI: Familiarity with the law.\n\nORAL: Oral skills.\n\nWRIT: Written skills.\n\nPHYS: Physical ability.\n\nRTEN: Willingness to follow trends.\n\nThis dataset is useful for analyzing various performance metrics of judges and can be used to explore relationships between different aspects of judicial performance. In the following examples, we’ll consider two variables:\n\n\nCONT: Number of contacts of lawyer with judge. Positively skewed.\n\nPHYS: Physical ability. Negatively skewed\n\n14.4.2 Loading the Data\n\n# Load the USJudgeRatings dataset\ndata(\"USJudgeRatings\")\ndf &lt;- USJudgeRatings\n\n# Display the first few rows of the dataset\nhead(df)\n\n               CONT INTG DMNR DILG CFMG DECI PREP FAMI ORAL WRIT PHYS RTEN\nAARONSON,L.H.   5.7  7.9  7.7  7.3  7.1  7.4  7.1  7.1  7.1  7.0  8.3  7.8\nALEXANDER,J.M.  6.8  8.9  8.8  8.5  7.8  8.1  8.0  8.0  7.8  7.9  8.5  8.7\nARMENTANO,A.J.  7.2  8.1  7.8  7.8  7.5  7.6  7.5  7.5  7.3  7.4  7.9  7.8\nBERDON,R.I.     6.8  8.8  8.5  8.8  8.3  8.5  8.7  8.7  8.4  8.5  8.8  8.7\nBRACKEN,J.J.    7.3  6.4  4.3  6.5  6.0  6.2  5.7  5.7  5.1  5.3  5.5  4.8\nBURNS,E.B.      6.2  8.8  8.7  8.5  7.9  8.0  8.1  8.0  8.0  8.0  8.6  8.6\n\n\n\n14.4.3 Visualizations of CONT and PHYS Variables\n\nggplot(df) + \n  aes(x = CONT) + \n  scale_x_continuous(limits = c(3, 12))+\n  labs(title = \"Density Plot of CONT\", x = \"CONT\", y = \"Density\") +\n  geom_density(fill = \"lightgray\") +\n  stat_function(\n    fun = dnorm,\n    args = list(mean = mean(df$CONT), sd = sd(df$CONT)),\n    color = \"red\",\n    linetype = \"dashed\"\n  ) \n\n\n\n\n\n\nFigure 14.1: Distribution of CONT Variable\n\n\n\n\n\nggplot(df) +\n  aes(x = PHYS) + \n  scale_x_continuous(limits = c(3, 12)) +\n  labs(title = \"Density Plot of PHYS\", x = \"PHYS\", y = \"Density\") +\n  geom_density(fill = \"lightgray\") +\n  stat_function(\n    fun = dnorm,\n    args = list(mean = mean(df$PHYS), sd = sd(df$PHYS)),\n    color = \"red\",\n    linetype = \"dashed\"\n  )\n\n\n\n\n\n\nFigure 14.2: Distribution of PHYS Variable\n\n\n\n\n\n14.4.4 Summary Statistics for CONT and PHYS Variables\n\n# Get the summary statistics for CONT and PHYS variables; note that the\n#   summary() function returns a named numeric vector, so to preserve the names\n#   we transform this vector to a matrix first (before creating the data frame).\nsummary_df &lt;- data.frame(\n  CONT = as.matrix(summary(df$CONT)),\n  PHYS = as.matrix(summary(df$PHYS))\n)\n\n# Display summary statistics as a table\nkable(summary_df)\n\n\nTable 14.1: Summary Statistics for CONT and PHYS Variables\n\n\n\n\n\nCONT\nPHYS\n\n\n\nMin.\n5.700000\n4.700000\n\n\n1st Qu.\n6.850000\n7.700000\n\n\nMedian\n7.300000\n8.100000\n\n\nMean\n7.437209\n7.934884\n\n\n3rd Qu.\n7.900000\n8.500000\n\n\nMax.\n10.600000\n9.100000\n\n\n\n\n\n\n\n\n\n14.4.5 Skewness and Kurtosis for CONT and PHYS Variables\n\n# Calculate skewness and kurtosis for CONT and PHYS variables with moments:: \nskewness_vals &lt;- sapply(df[, c(\"CONT\", \"PHYS\")], moments::skewness)\nkurtosis_vals &lt;- sapply(df[, c(\"CONT\", \"PHYS\")], moments::kurtosis)\n\n# Create a data frame to display skewness and kurtosis\nskew_kurt_df &lt;- data.frame(\n  Skewness = skewness_vals,\n  Kurtosis = kurtosis_vals\n)\n\n# Display the data frame as a table\nkable(skew_kurt_df)\n\n\nTable 14.2: Skewness and Kurtosis for CONT and PHYS Variables\n\n\n\n\n\nSkewness\nKurtosis\n\n\n\nCONT\n1.085973\n4.729637\n\n\nPHYS\n-1.558215\n5.408086\n\n\n\n\n\n\n\n\n\n14.4.6 Visualizations of Transformed CONT and PHYS Variables\nWe will first apply a natural log transformation to the “controlling/authoritarian” variable.\n\n# Apply log transformation to CONT variable\ndf$LOG_CONT &lt;- log(df$CONT)\n\n# Plot density of log-transformed CONT variable\nggplot(df) + \n  aes(x = LOG_CONT) +\n  geom_density(fill = \"lightgray\") +\n  labs(\n    title = \"Density Plot of Log-Transformed CONT\",\n    x = \"Log-Transformed CONT\",\n    y = \"Density\"\n  ) +\n  stat_function(\n    fun = dnorm,\n    args = list(mean = mean(df$LOG_CONT), sd = sd(df$LOG_CONT)),\n    color = \"red\",\n    linetype = \"dashed\"\n  )\n\n\n\n\n\n\nFigure 14.3: Distribution of log transformed CONT Variable\n\n\n\n\nNow we will apply a Box-Cox transformation to the “physical ability” variable.\n\n# Apply Box-Cox transformation to PHYS using MASS:: package\nbc &lt;- MASS::boxcox(df$PHYS ~ 1, lambda = seq(-5, 5, 0.1), plotit = TRUE)\nlambda &lt;- bc$x[which.max(bc$y)]\ndf$BOX_COX_PHYS &lt;- (df$PHYS^lambda - 1) / lambda\n\n\n\n\n\n\nFigure 14.4: Distribution of optimal lambda determined by the boxcox function\n\n\n\n\n\n# Plot density of Box-Cox transformed PHYS variable\nggplot(df) +\n  aes(x = BOX_COX_PHYS) +\n  labs(\n    title = \"Density Plot of Box-Cox Transformed PHYS\",\n    x = \"Box-Cox Transformed PHYS\",\n    y = \"Density\"\n  ) + \n  geom_density(fill = \"lightgray\") +\n  stat_function(\n    fun = dnorm,\n    args = list(mean = mean(df$BOX_COX_PHYS), sd = sd(df$BOX_COX_PHYS)),\n    color = \"red\",\n    linetype = \"dashed\"\n  )\n\n\n\n\n\n\nFigure 14.5: Distribution of Box-Cox transformed PHYS Variable\n\n\n\n\n\n14.4.7 Skewness and Kurtosis for Transformed CONT and PHYS Variables\n\n# Calculate skewness and kurtosis for transformed variables\nskewness_vals_trans &lt;- sapply(df[, c(\"LOG_CONT\", \"BOX_COX_PHYS\")], skewness)\nkurtosis_vals_trans &lt;- sapply(df[, c(\"LOG_CONT\", \"BOX_COX_PHYS\")], kurtosis)\n\n# Create a data frame to display skewness and kurtosis for transformed variables\nskew_kurt_df_trans &lt;- data.frame(\n  Skewness = skewness_vals_trans,\n  Kurtosis = kurtosis_vals_trans\n)\n\n# Display the data frame as a table\nkable(skew_kurt_df_trans)\n\n\nTable 14.3: Skewness and Kurtosis for Transformed Variables (LOG_CONT and BOX_COX_PHYS)\n\n\n\n\n\nSkewness\nKurtosis\n\n\n\nLOG_CONT\n0.6555572\n3.758254\n\n\nBOX_COX_PHYS\n-0.3813574\n2.501916",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Transformations to Normality</span>"
    ]
  },
  {
    "objectID": "lessons/02_transformations.html#results",
    "href": "lessons/02_transformations.html#results",
    "title": "\n14  Transformations to Normality\n",
    "section": "\n14.5 Results",
    "text": "14.5 Results\nThe application of log and Box-Cox transformations has effectively improved the normality of the CONT and PHYS variables, respectively. For the CONT variable, the original distribution exhibited a positive skewness of 1.086 and a kurtosis of 4.730, indicating a right-skewed distribution with heavy tails and a sharp peak. The log transformation reduced the skewness to 0.656 and the kurtosis to 3.758, demonstrating a significant move towards normality, though the distribution still retains some right-skewness and heavier tails compared to a normal distribution. The PHYS variable originally had a negative skewness of -1.558 and a kurtosis of 5.408, reflecting a left-skewed distribution with heavy tails and a pronounced peak. Following the Box-Cox transformation, the skewness was reduced to -0.381 and the kurtosis to 2.502. These results indicate that the transformed PHYS distribution is much closer to normality, with reduced skewness and lighter tails, achieving a more symmetric distribution. In summary, the transformations have substantially mitigated the skewness and kurtosis of both variables, enhancing their suitability for statistical analyses that assume normality. This adjustment ensures more reliable and valid results in subsequent analyses.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Transformations to Normality</span>"
    ]
  },
  {
    "objectID": "lessons/02_transformations.html#conclusion-and-discussion",
    "href": "lessons/02_transformations.html#conclusion-and-discussion",
    "title": "\n14  Transformations to Normality\n",
    "section": "\n14.6 Conclusion and Discussion",
    "text": "14.6 Conclusion and Discussion\nThe transformations applied to the CONT and PHYS variables demonstrate the effectiveness of data transformation techniques in improving the normality of distributions. By addressing skewness and kurtosis, transformations like the log and Box-Cox methods help in stabilizing variance and making data more symmetric. This enhancement is crucial for statistical analyses that rely on the assumption of normality, ensuring more accurate and reliable results. Overall, the use of appropriate transformations is a vital step in data preprocessing, significantly enhancing the suitability of data for various analytical procedures and improving the robustness of statistical inferences. However, caution is warranted in the interpretation of results after transformation. Transformed data can sometimes complicate the understanding of results and their real-world implications, as the transformed scale may not directly relate to the original measurements. It is essential to back-transform results when interpreting findings to ensure they are meaningful and relevant to the original context.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Transformations to Normality</span>"
    ]
  },
  {
    "objectID": "lessons/02_transformations.html#references",
    "href": "lessons/02_transformations.html#references",
    "title": "\n14  Transformations to Normality\n",
    "section": "\n14.7 References",
    "text": "14.7 References\n\nManikandan S. (2010). Data transformation. Journal of pharmacology & pharmacotherapeutics, 1(2), 126–127. https://doi.org/10.4103/0976-500X.72373\nWest R. M. (2022). Best practice in statistics: The use of log transformation. Annals of clinical biochemistry, 59(3), 162–165. https://doi.org/10.1177/00045632211050531\nLee D. K. (2020). Data transformation: a focus on the interpretation. Korean journal of anesthesiology, 73(6), 503–508. https://doi.org/10.4097/kja.20137",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Transformations to Normality</span>"
    ]
  },
  {
    "objectID": "lessons/02_fisher_exact_test.html",
    "href": "lessons/02_fisher_exact_test.html",
    "title": "\n15  Fisher’s Exact Test\n",
    "section": "",
    "text": "15.1 Introduction to Fisher’s Exact test",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Fisher's Exact Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_fisher_exact_test.html#introduction-to-fishers-exact-test",
    "href": "lessons/02_fisher_exact_test.html#introduction-to-fishers-exact-test",
    "title": "\n15  Fisher’s Exact Test\n",
    "section": "",
    "text": "Fisher’s exact test is a non-parametric statistical test used to test an association between categorical variables.\nIt is analogous to Chi-square test, but Fisher’s exact test is conducted when rule of Chi-square test cannot be applied, such as when the sample size in small and more than 20% of cells have expected frequency count of &lt;5 in a contingency table (Bower 2003).\nUsed to assess whether the proportions of categories in two group variables significantly differ from each other.\nUses (hypergeometric) marginal distribution to compute exact p-values which are not approximated, which are also somewhat conservative.\nThis particular test is used to obtain the probability of observing the combination of frequencies that we can actually see.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Fisher's Exact Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_fisher_exact_test.html#assumptions",
    "href": "lessons/02_fisher_exact_test.html#assumptions",
    "title": "\n15  Fisher’s Exact Test\n",
    "section": "\n15.2 Assumptions",
    "text": "15.2 Assumptions\n\nAssumes that the individual observations are independent - variable are not paired or related.\nAssumes that the row and column totals are fixed or conditioned.\nThe variables are categorical and randomly sampled.\nObservations are count data.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Fisher's Exact Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_fisher_exact_test.html#hypotheses",
    "href": "lessons/02_fisher_exact_test.html#hypotheses",
    "title": "\n15  Fisher’s Exact Test\n",
    "section": "\n15.3 Hypotheses",
    "text": "15.3 Hypotheses\nThe hypotheses of Fisher’s exact test are similar to Chi-square test:\n\nNull hypothesis:\\((H_0)\\) There is no significant relationship between the categorical variables (variables are independent).\nAlternative hypothesis: \\((H_1)\\) There is a significant relationship between the categorical variables (variables are dependent).",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Fisher's Exact Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_fisher_exact_test.html#mathematical-definition-of-fishers-exact-test",
    "href": "lessons/02_fisher_exact_test.html#mathematical-definition-of-fishers-exact-test",
    "title": "\n15  Fisher’s Exact Test\n",
    "section": "\n15.4 Mathematical definition of Fisher’s Exact test",
    "text": "15.4 Mathematical definition of Fisher’s Exact test\nThis test is usually used as a one-tailed test. It can also be used as a two tailed test. Fisher’s exact test for a one-tailed \\(p\\)-value is calculated using the following formula:\n\\[\np = {(a+b)!(c+d)!(a+c)!(b+d)! \\over a! b! c! d! n!},\n\\] where \\(a\\),\\(b\\),\\(c\\), and \\(d\\) are the individual frequencies on the 2x2 contingency table and \\(n\\) is the population size (total frequency).",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Fisher's Exact Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_fisher_exact_test.html#intalling-required-packages",
    "href": "lessons/02_fisher_exact_test.html#intalling-required-packages",
    "title": "\n15  Fisher’s Exact Test\n",
    "section": "\n15.5 Intalling required packages",
    "text": "15.5 Intalling required packages\nFirst, we installed and loaded the packages needed for this presentation.\n\n#Installing Required Packages\n#install.packages(\"public.ctn0094data\")\n#install.packages(\"gtsummary\")\n#Install ggstatsplot package\n#install.packages(\"ggstatsplot\")\n#install.packages(\"ggmosaic\")\n#install.packages(\"tidyverse\")\n\n# Loading Required Packages\nlibrary(public.ctn0094data)\nlibrary(gtsummary)\nlibrary(ggstatsplot)\nlibrary(ggmosaic) \nlibrary(tidyverse)",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Fisher's Exact Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_fisher_exact_test.html#data-source-and-description",
    "href": "lessons/02_fisher_exact_test.html#data-source-and-description",
    "title": "\n15  Fisher’s Exact Test\n",
    "section": "\n15.6 Data source and description",
    "text": "15.6 Data source and description\nFor this demonstration of the Fisher’s Exact test , we utilized the demographics, and psychiatric data sets from the public.ctn0094data package the public.ctn0094data package. The public.ctn0094data package contains de-identified and harmonized datasets from the Clinical Trials Network (CTN) protocol number 0094. This project, funded by the US National Institute on Drug Abuse (NIDA), focuses on opioid use disorder (OUD) and includes data from three clinical trials: CTN-0027, CTN-0030, and CTN-0051.The data describe the experiences of patients seeking care for opoid use disorder (OUD).\nThe demographics dataset contains the demographic variables such as age, sex, race, living condition, marital status etc. The psychiatric dataset contains data on different mental health issues and susbstance use, including bipolar, depression, schizophrenia, cocaine use etc.\n\n# # Search for suitable data sets; this lists of all datasets in package\n# data(package = \"public.ctn0094data\") \n\ndata(demographics, package = \"public.ctn0094data\")\ncolnames(demographics)\n\n[1] \"who\"              \"age\"              \"is_hispanic\"      \"race\"            \n[5] \"job\"              \"is_living_stable\" \"education\"        \"marital\"         \n[9] \"is_male\"         \n\ndata(psychiatric, package = \"public.ctn0094data\")\ncolnames(psychiatric) \n\n [1] \"who\"                 \"has_schizophrenia\"   \"has_major_dep\"      \n [4] \"has_bipolar\"         \"has_anx_pan\"         \"has_brain_damage\"   \n [7] \"has_epilepsy\"        \"depression\"          \"anxiety\"            \n[10] \"schizophrenia\"       \"has_opiates_dx\"      \"has_alcol_dx\"       \n[13] \"has_amphetamines_dx\" \"has_cannabis_dx\"     \"has_cocaine_dx\"     \n[16] \"has_sedatives_dx\"   \n\n\n\n15.6.1 Create a model data frame\nWe joined the demographics and psychiatric data sets within the public.ctn0094data package by participants ID (who variable) to create new data frame.\n\n# Joining data sets: \nmodel_df &lt;- \n  demographics %&gt;% \n  left_join(psychiatric, by = \"who\") %&gt;% \n  # Selecting variables of interest for our analysis\n  select(\n    age, race, education, is_male, marital, is_living_stable, has_schizophrenia\n  )\n\n\n15.6.2 Participants characteristics Summary table\nHere, we want to view the frequency of the variables in our dataset using the table summary function tbl_summary ().\n\n# Create Table 1, change the Label using the label function and also view the missing values\nmodel_df %&gt;% \n  tbl_summary(\n    label = list(\n      age = \"Age\",\n      race = \"Race\",\n      education = \"Education_Level\",\n      is_male = \"Male\",\n      marital = \"Marital_Status\",\n      is_living_stable = \"Living_Condition\",\n      has_schizophrenia = \"Schizophrenia\"\n    ),\n    missing_text = \"(Missing)\"\n  )\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nN = 3,5601\n\n\n\n\nAge\n34 (27, 45)\n\n\n    (Missing)\n208\n\n\nRace\n\n\n\n    Black\n365 (10%)\n\n\n    Other\n506 (14%)\n\n\n    Refused/missing\n58 (1.6%)\n\n\n    White\n2,631 (74%)\n\n\nEducation_Level\n\n\n\n    HS/GED\n691 (39%)\n\n\n    Less than HS\n352 (20%)\n\n\n    More than HS\n724 (41%)\n\n\n    (Missing)\n1,793\n\n\nMale\n2,351 (66%)\n\n\n    (Missing)\n4\n\n\nMarital_Status\n\n\n\n    Married or Partnered\n329 (19%)\n\n\n    Never married\n1,028 (59%)\n\n\n    Separated/Divorced/Widowed\n394 (23%)\n\n\n    (Missing)\n1,809\n\n\nLiving_Condition\n1,535 (96%)\n\n\n    (Missing)\n1,962\n\n\nSchizophrenia\n73 (2.4%)\n\n\n    (Missing)\n469\n\n\n\n\n1 Median (IQR); n (%)\n\n\n\n\n\n\n\n15.6.3 Recode to indicate variable factor levels\n\n# Recoding `is_living_stable` and has_schizophrenia`converting all NA to 99\nmodel_df &lt;- \n  model_df %&gt;% \n  mutate(\n    Living_stable = ifelse(is.na(is_living_stable), 99, is_living_stable),\n    Living_stable = factor(\n      Living_stable, levels = c(1, 2, 99), labels = c(\"No\", \"Yes\", \"Missing\")\n    )\n  ) %&gt;% \n  mutate(\n    schizophrenia = ifelse(is.na(has_schizophrenia), 99, has_schizophrenia),\n    schizophrenia = factor(\n      schizophrenia, levels = c(1, 2, 99), labels = c(\"No\", \"Yes\", \"Missing\")\n    )\n  ) \n\nmodel_df \n\n# A tibble: 3,560 × 9\n     age race  education    is_male marital   is_living_stable has_schizophrenia\n   &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;        &lt;fct&gt;   &lt;fct&gt;     &lt;fct&gt;            &lt;fct&gt;            \n 1    43 White More than HS Yes     Married … Yes              No               \n 2    30 White More than HS No      Never ma… Yes              No               \n 3    23 Black More than HS No      Never ma… Yes              No               \n 4    19 White More than HS Yes     Never ma… Yes              No               \n 5    31 White &lt;NA&gt;         No      &lt;NA&gt;      &lt;NA&gt;             &lt;NA&gt;             \n 6    43 White HS/GED       Yes     Married … Yes              No               \n 7    33 White More than HS No      Never ma… Yes              No               \n 8    44 White &lt;NA&gt;         Yes     &lt;NA&gt;      &lt;NA&gt;             &lt;NA&gt;             \n 9    25 Black HS/GED       No      Never ma… Yes              No               \n10    29 Other More than HS No      Never ma… Yes              Yes              \n# ℹ 3,550 more rows\n# ℹ 2 more variables: Living_stable &lt;fct&gt;, schizophrenia &lt;fct&gt;",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Fisher's Exact Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_fisher_exact_test.html#contingency-table-with-tbl_summary-function",
    "href": "lessons/02_fisher_exact_test.html#contingency-table-with-tbl_summary-function",
    "title": "\n15  Fisher’s Exact Test\n",
    "section": "\n15.7 Contingency Table with tbl_summary Function",
    "text": "15.7 Contingency Table with tbl_summary Function\n\nThis is a table that shows the distribution of a variable in the rows and columns. Sometimes referred to as a 2x2 table. They are useful in summarizing categorical variables.\nWe want to create a contingency table of the demographic variable by living_stable to Check the distribution of the frequency count of variables (is_living_stable, Yes = stable and No = unstable has_schizophrenia, Yes = schizophrenia diagnosed and No = no schizophrenia).\n\n\n# creating new data frame keeping only the categorical variable of interest\n#   for our contingency table in the next section\n\nfinalModel_df &lt;- select(model_df, schizophrenia, Living_stable)\n\n# Adding label and overall number \n\nfinalModel_df %&gt;% \n  tbl_summary(by = Living_stable) %&gt;%\n  #add_n() %&gt;%\n  add_overall() %&gt;%\n  modify_spanning_header(c(\"stat_1\", \"stat_2\") ~ \"**Living_stable**\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nOverall, N = 3,5601\n\nLiving_stable\n\nMissing, N = 1,9621\n\n\n\n\nNo, N = 631\n\n\nYes, N = 1,5351\n\n\n\n\n\nschizophrenia\n\n\n\n\n\n\n    No\n3,018 (85%)\n59 (94%)\n1,485 (97%)\n1,474 (75%)\n\n\n    Yes\n73 (2.1%)\n2 (3.2%)\n21 (1.4%)\n50 (2.5%)\n\n\n    Missing\n469 (13%)\n2 (3.2%)\n29 (1.9%)\n438 (22%)\n\n\n\n\n1 n (%)\n\n\n\n\n\n\nFrom the table, it seems like the patients who were homeless (answered no to living_stable) were less likely to be diagnosed with schizophrenia. However, this is not conclusive as we cannot tell if this relationship was a true correlation or it was due to random sampling error. So we will perform the Fisher’s Exact test to confirm the relationship.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Fisher's Exact Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_fisher_exact_test.html#performing-the-fishers-exact-test-using-the-function-fisher.test",
    "href": "lessons/02_fisher_exact_test.html#performing-the-fishers-exact-test-using-the-function-fisher.test",
    "title": "\n15  Fisher’s Exact Test\n",
    "section": "\n15.8 Performing the Fisher’s exact test using the function fisher.test()",
    "text": "15.8 Performing the Fisher’s exact test using the function fisher.test()\n\nA priori, we hypothesized that people who are diagnosed as being schizophrenic are more likely to homeless (unstable living). So we conducted a one-tailed Fisher’s Exact test and specify the direction of the test as “greater”.\nFor a two-tailed test, the alternative argument has a default value of \"two.sided\".\n\n\n# running one-tailed fisher's exact test\nfModelGreater_ls &lt;- fisher.test(\n  x = model_df$is_living_stable, \n  y = model_df$has_schizophrenia, \n  alternative = \"greater\"\n)\nfModelGreater_ls\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  model_df$is_living_stable and model_df$has_schizophrenia\np-value = 0.9429\nalternative hypothesis: true odds ratio is greater than 1\n95 percent confidence interval:\n 0.1159862       Inf\nsample estimates:\nodds ratio \n 0.4175209 \n\n# running two-tailed Fisher's exact test\nfisher.test(\n  x = model_df$is_living_stable, \n  y = model_df$has_schizophrenia\n)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  model_df$is_living_stable and model_df$has_schizophrenia\np-value = 0.2246\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 0.09821534 3.75679596\nsample estimates:\nodds ratio \n 0.4175209",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Fisher's Exact Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_fisher_exact_test.html#interpretation-of-results",
    "href": "lessons/02_fisher_exact_test.html#interpretation-of-results",
    "title": "\n15  Fisher’s Exact Test\n",
    "section": "\n15.9 Interpretation of results",
    "text": "15.9 Interpretation of results\n\n15.9.1 One-Tailed test\n\nNull Hypothesis, \\(H_0\\): people who reported unstable living (homeless) are not more often diagnosed with schizophrenia.\nAlternative Hypothesis, \\(H_A\\): people who reported unstable living(homeless) are more often diagnosed with schizophrenia.\n\nThe \\(p\\) value is greater than 0.05 (\\(p\\) = 0.9429), so we fail to reject the null hypothesis. We conclude that people who reported unstable living are not significantly more likely to be diagnosed with schizophrenia than those who reported stable living.\n\n15.9.2 Two-tailed analysis\n\nNull Hypothesis, \\(H_0\\): There is no association between living-stable and schizophrenia diagnosis variable\nAlternative Hypothesis, \\(H_A\\): There is an association between living condition and schizophrenia diagnosis.\n\nOur \\(p\\)-value = 0.2246, so we fail to reject the null hypothesis, indicating that there is no statistically significant association between living condition and schizophrenia diagnosis.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Fisher's Exact Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_fisher_exact_test.html#visualization-of-the-statistical-test-output",
    "href": "lessons/02_fisher_exact_test.html#visualization-of-the-statistical-test-output",
    "title": "\n15  Fisher’s Exact Test\n",
    "section": "\n15.10 Visualization of the statistical test output",
    "text": "15.10 Visualization of the statistical test output\nWe would like to have a visual representation of the distribution of the categories of our analysis. As shown in the Fisher’s Exact test, there was no statistical significant association between living-stability and schizophrenia diagnosis. Hence, the interpretation of the plots below is based on description of the charts.\n\n15.10.1 Barplots\nWe will first construct a barplot using ggstatsplot:: (for statistical details).\n\ndisplayP_char &lt;- ifelse(\n  test = fModelGreater_ls$p.value &lt; 0.001,\n  yes = \"&lt; 0.001\",\n  no = as.character(round(fModelGreater_ls$p.value, 3))\n)\n\n# combine plot and statistical test with ggbarstats\nggbarstats( \n  data = finalModel_df,\n  x = Living_stable,\n  y = schizophrenia,\n  results.subtitle = FALSE,\n  subtitle = paste0(\"Fisher's exact test, p-value = \", displayP_char)\n)\n\n\n\n\n\n\n\nPatients who indicated living_stable (not homeless) had the highest proportion of not having being diagnosed with schizophrenia. Patients who indicated not having stable living (homeless) had the highest proportion of being diagnosed with schizophrenia.\n\n15.10.2 Mosaic plots\n\n# Basic Mosaic Plot\nmosaic_basic &lt;- finalModel_df %&gt;% \n  ggplot() +\n  geom_mosaic(\n    aes(\n      x = product(schizophrenia),\n      fill = Living_stable\n    )\n  ) +\n  labs(\n    #y = \"Living_stable\",\n    #x = \"schizophrenia\",\n    title = \"Mosaic Plot of schizophrenia by living_stability\") +\n  # Specifies default `geom_mosaic` aesthetics, e.g white panel background, \n  # removes grid lines, adjusts widths and heights of rows and columns to \n  # reflect frequencies\n  theme_mosaic() +\n  theme(legend.position = \"None\") +\n  theme(axis.text.x = element_text(angle = 60, hjust = 1, vjust = 1))\n  \nmosaic_basic\n\n\n\n\n\n\n\nCompared to those who are not homeless, those who are homeless had the highest proportion of being diagnosed with schizophrenia.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Fisher's Exact Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_fisher_exact_test.html#references",
    "href": "lessons/02_fisher_exact_test.html#references",
    "title": "\n15  Fisher’s Exact Test\n",
    "section": "\n15.11 References",
    "text": "15.11 References\n\nBower, Keith M. 2003. “When to Use Fisher’s Exact Test.” In American Society for Quality, Six Sigma Forum Magazine, 2:35–37. 4.\nMcCrum-Gardner, Evie. 2008. “Which Is the Correct Statistical Test to Use?” British Journal of Oral and Maxillofacial Surgery 46 (1): 38–41.\nWong KC. Chi squared test versus Fisher’s exact test. Hong Kong Med J. 2011 Oct;17(5):427\nPatil, I. (2021). Visualizations with statistical details: The ‘ggstatsplot’ approach. Journal of Open Source Software, 6(61), 3167, doi:10.21105/joss.03167\nZach Bobbit. (2021). Fisher’s Exact Test: Definition, Formula, and Example",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Fisher's Exact Test</span>"
    ]
  },
  {
    "objectID": "03_header_two-sample.html",
    "href": "03_header_two-sample.html",
    "title": "Two-Sample Tests",
    "section": "",
    "text": "Text Outline\nThis text covers the basics of:",
    "crumbs": [
      "Two-Sample Tests"
    ]
  },
  {
    "objectID": "03_header_two-sample.html#text-outline",
    "href": "03_header_two-sample.html#text-outline",
    "title": "Two-Sample Tests",
    "section": "",
    "text": "Exploring data\nOne-sample tests\nTwo-sample tests\nLinear regression and ANOVA\nGeneralized linear models\nSome special topics\nMaybe a bit about power calculations…",
    "crumbs": [
      "Two-Sample Tests"
    ]
  },
  {
    "objectID": "03_header_two-sample.html#part-outline",
    "href": "03_header_two-sample.html#part-outline",
    "title": "Two-Sample Tests",
    "section": "Part Outline",
    "text": "Part Outline\nThis part of the text contains the chapters on various two-sample statistical tests:\n\n\\(t\\)-test\nWelch’s \\(t\\)-test\nMann-Whitney \\(U\\) test\nCochran’s \\(Q\\) test\n\\(\\chi^2\\) Test for Independence",
    "crumbs": [
      "Two-Sample Tests"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_ttest.html",
    "href": "lessons/03_two_sample_ttest.html",
    "title": "\n16  Two sample \\(t\\)-test\n",
    "section": "",
    "text": "16.1 Two sample \\(t\\)-test\nThis is also called the independent sample t test. It is used to see whether the population means of two groups are equal or different. This test requires one variable which can be the exposure x and another variable which can be the outcome y. If there are more than two groups, Analysis of Variance (ANOVA) would be more suitable. If data is nonparametric then an alternative test to use would be the Mann Whitney U test. Cressie, N.A., 1986\nThere are two types of independent t tests: the first is the Student’s t test, which assumes the variance of the two groups is equal, and the second being the Welch’s t test (default in R), which assumes the variance in the two groups is different.\nIn this article we will be discussing the Student’s t test.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Two sample $t$-test</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_ttest.html#two-sample-t-test",
    "href": "lessons/03_two_sample_ttest.html#two-sample-t-test",
    "title": "\n16  Two sample \\(t\\)-test\n",
    "section": "",
    "text": "16.1.1 Assumptions\n\nMeasurements for one observation do not affect measurements for any other observation (assumes independence).\nData values in dependent variable are continuous.\nData in each group are normally distributed.\nThe variances for the two independent groups are equal in the Student’s t test.\nThere should be no significant outliers.\n\n16.1.2 Hypotheses\n\n\\((H_0)\\): the mean of group A \\((m_A)\\) is equal to the mean of group B \\((m_B)\\)- two tailed test.\n\\((H_0)\\): \\((m_A)\\ge (m_B)\\)- one tailed test.\n\\((H_0)\\): \\((m_A)\\le (m_B)\\)- one tailed test.\n\nThe corresponding alternative hypotheses would be as follows:\n\n\n\\((H_1)\\): \\((m_A)\\neq(m_B)\\)- two tailed test.\n\n\\((H_1)\\): \\((m_A)&lt;(m_B)\\)- one tailed test.\n\n\\((H_1)\\): \\((m_A)&gt; (m_B)\\)- one tailed test.\n\n16.1.3 Statistical hypotheses formula\nFor the Student’s t test which assumes equal variance, here is an example of how the |t| statistic may be calculated using groups A and B:\n\\(t ={ {m_{A} - m_{B}} \\over \\sqrt{ {S^2 \\over n_{A} } + {S^2 \\over n_{B}}   }}\\)\nThis can be described as the sample mean difference divided by the sample standard deviation of the sample mean difference where:\n\\(m_A\\) and \\(m_B\\) are the mean values of A and B,\n\\(n_A\\) and \\(n_B\\) are the size of group A and B,\n\\(S^2\\) is the estimator for the pooled variance, with the degrees of freedom (df) = \\(n_A + n_B - 2\\),\nand \\(S^2\\) is calculated as follows:\n\\(S^2 = { {\\sum{ (x_A-m_{A})^2} + \\sum{ (x_B-m_{B})^2}} \\over {n_{A} + n_{B} - 2 }}\\)\nWhat if the data is not independent?\nIf the data is not independent such as paired data in the form of matched pairs which are correlated, we use the paired t test. This test checks whether the means of two paired groups are different from each other. It is usually applied in clinical trial studies with a “before and after” or case control studies with matched pairs. For this test we only assume the difference of each pair to be normally distributed (the paired groups are the ones important for analysis) unlike the independent t test which assumes that data from both samples are independent and variances are equal. Xu, M., 2017\n\n16.1.4 Example\n\n16.1.4.1 Prerequisites\n\n\ntidyverse: data manipulation and visualization.\n\nrstatix: providing pipe friendly R functions for easy statistical analysis.\n\ncar: providing variance tests.\n\n\n#install.packages(\"ggstatplot\") \n#install.packages(\"car\")\n#install.packages(\"rstatix\")\n#install.packages(tidyVerse)\n\n\n16.1.4.2 Dataset\nThis example dataset sourced from kaggle was obtained from surveys of students in Math and Portuguese classes in secondary school. It contains demographic information on gender, social and study information.\n\n# load relevant libraries\nlibrary(rcompanion)\nlibrary(car)\nlibrary (gt)\nlibrary(gtsummary)\nlibrary(ggpubr)\nlibrary(rstatix)\nlibrary(tidyverse)\n\n\n# load the dataset\nstu_math &lt;- read_csv(\"../data/03_student-mat.csv\")\n\nRows: 395 Columns: 33\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (17): school, sex, address, famsize, Pstatus, Mjob, Fjob, reason, guardi...\ndbl (16): age, Medu, Fedu, traveltime, studytime, failures, famrel, freetime...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nChecking the data\n\n# check the data\nglimpse(stu_math)\n\nRows: 395\nColumns: 33\n$ school     &lt;chr&gt; \"GP\", \"GP\", \"GP\", \"GP\", \"GP\", \"GP\", \"GP\", \"GP\", \"GP\", \"GP\",…\n$ sex        &lt;chr&gt; \"F\", \"F\", \"F\", \"F\", \"F\", \"M\", \"M\", \"F\", \"M\", \"M\", \"F\", \"F\",…\n$ age        &lt;dbl&gt; 18, 17, 15, 15, 16, 16, 16, 17, 15, 15, 15, 15, 15, 15, 15,…\n$ address    &lt;chr&gt; \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\",…\n$ famsize    &lt;chr&gt; \"GT3\", \"GT3\", \"LE3\", \"GT3\", \"GT3\", \"LE3\", \"LE3\", \"GT3\", \"LE…\n$ Pstatus    &lt;chr&gt; \"A\", \"T\", \"T\", \"T\", \"T\", \"T\", \"T\", \"A\", \"A\", \"T\", \"T\", \"T\",…\n$ Medu       &lt;dbl&gt; 4, 1, 1, 4, 3, 4, 2, 4, 3, 3, 4, 2, 4, 4, 2, 4, 4, 3, 3, 4,…\n$ Fedu       &lt;dbl&gt; 4, 1, 1, 2, 3, 3, 2, 4, 2, 4, 4, 1, 4, 3, 2, 4, 4, 3, 2, 3,…\n$ Mjob       &lt;chr&gt; \"at_home\", \"at_home\", \"at_home\", \"health\", \"other\", \"servic…\n$ Fjob       &lt;chr&gt; \"teacher\", \"other\", \"other\", \"services\", \"other\", \"other\", …\n$ reason     &lt;chr&gt; \"course\", \"course\", \"other\", \"home\", \"home\", \"reputation\", …\n$ guardian   &lt;chr&gt; \"mother\", \"father\", \"mother\", \"mother\", \"father\", \"mother\",…\n$ traveltime &lt;dbl&gt; 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 3, 1, 2, 1, 1, 1, 3, 1, 1,…\n$ studytime  &lt;dbl&gt; 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 3, 1, 2, 3, 1, 3, 2, 1, 1,…\n$ failures   &lt;dbl&gt; 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0,…\n$ schoolsup  &lt;chr&gt; \"yes\", \"no\", \"yes\", \"no\", \"no\", \"no\", \"no\", \"yes\", \"no\", \"n…\n$ famsup     &lt;chr&gt; \"no\", \"yes\", \"no\", \"yes\", \"yes\", \"yes\", \"no\", \"yes\", \"yes\",…\n$ paid       &lt;chr&gt; \"no\", \"no\", \"yes\", \"yes\", \"yes\", \"yes\", \"no\", \"no\", \"yes\", …\n$ activities &lt;chr&gt; \"no\", \"no\", \"no\", \"yes\", \"no\", \"yes\", \"no\", \"no\", \"no\", \"ye…\n$ nursery    &lt;chr&gt; \"yes\", \"no\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes…\n$ higher     &lt;chr&gt; \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"ye…\n$ internet   &lt;chr&gt; \"no\", \"yes\", \"yes\", \"yes\", \"no\", \"yes\", \"yes\", \"no\", \"yes\",…\n$ romantic   &lt;chr&gt; \"no\", \"no\", \"no\", \"yes\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\"…\n$ famrel     &lt;dbl&gt; 4, 5, 4, 3, 4, 5, 4, 4, 4, 5, 3, 5, 4, 5, 4, 4, 3, 5, 5, 3,…\n$ freetime   &lt;dbl&gt; 3, 3, 3, 2, 3, 4, 4, 1, 2, 5, 3, 2, 3, 4, 5, 4, 2, 3, 5, 1,…\n$ goout      &lt;dbl&gt; 4, 3, 2, 2, 2, 2, 4, 4, 2, 1, 3, 2, 3, 3, 2, 4, 3, 2, 5, 3,…\n$ Dalc       &lt;dbl&gt; 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1,…\n$ Walc       &lt;dbl&gt; 1, 1, 3, 1, 2, 2, 1, 1, 1, 1, 2, 1, 3, 2, 1, 2, 2, 1, 4, 3,…\n$ health     &lt;dbl&gt; 3, 3, 3, 5, 5, 5, 3, 1, 1, 5, 2, 4, 5, 3, 3, 2, 2, 4, 5, 5,…\n$ absences   &lt;dbl&gt; 6, 4, 10, 2, 4, 10, 0, 6, 0, 0, 0, 4, 2, 2, 0, 4, 6, 4, 16,…\n$ G1         &lt;dbl&gt; 5, 5, 7, 15, 6, 15, 12, 6, 16, 14, 10, 10, 14, 10, 14, 14, …\n$ G2         &lt;dbl&gt; 6, 5, 8, 14, 10, 15, 12, 5, 18, 15, 8, 12, 14, 10, 16, 14, …\n$ G3         &lt;dbl&gt; 6, 6, 10, 15, 10, 15, 11, 6, 19, 15, 9, 12, 14, 11, 16, 14,…\n\n\nIn total there are 395 observations and 33 variables. We will drop the variables we do not need and keep the variables that will help us answer the following: Is there a difference between the finals grades of boys and girls in maths?\n\\(H_0\\): There is no statistical difference between the final grades between boys and girls.\n\\(H_1\\): There is a statistically significant differencehe in the final grades between the two groups.\n\n# creating a subset of the data \nmath &lt;- stu_math %&gt;% subset %&gt;% select (sex, G3)\nglimpse(math)\n\nRows: 395\nColumns: 2\n$ sex &lt;chr&gt; \"F\", \"F\", \"F\", \"F\", \"F\", \"M\", \"M\", \"F\", \"M\", \"M\", \"F\", \"F\", \"M\", \"…\n$ G3  &lt;dbl&gt; 6, 6, 10, 15, 10, 15, 11, 6, 19, 15, 9, 12, 14, 11, 16, 14, 14, 10…\n\n\nSummary statistics The dependent variable is continuous (grades=G3) and the independent variable is character but binary (sex).\n\n# summarizing our data\n summary(math)\n\n     sex                  G3       \n Length:395         Min.   : 0.00  \n Class :character   1st Qu.: 8.00  \n Mode  :character   Median :11.00  \n                    Mean   :10.42  \n                    3rd Qu.:14.00  \n                    Max.   :20.00  \n\n\nWe see that data ranges from 0-20 with 0 being people who were absent and could not take the test therefore missing data.\nIdentifying outliers - Outliers can also be identified through boxplot.\n\n# creating a boxplot to visualize the outliers (without removing score of zero)\nggplot(data = math) + \n  aes(\n    x = sex, \n    y = G3\n  ) +\n  labs(\n    title = \"Boxplot to visualize the outlier\",\n    x  = \"sex\", \n    y = \"Maths Grades\"\n  ) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n# creating a boxplot to visualize the data with no outliers\nmath2 &lt;- math %&gt;% filter(G3&gt;0)\nggplot(data = math2) + \n  aes(\n    x = sex, \n    y = G3\n  ) +\n  labs(\n    title = \"Boxplot without the outlier\",\n    x  = \"sex\", \n    y = \"Maths Grades\"\n  ) +\n  geom_boxplot()\n\n\n\n\n\n\n\nThe second box plot shows us that there are no outliers as students with a score of zero have been removed. This score is not truly reflective of the performance between boys and girls as a grade of 0 may represent absence or other reasons for the test not been taken.\nWe remove the outliers before running the t test. However, other models can be considered such as the zero inflated model to differentiate those who truly got a 0 and those who were not present to take test.\n\n# finding the mean for the groups with outliers (score of zero included)\nmean(math$G3[math$sex==\"F\"])\n\n[1] 9.966346\n\nmean(math$G3[math$sex==\"M\"])\n\n[1] 10.91444\n\n# finding the mean for the groups without outliers (score of zero not included)\nmean(math2$G3[math2$sex==\"F\"])\n\n[1] 11.20541\n\nmean(math2$G3[math2$sex==\"M\"])\n\n[1] 11.86628\n\n\nThe mean has increased slightly in both groups and the difference in mean has been decreased in both groups after removing the outliers.\nVisualizing the data - We can use barplots to look at the difference in sample sizes between the groups and histograms check distribution of the data for normality.\n\nsample_size &lt;- table(math2$sex)\nsample_size_df &lt;- as.data.frame(sample_size)\ncolnames(sample_size_df) &lt;- c(\"sex\", \"count\")\n\n# plotting bar plot to see the distribution in sample size\nggplot(data = sample_size_df) + \n  aes(\n    x = sex, \n    y = count,\n  ) +\n  labs(\n    title = \"Distribution of sample size by sex\",\n    x = \"Sex\",\n    y = \"Number of students\"\n  ) +\n  geom_bar(stat = \"identity\")\n\n\n\n\n\n\n\nThe bar graph shows that there are slightly more females in the sample than males.\n\n# Histograms for data by groups \n\nmale = math2$G3[math2$sex == \"M\"]\nfemale = math2$G3[math2$sex == \"F\"]\n\n# plotting distribution for males\nplotNormalHistogram(\n  male, \n  breaks= 20,\n  xlim=c(0,20),\n  main=\"Distribution of the grades for males \", \n  xlab= \"Math Grades\"\n  )\n\n\n\n\n\n\n\nThe final grades for males seem to be normally distributed. If it is difficult to be certain whether the data is normally distributed, the histogram of the square root transformed data can be plotted. It can help reveal underlying normality in skewed data by compressing the spread.\n\n# plotting square root transformed distribution for males\nplotNormalHistogram(\n  sqrt(male), \n  breaks= 20,\n  main=\"Distribution of the grades for males \", \n  xlab= \"Math Grades\"\n  )\n\n\n\n\n\n\n\nA highly skewed data might still not appear normal after square root transformation, so this is not a solution to a skewed data, but rather an approach to check normality more precisely.\n\n# plotting distribution for females\nplotNormalHistogram(\n  female, \n  breaks= 20,\n  xlim=c(0,20),\n  main=\"Distribution of the grades for females \", \n  xlab= \"Math Grades\"\n  )\n\n\n\n\n\n\n\nSimilar to males, looking at the square root transformed distribution for females\n\n# plotting square root transformed distribution for males\nplotNormalHistogram(\n  sqrt(female), \n  breaks= 20,\n  main=\"Distribution of the grades for females \", \n  xlab= \"Math Grades\"\n  )\n\n\n\n\n\n\n\nFinal grades for females also appear to be normally distributed. The final score across both is almost evenly distributed.\nCheck the equality of variances (homogeneity) - By looking at the two box plots above for two groups, it does not appear that the variances are different between the two groups.\nWe can use the Levene’s test or the Bartlett’s test to check for homogeneity of variances. The former is in the car library and the later in the rstatix library. If the variances are homogeneous, the p value will be greater than 0.05.\nOther tests include F test 2 sided, Brown-Forsythe and O’Brien but we shall not cover these.\n\n# running the Levene's test to check equal variance\nmath2 %&gt;% levene_test(G3~sex)\n\n# A tibble: 1 × 4\n    df1   df2 statistic     p\n  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1     1   355     0.614 0.434\n\n#don't do this unless worried about the data\n\n\n# running the Bartlett's test to check equal variance\nbartlett.test(G3~sex, data=math2)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  G3 by sex\nBartlett's K-squared = 0.12148, df = 1, p-value = 0.7274\n\n#don't do this unless worried about the data\n\nThe p value is greater than 0.05 from both the test suggesting there is no difference between the variances of the two groups.\n\n16.1.4.3 Assessment\n\nData is continuous(G3)\nData is independent (males and females are distinct and not the same individual)\nData is normally distributed. We might still want to do a square root transformation.\nNo significant outliers.\nThere are equal variances.\n\nAs the assumptions are met we go ahead to perform the Student’s \\(t\\)-test.\n\n16.1.4.4 Performing the two-sample \\(t\\)-test\nSince the default is the Welch t test we use the \\(\\color{blue}{\\text{var.eqaul = TRUE }}\\) code to signify a Student’s t test.\n\n# perfoming the two sample t test\nstat.test &lt;- math2 %&gt;% \n  #mutate(sqrt_G3 = sqrt(G3)) %&gt;%\n  t_test(G3~ sex, var.equal=TRUE) %&gt;%\n  add_significance()\nstat.test\n\n# A tibble: 1 × 9\n  .y.   group1 group2    n1    n2 statistic    df      p p.signif\n  &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   \n1 G3    F      M        185   172     -1.94   355 0.0531 ns      \n\n\n\nstat.test$statistic\n\n        t \n-1.940477 \n\n\nIf you decided to work with square root transformed data, you could use the mutate function to create the ‘sqrt_G3’ variable in the ‘stat.test’ dataset, which produces quite similar p-value and t test statistic.\nThe results are represented as follows;\n\ny - dependent variable\ngroup1, group 2 - compared groups (independent variables)\ndf - degrees of freedom\np - p value\n\ngtsummary table of results\n\n math2 |&gt; \n  tbl_summary(\n    by = sex,\n    statistic =\n      list(\n        all_continuous() ~ \"{mean} ({sd})\")\n    ) |&gt; \n   add_n() |&gt; \n  add_overall() |&gt; \n  add_difference()\n\n\n\n\n\n\nCharacteristic\nN\n\nOverall, N = 3571\n\n\nF, N = 1851\n\n\nM, N = 1721\n\n\nDifference2\n\n\n95% CI2,3\n\n\np-value2\n\n\n\nG3\n357\n11.5 (3.2)\n11.2 (3.2)\n11.9 (3.3)\n-0.66\n-1.3, 0.01\n0.053\n\n\n\n\n1 Mean (SD)\n\n\n\n2 Welch Two Sample t-test\n\n\n\n3 CI = Confidence Interval\n\n\n\n\n\n\n\nInterpretation of results\nFor the Student’s t test, the obtained t statistic of -1.940477 is greater than the critical value at 355 degree of freedom (n1+n2-2) i.e. -1.984, due to which it is not statistically significant. Also, the p-value is greater than alpha of 0.05, due to which we we fail to reject the null hypothesis and conclude that there is no statistical difference between the mean grades of boys and girls. (A significant |t| would be equal to -1.984 or smaller; or equal to 1.984 or greater).\nEffect size Note: Effect size should only be calculated if the null hypothesis is rejected. We are showing how to do this for pedagogical reasons. In practice, we would not calculate an effect size because we fail to reject the null hypothesis.\nCohen’s d can be an used as an effect size statistic for the two sample t test. It is the difference between the means of each group divided by the pooled standard deviation.\n\\(d= {m_A-m_B \\over SDpooled}\\)\nIt ranges from 0 to infinity, with 0 indicating no effect where the means are equal. 0.5 means that the means differ by half the standard deviation of the data and 1 means they differ by 1 standard deviation. It is divided into small, medium or large using the following cut off points.\n\nsmall 0.2-&lt;0.5\nmedium 0.5-&lt;0.8\nlarge &gt;=0.8\n\nFor the above test the following is how we can find the effect size;\n\n#perfoming cohen's d\nmath2 %&gt;% \n  cohens_d(G3~sex,var.equal = TRUE)\n\n# A tibble: 1 × 7\n  .y.   group1 group2 effsize    n1    n2 magnitude\n* &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;ord&gt;    \n1 G3    F      M       -0.206   185   172 small    \n\n\nThe effect size is small d= -0.20.\nIn conclusion, a two-samples t-test showed that the difference was not statistically significant, t(355) = -1.940477, p &lt; 0.0531, d = -0.20; where, t(355) is shorthand notation for a t-statistic that has 355 degrees of freedom and d is Cohen’s d. We can conclude that the females mean final grade is greater than males final grade (d= -0.20) but this result is not significant.\nWhat if it is one tailed t test?\nUse the \\(\\color{blue}{\\text{alternative =}}\\) option to determine if one group is \\(\\color{blue}{\\text{\"less\"}}\\) or \\(\\color{blue}{\\text{\"greater\"}}\\). For example if we want to check the null hypothesis whether the final grades for females are greater than or equal to males we can use the following code:\n\n# perfoming the one tailed two sample t test\nstat.test &lt;- math2 %&gt;% \n  t_test(G3~ sex, var.equal=TRUE, alternative = \"less\") %&gt;%\n  add_significance()\nstat.test\n\n# A tibble: 1 × 9\n  .y.   group1 group2    n1    n2 statistic    df      p p.signif\n  &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   \n1 G3    F      M        185   172     -1.94   355 0.0266 *       \n\n\nSince, the p value is smaller than 0.05 (p=0.027), we reject the null hypothesis. We conclude that the final grades for females are significantly lesser than that for males.\nWhat about running the paired sample t test?\nWe can simply add the syntax \\(\\color{blue}{\\text{paired= TRUE}}\\) to the t_test() function to run the analysis for matched pairs data.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Two sample $t$-test</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_ttest.html#conclusion",
    "href": "lessons/03_two_sample_ttest.html#conclusion",
    "title": "\n16  Two sample \\(t\\)-test\n",
    "section": "\n16.2 Conclusion",
    "text": "16.2 Conclusion\nThis article covers the Student’s t test and how we run it in R. It also shows how we find the effect size and how we can conclude the results.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Two sample $t$-test</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_ttest.html#references",
    "href": "lessons/03_two_sample_ttest.html#references",
    "title": "\n16  Two sample \\(t\\)-test\n",
    "section": "\n16.3 References",
    "text": "16.3 References\n\nCressie, N.A.C. and Whitford, H.J. (1986). How to Use the Two Sample t-Test. Biom. J., 28: 131-148. https://doi.org/10.1002/bimj.4710280202\nXu, M., Fralick, D., Zheng, J. Z., Wang, B., Tu, X. M., & Feng, C. (2017). The Differences and Similarities Between Two-Sample T-Test and Paired T-Test. Shanghai archives of psychiatry, 29(3), 184–188. https://doi.org/10.11919/j.issn.1002-0829.217070",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Two sample $t$-test</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_welch_ttest.html",
    "href": "lessons/03_two_sample_welch_ttest.html",
    "title": "\n17  Two sample Welchs test or Welchs t-test\n",
    "section": "",
    "text": "17.1 Packages for this lesson\nWe will need to load tidyverse (which contains ggplot2 and dplyr) and e1071. We are going to use the mtcars dataset included in r studio.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Two sample Welchs test or Welchs t-test</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_welch_ttest.html#introduction",
    "href": "lessons/03_two_sample_welch_ttest.html#introduction",
    "title": "\n17  Two sample Welchs test or Welchs t-test\n",
    "section": "\n17.2 Introduction",
    "text": "17.2 Introduction\nWelch’s t-test, also known as Welch’s unequal variances t-test, is a statistical test used to determine if there is a significant difference between the means of two independent samples. It is an adaptation of Student’s t-test and is more reliable under the next conditions:\n\n17.2.1 When to use Welch´s t-test\nWelch’s t-test is particularly useful in the following scenarios:\n\n\nThe two samples have unequal variances (heteroscedasticity).\n\nThe two samples have unequal sample sizes.\n\nIn contrast, Student’s t-test assumes that the two samples have equal variances (homoscedasticity). If this assumption is violated, Welch’s t-test provides a more accurate p-value (Moore, McCabe, & Craig, 2016).",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Two sample Welchs test or Welchs t-test</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_welch_ttest.html#assumptions",
    "href": "lessons/03_two_sample_welch_ttest.html#assumptions",
    "title": "\n17  Two sample Welchs test or Welchs t-test\n",
    "section": "\n17.3 Assumptions",
    "text": "17.3 Assumptions\n\n\nIndependence: the observations in each sample should be independent of each other.\n\nNormality: the data in each group should be approximately normally distributed. Welch’s t-test is fairly robust to deviations from normality, especially with larger sample sizes &gt;30.\n\nUnequal Variances: Unlike Student’s t-test, Welch’s t-test does not assume equal variances between the two groups (Newbold, Carlson, & Thorne, 2012).\n\nThe formula for the two-sample Welch’s t-test is given by (Agresti & Franklin, 2017):\n\\[\nt = \\frac{\\bar{X}_1 - \\bar{X}_2}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\n\\]\nWhere: \\[\\begin{align*}\n\\bar{X}_1 & \\text{ is the sample mean of the first group (4-cylinder cars)} \\\\\n\\bar{X}_2 & \\text{ is the sample mean of the second group (6-cylinder cars)} \\\\\ns_1^2 & \\text{ is the sample variance of the first group} \\\\\ns_2^2 & \\text{ is the sample variance of the second group} \\\\\nn_1 & \\text{ is the sample size of the first group} \\\\\nn_2 & \\text{ is the sample size of the second group}\n\\end{align*}\\]\nThe degrees of freedom (DoF) for the Welch’s t-test are calculated using the Satterthwaite approximation (Khan Academy, n.d.):\n\\[\n\\text{DoF} \\approx \\frac{\\left( \\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2} \\right)^2}{\\frac{\\left( \\frac{s_1^2}{n_1} \\right)^2}{n_1 - 1} + \\frac{\\left( \\frac{s_2^2}{n_2} \\right)^2}{n_2 - 1}}\n\\]",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Two sample Welchs test or Welchs t-test</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_welch_ttest.html#welchs-t-test-vs.-students-t-test",
    "href": "lessons/03_two_sample_welch_ttest.html#welchs-t-test-vs.-students-t-test",
    "title": "\n17  Two sample Welchs test or Welchs t-test\n",
    "section": "\n17.4 Welch’s t-Test vs. Student’s t-Test",
    "text": "17.4 Welch’s t-Test vs. Student’s t-Test\n\n\nEqual Variances (Student’s t-Test): Use when you have equal variances and possibly equal sample sizes.\n\nUnequal Variances (Welch’s t-Test): Use when the variances and/or sample sizes are unequal.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Two sample Welchs test or Welchs t-test</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_welch_ttest.html#the-reliability-of-the-welchs-t-test",
    "href": "lessons/03_two_sample_welch_ttest.html#the-reliability-of-the-welchs-t-test",
    "title": "\n17  Two sample Welchs test or Welchs t-test\n",
    "section": "\n17.5 The reliability of the Welch’s t-test",
    "text": "17.5 The reliability of the Welch’s t-test\nLike other statistical tests, can be influenced by the sample size. However, the Welch’s t-test is designed to be more robust than the traditional Student’s t-test, particularly when the sample sizes are unequal and the variances of the two groups are different. Here are some general guidelines to consider (Wikipedia contributors, n.d.):\n\n17.5.1 Minimum Sample Sizes\nSmall Sample Sizes: The Welch’s t-test can be used with small sample sizes, but very small sample sizes (e.g., fewer than 5 per group) can limit the power of the test and make it more difficult to detect significant differences.\nRecommended Minimum: a common recommendation is to have at least 10-15 observations per group to ensure a reasonable level of reliability and power. This isn’t a hard rule, but a guideline to aim for (Glen, 2022).\n\n17.5.2 Power and Effect Size\nPower Analysis: To determine the appropriate sample size for your specific context, you can perform a power analysis. This involves specifying the expected effect size, the desired power (commonly 0.80), and the significance level (commonly 0.05). The power analysis will help you estimate the necessary sample size to detect a significant difference if one exists (Ahad & Yahaya, 2014), (Zhou, Ren, & Brooks, 2023).\n\n17.5.3 Factors Affecting Sample Size Requirements\nEffect Size: larger effect sizes (i.e., larger differences between group means) can be detected with smaller sample sizes. Smaller effect sizes require larger sample sizes to be detected, in others worlds if you want to detect an smaller difference or effect you need more power, and the way to increase it is enlarging the sample, but this is not always feasible in real life due to logistical and cost considerations (Haas, 2012).\nVariance: if the variances of the two groups are very different, larger sample sizes may be needed to ensure the robustness of the test.\nSignificance Level and Power: lowering the significance level (e.g., to 0.01) or increasing the desired power (e.g., to 0.90) typically requires larger sample sizes (Haas, 2012).\nNon normality: it is moderately robust to non-normality, especially as sample sizes increase. For larger samples, the Central Limit Theorem (CLT) helps ensure that the sampling distribution of the test statistic approaches normality, making the test more reliable. For small sample sizes, the reliability of Welch’s t-test can be compromised if the data are significantly non-normal. In such cases, the test may not perform well, and the results might be less trustworthy, in these cases consider using non-parametric alternatives like the Mann-Whitney U test, which does not assume normality. Also consider data transformation and Bootstrap methods, if you want to keep with parametric methods, considering that non-parametric methods have less power.\nIf sample sizes are reasonably large (generally n &gt; 30 per group), Welch’s t-test tends to be reliable even when the data are not perfectly normal (Lund Research Ltd, n.d.).\n\n17.5.4 Let´s remember\nThe significance level, denoted as α, is a threshold in hypothesis testing that determines whether to reject the null hypothesis. It is the probability of making a Type I error, which occurs when the null hypothesis is true but incorrectly rejected. Common values for α are 0.05, 0.01, and 0.10 (Labovitz, 2017).\nFor α = 0.05, there is a 5% chance of rejecting the null hypothesis when it is true. If the p-value from a statistical test is less than or equal to α, we reject the null hypothesis; if it is greater, we do not reject the null hypothesis.\nThe Central Limit Theorem (CLT) states that the distribution of the sample mean will approximate a normal distribution as the sample size becomes large, regardless of the population’s distribution, provided the samples are independent and identically distributed (Kwak & Kim, 2017).\nIn the context of Welch’s test, the CLT implies that as the sample size increases, the distribution of the test statistic approaches normality, reducing the impact of non-normality in the original data. Thus, Welch’s test becomes more reliable with larger samples because the sampling distribution of the mean difference tends to be normal.\nIf the samples are not identically distributed, meaning they come from populations with different distributions, the assumptions underlying the Central Limit Theorem (CLT) and many statistical tests, including Welch’s t-test, may be violated. This can have several consequences:\nReduced Accuracy: The approximation to the normal distribution for the sample mean may not hold, leading to inaccurate p-values and confidence intervals.\nIncreased Type I and Type II Errors: There may be an increased risk of Type I errors (incorrectly rejecting a true null hypothesis) and Type II errors (failing to reject a false null hypothesis).\nBiased Results: The test results may be biased, reflecting the differences in the underlying distributions rather than the true differences between the population means (Ruxton, 2006).\n\n17.5.5 Suspecting Different Variances:\nYou might suspect different variances when comparing two groups (samples) and their spread appears noticeably different. For example, if one group’s data points are more dispersed than the other, it could indicate unequal variances.The welch´s test dont need equal variances to perform well but you can asses variances with graphical methods and using the Levene´s test.\n\n17.5.6 Assessing normality:\nShapiro-Wilk Test: Purpose: Determines if a sample comes from a normally distributed population. Suitability for Small Samples: Good power even with small sample sizes (&lt; 50). How It Works: Compares the sample data to a normal distribution. Interpretation: If p-value &lt; chosen alpha level, data is not normally distributed. Advantages: Sensitive to deviations in both location and shape. Limitations: May detect trivial deviations due to large sample size1 (Razali & Wah, 2011).\nKolmogorov-Smirnov (K-S) Test: Purpose: Compares two samples or tests if a sample matches a reference distribution. Suitability for Small Samples: Useful for small to large sample sizes. How It Works: Quantifies the distance between empirical and reference cumulative distribution functions. Interpretation: Compares observed data to expected distribution. Advantages: Sensitive to differences in both location and shape. Limitations: May not be better than Shapiro-Wilk for small samples (Razali & Wah, 2011).\nAnderson-Darling Test: Purpose: Tests if a sample comes from a specific distribution (e.g., normal). Suitability for Small Samples: Similar to Shapiro-Wilk. How It Works: Compares observed data to expected distribution. Interpretation: Reject null hypothesis if p-value &lt; chosen alpha level. Advantages: Generalizes well for various distributions. Limitations: Adjust for parameter estimation if needed (Razali & Wah, 2011).\nGraphical Inspection: Plot histograms or box plots for each group. Look for differences in spread!!",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Two sample Welchs test or Welchs t-test</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_welch_ttest.html#defyning-the-question-and-hypotheses",
    "href": "lessons/03_two_sample_welch_ttest.html#defyning-the-question-and-hypotheses",
    "title": "\n17  Two sample Welchs test or Welchs t-test\n",
    "section": "\n17.6 Defyning the question and hypotheses",
    "text": "17.6 Defyning the question and hypotheses\nNull hypothesis (H0): 4 cylinder cars and 6 cylinder cars have equal miles per galon (mpg) mean. Alternative hypothesis (HA): 4 cylinder cars and 6 cylinder cars have not equal miles per galon (mpg) mean.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Two sample Welchs test or Welchs t-test</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_welch_ttest.html#dataset-visualization",
    "href": "lessons/03_two_sample_welch_ttest.html#dataset-visualization",
    "title": "\n17  Two sample Welchs test or Welchs t-test\n",
    "section": "\n17.7 Dataset visualization",
    "text": "17.7 Dataset visualization\nWe will use the mtcars dataset in R for this demonstration. This dataset contains various attributes of different car models and at priori we are not sure if the assumptions are meet, so we have to assess the data first:\n\n\n'data.frame':   32 obs. of  11 variables:\n $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n $ disp: num  160 160 108 258 360 ...\n $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n $ qsec: num  16.5 17 18.6 19.4 17 ...\n $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\n\n\nAs we can see, we have a data set with the information about different types of cars, with 11 different variables including miles per gallon (mpg), cylinder (cyl), horse power (hp), etc., and there are 32 observations for each variable.\n\n\n      mpg             cyl             disp             hp       \n Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n 1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \n Median :19.20   Median :6.000   Median :196.3   Median :123.0  \n Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n 3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \n Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n      drat             wt             qsec             vs        \n Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n 1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \n Median :3.695   Median :3.325   Median :17.71   Median :0.0000  \n Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n 3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \n Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n       am              gear            carb      \n Min.   :0.0000   Min.   :3.000   Min.   :1.000  \n 1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  \n Median :0.0000   Median :4.000   Median :2.000  \n Mean   :0.4062   Mean   :3.688   Mean   :2.812  \n 3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  \n Max.   :1.0000   Max.   :5.000   Max.   :8.000",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Two sample Welchs test or Welchs t-test</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_welch_ttest.html#plotting-mtcars-data",
    "href": "lessons/03_two_sample_welch_ttest.html#plotting-mtcars-data",
    "title": "\n17  Two sample Welchs test or Welchs t-test\n",
    "section": "\n17.8 Plotting Mtcars data",
    "text": "17.8 Plotting Mtcars data\n\n\n\n\nCars by gears and cylinders\n\n\n\nOn this bar graph we observe the count of cars by number of cylinders and gears.\n\n\n\n\nmpg v.s hp\n\n\n\nJust to give us an idea of the data set, on this plot we observed the relation between horse power and cylinders. The cars with more cylinders have more hp and probably do less miles per gallon.\n\n17.8.1 Assessing the distribution\n\n\n\n\nHistogram 4-cylinder vs 6-cylinder\n\n\n\nThis histogram shows there is a difference distribution on the miles per gallon (MPG) variable between 4 and 6 cylinder cars. 6 cylinder cars appear to have a normal distribution but 4 cylinder cars not.\n\n\n\n\nBoxplot 4-cylinder vs 6 cylinder\n\n\n\nThis boxplot also shows the difference between the two groups. 4 cylinder cars have a bigger variance on the mpg variable.\n\n\n\n\nQ-Q Plot 4-cylinder vs 6 cylinder\n\n\n\nThis Q-Q plots shows a red line and a blue line, if both diverge it means the two groups have different distributions.\n\n17.8.2 Assessing skewness and kurtosis\n\n# Calculate skewness and kurtosis for mpg_4_cyl\nskewness_4_cyl &lt;- skewness(mpg_4_cyl)\nkurtosis_4_cyl &lt;- kurtosis(mpg_4_cyl)\n\n# Calculate skewness and kurtosis for mpg_6_cyl\nskewness_6_cyl &lt;- skewness(mpg_6_cyl)\nkurtosis_6_cyl &lt;- kurtosis(mpg_6_cyl)\n\n# Print the results\ncat(\"Skewness for mpg_4_cyl:\", skewness_4_cyl, \"\\n\")\n\nSkewness for mpg_4_cyl: 0.2591965 \n\ncat(\"Kurtosis for mpg_4_cyl:\", kurtosis_4_cyl, \"\\n\")\n\nKurtosis for mpg_4_cyl: -1.645012 \n\ncat(\"Skewness for mpg_6_cyl:\", skewness_6_cyl, \"\\n\")\n\nSkewness for mpg_6_cyl: -0.1583137 \n\ncat(\"Kurtosis for mpg_6_cyl:\", kurtosis_6_cyl, \"\\n\")\n\nKurtosis for mpg_6_cyl: -1.906971 \n\n\nSkewness: Skewness measures the asymmetry of the distribution of a variable. A normal distribution has a skewness value of zero, indicating symmetry. Positive skewness means the right tail is longer (values cluster to the left of the mean), while negative skewness means the left tail is longer. If skewness is substantial (e.g., greater than 2.1), it suggests departure from normality. It is not the case here!!\nKurtosis: Kurtosis measures the peakedness of a distribution. The original kurtosis value is sometimes called “kurtosis (proper).” A normal distribution has kurtosis (proper) equal to 3. Excess kurtosis (obtained by subtracting 3 from the proper kurtosis) is often used. Substantial departure from normality occurs when excess kurtosis is greater than 7.1\n\n17.8.3 Normality test\nShapiro-Wilk Test:\n\n# Perform Shapiro-Wilk normality test\nshapiro.test(mpg_4_cyl)\n\n\n    Shapiro-Wilk normality test\n\ndata:  mpg_4_cyl\nW = 0.91244, p-value = 0.2606\n\nshapiro.test(mpg_6_cyl)\n\n\n    Shapiro-Wilk normality test\n\ndata:  mpg_6_cyl\nW = 0.89903, p-value = 0.3252\n\n\nFor mpg_4_cyl: the test statistic (W = 0.91244) indicates that the data is relatively close to a normal distribution. The p-value (0.2606) is not statistically significant (above the typical threshold of 0.05). Interpretation: The data for mpg_4_cyl is not significantly different from a normal distribution. However, with small sample sizes, the test may have limited power to detect departures from normality.\nFor mpg_6_cyl: the test statistic (W = 0.89903) is slightly lower than for mpg_4_cyl but still indicates a relatively normal distribution. The p-value (0.3252) is not statistically significant (above 0.05). Interpretation: The data for mpg_6_cyl is also not significantly different from a normal distribution based on the Shapiro-Wilk test.\nAgain, note that with small sample sizes, the ability to detect deviations from normality may be limited. Small sample sizes can limit the power of statistical tests to detect departures from normality, and it’s important to consider the context and potential limitations when interpreting such results.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Two sample Welchs test or Welchs t-test</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_welch_ttest.html#performing-the-welchs-t-test",
    "href": "lessons/03_two_sample_welch_ttest.html#performing-the-welchs-t-test",
    "title": "\n17  Two sample Welchs test or Welchs t-test\n",
    "section": "\n17.9 Performing the Welchs t-test",
    "text": "17.9 Performing the Welchs t-test\nWe will perform a Welch’s t-test to compare the mean miles per gallon (mpg) between cars with 4 cylinders and cars with 6 cylinders.\nHypotheses Null Hypothesis (H0): The mean mpg of cars with 4 cylinders is equal to the mean mpg of cars with 6 cylinders. Alternative Hypothesis (H1): The mean mpg of cars with 4 cylinders is not equal to the mean mpg of cars with 6 cylinders.\n\n17.9.1 Welchs test results\n\n## Extract the mpg values for cars with 4 and 6 cylinders\nmpg_4_cyl &lt;- mtcars$mpg[mtcars$cyl == 4]\nmpg_6_cyl &lt;- mtcars$mpg[mtcars$cyl == 6]\n\n## Perform Welch's t-test\nt_test_result &lt;- t.test(mpg_4_cyl, mpg_6_cyl, var.equal = FALSE)\nt_test_result\n\n\n    Welch Two Sample t-test\n\ndata:  mpg_4_cyl and mpg_6_cyl\nt = 4.7191, df = 12.956, p-value = 0.0004048\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n  3.751376 10.090182\nsample estimates:\nmean of x mean of y \n 26.66364  19.74286 \n\n## Extract confidence interval\nconf_interval &lt;- t_test_result$conf.int\nconf_interval\n\n[1]  3.751376 10.090182\nattr(,\"conf.level\")\n[1] 0.95\n\n\n\n17.9.2 Interpretation or results:\nLet’s break down the interpretation of the Welch Two Sample t-test results for this data:\nData: The test was performed on two samples, mpg_4_cyl and mpg_6_cyl.\nt-value: the t-test statistic is 4.7191.\nDegrees of Freedom (df): the degrees of freedom associated with the t-test statistic is approximately 12.956.\np-value: the p-value is 0.0004048. This small p-value suggests strong evidence against the null hypothesis.\nAlternative Hypothesis: the alternative hypothesis states that the true difference in means between the two groups is not equal to zero.\n95% Confidence Interval: The confidence interval for the true difference in means lies between 3.751376 and 10.090182. (Always have in mind the repeated sample paradigm)\nSample Estimates: The sample mean of mpg_4_cyl is 26.66364, and the sample mean of mpg_6_cyl is 19.74286.\nIn summary, based on the p-value, we reject the null hypothesis and conclude that there is a significant difference in means between the two groups. The confidence interval provides a range for this difference. If the interval does not include zero, it supports the alternative hypothesis.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Two sample Welchs test or Welchs t-test</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_welch_ttest.html#satterthwaite-degrees-of-freedom",
    "href": "lessons/03_two_sample_welch_ttest.html#satterthwaite-degrees-of-freedom",
    "title": "\n17  Two sample Welchs test or Welchs t-test\n",
    "section": "\n17.10 Satterthwaite Degrees of Freedom",
    "text": "17.10 Satterthwaite Degrees of Freedom\nIn the context of Welch’s t-test, the Satterthwaite approximation is used to calculate an approximation of the degrees of freedom. This method provides a more accurate estimation compared to the standard t-test when the variances of the two samples are not equal.\nThe formula for the Satterthwaite degrees of freedom is: \\[\n\\nu \\approx \\frac{\\left( \\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2} \\right)^2}{\\frac{\\left( \\frac{s_1^2}{n_1} \\right)^2}{n_1 - 1} + \\frac{\\left( \\frac{s_2^2}{n_2} \\right)^2}{n_2 - 1}}\n\\]\nWhere: \\[ s_1^2 \\] is the sample variance of the first sample (4-cylinder cars), \\[ s_2^2 \\] is the sample variance of the second sample (6-cylinder cars), \\[ n_1 \\] is the sample size of the first sample (4-cylinder cars) and \\[ n_2 \\] is the sample size of the second sample (6-cylinder cars).\n\n# Sample sizes\nn1 &lt;- length(mpg_4_cyl)\nn2 &lt;- length(mpg_6_cyl)\n\n# Sample variances\n\ns1_sq &lt;- var(mpg_4_cyl)\ns2_sq &lt;- var(mpg_6_cyl)\n\n# Satterthwaite degrees of freedom\nnumerator &lt;- (s1_sq / n1 + s2_sq / n2)^2\ndenominator &lt;- ((s1_sq / n1)^2 / (n1 - 1)) + ((s2_sq / n2)^2 / (n2 - 1))\ndf &lt;- numerator / denominator\n\n# Print the result\ndf\n\n[1] 12.95598\n\n\n\n17.10.1 Degrees of Freedom (DoF) explanation:\nThe degrees of freedom indicate the number of independent values or quantities which can vary in the analysis without breaking any constraints. In this context, the degrees of freedom are adjusted to better reflect the reliability of the variance estimates from the two samples (Huang, 2016).\nSatterthwaite Approximation: this method provides an adjusted degrees of freedom value that accounts for differences in variances between the two samples. The formula combines the sample variances and sizes to compute a more accurate degrees of freedom for the Welch’s t-test.\nImplications: using the Satterthwaite approximation leads to a more robust test when comparing means from two samples with unequal variances. The resulting degrees of freedom are used to determine the critical value from the t-distribution, which is crucial for calculating the p-value and making statistical inferences (Derrick, Toher, & White, 2016).\nDegrees of Freedom Calculation for the two sample t-test: can be calculated as follows:\n\\[\n\\text{DoF} = (n_1 - 1) + (n_2 - 1) = (n_1 + n_2 - 2)\n\\]\nWhere: \\[ n_1 \\] is the number of observations in the first group. \\[ n_2 \\] is the number of observations in the second group.\nWhy Subtract the Number of Groups?:\nWe subtract 2 because we are estimating one parameter (the mean) for each of the two groups. Each estimation reduces the degrees of freedom by 1. Thus, for two groups, we subtract 2 from the total number of observations to account for the two estimated means.\nSatterthwaite vs t-test degrees of freedom:\nCalculated Degrees of Freedom using the Satterthwaite method: approximately 12.956\nTraditional t-test Degrees of Freedom: n1+n2-2=11+7-2=16\nThe Satterthwaite degrees of freedom (12.956) are lower than the traditional degrees of freedom (16). This adjustment accounts for the unequal variances between the two samples (4-cylinder and 6-cylinder cars) and provides a more accurate measure for the t-distribution used in the Welch’s t-test.\n\n17.10.2 Important concepts about the degrees of Freedom\n\n\nDegrees of Freedom (DoF)\n\n\nRepresent the number of independent values that can vary without breaking constraints.\n\n\nTraditional t-Test\n\n\nFor two groups, the DoF is the total number of observations minus 2 because each group’s mean estimation uses one degree of freedom.\n\n\nSatterthwaite Approximation\n\n\nAdjusts the DoF for unequal variances, usually resulting in a lower and non-integer value, providing a more accurate basis for hypothesis testing.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Two sample Welchs test or Welchs t-test</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_welch_ttest.html#summary-and-conclusion",
    "href": "lessons/03_two_sample_welch_ttest.html#summary-and-conclusion",
    "title": "\n17  Two sample Welchs test or Welchs t-test\n",
    "section": "\n17.11 Summary and Conclusion",
    "text": "17.11 Summary and Conclusion\nWelch’s t-test, useful for comparing means of two independent samples with unequal variances and sizes, was applied to the mtcars dataset to compare 4-cylinder and 6-cylinder cars’ MPG. The test revealed a significant difference in means, with a p-value of 0.0004048, rejecting the null hypothesis. The Satterthwaite approximation, yielding degrees of freedom at 12.956, provided a more accurate assessment than the traditional t-test (DoF = 16), ensuring robust statistical inference under unequal variances. This method enhances reliability, especially with large samples, where normality is approached due to the Central Limit Theorem.\n\n\n\n\nAgresti, A., & Franklin, C. (2017). Statistics: The art and science of learning from data (4th ed.). Pearson.\n\n\nAhad, N. A., & Yahaya, S. S. S. (2014). Sensitivity analysis of welch’st-test. In AIP conference proceedings (Vol. 1605, pp. 888–893). American Institute of Physics.\n\n\nDerrick, B., Toher, D., & White, P. (2016). Why welch’s test is type I error robust. The quantitative methods for. Psychology, 12(1), 30–38.\n\n\nGlen, S. (2022). Welch’s t-test: Definition, formula and example.\n\n\nHaas, J. P. (2012). Sample size and power. Am. J. Infect. Control, 40(8), 766–767.\n\n\nHuang, H. (2016). On the Welch-Satterthwaite formula for uncertainty estimation: A paradox and its resolution. Cal Lab the International Journal of Metrology, 23(4), 20–28.\n\n\nKhan Academy. (n.d.). Statistics and probability.\n\n\nKwak, S. G., & Kim, J. H. (2017). Central limit theorem: The cornerstone of modern statistics. Korean J. Anesthesiol., 70(2), 144–156.\n\n\nLabovitz, S. (2017). Criteria for selecting a significance level:: A note on the sacredness of. 05. In The significance test controversy (pp. 166–171).\n\n\nLund Research Ltd. (n.d.). Welch’s t-test.\n\n\nMoore, D. S., McCabe, G. P., & Craig, B. A. (2016). Introduction to the practice of statistics (9th ed.). W. H. Freeman; Company.\n\n\nNewbold, P., Carlson, W. L., & Thorne, B. (2012). Statistics for business and economics (8th ed.). Pearson.\n\n\nRazali, N. M., & Wah, Y. B. (2011). Power comparisons of shapiro-wilk, kolmogorov-smirnov, lilliefors and anderson-darling tests. Journal of Statistical Modeling and Analytics, 2(1), 21–33.\n\n\nRuxton, G. D. (2006). The unequal variance t-test is an underused alternative to student’s t-test and the mann–whitney u test. Behavioral Ecology, 17(4), 688–690. https://doi.org/10.1093/beheco/ark016\n\n\nWikipedia contributors. (n.d.). Welch’s t-test.\n\n\nZhou, Y., Ren, X., & Brooks, G. (2023). Which effect size calculation is the best to estimate the population effect size in the welch T test? J. Mod. Appl. Stat. Methods, 22.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Two sample Welchs test or Welchs t-test</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_mann_whitney.html",
    "href": "lessons/03_two_sample_mann_whitney.html",
    "title": "\n18  Mann-Whitney-U Test Example\n",
    "section": "",
    "text": "18.1 Introduction\nOverall goal: Identify whether the distribution of two groups significantly differs.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Mann-Whitney-U Test Example</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_mann_whitney.html#introduction",
    "href": "lessons/03_two_sample_mann_whitney.html#introduction",
    "title": "\n18  Mann-Whitney-U Test Example\n",
    "section": "",
    "text": "Mann Whitney U test, also known as the Wilcoxon Rank-Sum test, is a nonparametric statistical test of the null hypothesis, which is commonly used to compare the means or medians of two independent groups with the assumption that the at least one group is not normally distributed and when the sample size is small.\n\nThe Welch U test should be used when signs of skewness and variance of heterogeneity.\n\n\n\n\nIt is useful for numerical/continuous variables.\n\nFor example, if researchers want to compare the age or height of two different groups (as continuous variables) in a study with non-normally distributed data.\n\n\n\nWhen conducting this test, aside from reporting the p-value, the spread and the shape of the data should be described.\n\n\n\n18.1.0.1 Assumptions\n\nSamples are independent: Each dependent variable must be related to only one independent variable.\nThe response variable is ordinal or continuous.\nAt least one variable is not normally distributed.\n\n18.1.1 Hypotheses\nNull Hypothesis (H0): Distribution1 = Distribution2\n\nMean/Median Ranks of two levels are equal.\n\n\nAlternate Hypothesis (H1): Distribution1 ≠ Distribution2\n\nMean/Median Ranks of two levels are significantly different.\n\n\n\n18.1.1.1 Mathematical Equation\n\\(U_1 = n_1n_2 + \\frac{n_1 \\cdot (n_1 + 1)}{2} - R_1\\)\n\\(U_2 = n_1n_2 + \\frac{n_2 \\cdot (n_2 + 1)}{2} - R_2\\)\nWhere:\n\n\n\\(U_1\\) and \\(U_2\\) represent the test statistics for two groups (Male & Female).\n\n\\(R_1\\) and \\(R_2\\) represent the sum of the ranks of the observations for two groups.\n\n\\(n_1\\) and \\(n_2\\) are the sample sizes for two groups.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Mann-Whitney-U Test Example</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_mann_whitney.html#performing-mann-whitney-u-test-in-r",
    "href": "lessons/03_two_sample_mann_whitney.html#performing-mann-whitney-u-test-in-r",
    "title": "\n18  Mann-Whitney-U Test Example\n",
    "section": "\n18.2 Performing Mann-Whitney U Test in R",
    "text": "18.2 Performing Mann-Whitney U Test in R\n\n18.2.1 Data Source\nIn this example, we will perform the Mann-Whitney U Test using wave 8 (2012-2013) data of a longitudinal epidemiological study titled Hispanic Established Populations For the Epidemiological Study of Elderly (HEPESE).\nThe HEPESE provides data on risk factors for mortality and morbidity in Mexican Americans in order to contrast how these factors operate differently in non-Hispanic White Americans, African Americans, and other major ethnic groups. The data is publicly available and can be obtained from the University of Michigan website. For the purposes of this report/chapter, the example in the analysis uses synthetic data. Using this data, we want to explore whether there are significant gender differences in age when Type 2 diabetes mellitus (T2DM) is diagnosed. Type 2 diabetes is a chronic disease condition that has affected 37 million people living in the United States. Type 2 diabetes is the eighth leading cause of death and disability in US. Type 2 diabetes generally occurs among adults aged 45 or older, but may also occur amongst young adults and children. Diabetes and its complications are often preventable by following lifestyle guidelines and taking medication in a timely manner. 1 in 5 of US people don’t know they have diabetes.\nResearch has shown that men are more likely to develop type 2 diabetes, while women are more likely to experience complications from type 2 diabetes, including heart and kidney disease.\nIn this report, we want to test whether there are significant differences in age at which diabetes is diagnosed among males and females.\nDependent Response Variable: ageAtDx = Age_Diagnosed = Age at which diabetes is diagnosed.\nIndependent Variable: isMale = Gender\nResearch Question: Does the age at which diabetes is diagnosed significantly differ among men and women?\nNull Hypothesis (H0): Mean rank of age at which diabetes is diagnosed is equal among men and women.\nAlternate Hypothesis (H1): Mean rank of age at which diabetes is diagnosed is not equal among men and women.\n\n18.2.2 Packages\n\ngmodels: Helps to compute and display confidence intervals (CI) for model estimates.\nDescTools: Provides tools for basic statistics e.g. to compute Median CI for an efficient data description.\nggplot2: Helps to create boxplots.\nqqplotr: Helps to create QQ plot.\ndplyr: Used to manipulate data and provide summary statistics.\nhaven: Helps to import SPSS data into r.\n\nDependencies = TRUE : Indicates that while installing packages, it must also install all dependencies of the specified package.\n\n# install.packages(\"gmodels\", dependencies = TRUE)\n# install.packages(\"car\", dependencies = TRUE)\n# install.packages(\"DescTools\", dependencies = TRUE)\n# install.packages(\"ggplot2\", dependencies = TRUE)\n# install.packages(\"qqplotr\", dependencies = TRUE)\n# install.packages(\"gtsummary\", dependencies = TRUE)\n\nLoading Library\n\nsuppressPackageStartupMessages(library(haven))\nsuppressPackageStartupMessages(library(ggpubr))\nsuppressPackageStartupMessages(library(gmodels))\nsuppressPackageStartupMessages(library(DescTools))\nsuppressPackageStartupMessages(library(ggplot2))\nsuppressPackageStartupMessages(library(qqplotr))\nsuppressPackageStartupMessages(library(gtsummary))\nsuppressPackageStartupMessages(library(tidyverse))\n\nData Importing\n\nHEPESE &lt;- read_csv(\"../data/03_HEPESE_synthetic_20240510.csv\")\n\nRows: 744 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (1): ageAtDx\nlgl (1): isMale\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n18.2.3 Data Exploration\n\n# str(HEPESE)\nstr(HEPESE$isMale)\n\n logi [1:744] FALSE FALSE FALSE TRUE FALSE TRUE ...\n\nstr(HEPESE$ageAtDx)\n\n num [1:744] 87 70 68 60 55 33 38 65 50 68 ...\n\n\nAfter inspecting the data, we found that values of our dependent and independent variable values are in character format. We want them to be numerical and categorical, respectively. First, we will convert the dependent variable into numerical form, and our independent variable into categorical. Then, we will recode the factors as male and female. For simplicity’s sake, we will also rename our dependent and independent variable.\n\n# convert to number and factor\nHEPESE$ageAtDx &lt;- as.numeric(HEPESE$ageAtDx)\nclass(HEPESE$ageAtDx)\n\n[1] \"numeric\"\n\nHEPESE$isMale &lt;- as_factor(HEPESE$isMale)\nclass(HEPESE$isMale)\n\n[1] \"factor\"\n\n\nThe next step is to calculate some of the descriptive data to give us a better idea of the data that we are dealing with. This can be done using the summarise function.\nDescriptive Data\n\nDes &lt;- \n HEPESE %&gt;% \n select(isMale, ageAtDx) %&gt;% \n group_by(isMale) %&gt;%\n summarise(\n   n = n(),\n   mean = mean(ageAtDx, na.rm = TRUE),\n   sd = sd(ageAtDx, na.rm = TRUE),\n   stderr = sd/sqrt(n),\n   LCL = mean - qt(1 - (0.05 / 2), n - 1) * stderr,\n   UCL = mean + qt(1 - (0.05 / 2), n - 1) * stderr,\n   median = median(ageAtDx, na.rm = TRUE),\n   min = min(ageAtDx, na.rm = TRUE), \n   max = max(ageAtDx, na.rm = TRUE),\n   IQR = IQR(ageAtDx, na.rm = TRUE),\n   LCLmed = MedianCI(ageAtDx, na.rm = TRUE)[2],\n   UCLmed = MedianCI(ageAtDx, na.rm = TRUE)[3]\n )\n\nDes\n\n# A tibble: 2 × 13\n  isMale     n  mean    sd stderr   LCL   UCL median   min   max   IQR LCLmed\n  &lt;fct&gt;  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 FALSE    455  67.6  14.1  0.661  66.3  68.9     70    18    93    19     68\n2 TRUE     289  67.1  15.1  0.886  65.3  68.8     70    20    94    18     68\n# ℹ 1 more variable: UCLmed &lt;dbl&gt;\n\n\n\nn: The number of observations for each gender.\nmean: The mean age when diabetes is diagnosed for each gender.\nsd: The standard deviation of each gender.\nstderr: The standard error of each gender level. That is the standard deviation / sqrt (n).\nLCL, UCL: The upper and lower confidence intervals of the mean. This values indicates the range at which we can be 95% certain that the true mean falls between the lower and upper values specified for each gender group assuming a normal distribution.\nmedian: The median value for each gender.\nmin, max: The minimum and maximum value for each gender.\nIQR: The interquartile range of each gender. That is the 75th percentile – 25th percentile.\nLCLmed, UCLmed: The 95% confidence interval for the median.\n\nChecking Assumptions and Visualizing the Data\nThe next step is to visualize the data. This can be done using different functions under the ggplot package.\n1) Box plot\n\nggplot(\n HEPESE, \n aes(\n   x = isMale, \n   y = ageAtDx, \n   fill = isMale\n )\n) +\n stat_boxplot(\n   geom = \"errorbar\", \n   width = 0.5\n ) +\n geom_boxplot(\n   fill = \"light blue\"\n ) + \n stat_summary(\n   fun = mean, \n   geom = \"point\", \n   shape = 10, \n   size = 3.5, \n   color = \"black\"\n ) + \n ggtitle(\n   \"Boxplot of Gender\"\n ) + \n theme_bw() + \n theme(\n   legend.position = \"none\"\n )\n\n\n\n\n\n\n\n2) QQ plot\n\nlibrary(conflicted)\nconflict_prefer(\"stat_qq_line\", \"qqplotr\", quiet = TRUE)\n\n\n# Perform QQ plots by group\nQQ_Plot &lt;- \nggplot(\n data = HEPESE, \n aes(\n   sample = ageAtDx, \n   color = isMale, \n   fill = isMale\n )\n) +\n stat_qq_band(\n   alpha = 0.5, \n   conf = 0.95, \n   qtype = 1, \n   bandType = \"boot\"\n ) +\n stat_qq_line(\n   identity = TRUE\n ) +\n stat_qq_point(\n   col = \"black\"\n ) +\n facet_wrap(\n   ~ isMale, scales = \"free\"\n ) +\n labs(\n   x = \"Theoretical Quantiles\", \n   y = \"Sample Quantiles\"\n ) + theme_bw()\n\nQQ_Plot\n\n\n\n\n\n\n\n\nstat_qq_line: Draws a reference line based on the data quantiles.\n\nStat_qq_band: Draws confidence bands based on three methods; “pointwise”/“boot”,“Ks” and “ts”.\n\n\"pointwise\" constructs simultaneous confidence bands based on the normal distribution;\n\"boot\" creates pointwise confidence bands based on a parametric boostrap;\n\"ks\" constructs simultaneous confidence bands based on an inversion of the Kolmogorov-Smirnov test;\n\"ts\" constructs tail-sensitive confidence bands\n\n\n\nStat_qq_Point: Is a modified version of ggplot: : stat_qq with some parameters adjustments and a new option to detrend the points.\n3) Histogram\nA histogram is the most commonly used graph to show frequency distributions.\n\n\n\nggplot(HEPESE) +\n  aes(x = ageAtDx, fill = isMale) +\n  geom_histogram() +\n  facet_wrap(~ isMale) \n\n\n\n\n\n\n\n**3b) Density curve in Histogram**\nA density curve gives us a good idea of the “shape” of a distribution, including whether or not a distribution has one or more “peaks” of frequently occurring values and whether or not the distribution is skewed to the left or the right.\n\nggplot(HEPESE) +\n  aes(\n    x = ageAtDx,\n    fill = isMale\n  ) +\n  labs(\n    x = \"Age When diabetes is diagnosed\",\n    y = \"Density\",\n    fill = \"Gender\",\n    title = \"A Density Plot of Age when diabetes is diagnosed\",\n    caption = \"Data Source: HEPESE Wave 8 (ICPSR 36578)\"\n  ) + \n  geom_density() +\n  facet_wrap(~isMale)\n\n\n\n\n\n\n\nThis density curve shows that our data does not have a bell shaped distribution and it is slightly skewed towards the left.\n4) Statistical test for normality\n\nHEPESE %&gt;%\n  group_by(isMale) %&gt;%\n  summarise(\n    `W Stat` = shapiro.test(ageAtDx)$statistic,\n    p.value = shapiro.test(ageAtDx)$p.value\n  )\n\n# A tibble: 2 × 3\n  isMale `W Stat`  p.value\n  &lt;fct&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 FALSE     0.959 6.50e-10\n2 TRUE      0.937 9.99e-10\n\n\nInterpretation\nFrom the above table, we see that the value of the Shapiro-Wilk Test is 0.0006 and 0.000002, which are both less than 0.05. Therefore we have enough evidence to reject the null hypothesis and confirm that the data significantly deviates from a normal distribution.\n\n18.2.4 Mann Whitney U Test\n\nresult &lt;- wilcox.test(\n  ageAtDx ~ isMale, \n  data = HEPESE, \n  na.rm = TRUE, \n  exact = FALSE, \n  conf.int = TRUE\n)\n\ntibble(\n  Test_Statistic = result$statistic,\n  P_Value = result$p.value,\n  Method = result$method\n)\n\n# A tibble: 1 × 3\n  Test_Statistic P_Value Method                                           \n           &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                                            \n1          66178   0.880 Wilcoxon rank sum test with continuity correction",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Mann-Whitney-U Test Example</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_mann_whitney.html#results",
    "href": "lessons/03_two_sample_mann_whitney.html#results",
    "title": "\n18  Mann-Whitney-U Test Example\n",
    "section": "\n18.3 Results",
    "text": "18.3 Results\nWhile the analysis above is for synthetic data, we see that the mean age at which diabetes is diagnosed is not significantly different in males (69 years old) and females (66 years old). Of note, the Mann-Whitney U-Test applied in the real data (not shown in this report) showed that this difference is not statistically significant at 0.05 level of significance because the statistical p value (p=.155) is greater than the critical value (p=0.05). For the real data, the test statistic is W = 5040.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Mann-Whitney-U Test Example</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_mann_whitney.html#conclusion",
    "href": "lessons/03_two_sample_mann_whitney.html#conclusion",
    "title": "\n18  Mann-Whitney-U Test Example\n",
    "section": "\n18.4 Conclusion",
    "text": "18.4 Conclusion\nFrom the above result, we can conclude that gender does not play a significant role in the age at which one is diagnosed with diabetes. Diabetes is the 8th leading cause of death and disability in the US, and 1 in 5 US adults are currently unaware of their diabetes condition. This urges the need for increased policy efforts towards timely diabetes testing and diagnosing. Although previous research has suggested that there are gender based differences in diabetes related severity of inquiries, our findings suggest that this difference is not due to age, and may be due to other gender based differences, such as willingness to seek medical care, underlying health issues, etc. There may not necessarily be a need for gender-based approaches to interventions aimed at increasing diabetes surveillance, and efforts should focus on targeting the population as a whole.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Mann-Whitney-U Test Example</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_bootstrap_conf_int.html",
    "href": "lessons/03_two_sample_bootstrap_conf_int.html",
    "title": "\n19  Bootstrap Tests\n",
    "section": "",
    "text": "19.1 Introduction to Bootstrapping\nBootstrapping, introduced by Brad Efron in 1979, is founded on a straightforward concept: when our data is a sample from a larger population, why not generate additional samples by resampling from our existing data? However, since we lack access to new data, we resort to repeatedly sampling from our dataset with replacement.\nThe primary objective of bootstrapping is to augment the sample size for analysis, particularly in scenarios where the provided sample size is limited.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Bootstrap Tests</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_bootstrap_conf_int.html#mathematical-definition",
    "href": "lessons/03_two_sample_bootstrap_conf_int.html#mathematical-definition",
    "title": "\n19  Bootstrap Tests\n",
    "section": "\n19.2 Mathematical definition",
    "text": "19.2 Mathematical definition\nBootstrapping involves randomly selecting n observations from a sample with replacement to create a bootstrap sample. The process of sampling with replacement allows each observation in the original dataset to be selected multiple times or not at all in the bootstrap sample.\nOnce a bootstrap sample is obtained, the statistic of interest (e.g., mean, median, standard deviation) is calculated from this sample. This process is repeated multiple times to generate a distribution of the statistic under the assumption that the original dataset is representative of the population.\nBootstrapping can be used to construct confidence intervals for a population parameter (e.g., mean, median) by calculating the desired quantiles (e.g., percentiles) of the bootstrap distribution of the statistic.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Bootstrap Tests</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_bootstrap_conf_int.html#penguins-data",
    "href": "lessons/03_two_sample_bootstrap_conf_int.html#penguins-data",
    "title": "\n19  Bootstrap Tests\n",
    "section": "\n19.3 penguins Data",
    "text": "19.3 penguins Data\nThe penguins dataset contains measurements collected in the Palmer Archipelago of Antarctica, made available by Dr. Kristen Gorman and the Palmer Station Long Term Ecological Research (LTER) Program. This dataset, included in the palmerpenguins package, comprises observations on various attributes of penguins, including species, island of origin, physical measurements (such as flipper length, body mass, and bill dimensions), sex, and year of observation. In total, the dataset consists of 344 rows and 8 variables.\nIn this tests we are going to obtain the 95% confidence interval for flipper length of the Adelie penguin from two different Islands.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Bootstrap Tests</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_bootstrap_conf_int.html#cleaning-the-data",
    "href": "lessons/03_two_sample_bootstrap_conf_int.html#cleaning-the-data",
    "title": "\n19  Bootstrap Tests\n",
    "section": "\n19.4 Cleaning the data",
    "text": "19.4 Cleaning the data\n\nCodenew_penguins_df &lt;- \n  filter(penguins, species == \"Adelie\", island != \"Dream\") %&gt;% \n  select(species, island, flipper_length_mm) %&gt;% \n  arrange(island, .by_group = TRUE) %&gt;% \n  drop_na()\n\nview(new_penguins_df)\n\n\nThe island of Dream penguin population was excluded because their population size was much larger compared to Torgersen and Biscoe populations.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Bootstrap Tests</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_bootstrap_conf_int.html#assumptions-of-bootstrap",
    "href": "lessons/03_two_sample_bootstrap_conf_int.html#assumptions-of-bootstrap",
    "title": "\n19  Bootstrap Tests\n",
    "section": "\n19.5 Assumptions of Bootstrap",
    "text": "19.5 Assumptions of Bootstrap\n\nThe dataset is a random sample drawn representative of the population of interest.\nResampling with replacement accurately simulates the sampling process.\nObservations in the dataset are independent and identically distributed",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Bootstrap Tests</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_bootstrap_conf_int.html#checking-distribution",
    "href": "lessons/03_two_sample_bootstrap_conf_int.html#checking-distribution",
    "title": "\n19  Bootstrap Tests\n",
    "section": "\n19.6 Checking Distribution",
    "text": "19.6 Checking Distribution\n\nCode# check the boxplot of the data\nboxplot(\n  new_penguins_df$flipper_length_mm ~ new_penguins_df$island, las = 1, \n  ylab = \"Flipper Length (mm)\",\n  xlab = \"Island\",\n  main = \"Flipper Length by Island\"\n)\n\n\n\n\n\n\nCode# check the histogram of the data\nhist(\n  x = new_penguins_df$flipper_length_mm,\n  main = \"Distribution of Flipper Length (mm)\",\n  xlab = \"Flipper Length\"\n)",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Bootstrap Tests</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_bootstrap_conf_int.html#code-to-run-bootstrap",
    "href": "lessons/03_two_sample_bootstrap_conf_int.html#code-to-run-bootstrap",
    "title": "\n19  Bootstrap Tests\n",
    "section": "\n19.7 Code to run Bootstrap",
    "text": "19.7 Code to run Bootstrap\n\nCode# set a seed so that our random results can be replicated by other people:\nset.seed(20150516)\n\n# take a random re-sample of the data that is the *same size*\nN &lt;- length(new_penguins_df$flipper_length_mm)\n\n# a random sample:\nsample(new_penguins_df$flipper_length_mm, size = N, replace = TRUE)\n\n [1] 184 192 198 195 195 176 188 183 184 193 199 198 184 190 198 195 195 199 193\n[20] 197 198 189 197 188 189 199 200 190 183 198 194 190 191 196 189 195 198 197\n[39] 191 184 198 180 195 186 193 193 191 195 190 198 189 181 197 196 182 200 188\n[58] 184 202 189 197 186 181 195 181 191 185 193 196 185 192 199 186 196 180 190\n[77] 190 195 197 193 191 181 195 190 186 189 192 187 190 195 195 182 172 194 181\n\nCode# number of bootstrap samples\nB_int &lt;- 10000\n\n# create a list of these thousands of samples \nbootstrapSamples_ls &lt;- map(\n  .x = 1:B_int,\n  .f = ~{\n    sample(new_penguins_df$flipper_length_mm, size = N, replace = TRUE)\n  }\n)\n\n# subset of the random samples\nbootstrapSamples_ls[1:3]\n\n[[1]]\n [1] 183 190 189 188 181 198 181 172 187 189 189 193 180 197 191 190 196 191 195\n[20] 181 193 190 190 186 188 195 190 197 198 190 180 198 194 188 195 191 203 199\n[39] 190 189 195 186 189 199 202 197 189 190 194 190 181 190 190 181 186 196 174\n[58] 185 174 202 191 184 181 184 193 190 190 190 191 196 189 195 195 198 193 190\n[77] 197 184 186 188 193 190 191 195 198 180 191 185 189 192 183 192 199 186 195\n\n[[2]]\n [1] 187 194 187 189 184 188 187 187 184 197 193 191 187 189 190 172 187 186 180\n[20] 193 191 195 195 180 184 189 197 191 187 186 186 187 184 188 190 193 198 190\n[39] 195 198 184 197 195 195 195 198 194 191 198 197 198 186 194 195 189 186 181\n[58] 180 191 180 191 193 196 191 202 191 187 181 199 172 181 191 195 195 194 198\n[77] 191 191 190 192 190 199 195 193 195 197 188 181 190 185 186 191 174 193 195\n\n[[3]]\n [1] 191 196 203 195 185 195 193 186 186 202 186 203 187 180 185 186 192 202 186\n[20] 192 200 195 184 185 195 193 199 190 189 185 181 181 188 197 181 190 188 185\n[39] 187 184 184 195 199 186 200 186 192 195 190 182 189 191 203 193 195 191 191\n[58] 199 195 198 187 191 195 190 190 187 189 192 186 199 193 190 187 181 190 191\n[77] 190 190 183 193 190 197 181 190 187 198 187 190 200 184 190 184 186 191 193\n\n\n\nCode# The Sample Mean\nbootMeans_num &lt;-\n  bootstrapSamples_ls %&gt;%\n  # the map_dbl() function takes in a list and returns an atomic vector of type\n  #   double (numeric)\n  map_dbl(mean)\n\n# a normally distributed histogram using the samples from bootstrapping\nhist(bootMeans_num)\n\n\n\n\n\n\nCode# 95% confidence interval?\nquantile(bootMeans_num, probs = c(0.025, 0.975))\n\n    2.5%    97.5% \n188.7682 191.3684 \n\n\n\n19.7.1 Code for spearman correlation\n\nCode# Custom function to find correlation between the bill length and depth \ncorr.fun &lt;- function(data, idx) {\n  \n# vector of indices that the boot function uses\n  df &lt;- data[idx, ]\n\n# Find the spearman correlation between\n# the 3rd (length) and 4th (depth) columns of dataset\n  cor(df[, 3], df[, 4], method = 'spearman')\n}\n\n# Setting the seed for reproducability of results\nset.seed(42)\n\n# Calling the boot function with the dataset\nbootstrap &lt;- boot(iris, corr.fun, R = 1000)\n\n# Display the result of boot function\nbootstrap\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = iris, statistic = corr.fun, R = 1000)\n\n\nBootstrap Statistics :\n     original       bias    std. error\nt1* 0.9376668 -0.002717295 0.009436212\n\nCode# Plot the bootstrap sampling distribution using ggplot\nplot(bootstrap)\n\n\n\n\n\n\nCode# Function to find the bootstrap CI\nboot.ci(\n  boot.out = bootstrap,\n    type = \"perc\"\n)\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 1000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = bootstrap, type = \"perc\")\n\nIntervals : \nLevel     Percentile     \n95%   ( 0.9142,  0.9519 )  \nCalculations and Intervals on Original Scale\n\n\nThis code utilizes bootstrapping to estimate the sampling distribution and confidence interval of the Spearman correlation coefficient between bill length and depth in the iris dataset. The boot() function generates bootstrap samples, while the boot.ci() function calculates the bootstrap confidence interval. Visualizations are provided to aid in understanding the sampling distribution.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Bootstrap Tests</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_bootstrap_conf_int.html#conclusion",
    "href": "lessons/03_two_sample_bootstrap_conf_int.html#conclusion",
    "title": "\n19  Bootstrap Tests\n",
    "section": "\n19.8 Conclusion",
    "text": "19.8 Conclusion\nIn conclusion, bootstrap testing emerges as a valuable tool, particularly when confronted with small sample sizes. By leveraging resampling techniques, it offers a robust method to estimate parameters, assess uncertainty, and make reliable inferences about population statistics.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Bootstrap Tests</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_chi_sq_independence.html",
    "href": "lessons/03_two_sample_chi_sq_independence.html",
    "title": "\n20  Chi-Square Test of Independence\n",
    "section": "",
    "text": "20.1 What is a Chi-Square Test of Independence?\nThe Chi-Square Test of Independence is used to determine if there is a significant association between two categorical variables.\nPurpose: The test assesses whether the observed frequencies in a contingency table differ from the frequencies expected under the assumption of independence.\nWhat Does This Mean!?!?\nObserved Frequencies: This refers to actual counts or numbers recorded in each cell of the contingency table.\nExample: In a survey, 20 males preferred Product A, 30 males preferred Product B, 25 females preferred Product A, and 25 females preferred Product B.\nThese counts would be expected in each cell if the two variables were completely independent of each other.\nContingency Table: A contingency table displays the frequency distribution of variables in a matrix format, showing the observed counts of occurrences across categories of two variables.\nExpected Frequencies: These are the counts we would expect to see in each cell if the two variables were completely independent of each other. Calculated using the formula:\n\\[\nE_{ij} = \\frac{\\text{RowTotal} * \\text{ColumnTotal}}{\\text{GrandTotal}}\n\\]\nExample: If gender and product preference are independent, the expected frequency for males preferring Product A is calculated based on the overall proportions of males and Product A preferences in the total sample.\nAssumption of Independence: Under the assumption of independence, the occurrence of one variable does not affect the occurrence of the other variable.\nExample: Gender has no influence on product preference, meaning the distribution of preferences should be the same for both males and females.\nComparing Observed and Expected Frequencies: The Chi-Square test compares the observed frequencies with the expected frequencies. If the observed frequencies match the expected frequencies closely, it suggests that there is independence between the variables. Significant differences between observed and expected frequencies suggest an association between the variables.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Chi-Square Test of Independence</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_chi_sq_independence.html#what-is-a-chi-square-test-of-independence",
    "href": "lessons/03_two_sample_chi_sq_independence.html#what-is-a-chi-square-test-of-independence",
    "title": "\n20  Chi-Square Test of Independence\n",
    "section": "",
    "text": "Preference A\nPreference B\nTotal\n\n\n\nMale\n20\n30\n50\n\n\nFemale\n25\n25\n50\n\n\nTotal\n45\n55\n100",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Chi-Square Test of Independence</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_chi_sq_independence.html#statistic-for-chi-square-test",
    "href": "lessons/03_two_sample_chi_sq_independence.html#statistic-for-chi-square-test",
    "title": "\n20  Chi-Square Test of Independence\n",
    "section": "\n20.2 Statistic for Chi-Square test",
    "text": "20.2 Statistic for Chi-Square test\n\n\n\nPreference A\nPreference B\nTotal\n\n\n\nMale\n20\n30\n50\n\n\nFemale\n25\n25\n50\n\n\nTotal\n45\n55\n100\n\n\n\n1. Calculate Expected Frequencies: For each cell, the expected frequency is calculated as below Example for cell (Male, Preference A): \\(E_{Male,A}\\) = (50×45) 100 = 22.5\n2. Compute the Chi-Square Statistic: calculated using the formula:\n\\[\nx^2 = \\sum\\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\n\\]\nwhere \\(O_{ij}\\) is the observed frequency and \\(E_{ij}\\) is the expected frequency.\nSum this calculation over all cells in the table.\nDetermine Degrees of Freedom (df): df=(Number of Rows−1)×(Number of Columns−1)\n3. Find the Critical Value and Compare:\nUsing the degrees of freedom and the significance level (e.g., 0.05), find the critical value from the Chi-Square distribution table.\nCompare the calculated \\(x^2\\) to the critical value to determine whether to reject the null hypothesis.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Chi-Square Test of Independence</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_chi_sq_independence.html#conducting-the-test",
    "href": "lessons/03_two_sample_chi_sq_independence.html#conducting-the-test",
    "title": "\n20  Chi-Square Test of Independence\n",
    "section": "\n20.3 Conducting the test",
    "text": "20.3 Conducting the test\nResearch Question and Hypotheses The general question for a Chi-Square Test of Independence is: Is there a statistically significant association between two categorical variables?\nExample questions include:\n\nIs there an association between gender and voting preference?\nIs there a relationship between education level and job satisfaction?\nDoes the use of a new teaching method depend on the grade level of students?\n\nHypotheses From the research question, you derive the hypotheses:\nNull Hypothesis (\\(H_{0}\\)): There is no association between the two categorical variables (they are independent). Alternative Hypothesis (\\(H_{1}\\)): There is an association between the two categorical variables (they are not independent).\nBefore performing the Chi-Square test, check these assumptions:\nCategorical Data: Both variables should be categorical.\nIndependence of Observations: Each observation should contribute to only one cell in the contingency table.\nExpected Frequency: Expected frequency in each cell should be at least 5 for the test to be valid.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Chi-Square Test of Independence</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_chi_sq_independence.html#data-format-contingency-tables",
    "href": "lessons/03_two_sample_chi_sq_independence.html#data-format-contingency-tables",
    "title": "\n20  Chi-Square Test of Independence\n",
    "section": "\n20.4 Data format: Contingency tables",
    "text": "20.4 Data format: Contingency tables\nWe’ll use housetasks data sets from STHDA:http://www.sthda.com/sthda/RDoc/data/housetasks.txt. link\n\n# Import the data\n# file_path &lt;- \"http://www.sthda.com/sthda/RDoc/data/housetasks.txt\"\nhousetasks &lt;- read.delim(\"../data/03_housetasks.txt\", row.names = 1)\n\n# Show top rows of data\nhead(housetasks)\n\n           X.Wife Alternating Husband Jointly\nLaundry       156          14       2       4\nMain_meal     124          20       5       4\nDinner         77          11       7      13\nBreakfeast     82          36      15       7\nTidying        53          11       1      57\nDishes         32          24       4      53\n\n\nThe data is a contingency table containing 13 house tasks and their distribution in the couple:  1. by the wife only  2. alternatively  3. by the husband only  4. jointly",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Chi-Square Test of Independence</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_chi_sq_independence.html#graphical-display-of-contengency-tables",
    "href": "lessons/03_two_sample_chi_sq_independence.html#graphical-display-of-contengency-tables",
    "title": "\n20  Chi-Square Test of Independence\n",
    "section": "\n20.5 Graphical display of contengency tables",
    "text": "20.5 Graphical display of contengency tables\nContingency table can be visualized using the function balloonplot() [in gplots package]. This function draws a graphical matrix where each cell contains a dot whose size reflects the relative magnitude of the corresponding component.\n\n# Convert the data as a table\n\ndt &lt;- as.table(as.matrix(housetasks))\n\n\n# Graph of contingency table\nballoonplot(\n  t(dt), \n  main =\"housetasks\", \n  xlab =\"\", \n  ylab=\"\",\n  label = FALSE\n) \n\n\n\n\n\n\n\n\nThe size of the blue bubbles in the balloon plot represents the frequency or count of how often a specific task is performed by a particular household member or group of members. Larger bubbles indicate higher frequencies, meaning that the task is performed more often by that household member or group.\n\nOverall Summary:  1. The Wife appears to handle the majority of routine daily tasks such as cooking, cleaning, and shopping.  2. The Husband is more involved in tasks like Repairs, Driving, and Finances.  3. Some tasks are done jointly, showing cooperation between household members for tasks like Shopping, Official duties, and Holidays.  4. Alternating responsibility is the least common method of task distribution.\n\nRow and column sums are printed by default in the bottom and right margins, respectively. These values can be hidden using the argument show.margins = FALSE.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Chi-Square Test of Independence</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_chi_sq_independence.html#compute-chi-square-test-in-r",
    "href": "lessons/03_two_sample_chi_sq_independence.html#compute-chi-square-test-in-r",
    "title": "\n20  Chi-Square Test of Independence\n",
    "section": "\n20.6 Compute chi-square test in R",
    "text": "20.6 Compute chi-square test in R\nChi-square statistic can be easily computed using the function chisq.test() as follow:\n\nchisq &lt;- chisq.test(housetasks)\nchisq\n\n\n    Pearson's Chi-squared test\n\ndata:  housetasks\nX-squared = 1944.5, df = 36, p-value &lt; 2.2e-16\n\n\n\nAs the results shown above, p-value &lt; 2.2e-16, which means we can reject the null hypothesis, and saying that the row and the column variables are statistically significantly associated.\n\n\n20.6.1 Nature of the dependence between the row and the column variables\nIf you want to know the most contributing cells to the total Chi-square score, you just have to calculate the Pearson residuals (r) for each cell (or standardized residuals).Pearson residuals can be easily extracted from the output of the function chisq.test():\n\nCells with the highest absolute standardized residuals contribute the most to the total Chi-square score.\n\nVisualize Pearson residuals using the package corrplot:\n\n# Contibution in percentage (%)\n\ncontrib &lt;- 100*chisq$residuals^2/chisq$statistic\n\n\n# Visualize the contribution\n\ncorrplot(\n  contrib, \n  is.cor = FALSE\n)\n\n\n\n\n\n\n\nFor a given cell, the size of the circle is proportional to the amount of the cell contribution.\nIt can be seen that:  1. The column “Wife” is strongly associated with Laundry, Main_meal, Dinner  2. The column “Husband” is strongly associated with the row Repairs  3. The column jointly is frequently associated with the row Holidays",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Chi-Square Test of Independence</span>"
    ]
  },
  {
    "objectID": "04_header_anova-and-regression.html",
    "href": "04_header_anova-and-regression.html",
    "title": "ANOVA and Regression",
    "section": "",
    "text": "Text Outline\nThis text covers the basics of:",
    "crumbs": [
      "ANOVA and Regression"
    ]
  },
  {
    "objectID": "04_header_anova-and-regression.html#text-outline",
    "href": "04_header_anova-and-regression.html#text-outline",
    "title": "ANOVA and Regression",
    "section": "",
    "text": "Exploring data\nOne-sample tests\nTwo-sample tests\nANOVA and linear regression\nGeneralized linear models\nSome special topics\nMaybe a bit about power calculations…",
    "crumbs": [
      "ANOVA and Regression"
    ]
  },
  {
    "objectID": "04_header_anova-and-regression.html#part-outline",
    "href": "04_header_anova-and-regression.html#part-outline",
    "title": "ANOVA and Regression",
    "section": "Part Outline",
    "text": "Part Outline\nThis part of the text contains the chapters on various ways to perform ANOVA and linear regression:\n\nOne-Way ANOVA\nTwo-way ANOVA\nWelch’s ANOVA\nKruskal-Wallace Test\nTukey HSD Post-Hoc Test\nRepeated Measures ANOVA\nRandom Intercept Models\nCorrelation Matrices and Covariances\nMultiple Regression (linear)\nPolynomial regression",
    "crumbs": [
      "ANOVA and Regression"
    ]
  },
  {
    "objectID": "lessons/04_anova_one_way.html",
    "href": "lessons/04_anova_one_way.html",
    "title": "\n21  One-Way ANOVA\n",
    "section": "",
    "text": "21.1 Introduction to one-way ANOVA\nOne-way analysis of variance (ANOVA) is an extension of a two-samples t-test for comparing means between three or more independent groups. A single factor variable is employed to categorize the data into several groups.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>One-Way ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_one_way.html#introduction-to-one-way-anova",
    "href": "lessons/04_anova_one_way.html#introduction-to-one-way-anova",
    "title": "\n21  One-Way ANOVA\n",
    "section": "",
    "text": "Hypothesis Testing\n\n\n\nNull Hypothesis \\((H_0)\\) the means between groups are not statistically different (they ARE the same).  Alternative Hypothesis \\((H_a)\\) the means between groups are statistically different (they ARE NOT the same).",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>One-Way ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_one_way.html#mathematical-definition-of-one-way-anova",
    "href": "lessons/04_anova_one_way.html#mathematical-definition-of-one-way-anova",
    "title": "\n21  One-Way ANOVA\n",
    "section": "\n21.2 Mathematical definition of one-way ANOVA",
    "text": "21.2 Mathematical definition of one-way ANOVA\n\\[\nY_{ij} = \\mu + \\tau_{i} + \\epsilon_{ij}\n\\]\nWhere,\n\n\n\\(Y_{ij}\\) represents the j-th observation (j = 1, 2, …, \\(n_{i}\\)) on the i-th treatment (i = 1, 2, …, k levels).\n\nSo, \\(Y_{23}\\) represents the third observation for the second factor level.\n\n\n\n\\(\\mu\\) is the common effect.\n\n\\(\\tau_{i}\\) represents the i-th treatment effect.\n\n\\(\\epsilon_{ij}\\) represents the random error present in the j-th observation on the i-th treatment.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>One-Way ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_one_way.html#data-description",
    "href": "lessons/04_anova_one_way.html#data-description",
    "title": "\n21  One-Way ANOVA\n",
    "section": "\n21.3 Data Description",
    "text": "21.3 Data Description\nThe PlantGrowth data set is already included in {base R} within the {datasets} package. The PlantGrowth data set includes results from an experiment to compare yield obtained under a control condition and two different treatment conditions. The measurements were obtained through the dried weight of each plant.\n\n# Call the data set into the Global environment\ndata(\"PlantGrowth\")\n\n# Use the `glimpse()` or `head()` function to skim observations\n# glimpse(PlantGrowth)\nhead(PlantGrowth)\n\n  weight group\n1   4.17  ctrl\n2   5.58  ctrl\n3   5.18  ctrl\n4   6.11  ctrl\n5   4.50  ctrl\n6   4.61  ctrl\n\n# Optional: You can use the `summary()` function to confirm that all variables\n# are being read correctly. E.g. the `group` variable is displayed as counts\n# for each level\nsummary(PlantGrowth)\n\n     weight       group   \n Min.   :3.590   ctrl:10  \n 1st Qu.:4.550   trt1:10  \n Median :5.155   trt2:10  \n Mean   :5.073            \n 3rd Qu.:5.530            \n Max.   :6.310            \n\n# Optional: You can use the `str()` function to view the factor levels and\n# confirm the appropriate reference level for analysis\nstr(PlantGrowth)\n\n'data.frame':   30 obs. of  2 variables:\n $ weight: num  4.17 5.58 5.18 6.11 4.5 4.61 5.17 4.53 5.33 5.14 ...\n $ group : Factor w/ 3 levels \"ctrl\",\"trt1\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n# R assigns reference level in alphabetical order. E.g. From our factor levels \n# (ctrl, trt1, trt2) - ctrl will be assigned as the reference level \n# automatically by R.\n# Here, we see that `group` is a factor with 3 levels,\n# with 1 corresponding to 1 (the first level).\n\n# If you need to re-assign the reference level, you can `relevel()` your factor\n# The first variable after `Levels:` is your new reference group.\nrelevel(PlantGrowth$group, ref = \"ctrl\")\n\n [1] ctrl ctrl ctrl ctrl ctrl ctrl ctrl ctrl ctrl ctrl trt1 trt1 trt1 trt1 trt1\n[16] trt1 trt1 trt1 trt1 trt1 trt2 trt2 trt2 trt2 trt2 trt2 trt2 trt2 trt2 trt2\nLevels: ctrl trt1 trt2",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>One-Way ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_one_way.html#assumptions-of-one-way-anova",
    "href": "lessons/04_anova_one_way.html#assumptions-of-one-way-anova",
    "title": "\n21  One-Way ANOVA\n",
    "section": "\n21.4 Assumptions of One-Way ANOVA",
    "text": "21.4 Assumptions of One-Way ANOVA\nThe assumptions of one-way ANOVA are as follows:\n\n\nAssumption 1: All observations are independent and randomly selected from the population as defined by the factor variable.\n\nAssumption 2: The data within each factor level are approximately normally distributed.\n\nAssumption 3: The variance of the data of interest is similar across each factor level (aka: Homogeneity of variance).",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>One-Way ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_one_way.html#checking-the-assumptions-of-the-one-way-anova",
    "href": "lessons/04_anova_one_way.html#checking-the-assumptions-of-the-one-way-anova",
    "title": "\n21  One-Way ANOVA\n",
    "section": "\n21.5 Checking the Assumptions of the One-Way ANOVA",
    "text": "21.5 Checking the Assumptions of the One-Way ANOVA\nAssumption 2: The data within each factor level are approximately normally distributed.\nTo check this assumption, we can examine if the data are approximately normally distributed across groups with 2 plots, the Q-Q plot or histograms, or with the Shapiro-Wilk test.\nFirst, we will look at the Q-Q plot:\n\n# Check for missing values to ensure balanced data\ncolSums(is.na(PlantGrowth))\n\nweight  group \n     0      0 \n\n# No missing datapoints\n\n# Normality assumption: Q-Q plot\nqq_plot_plantgrowth &lt;- ggplot(PlantGrowth) +\n  aes(sample = weight) +\n  facet_wrap(~ group) +\n  stat_qq() +\n  stat_qq_line()\n\nqq_plot_plantgrowth\n\n\n\n\n\n\n\nThe weight variable seems to be approximately normally distributed across each of the three groups based on the Q-Q plots.\nNext, we will look at histograms:\n\n# Normality assumption: Histograms\nhistogram_plantgrowth &lt;- ggplot(PlantGrowth) +\n  aes(x = weight) +\n  geom_histogram(binwidth = 0.5) +\n  facet_wrap(~ group)\n\nhistogram_plantgrowth\n\n\n\n\n\n\n\nAgain, the weight variable appears to be approximately normally distributed across the groups.\nLastly, we can use the Shapiro-Wilk test to test if the data is approximately normally distributed. P-values greater than 0.05 indicate that the data is likely approximately normally distributed.\n\n# Normality: Shapiro Wilks\nshapiro_plantgrowth &lt;- PlantGrowth %&gt;%\n  group_by(group) %&gt;%\n  summarise(\n    statistic = shapiro.test(weight)$statistic,\n    p.value = shapiro.test(weight)$p.value\n  )\n\nshapiro_plantgrowth\n\n# A tibble: 3 × 3\n  group statistic p.value\n  &lt;fct&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 ctrl      0.957   0.747\n2 trt1      0.930   0.452\n3 trt2      0.941   0.564\n\n\nThe p-value for each of our groups is above 0.05, so we can assume that weight is approximately normally distributed within each group.\nAssumption 3: Homogeneity of variance.\nWe can check this assumption with Levene’s test. A p-value greater than 0.05 indicates that the variance is similar across groups, and we therefore meet the requirements of this assumption.\n\n# Homogeneity of Variance: Levene's test\nlevenes_plantgrowth &lt;- levene_test(weight ~ group, data = PlantGrowth)\nlevenes_plantgrowth\n\n# A tibble: 1 × 4\n    df1   df2 statistic     p\n  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1     2    27      1.12 0.341\n\n\nThe p-value is 0.341, so we can therefore assume that our data meets the assumption of homogeneity of variance.\nWe can also check assumption 3 by visually examining a box plot of the outcome variable across groups:\n\n# Variance assumption: Box plot for weight by group\nbox_plantgrowth &lt;- ggplot(PlantGrowth) +\n  aes(y = weight) +\n  geom_boxplot() +\n  facet_wrap(~ group)\n\nbox_plantgrowth\n\n\n\n\n\n\n\nTo compare the box plot visually, we take a look at the interquartile range between each three plots. All three have approximately similar size and length, which satisfies our assumption homogeneity of variance.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>One-Way ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_one_way.html#code-to-run-one-way-anova",
    "href": "lessons/04_anova_one_way.html#code-to-run-one-way-anova",
    "title": "\n21  One-Way ANOVA\n",
    "section": "\n21.6 Code to Run One-Way ANOVA",
    "text": "21.6 Code to Run One-Way ANOVA\n\n# Include an outcome variable, `weight`, and predictor(s), `group`. as well as \n# which data set to pull variables from with `data = ` argument\nanova_fit &lt;- aov(weight ~ group, data = PlantGrowth)\n\n# Call summary statistics from `anova_fit` object\nsummary(anova_fit)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)  \ngroup        2  3.766  1.8832   4.846 0.0159 *\nResiduals   27 10.492  0.3886                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe summary includes the independent variable group being tested within the model. All variation that is not explained by group is considered residual variance.\n\nThe Df column tabulates the degrees of freedom for the independent variable, which is the number of levels in the variable minus 1. For example, group has 3 levels (3 - 1 = 2 degrees of freedom).\nThe Sum Sq column tabulates the sum of squares (total variation between group means and overall mean).\nThe Mean Sq column tabulates the mean of the sum of squares, which is calculated by dividing the sum of squares by the degrees of freedom for each parameter.\nThe F value column tabulates the test statistic from the F test, which is the mean square of each independent variable (only one in this case) by the mean square of the residuals.\nThe Pr(&gt;F) column is the resulting p-value of the F statistic. Remember: the p-value shows how likely it is that the calculated F value would have occurred if the null hypothesis were true.\n\nThe p value of the fertilizer variable is low (p &lt; 0.01), so it appears that the type of fertilizer used (group) has a real impact on the final crop yield (weight).\n\n21.6.1 Post-Hoc Test for Pairwise Comparisons\nTo compare the group means directly, we can conduct a post-hoc test to see which group means are different from one another. One post-hoc test is Tukey’s Test.\n\n# perform Tukey's Test for multiple comparisons\nanova_post_hoc &lt;- TukeyHSD(anova_fit, conf.level=.95)\n\nanova_post_hoc\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = weight ~ group, data = PlantGrowth)\n\n$group\n            diff        lwr       upr     p adj\ntrt1-ctrl -0.371 -1.0622161 0.3202161 0.3908711\ntrt2-ctrl  0.494 -0.1972161 1.1852161 0.1979960\ntrt2-trt1  0.865  0.1737839 1.5562161 0.0120064\n\n\nIn the above output, we can see that:\n\n\ntrt1-ctrl: P-value is approximately 0.391 when comparing treatment 1 (trt1) and control (ctrl), which is greater than 0.05. This indicates there is not a statistically significant difference in mean between these groups. Their means are the SAME.\n\ntrt2-ctrl: P-value is approximately 0.198 when comparing treatment 2 (trt2) and control (ctrl), which is greater than 0.05. This indicates there is not a statistically significant difference in mean between these groups. Their means are the SAME.\n\ntrt2-trt1: P-value is 0.012 when comparing treatment 1 (trt1) and treatment 2 (trt2), which is less than 0.05. This indicates there is a statistically significant difference in mean between these groups. Their means are NOT the SAME.\n\n21.6.2 Regression for Comparison\nTo compare the ANOVA results with those of a simple linear regression, we can run a regression model to examine if there are differences between the control group and the two treatment groups.\n\n# Simple linear regression\nlm_plantgrowth &lt;- lm(weight ~ group, data = PlantGrowth)\n\nsummary(lm_plantgrowth)\n\n\nCall:\nlm(formula = weight ~ group, data = PlantGrowth)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.0710 -0.4180 -0.0060  0.2627  1.3690 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   5.0320     0.1971  25.527   &lt;2e-16 ***\ngrouptrt1    -0.3710     0.2788  -1.331   0.1944    \ngrouptrt2     0.4940     0.2788   1.772   0.0877 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6234 on 27 degrees of freedom\nMultiple R-squared:  0.2641,    Adjusted R-squared:  0.2096 \nF-statistic: 4.846 on 2 and 27 DF,  p-value: 0.01591\n\n\nFrom the above output, we can see that:\n\nThe F-statistic is 4.846, which is the same F-statistic produced by the ANOVA.\nThe p-value of the model (0.01591) is the same as the p-value produced by the ANOVA.\nThe degrees of freedom for the ANOVA and linear model are also the same (2 and 27).\nThe Multiple R-squared value from the regression (0.2641) is related to the Sum of Squares for our factor and the Sum of Squares for the residual in the following way: \\(R^{2} = \\frac{SS_{factor}}{(SS_{factor} + SS_{residuals})}\\).\nThe linear model confirms that there is no difference in the treatment groups compared to the control group since the p-values for the treatment groups are larger than 0.05 (0.194 and 0.088 for treatment 1 and treatment 2 versus control, respectively).",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>One-Way ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_one_way.html#brief-interpretation-of-the-output",
    "href": "lessons/04_anova_one_way.html#brief-interpretation-of-the-output",
    "title": "\n21  One-Way ANOVA\n",
    "section": "\n21.7 Brief Interpretation of the Output",
    "text": "21.7 Brief Interpretation of the Output\nThe resulting p-value of the one-way ANOVA is 0.0159, which is less than our significance level of 0.05. Therefore we must reject our null hypothesis as the data supports that the mean weight between groups is statistically different. In other words, the mean weight(s) are NOT the same between group (ctrl, trt1, trt2).\nThe Tukey test results illustrate a true difference that only exists between treatment groups 1 and 2 (trt1 and trt2), and not between the control group and either of the treatment groups. This is confirmed by the output from the simple linear regression model.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>One-Way ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_two_way.html",
    "href": "lessons/04_anova_two_way.html",
    "title": "\n22  Two-Way ANOVA\n",
    "section": "",
    "text": "22.1 Introduction\nThe following two-way ANOVA lesson is based on “Two-Way ANOVA | Examples & When to Use It”, a tutorial by Rebecca Bevans.\nA two-way ANOVA examines the influence of two or more independent categorical variables, also known as factors, on a continuous dependent variable. Each factor must have at least two levels, which are the groups within each factor being analyzed. In this lesson, we will see an example of how to test for these assumptions and how to conduct a type I two-way ANOVA once we know they have been met.\nAn interaction occurs when the effect of one factor depends on the level of another factor. The two-way ANOVA method allows us to measure:",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Two-Way ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_two_way.html#introduction",
    "href": "lessons/04_anova_two_way.html#introduction",
    "title": "\n22  Two-Way ANOVA\n",
    "section": "",
    "text": "Main Effects - the individual effect of one factor on the dependent variable, and\nInteraction Effects - the extent to which the effects of one factor change across different levels of another factor.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Two-Way ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_two_way.html#how-does-a-two-way-anova-work",
    "href": "lessons/04_anova_two_way.html#how-does-a-two-way-anova-work",
    "title": "\n22  Two-Way ANOVA\n",
    "section": "\n22.2 How Does a Two-Way ANOVA Work?",
    "text": "22.2 How Does a Two-Way ANOVA Work?\nThe F test, a group-wise comparison test, is used in an ANOVA to determine statistical significance. The outcome of an F test, the F statistic, measures how different a groups’ variances are to the overall variance of the dependent variable. When the variance is higher between groups than within the group, the F statistic will be greater. A large F statistic suggests there’s a low likelihood that the observed difference is due to chance, and thus, the factor likely has an effect on the outcome.\nFor comparison, a critical value is calculated based on the desired alpha (α) and degrees of freedom of all groups. If the F statistic is greater than the F critical value, the results are considered statistically significant. For example, if the F statistic is greater than the F critical value at α = 0.05, then the p-value will be less than 0.05. For a model with two factors, we can generate three F statistics and three associated p values to test three possible hypotheses:\n\n\n\n\n\n\nNull Hypotheses\n\n\n\n\nH01: The population means of the first factor are equal.\n\nH02: The population means of the second factor are equal.\n\nH03: There is no interaction effect, that is, the effect of one factor does not depend on the value of the other factor.\n\n\n\n\n\n\n\n\n\n\nAlternative Hypotheses\n\n\n\n\nHA1: The population means of the first factor are not equal.\n\nHA2: The population means of the second factor are not equal.\n\nHA3: There is an interaction effect, that is, the effect of one factor depends on the value of the other factor.\n\n\n\n\n\n22.2.1 Assumptions\nThree assumptions must be met before conducting a two-way ANOVA:\n\nHomogeneity of variance: The variances for each group should be roughly equal. If the groups do not have equal variances, a non-parametric test, such as the Kruskal-Wallis test, may be used.\nIndependence of observations: The observations in each group are independent of each other and the observations within groups were obtained by a random sample. This can be assumed to be true so long as the experiment is properly designed. If observations are grouped in categories, this effect should be accounted for with the use of a blocking variable or a repeated-measures ANOVA.\nNormally distributed dependent variable: The values of the dependent variable should follow a normal distribution.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Two-Way ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_two_way.html#two-way-anova-table",
    "href": "lessons/04_anova_two_way.html#two-way-anova-table",
    "title": "\n22  Two-Way ANOVA\n",
    "section": "\n22.3 Two-Way ANOVA Table",
    "text": "22.3 Two-Way ANOVA Table\nThe following table includes the calculations used in a two-way ANOVA. For more detailed information on these calculations, please read Models and Calculations for the Two-Way ANOVA, a lesson by the National Institute of Standards and Technology.\nIn the ANOVA table, the main effect A has k levels and the main effect B has l levels. N represents the total sample size.\n\n\n\n\n\n\n\n\n\nSource of Variation\nSum of Squares\nDegrees of freedom\nMean Squares\nF value\n\n\n\nFactor A\n\\(SS_A\\)\n\\(k-1\\)\n\\(MS_A\\)\n\\(F_A\\)\n\n\nFactor B\n\\(SS_B\\)\n\\(l-1\\)\n\\(MS_B\\)\n\\(F_B\\)\n\n\nInteraction AB\n\\(SS_{AB}\\)\n\\((k-1)(l-1)\\)\n\\(MS_{AB}\\)\n\\(F_{AB}\\)\n\n\nError\n\\(SS_E\\)\n\\(N - kl\\)\n\\(MS_E\\)\n\n\n\nTotal\n\\(SS_T\\)\n\\(N-1\\)\n\n\n\n\n\nWhere\n\n\\[ MS_E := \\frac{SS_E}{N-kl} \\]\n\\[ MS_A := \\frac{SS_A}{k-1} \\text{ and } F_A := \\frac{MS_A}{MS_E} \\]\n\\[ MS_B := \\frac{SS_B}{l-1} \\text{ and } F_B := \\frac{MS_B}{MS_E} \\]\n\\[ MS_{AB} := \\frac{SS_{AB}}{(k-1)(l-1)} \\text{ and } F_{AB} := \\frac{MS_{AB}}{MS_E} \\]\n\nWe explain these components as:\n\n\n\\(SS_A\\): Factor \\(A\\) main effect sums of squares, df = \\(k-1\\)\n\n\n\\(SS_B\\): Factor \\(B\\) main effect sums of squares, df = \\(l-1\\)\n\n\n\\(SS_{AB}\\): interaction sum of squares, df = \\((k-1)(l-1)\\)\n\n\n\\(SS_E\\): error sum of squares, df = \\(N-kl\\)\n\n\n\\(SS_T\\): Total sums of squares, df = \\(N-1\\)",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Two-Way ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_two_way.html#crop-yield-two-way-anova-example",
    "href": "lessons/04_anova_two_way.html#crop-yield-two-way-anova-example",
    "title": "\n22  Two-Way ANOVA\n",
    "section": "\n22.4 Crop Yield Two-Way ANOVA Example",
    "text": "22.4 Crop Yield Two-Way ANOVA Example\nWe will use the agricultural crop yield dataset from the Two-Way ANOVA lesson lesson in our example, which can be directly downloaded here.\nIn this example, corn was planted in one of four blocks within a field, in either high or low density, and fertilized with one of three types of fertilizer. Their yield in bushels per acre was then measured. The three factors of this experiment with their associated levels are:\n\nType of fertilizer (type 1, 2, or 3)\nPlanting density (1 = low, 2 = high)\nBlock number in the field (block 1, 2, 3, or 4)\n\n\n22.4.1 Hypotheses\nSuppose we want to use a two-way ANOVA to examine whether the type of fertilizer and planting density (factors) have an effect on the average crop yield (dependent variable). To answer this question, the following three hypotheses will be tested:\n\n\n\n\n\n\nNull Hypotheses\n\n\n\n\nH01: Fertilizer type has no effect on average crop yield\n\nH02: Planting density has no effect on average crop yield\n\nH03: The effects of fertilizer type and planting density on average yield are independent of each other (no interaction exists)\n\n\n\n\n\n\n\n\n\n\nAlternative Hypotheses\n\n\n\n\nH11: Fertilizer type has an effect on average crop yield\n\nH12: Planting density has an effect on average crop yield\n\nH13: The effects of fertilizer type and planting density on average yield are not independent of each other (no interaction occurs)\n\n\n\n\n\n22.4.2 Loading Libraries and Data\nFirst, we will load our required packages and read in the crop yield dataset. The tidyverse package will be used to transform the data. The tidymodels and gt packages are necessary for creating presentation-ready tables of results. Finally, the AICcmodavg package will be used to construct an AIC table with which to compare our models (more on this later).\n\n# Uncomment the following code to install required packages\n\n# gt and tidymodels used for presentation-ready table:\n# install.packages(\"gt\")\n# install.packages(\"tidymodels\")\n\n# AICcmodavg used for AIC table:\n# install.packages(\"AICcmodavg\")\n\n# tidyverse for data transformation:\n# install.packages(\"tidyverse\")\n\n# Load the required packages\nlibrary(gt)\nlibrary(tidymodels)\nlibrary(AICcmodavg)\nlibrary(tidyverse)\n\n# read in the crop data dataset downloaded from\n# https://www.scribbr.com/wp-content/uploads//2020/03/crop.data_.anova_.zip\ncrop_data_df &lt;- read_csv(\"../data/04_crop_data.csv\")\n\n\n22.4.3 Data Exploration\nThe first few datapoints of the crop yield dataset were inspected and summary statistics were performed on the entire dataset to examine the structure, center, and spread of the data.\n\n# show first six rows of the dataset\nhead(crop_data_df)\n\n# overview of summary statistics\nsummary(crop_data_df)\n\n# A tibble: 6 × 4\n  density block fertilizer yield\n    &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;\n1       1     1          1  177.\n2       2     2          1  178.\n3       1     3          1  176.\n4       2     4          1  178.\n5       1     1          1  177.\n6       2     2          1  177.\n    density        block        fertilizer     yield      \n Min.   :1.0   Min.   :1.00   Min.   :1    Min.   :175.4  \n 1st Qu.:1.0   1st Qu.:1.75   1st Qu.:1    1st Qu.:176.5  \n Median :1.5   Median :2.50   Median :2    Median :177.1  \n Mean   :1.5   Mean   :2.50   Mean   :2    Mean   :177.0  \n 3rd Qu.:2.0   3rd Qu.:3.25   3rd Qu.:3    3rd Qu.:177.4  \n Max.   :2.0   Max.   :4.00   Max.   :3    Max.   :179.1  \n\n\n\n22.4.4 Transform Treatment Factors Into R Factors\nNote that the fertilizer, density, and block vectors are read in as numeric class objects, so we must convert them to the factor class. We will also specify the reference groups for each factor: fertilizer 1, low density, and block 1. If not specified, reference groups are automatically assigned in alphabetical order.\n\n# transform numeric vectors to factors and assign reference groups\ncrop_data_df$fertilizer &lt;- as.factor(crop_data_df$fertilizer) %&gt;% \n  relevel(ref = 1)\ncrop_data_df$density &lt;- as.factor(crop_data_df$density) %&gt;% \n  relevel(ref = 1)\ncrop_data_df$block &lt;- as.factor(crop_data_df$block) %&gt;% \n  relevel(ref = 1)\n\n\n22.4.4.1 Boxplots of Factors\nBox plots for each factor were created to further examine the spread and center of the data using ggplot2.\n\n# create box plot of yield by fertilizer\nplot_fertilizer &lt;- \n  ggplot(data = crop_data_df) +\n    aes(\n      x = as.factor(fertilizer),\n      y = yield,\n      color = fertilizer \n    ) +\n  labs(\n    title = \"Crop Yield by Fertilizer Type\",\n    x = \"Fertilizer Type\",\n    y = \"Crop Yield\"\n  ) +\n  geom_boxplot()\n\nplot_fertilizer\n\n\n\n\n\n\n# yield by density\nplot_density &lt;- \n  ggplot(data = crop_data_df) +\n    aes(\n      x = as.factor(density),\n      y = yield,\n      color = density\n    ) +\n  labs(\n    title = \"Crop Yield by Planting Density\",\n    x = \"Planting Density\",\n    y = \"Crop Yield\"\n  ) +\n  geom_boxplot()\n\nplot_density\n\n\n\n\n\n\n# yield by block\nplot_block &lt;- \n  ggplot(data = crop_data_df) +\n    aes(\n      x = as.factor(block),\n      y = yield,\n      color = block \n    ) +\n  labs(\n    title = \"Crop Yield by Block\",\n    x = \"Block\",\n    y = \"Crop Yield\"\n  ) +\n  geom_boxplot()\n\nplot_block\n\n\n\n\n\n\n\nThe data for each level appear to be approximately normally distributed with roughly equal variance and few outliers.\n\n22.4.4.2 Combined Boxplot of Crop Yield by Fertilizer and Density\nTo help visualize the data, a combined box plot visualizing the effects of fertilizer and density on average crop yield was created. The y-axis indicates the average crop yield. The x-axis is divided by fertilizer type (labeled at the top of the graph) and indicates the planting density for each level of the fertilizer factor.\n\nanova_plot &lt;- \n  ggplot(crop_data_df) +\n  aes(\n    x = density, \n    y = yield, \n    group = fertilizer,\n    color = fertilizer\n  ) +\n  geom_point(\n    cex = 1.5, \n    pch = 1.0, \n# set point positions to randomly jitter so points don't overlap\n    position = position_jitter(w = 0.1, h = 0.1)\n  )\n\nanova_plot &lt;- anova_plot + \n# compute summary stats to create error bars\n  stat_summary(\n# set calculation for error bar parameters to mean_se\n    fun.data = 'mean_se',\n    geom = 'errorbar',\n    width = 0.2,\n    color = \"grey50\"\n  ) +\n# create points for level means\n  stat_summary(\n# set calculation for mean parameter to mean_se\n    fun.data = 'mean_se',\n    geom = 'pointrange'\n  )\n\nanova_plot &lt;- anova_plot +\n# create three boxplots side by side for each fertilizer level\n  facet_wrap(~ fertilizer)\n\nanova_plot &lt;- anova_plot +\n  theme_classic() +\n  labs(\n    title = \"Crop Yield Averages by Fertilizer Types and Planting Density\",\n    x = \"Planting Density (1 = low density, 2 = high density)\",\n    y = \"Average Yield\"\n  )\n\nanova_plot\n\n\n\n\n\n\n\nThe differing means for each level of fertilizer and density suggest that the factors have an effect on average yield. Before we can test whether these potential effects are statistically significant, we must first check that the data meet the ANOVA assumptions. To do this, we will generate a two-way ANOVA and run diagnostics on the model.\n\n22.4.5 Performing the Two-Way ANOVA with Interaction and Blocking Variables\nRecall that the two-way ANOVA can actually account for more than two factors. The crops were planted across various blocks whose conditions may differ in terms sunlight, moisture, etc. This could possibly lead to confounding, so it is important to control for the possible effect of these differences by adding this third factor to our model.\nA two-way ANOVA was created to model the effects of fertilizer, density, the interaction between fertilizer and density, and the blocking factor. In the following code, the argument fertilizer * density is equivalent to fertilizer + density + fertilizer : density, where fertilizer : density represents the interaction term.\n\n\n\n\n\n\nAre your data balanced?\n\n\n\nBecause the crop data are balanced (the sample sizes across the levels within each factor are equal), we will use the base-R function aov(), which uses Type I sums of squares. If your data are unbalanced, use a different function to conduct a Type II ANOVA (for data with no significant interaction) or Type III ANOVA (for data with significant interaction).\n\n\n\n# performing two-way anova with fertilizer, density, fertilizer:density \n#   interaction and blocking factor\nfull_model &lt;- \n# 'fertilizer * density' = fertilizer + density + fertilizer:density interaction\n  aov(yield ~ fertilizer * density + block, data = crop_data_df)\n\n\n22.4.6 Checking ANOVA Assumptions\nWe will now create diagnostic plots, which include a residuals vs fitted plot, scale-location plot, Q-Q plot, and constant leverage plot, to evaluate the homoscedasticity of our data. Please read Understanding Diagnostic Plots for Linear Regression Analysis for more information on these plots.\n\n# set plot parameter to display 2 x 2 plots in output\npar(mfrow=c(2,2))\n# plot default diagnostic plots of full_model\nplot(full_model)\n\n\n\n\n\n\n# set plot parameter back to 1 x 1 format\npar(mfrow=c(1,1))\n\nThe residual vs fitted plot shows the data are randomly spread about the “0” line with no large outliers, so we can assume the factors have equal variances. Similarly, the scale-location plot shows an approximately horizontal line with randomly spread points, indicating equal variances. Thus, the homoscedasticity assumption is met.\nThe points of the Q-Q residuals plot roughly follow the reference line, indicating the data are normally distributed. We can conclude the normality assumption is met.\nThe constant leverage plot displays a horizontal “0” value line with points randomly spread around it, indicating the spread of the points are the same at different levels. No points lie outside of the critical value lines (not visible in the graph due to scale), indicating no outliers exist that could skew the data.\n\n22.4.7 Two-Way ANOVA - Full Model Interpretation\nNow that we have verified that the data satisfy the ANOVA assumptions, we may conduct the two-way ANOVA to test the hypotheses. Let’s interpret the results of the ANOVA of the full model:\n\ntable_full &lt;- full_model %&gt;% \n# turn model into data.frame of parameters\n  tidy() %&gt;% \n# create gt table\n  gt()\n\n# customizing gt table header\ntable_full |&gt;\n   tab_header(\n      title = \"Two-Way ANOVA of Crop Yield - Full Model\",\n      subtitle = \"for main full model (y ~ fertilizer * density + block)\"\n    )\n\n\n\n\n\n\n\nTwo-Way ANOVA of Crop Yield - Full Model\n\n\nfor main full model (y ~ fertilizer * density + block)\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nfertilizer\n2\n6.0680466\n3.0340233\n8.9443603\n0.0002909717\n\n\ndensity\n1\n5.1216812\n5.1216812\n15.0988167\n0.0001972614\n\n\nblock\n2\n0.4861389\n0.2430695\n0.7165735\n0.4912504731\n\n\nfertilizer:density\n2\n0.4278183\n0.2139091\n0.6306083\n0.5346558069\n\n\nResiduals\n88\n29.8505477\n0.3392108\nNA\nNA\n\n\n\n\n\n\n\nThe block variable’s p value of p = 0.49 and the interaction term’s p value of p = 0.53 are not significant at the α = 0.05 level. Hence, neither have a statistically significant effect on the crop yield. Because they do not add information to the model, They may be removed from the final model.\n\n22.4.8 Two-Way ANOVA - Density + Fertilizer + Interaction\nWe will now run a two-way ANOVA on a new model using the density, fertilizer, and interaction factors to investigate whether these terms are significant.\n\n# performing the two-way ANOVA with interaction\ninteraction &lt;- \n# 'fertilizer * density' = fertilizer + density + fertilizer:density interaction\n  aov(yield ~ fertilizer * density, data = crop_data_df)\n\ntable_int &lt;- interaction %&gt;%\n# turn model into data.frame of parameters\n  tidy() %&gt;% \n# create gt table\n  gt()\n\n# customizing table header\ntable_int |&gt;\n   tab_header(\n      title = \"Two-way ANOVA of Crop Yield - Interaction Model\",\n      subtitle = \"for interaction model (y ~ fertilizer * density)\"\n    )\n\n\n\n\n\n\n\nTwo-way ANOVA of Crop Yield - Interaction Model\n\n\nfor interaction model (y ~ fertilizer * density)\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nfertilizer\n2\n6.0680466\n3.0340233\n9.0010522\n0.0002731890\n\n\ndensity\n1\n5.1216812\n5.1216812\n15.1945174\n0.0001864075\n\n\nfertilizer:density\n2\n0.4278183\n0.2139091\n0.6346053\n0.5325000914\n\n\nResiduals\n90\n30.3366866\n0.3370743\nNA\nNA\n\n\n\n\n\n\n\nThe p value for the interaction term is greater than 0.05, hence we fail to reject the null hypothesis (H03) and conclude that there is no statistically significant interaction effect between fertilizer type and crop density on average yield. The interaction term should also be removed from the model.\n\n22.4.9 Two-Way ANOVA - Density + Fertilizer\nBecause the interaction term was not significant, we will remove it from our model and perform a two-way ANOVA on a model with only the density and fertilizer factors.\n\n# performing the two-way ANOVA without the interaction term\nmain_effects &lt;- \n  aov(yield ~ fertilizer + density, data = crop_data_df)\n\ntable_main &lt;- main_effects %&gt;% \n# turn model into data.frame of parameters\n  tidy() %&gt;%\n# create gt table\n  gt()\n\n# customizing table header\ntable_main |&gt;\n   tab_header(\n      title = \"Two-Way ANOVA of Crop Yield - Main Effects Model\",\n      subtitle = \"for main effects model (y ~ fertilizer + density)\"\n   )\n\n\n\n\n\n\n\nTwo-Way ANOVA of Crop Yield - Main Effects Model\n\n\nfor main effects model (y ~ fertilizer + density)\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nfertilizer\n2\n6.068047\n3.0340233\n9.073123\n0.0002532992\n\n\ndensity\n1\n5.121681\n5.1216812\n15.316179\n0.0001741418\n\n\nResiduals\n92\n30.764505\n0.3343968\nNA\nNA\n\n\n\n\n\n\n\nWithout the interaction term, we see that the p values for both fertilizer type and planting density are significant at the α = 0.05 level. Thus, we reject null hypotheses H01 and H02 and conclude that both fertilizer and density have a statistically significant main effect on crop yield.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Two-Way ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_two_way.html#determining-the-best-fitting-model-using-aic",
    "href": "lessons/04_anova_two_way.html#determining-the-best-fitting-model-using-aic",
    "title": "\n22  Two-Way ANOVA\n",
    "section": "\n22.5 Determining the Best-Fitting Model Using AIC",
    "text": "22.5 Determining the Best-Fitting Model Using AIC\nThe Akaike information criterion (AIC) is another method that can be used to determine the model that best fits the data. The model that explains the greatest amount of the variation in the data using the fewest possible independent variables is considered the best-fitting model. The lower the AIC value, the more variation is explained by the model.\n\n# creating a list of models to compare and their respective names\nmodel_set &lt;- list(main_effects, interaction, full_model)\nmodel_names &lt;- c(\"main_effects\", \"interaction\", \"full_model\")\n\n# using AICtab to compare models \ngt_fmt &lt;-\n  aictab(model_set, modnames = model_names) \n\n# create gt table using AIC\ngt_print &lt;- \n  gt(gt_fmt)\n\ngt_print\n\n\n\n\n\n\nModnames\nK\nAICc\nDelta_AICc\nModelLik\nAICcWt\nLL\nCum.Wt\n\n\n\nmain_effects\n5\n173.8562\n0.000000\n1.00000000\n0.81041300\n-81.59474\n0.8104130\n\n\ninteraction\n7\n177.1178\n3.261693\n0.19576377\n0.15864950\n-80.92256\n0.9690625\n\n\nfull_model\n9\n180.3873\n6.531150\n0.03817497\n0.03093749\n-80.14714\n1.0000000\n\n\n\n\n\n\n\nAs shown in this table, the main_effects model, which only includes the fertilizer and density variables without their interaction term, has the lowest AIC and is therefore the best fit for our crop data analysis.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Two-Way ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_two_way.html#post-hoc-testing-tukey-hsd",
    "href": "lessons/04_anova_two_way.html#post-hoc-testing-tukey-hsd",
    "title": "\n22  Two-Way ANOVA\n",
    "section": "\n22.6 Post-Hoc Testing (Tukey HSD)",
    "text": "22.6 Post-Hoc Testing (Tukey HSD)\nWe now know which parameters are significant, however, we are also interested in learning how the levels of the factors differ from each other. To quantify these differences, the Tukey’s Honestly-Significant-Difference test can be used.\n\n# Performing Tukey HSD on the final model (yield ~ fertilizer + density)\ntukey_crop &lt;- TukeyHSD(main_effects)  \n  \ntukey_crop %&gt;% \n# turn model into data.frame of parameters\n  tidy %&gt;% \n# create gt table\n  gt() %&gt;% \n  tab_header(\n      title = \"Tukey Multiple Comparisons of Means\",\n      subtitle = \"for main effects model (y ~ fertilizer + density)\"\n  )\n\n\n\n\n\n\n\nTukey Multiple Comparisons of Means\n\n\nfor main effects model (y ~ fertilizer + density)\n\n\nterm\ncontrast\nnull.value\nestimate\nconf.low\nconf.high\nadj.p.value\n\n\n\n\nfertilizer\n2-1\n0\n0.1761687\n-0.16822506\n0.5205625\n0.4452958212\n\n\nfertilizer\n3-1\n0\n0.5991256\n0.25473179\n0.9435194\n0.0002218678\n\n\nfertilizer\n3-2\n0\n0.4229569\n0.07856306\n0.7673506\n0.0119381379\n\n\ndensity\n2-1\n0\n0.4619560\n0.22752045\n0.6963916\n0.0001741423\n\n\n\n\n\n\n\nThis table shows the pairwise differences between each level of the factors. Comparisons with p values less than 0.05 are considered significant:\n\nfertilizer type 1 vs 3\nfertilizer type 2 vs 3\nlow vs high density\n\nTo visualize the differences between the levels of each factor, the 95% family-wise confidence intervals of the pairs for fertilizer and density were plotted below. The x-axes of the plots display the difference in means between the paired levels. The y-axis denotes the pair being compared.\n\n# plot tukey confidence intervals and set tick marks to horizontal (las = 1)\n#   position\nplot(tukey_crop, las = 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the significant confidence intervals do not include zero. From this plot, we see that only the fertilizer comparison of type 1 and 2 confidence interval includes 0. Thus, there is no statistically significant difference in the average crop yield produced by fertilizer 1 vs fertilizer 2. 95% family-wise confidence intervals for all other comparisons are statistically significant at the α = 0.05 level.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Two-Way ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_two_way.html#anova-vs.-linear-regression",
    "href": "lessons/04_anova_two_way.html#anova-vs.-linear-regression",
    "title": "\n22  Two-Way ANOVA\n",
    "section": "\n22.7 ANOVA vs. Linear Regression",
    "text": "22.7 ANOVA vs. Linear Regression\nThe model underlying the ANOVA we just performed is actually a linear regression. Let’s see what results we would get if we used linear regression to model the same relationships:\n\n# create linear regression model of yield ~ fertilizer + density\ncrop_lm &lt;- lm(yield ~ fertilizer + density, data = crop_data_df)\n\nsummary(crop_lm)\n\n\nCall:\nlm(formula = yield ~ fertilizer + density, data = crop_data_df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.16523 -0.30208 -0.05802  0.42576  1.47375 \n\nCoefficients:\n            Estimate Std. Error  t value Pr(&gt;|t|)    \n(Intercept) 176.5261     0.1180 1495.490  &lt; 2e-16 ***\nfertilizer2   0.1762     0.1446    1.219 0.226115    \nfertilizer3   0.5991     0.1446    4.144 7.57e-05 ***\ndensity2      0.4620     0.1180    3.914 0.000174 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5783 on 92 degrees of freedom\nMultiple R-squared:  0.2667,    Adjusted R-squared:  0.2428 \nF-statistic: 11.15 on 3 and 92 DF,  p-value: 2.601e-06\n\n\nThe output from the linear regression model indicate a significant difference in average yield exists between crops fertilized with fertilizer 3 and 1 (p = 7.57 e-05) but not fertilizers 1 and 2 (p = 0.22). Furthermore, the average yield for high density acres is significantly different than low density acres (p &lt; 0.001). These findings are consistent with the results of the Tukey’s HSD tests.\nRecall that the two-way ANOVA compares the variance within all levels of a factor to the total variation of the model. Similarly, the linear regression quantifies the variation of each group (or “level”) from the mean using dummy variables. Both use the same computations and thus the sum of squares for the independent variables are the same in the outputs of both the two-way ANOVA and linear regression. Because the density variable consists of two groups, its p value (p = 0.00017) is also the same in both models. The p value(s) for fertilizer differs between the two, however, because the three fertilizer groups are split into three dummy variables in the linear regression and compared against the group mean individually. The two-way ANOVA, on the other hand, is only concerned with the total variance within the fertilizer factor. Read “Common statistical tests are linear models” to learn more about how ANOVAs and many other statistical tests are simply different flavors of linear regression.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Two-Way ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_two_way.html#conclusions",
    "href": "lessons/04_anova_two_way.html#conclusions",
    "title": "\n22  Two-Way ANOVA\n",
    "section": "\n22.8 Conclusions",
    "text": "22.8 Conclusions\nThere is a statistically significant difference in average crop yield by both the fertilizer type and planting density variables with F values of 9.018 (p &lt; 0.001) and 15.316 (p &lt; 0.001) respectively. The interaction between these two terms was not significant.\nThe Tukey post-hoc test showed significant pairwise differences in average yield between fertilizer types 1 and 3 and between type 2 and 3. It also depicted significant differences in average yield between low and high planting density. Therefore, corn fertilized with fertilizer type 3 produced significantly more bushels per acre than those with fertilizers type 1 and 2, and acres of corn planted at high density produced more bushels than those planted at low density.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Two-Way ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_welch_two_way.html",
    "href": "lessons/04_anova_welch_two_way.html",
    "title": "\n23  Welch’s Anova\n",
    "section": "",
    "text": "23.1 Packages for this lesson\nWe will need to load tidyverse (which contains ggplot2, dplyr and tibble), gridExtra, emmeans and e1071. We are going to use the mtcars dataset included in r studio.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Welch's Anova</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_welch_two_way.html#introduction",
    "href": "lessons/04_anova_welch_two_way.html#introduction",
    "title": "\n23  Welch’s Anova\n",
    "section": "\n23.2 Introduction",
    "text": "23.2 Introduction\nWelch’s ANOVA, an extension of Welch’s t-test, is designed to compare the means of three or more groups when the assumption of equal variances (homoscedasticity) is violated. Unlike the traditional ANOVA, which assumes that the variances across groups are equal, Welch’s ANOVA is robust to heteroscedasticity, making it more reliable when dealing with real-world data that often exhibit unequal variances.\nThis statistical method is particularly useful in fields such as biology, medicine, and social sciences, where differences in group variances are common. By adjusting the degrees of freedom and using a modified F-ratio, Welch’s ANOVA provides a more accurate p-value for hypothesis testing under these conditions. This adaptation enhances the validity of the conclusions drawn from the data, ensuring that the results are not unduly influenced by variance inequality (Liu, 2015a)\n\n23.2.1 When to use Welch´s Anova\nWelch’s ANOVA is particularly useful in the following scenarios:\nUnequal Variances (Heteroscedasticity):\nWhen the variances across the groups being compared are not equal, Welch’s ANOVA should be used. Traditional ANOVA assumes homoscedasticity (equal variances), and when this assumption is violated, it can lead to inaccurate results. Welch’s ANOVA adjusts for unequal variances, providing more reliable results (Delacre, Leys, Mora, & Lakens, 2019).\nUnequal Sample Sizes:\nWhen the groups have different sample sizes, Welch’s ANOVA is more appropriate. Traditional ANOVA is sensitive to unequal sample sizes, especially when combined with unequal variances. Welch’s ANOVA is robust to these differences, ensuring that the statistical conclusions remain valid (Delacre et al., 2019).\nComparing Means of Three or More Groups:\nSimilar to the traditional ANOVA, Welch’s ANOVA is used when you need to compare the means of three or more groups. It extends the principles of Welch’s t-test (which compares two groups) to multiple groups (Delacre et al., 2019).\nReal-World Data with Variability:\nIn practical applications, data often do not meet the strict assumptions of equal variances and equal sample sizes. Welch’s ANOVA is more flexible and can handle the variability often found in real-world data, making it a preferred choice in many applied research settings.\nIncreased Reliability:\nWhen you require a statistical test that remains robust under less-than-ideal conditions, such as non-normal distributions and unequal group variances, Welch’s ANOVA provides increased reliability and accuracy compared to traditional ANOVA (Delacre et al., 2019).\nPractical Examples\nMedical Research: Comparing the effectiveness of different treatments across patient groups where the variance in treatment effects is different.\nEducational Studies: Analyzing test scores from students in different schools or classes where the variability in scores differs.\nBiological Experiments: Evaluating the growth of plants under different treatments when the growth variability is not consistent across treatments.\nBy using Welch’s ANOVA in these scenarios, researchers can ensure their statistical analyses are valid and robust, leading to more accurate and trustworthy conclusions.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Welch's Anova</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_welch_two_way.html#assumptions-of-welchs-anova",
    "href": "lessons/04_anova_welch_two_way.html#assumptions-of-welchs-anova",
    "title": "\n23  Welch’s Anova\n",
    "section": "\n23.3 Assumptions of Welch´s Anova",
    "text": "23.3 Assumptions of Welch´s Anova\nWelch’s ANOVA, like any statistical test, has specific assumptions that need to be met to ensure valid and reliable results. However, it relaxes some of the assumptions required by traditional ANOVA, making it more robust to real-world data. Here are the key assumptions of Welch’s ANOVA:\nIndependence of Observations:\nThe observations within each group and between groups should be independent. This means that the data points collected from different groups do not influence each other.\nNormality:\nThe dependent variable should be approximately normally distributed within each group. Welch’s ANOVA is fairly robust to deviations from normality, especially with larger sample sizes. However, significant departures from normality can affect the results, so it’s important to assess the distribution of the data.\nUnequal Variances:\nUnlike traditional ANOVA, Welch’s ANOVA does not assume equal variances across groups. It is specifically designed to handle heteroscedasticity (unequal variances) by adjusting the degrees of freedom and modifying the F-ratio. This makes Welch’s ANOVA more appropriate when the homogeneity of variances assumption is violated.(Liu, 2015b).\nThe formula for the Welch’s anova is given by:\n\\[\nF = \\frac{\\sum_{i=1}^{k} \\frac{n_i (\\bar{X}_i - \\bar{X})^2}{s_i^2}}{1 + \\frac{2(k-2)}{k^2 - 1} \\sum_{i=1}^{k} \\left( \\frac{1}{n_i - 1} \\right) \\left( \\frac{s_i^2}{s_{avg}^2} \\right)^2 }\n\\]\nWhere: \\[\\begin{align*}\n\\bar{X}_i & \\text{ is the sample mean of the i-th group} \\\\\n\\bar{X} & \\text{ is the grand mean (overall mean)} \\\\\nn_i & \\text{ is the sample size of the i-th group} \\\\\ns_i^2 & \\text{ is the sample variance of the i-th group} \\\\\ns_{avg}^2 & \\text{ is the average of the sample variances, } s_{avg}^2 = \\frac{1}{k} \\sum_{i=1}^{k} s_i^2 \\\\\nk & \\text{ is the number of groups}\n\\end{align*}\\]\nThe degrees of freedom for the numerator and the denominator are given by (Liu, 2015b):\n\\[\n\\text{df}_1 = k - 1\n\\]\n\\[\n\\text{df}_2 = \\frac{ \\left( 1 + \\frac{2(k-2)}{k^2 - 1} \\sum_{i=1}^{k} \\left( \\frac{1}{n_i - 1} \\right) \\left( \\frac{s_i^2}{s_{avg}^2} \\right)^2 \\right)^2 }{ \\frac{2(k-2)}{k^2 - 1} \\sum_{i=1}^{k} \\left( \\frac{1}{(n_i - 1)^2} \\right) \\left( \\frac{s_i^2}{s_{avg}^2} \\right)^2 }\n\\]",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Welch's Anova</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_welch_two_way.html#comparison-of-traditional-anova-and-welchs-anova",
    "href": "lessons/04_anova_welch_two_way.html#comparison-of-traditional-anova-and-welchs-anova",
    "title": "\n23  Welch’s Anova\n",
    "section": "\n23.4 Comparison of Traditional ANOVA and Welch’s ANOVA",
    "text": "23.4 Comparison of Traditional ANOVA and Welch’s ANOVA\nWelch’s ANOVA and traditional ANOVA are both used to compare the means of three or more groups, but they have different assumptions and are suitable for different scenarios. Here’s a detailed comparison:\n\n\n\n\n\n\nComparison Summary\n\n\n\n\n\n\n\n\n\n\nAspect\nTraditional ANOVA\nWelch’s ANOVA\n\n\n\nAssumptions\nEqual Variances (Homoscedasticity): Assumes that the variances across groups are equal.Normality: Assumes that the data within each group are normally distributed.Independence: Assumes that the observations are independent of each other.\nUnequal Variances (Heteroscedasticity): Does not assume equal variances across groups, making it suitable for data with heteroscedasticity.Normality: Assumes that the data within each group are normally distributed, similar to traditional ANOVA.Independence: Assumes that the observations are independent of each other.\n\n\nRobustness to Assumptions\nSensitive to Unequal Variances: If the assumption of equal variances is violated, the results of traditional ANOVA can be misleading.Robust to Normality: Fairly robust to deviations from normality, especially with larger sample sizes (Liu, 2015b).\nRobust to Unequal Variances: Specifically designed to handle unequal variances, providing more reliable results when variances differ across groups.Robust to Normality: Similar robustness to normality as traditional ANOVA, but still requires approximate normality within groups (Liu, 2015b).\n\n\nDegrees of Freedom\nDegrees of freedom for the numerator (df1) is k-1, where k is the number of groups.Degrees of freedom for the denominator (df2) is N-k, where N is the total number of observations (Liu, 2015b).\nDegrees of freedom for the numerator (df1) is k-1, similar to traditional ANOVA.Degrees of freedom for the denominator (df2) is adjusted based on the variances and sample sizes of the groups, making it more complex and generally lower than N-k (Liu, 2015b).\n\n\nApplication Scenarios\nUse when you have equal variances across groups and possibly equal sample sizes. Suitable for controlled experimental designs where assumptions can be maintained.\nUse when variances across groups are unequal or when sample sizes are unequal. Suitable for real-world data where assumptions of traditional ANOVA are often violated.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Welch's Anova</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_welch_two_way.html#reliability-of-welchs-anova",
    "href": "lessons/04_anova_welch_two_way.html#reliability-of-welchs-anova",
    "title": "\n23  Welch’s Anova\n",
    "section": "\n23.5 Reliability of Welch’s ANOVA",
    "text": "23.5 Reliability of Welch’s ANOVA\nWelch’s ANOVA is designed to be a robust alternative to traditional ANOVA, particularly when dealing with unequal variances across groups. Here’s a detailed look at factors affecting the reliability of Welch’s ANOVA, including minimum sample size, power, effect size, variance, significance level, and non-normality.\n1. Minimum Sample Size\nSmall Sample Sizes: Welch’s ANOVA can be used with small sample sizes, but its power is limited with very small samples (e.g., fewer than 5 observations per group). In such cases, the test might not detect significant differences even if they exist.\nRecommended Minimum: A common recommendation is to have at least 10-15 observations per group. This ensures a reasonable level of reliability and power. This isn’t a strict rule but a guideline to aim for.\n2. Power and Effect Size\nPower Analysis: Power is the probability of correctly rejecting the null hypothesis when it is false. To determine the appropriate sample size for a specific context, you can perform a power analysis. This involves specifying the expected effect size, the desired power (commonly 0.80), and the significance level (commonly 0.05).\nEffect Size: Larger effect sizes (i.e., larger differences between group means) can be detected with smaller sample sizes. Smaller effect sizes require larger sample sizes to be detected. Power analysis can help estimate the necessary sample size to achieve sufficient power to detect an effect.\n3. Variance\nUnequal Variances: Welch’s ANOVA is specifically designed to handle unequal variances. It adjusts the degrees of freedom and uses a modified F-ratio to account for this heteroscedasticity, providing more accurate p-values.\nLarge Variance Differences: When variances between groups are very different, larger sample sizes may be needed to ensure the robustness of the test.\n4. Significance Level\nCommon Significance Levels: The significance level (α) is the threshold for determining statistical significance, with common values being 0.05, 0.01, and 0.10.\nImpact of Significance Level: Lowering the significance level (e.g., from 0.05 to 0.01) makes the test more stringent, reducing the chance of Type I errors (false positives). However, this typically requires larger sample sizes to maintain power.\nType I and Type II Errors: Balancing the significance level and power is crucial to minimize both Type I errors (incorrectly rejecting the null hypothesis) and Type II errors (failing to reject a false null hypothesis).\n5. Non-Normality\nModerate Robustness: Welch’s ANOVA is moderately robust to deviations from normality, especially as sample sizes increase. This robustness is due to the Central Limit Theorem, which ensures that the sampling distribution of the test statistic approaches normality with larger sample sizes.\nSevere Non-Normality: For small sample sizes, significant departures from normality can affect the reliability of Welch’s ANOVA. In such cases, the test may not perform well, and the results might be less trustworthy.\nAlternative Approaches: When normality is a concern, consider non-parametric alternatives like the Kruskal-Wallis test, which does not assume normality.\n\n\n\n\n\n\nGuidelines for Choosing Welch’s ANOVA or Kruskal-Wallis Test\n\n\n\n\n\nUses Welch’s ANOVA if all sample sizes are ≥ 30 or Shapiro-Wilk test is ≥ 0.05 (normal distribution).\n\nUses Welch’s ANOVA if all sample sizes are ≥ 15 and the p-values from the Shapiro-Wilk test are &gt; 0.01 (indicating only moderate deviations from normality).\n\nUses the Kruskal-Wallis test if any sample size is &lt; 15 or if the p-values from the Shapiro-Wilk test are ≤ 0.01 (indicating severe deviations from normality).\n\n\n\nPractical Recommendations:\nAssess Normality and Variance: Before applying Welch’s ANOVA, assess the normality and variances of your data. Use tests like Shapiro-Wilk for normality and Levene’s test for variance homogeneity. Perform Power Analysis: Conduct a power analysis to determine the appropriate sample size for your study, considering the expected effect size and desired power. Consider Sample Size Guidelines: Aim for at least 10-15 observations per group to ensure reasonable power and reliability.\n\n23.5.1 Let’s Remember\n\n\n\n\n\n\nLet’s remember\n\n\n\nThe significance level, denoted as α, is a threshold in hypothesis testing that determines whether to reject the null hypothesis. It is the probability of making a Type I error, which occurs when the null hypothesis is true but incorrectly rejected. Common values for α are 0.05, 0.01, and 0.10 (Labovitz, 2017).\nFor α = 0.05, there is a 5% chance of rejecting the null hypothesis when it is true. If the p-value from a statistical test is less than or equal to α, we reject the null hypothesis; if it is greater, we do not reject the null hypothesis.\nThe Central Limit Theorem (CLT) states that the distribution of the sample mean will approximate a normal distribution as the sample size becomes large, regardless of the population’s distribution, provided the samples are independent and identically distributed (Kwak & Kim, 2017).\nIn the context of Welch’s test, the CLT implies that as the sample size increases, the distribution of the test statistic approaches normality, reducing the impact of non-normality in the original data. Thus, Welch’s test becomes more reliable with larger samples because the sampling distribution of the mean difference tends to be normal.\nIf the samples are not identically distributed, meaning they come from populations with different distributions, the assumptions underlying the Central Limit Theorem (CLT) and many statistical tests, including Welch’s t-test, may be violated. This can have several consequences:\n\n\nReduced Accuracy: The approximation to the normal distribution for the sample mean may not hold, leading to inaccurate p-values and confidence intervals.\n\nIncreased Type I and Type II Errors: There may be an increased risk of Type I errors (incorrectly rejecting a true null hypothesis) and Type II errors (failing to reject a false null hypothesis).\n\nBiased Results: The test results may be biased, reflecting the differences in the underlying distributions rather than the true differences between the population means (Ruxton, 2006).\n\n\n\n\n23.5.2 Suspecting Different Variances:\nYou might suspect different variances when comparing groups (samples) and their spread appears noticeably different. For example, if one group’s data points are more dispersed than the others, it could indicate unequal variances.The welch´s Anova don´t need equal variances to perform well but you can asses variances with graphical methods and using the Levene´s test.\n\n23.5.3 Assessing normality:\nShapiro-Wilk Test: Purpose: Determines if a sample comes from a normally distributed population. Suitability for Small Samples: Good power even with small sample sizes (&lt; 50). How It Works: Compares the sample data to a normal distribution. Interpretation: If p-value &lt; chosen alpha level, data is not normally distributed. Advantages: Sensitive to deviations in both location and shape. Limitations: May detect trivial deviations due to large sample size1 (Razali & Wah, 2011).\nKolmogorov-Smirnov (K-S) Test: Purpose: Compares two samples or tests if a sample matches a reference distribution. Suitability for Small Samples: Useful for small to large sample sizes. How It Works: Quantifies the distance between empirical and reference cumulative distribution functions. Interpretation: Compares observed data to expected distribution. Advantages: Sensitive to differences in both location and shape. Limitations: May not be better than Shapiro-Wilk for small samples (Razali & Wah, 2011).\nAnderson-Darling Test: Purpose: Tests if a sample comes from a specific distribution (e.g., normal). Suitability for Small Samples: Similar to Shapiro-Wilk. How It Works: Compares observed data to expected distribution. Interpretation: Reject null hypothesis if p-value &lt; chosen alpha level. Advantages: Generalizes well for various distributions. Limitations: Adjust for parameter estimation if needed (Razali & Wah, 2011).\nGraphical Inspection: Plot histograms or box plots for each group. Look for differences in spread!!",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Welch's Anova</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_welch_two_way.html#defyning-the-question-and-hypotheses",
    "href": "lessons/04_anova_welch_two_way.html#defyning-the-question-and-hypotheses",
    "title": "\n23  Welch’s Anova\n",
    "section": "\n23.6 Defyning the question and hypotheses",
    "text": "23.6 Defyning the question and hypotheses\nNull hypothesis (H0): 4 cylinder cars, 6 cylinder cars and 8 cylinder cars have equal miles per galon (mpg) mean. Alternative hypothesis (HA): 4 cylinder cars, 6 cylinder cars and 8 cylinder cars have not equal miles per galon (mpg) mean.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Welch's Anova</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_welch_two_way.html#dataset-visualization",
    "href": "lessons/04_anova_welch_two_way.html#dataset-visualization",
    "title": "\n23  Welch’s Anova\n",
    "section": "\n23.7 Dataset visualization",
    "text": "23.7 Dataset visualization\nWe will use the mtcars dataset in R for this demonstration. This dataset contains various attributes of different car models and at priori we are not sure if the assumptions are meet, so we have to assess the data first:\n\n\n'data.frame':   32 obs. of  11 variables:\n $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n $ disp: num  160 160 108 258 360 ...\n $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n $ qsec: num  16.5 17 18.6 19.4 17 ...\n $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\n\n\nAs we can see, we have a data set with the information about different types of cars, with 11 different variables including miles per gallon (mpg), cylinder (cyl), horse power (hp), etc., and there are 32 observations for each variable.\n\n\n      mpg             cyl             disp             hp       \n Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n 1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \n Median :19.20   Median :6.000   Median :196.3   Median :123.0  \n Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n 3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \n Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n      drat             wt             qsec             vs        \n Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n 1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \n Median :3.695   Median :3.325   Median :17.71   Median :0.0000  \n Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n 3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \n Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n       am              gear            carb      \n Min.   :0.0000   Min.   :3.000   Min.   :1.000  \n 1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  \n Median :0.0000   Median :4.000   Median :2.000  \n Mean   :0.4062   Mean   :3.688   Mean   :2.812  \n 3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  \n Max.   :1.0000   Max.   :5.000   Max.   :8.000",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Welch's Anova</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_welch_two_way.html#plotting-mtcars-data",
    "href": "lessons/04_anova_welch_two_way.html#plotting-mtcars-data",
    "title": "\n23  Welch’s Anova\n",
    "section": "\n23.8 Plotting Mtcars data",
    "text": "23.8 Plotting Mtcars data\n\n\n\n\nCars by gears and cylinders\n\n\n\nOn this bar graph we observe the count of cars by number of cylinders and gears.\n\n\n\n\nmpg v.s hp\n\n\n\nJust to give us an idea of the data set, on this plot we observed the relation between horse power and cylinders. The cars with more cylinders have more hp and probably do less miles per gallon.\n\n23.8.1 Assessing the distribution\n\n\nWarning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\n\n\n\nHistogram 4-cylinder vs 6-cylinder vs 8-cylinder\n\n\n\nThis histogram shows there is a difference distribution on the miles per gallon (MPG) variable between 4, 6 and 8 cylinder cars. 8 and 6 cylinder cars appear to have a normal distribution but 4 cylinder cars not.\n\n\n\n\nBoxplot 4-cylinder vs 6 cylinder vs 8 cylinder\n\n\n\nThis boxplot also shows the difference between the three groups. 4 cylinder cars have a bigger variance on the mpg variable.\n\n\n\n\nQ-Q Plots for 4-Cylinder, 6-Cylinder, and 8-Cylinder Cars\n\n\n\nThe divergence between the red line and the blue points in a Q-Q plot indicates the degree and nature of the difference between the two distributions being compared.\n\n23.8.2 Assessing skewness and kurtosis\n\n### Assessing skewness and kurtosis\n#| label: skewness and kurtosis\n#| echo: true\n#| message: false\n\n# Calculate skewness and kurtosis for mpg_4_cyl\nskewness_4_cyl &lt;- skewness(mpg_4_cyl)\nkurtosis_4_cyl &lt;- kurtosis(mpg_4_cyl)\n\n# Calculate skewness and kurtosis for mpg_6_cyl\nskewness_6_cyl &lt;- skewness(mpg_6_cyl)\nkurtosis_6_cyl &lt;- kurtosis(mpg_6_cyl)\n\n# Calculate skewness and kurtosis for mpg_8_cyl\nskewness_8_cyl &lt;- skewness(mpg_8_cyl)\nkurtosis_8_cyl &lt;- kurtosis(mpg_8_cyl)\n\n# Print the results\ncat(\"Skewness for mpg_4_cyl:\", skewness_4_cyl, \"\\n\")\n\nSkewness for mpg_4_cyl: 0.2591965 \n\ncat(\"Kurtosis for mpg_4_cyl:\", kurtosis_4_cyl, \"\\n\")\n\nKurtosis for mpg_4_cyl: -1.645012 \n\ncat(\"Skewness for mpg_6_cyl:\", skewness_6_cyl, \"\\n\")\n\nSkewness for mpg_6_cyl: -0.1583137 \n\ncat(\"Kurtosis for mpg_6_cyl:\", kurtosis_6_cyl, \"\\n\")\n\nKurtosis for mpg_6_cyl: -1.906971 \n\ncat(\"Skewness for mpg_8_cyl:\", skewness_8_cyl, \"\\n\")\n\nSkewness for mpg_8_cyl: -0.3628186 \n\ncat(\"Kurtosis for mpg_8_cyl:\", kurtosis_8_cyl, \"\\n\")\n\nKurtosis for mpg_8_cyl: -0.5655154 \n\n\nSkewness: Skewness measures the asymmetry of the distribution of a variable. A normal distribution has a skewness value of zero, indicating symmetry. Positive skewness means the right tail is longer (values cluster to the left of the mean), while negative skewness means the left tail is longer. If skewness is substantial (e.g., greater than 2.1), it suggests departure from normality. It is not the case here!!\nKurtosis: Kurtosis measures the peakedness of a distribution. The original kurtosis value is sometimes called “kurtosis (proper).” A normal distribution has kurtosis (proper) equal to 3. Excess kurtosis (obtained by subtracting 3 from the proper kurtosis) is often used. Substantial departure from normality occurs when excess kurtosis is greater than 7.1\n\n23.8.3 Normality test\nShapiro-Wilk Test:\n\n# Perform Shapiro-Wilk normality test\nshapiro.test(mpg_4_cyl)\n\n\n    Shapiro-Wilk normality test\n\ndata:  mpg_4_cyl\nW = 0.91244, p-value = 0.2606\n\nshapiro.test(mpg_6_cyl)\n\n\n    Shapiro-Wilk normality test\n\ndata:  mpg_6_cyl\nW = 0.89903, p-value = 0.3252\n\nshapiro.test(mpg_8_cyl)\n\n\n    Shapiro-Wilk normality test\n\ndata:  mpg_8_cyl\nW = 0.93175, p-value = 0.3229\n\n\nFor mpg_4_cyl: the test statistic (W = 0.91244) indicates that the data is relatively close to a normal distribution. The p-value (0.2606) is not statistically significant (above the typical threshold of 0.05). Interpretation: The data for mpg_4_cyl is not significantly different from a normal distribution. However, with small sample sizes, the test may have limited power to detect departures from normality.\nFor mpg_6_cyl: the test statistic (W = 0.89903) is slightly lower than for mpg_4_cyl but still indicates a relatively normal distribution. The p-value (0.3252) is not statistically significant (above 0.05). Interpretation: The data for mpg_6_cyl is also not significantly different from a normal distribution based on the Shapiro-Wilk test.\nFor mpg_8_cyl: the test statistic (W = 0.93175) is slightly higher than for mpg_4_cyl but still indicates a relatively normal distribution. The p-value (0.3229) is not statistically significant (above 0.05). Interpretation: The data for mpg_8_cyl is also not significantly different from a normal distribution based on the Shapiro-Wilk test.\nAgain, note that with small sample sizes, the ability to detect deviations from normality may be limited. Small sample sizes can limit the power of statistical tests to detect departures from normality, and it’s important to consider the context and potential limitations when interpreting such results.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Welch's Anova</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_welch_two_way.html#performing-the-welchs-anova",
    "href": "lessons/04_anova_welch_two_way.html#performing-the-welchs-anova",
    "title": "\n23  Welch’s Anova\n",
    "section": "\n23.9 Performing the Welchs Anova",
    "text": "23.9 Performing the Welchs Anova\nWe will perform a Welch’s Anova to compare the mean miles per gallon (mpg) between cars with 4 cylinders, with 6 cylinders and with 8 cylinders cars.\nHypotheses Null Hypothesis (H0): The mean mpg of cars with 4 cylinders, 6 cylinders and 8 cylinders is equal. Alternative Hypothesis (H1): The mean mpg of cars with 4 cylinders, 6 cylinders and 8 cylinders is not equal.\n\n23.9.1 Welchs Anova results\n\n\n\n    One-way analysis of means (not assuming equal variances)\n\ndata:  mpg and cylinders\nF = 31.624, num df = 2.000, denom df = 18.032, p-value = 1.271e-06\n\n\n cylinders emmean    SE df lower.CL upper.CL\n 4           26.7 0.972 29     24.7     28.7\n 6           19.7 1.218 29     17.3     22.2\n 8           15.1 0.861 29     13.3     16.9\n\nConfidence level used: 0.95 \n\n\n contrast                estimate   SE df t.ratio p.value\n cylinders4 - cylinders6     6.92 1.56 29   4.441  0.0003\n cylinders4 - cylinders8    11.56 1.30 29   8.905  &lt;.0001\n cylinders6 - cylinders8     4.64 1.49 29   3.112  0.0112\n\nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\n\n23.9.2 Interpretation or results:\nF-Value: The test statistic for the Welch’s ANOVA. num df: The degrees of freedom for the numerator. denom df: The degrees of freedom for the denominator. p-value: The p-value for the test. A small p-value (typically &lt; 0.05) indicates that there are significant differences between the group means.\n• *F-Statistic: 31.624 • Numerator Degrees of Freedom: 2.000 • Denominator Degrees of Freedom: 18.032 • P-Value: 1.271×10−6.\nThe extremely low p-value indicates that there is a statistically significant difference in the mean MPG across the three groups of cars (4-cylinder, 6-cylinder, and 8-cylinder).\n\nEstimated Marginal Means (MPG) for Each Cylinder Group\n\n\nCylinders\nEMM\nSE\ndf\nLower CI\nUpper CI\n\n\n\n4\n26.7\n0.972\n29\n24.7\n28.7\n\n\n6\n19.7\n1.218\n29\n17.3\n22.2\n\n\n8\n15.1\n0.861\n29\n13.3\n16.9\n\n\n\nInterpretation: - The average MPG for 4-cylinder cars is 26.7. - The average MPG for 6-cylinder cars is 19.7. - The average MPG for 8-cylinder cars is 15.1.\nThese intervals do not overlap, suggesting clear differences in the mean MPG across the different cylinder groups.\n\n\nPairwise Comparisons of Mean MPG\n\n\n\n\n\n\n\n\n\n\nContrast\nEstimate\nSE\ndf\nt-ratio\np-value\n\n\n\n4 Cylinders - 6 Cylinders\n6.92\n1.56\n29\n4.441\n0.0003\n\n\n4 Cylinders - 8 Cylinders\n11.56\n1.30\n29\n8.905\n&lt;.0001\n\n\n6 Cylinders - 8 Cylinders\n4.64\n1.49\n29\n3.112\n0.0112\n\n\n\nInterpretation: - 4 Cylinders vs. 6 Cylinders: The mean difference in MPG is 6.92, which is statistically significant (( p = 0.0003 )). - 4 Cylinders vs. 8 Cylinders: The mean difference in MPG is 11.56, which is highly statistically significant (( p &lt; 0.0001 )). - 6 Cylinders vs. 8 Cylinders: The mean difference in MPG is 4.64, which is statistically significant (( p = 0.0112 )).\nAll these differences are statistically significant, indicating that the MPG for 4-cylinder cars is significantly higher than for 6-cylinder and 8-cylinder cars, and the MPG for 6-cylinder cars is significantly higher than for 8-cylinder cars.\n\n\n\n\n\n\n\nMean: The mean is a basic measure of central tendency. It represents the arithmetic average of a set of values. It provides a single value that summarizes the data distribution. However, it doesn’t account for other factors or covariates.\nEstimated Marginal Mean (EMM): EMMs are adjusted means that consider other variables in the model. They account for the effects of covariates, making them more informative. EMMs are useful when comparing treatment levels in complex models. For example, in an ANOVA with interactions or mixed-effects models, EMMs provide adjusted group means. EMMs help address confounding and provide a clearer picture of treatment effects. In summary, while the mean is straightforward and unadjusted, EMMs offer a more nuanced understanding by considering covariates.\n\n\n\nConclusion\nThe analysis shows that there are significant differences in the mean MPG between cars with different numbers of cylinders. Specifically, 4-cylinder cars have the highest mean MPG, followed by 6-cylinder cars, and then 8-cylinder cars. The pairwise comparisons confirm that all these differences are statistically significant, with the 4-cylinder cars performing significantly better in terms of fuel efficiency compared to both 6-cylinder and 8-cylinder cars.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Welch's Anova</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_welch_two_way.html#satterthwaite-degrees-of-freedom",
    "href": "lessons/04_anova_welch_two_way.html#satterthwaite-degrees-of-freedom",
    "title": "\n23  Welch’s Anova\n",
    "section": "\n23.10 Satterthwaite Degrees of Freedom",
    "text": "23.10 Satterthwaite Degrees of Freedom\nIn the context of Welch’s t-test and Welchs Anova, the Satterthwaite approximation is used to calculate an approximation of the degrees of freedom. This method provides a more accurate estimation compared to the standard t-test and traditional Anova when the variances of the two samples are not equal.\nThe formula for the Satterthwaite degrees of freedom is: \\[\n\\nu \\approx \\frac{\\left( \\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2} \\right)^2}{\\frac{\\left( \\frac{s_1^2}{n_1} \\right)^2}{n_1 - 1} + \\frac{\\left( \\frac{s_2^2}{n_2} \\right)^2}{n_2 - 1}}\n\\]\nWhere: \\[ s_1^2 \\] is the sample variance of the first sample (4-cylinder cars), \\[ s_2^2 \\] is the sample variance of the second sample (6-cylinder cars), \\[ n_1 \\] is the sample size of the first sample (4-cylinder cars) and \\[ n_2 \\] is the sample size of the second sample (6-cylinder cars).\n\n# Sample sizes\nn1 &lt;- length(mpg_4_cyl)\nn2 &lt;- length(mpg_6_cyl)\n\n# Sample variances\n\ns1_sq &lt;- var(mpg_4_cyl)\ns2_sq &lt;- var(mpg_6_cyl)\n\n# Satterthwaite degrees of freedom\nnumerator &lt;- (s1_sq / n1 + s2_sq / n2)^2\ndenominator &lt;- ((s1_sq / n1)^2 / (n1 - 1)) + ((s2_sq / n2)^2 / (n2 - 1))\ndf &lt;- numerator / denominator\n\n# Print the result\ndf\n\n[1] 12.95598\n\n\n\n23.10.1 Degrees of Freedom (DoF) explanation:\nThe degrees of freedom indicate the number of independent values or quantities which can vary in the analysis without breaking any constraints. In this context, the degrees of freedom are adjusted to better reflect the reliability of the variance estimates from the samples (Huang, 2016).\nSatterthwaite Approximation: this method provides an adjusted degrees of freedom value that accounts for differences in variances between the samples. The formula combines the sample variances and sizes to compute a more accurate degrees of freedom for the Welch’s t-test and Welchs Anova.\nImplications: using the Satterthwaite approximation leads to a more robust test when comparing means from two samples with unequal variances. The resulting degrees of freedom are used to determine the critical value from the t-distribution, which is crucial for calculating the p-value and making statistical inferences (Derrick, Toher, & White, 2016).\nDegrees of Freedom Calculation for the two sample t-test: can be calculated as follows:\n\\[\n\\text{DoF} = (n_1 - 1) + (n_2 - 1) = (n_1 + n_2 - 2)\n\\]\nWhere: \\[ n_1 \\] is the number of observations in the first group. \\[ n_2 \\] is the number of observations in the second group.\nWhy Subtract the Number of Groups?:\nWe subtract 2 because we are estimating one parameter (the mean) for each of the two groups. Each estimation reduces the degrees of freedom by 1. Thus, for two groups, we subtract 2 from the total number of observations to account for the two estimated means.\nSatterthwaite vs t-test degrees of freedom:\nCalculated Degrees of Freedom using the Satterthwaite method: approximately 18\nTraditional t-test Degrees of Freedom: n1+n2+n3-3=11+7+14-3=29\nThe Satterthwaite degrees of freedom (18) are lower than the traditional degrees of freedom (29). This adjustment accounts for the unequal variances between the three samples (4-cylinder, 6-cylinder cars and 8-cylinder cars) and provides a more accurate measure for the t-distribution used in the Welch’s t-test and Welchs Anova.\n\n23.10.2 Important concepts about the degrees of Freedom\n\n\nDegrees of Freedom (DoF)\n\n\nRepresent the number of independent values that can vary without breaking constraints.\n\n\nTraditional t-Test and Traditional Anova\n\n\nDoF is the total number of observations minus the number of groups because each group’s mean estimation uses one degree of freedom.\n\n\nSatterthwaite Approximation\n\n\nAdjusts the DoF for unequal variances, usually resulting in a lower and non-integer value, providing a more accurate basis for hypothesis testing.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Welch's Anova</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_welch_two_way.html#summary-and-conclusion",
    "href": "lessons/04_anova_welch_two_way.html#summary-and-conclusion",
    "title": "\n23  Welch’s Anova\n",
    "section": "\n23.11 Summary and Conclusion",
    "text": "23.11 Summary and Conclusion\nWelch’s Anova, useful for comparing means of three independent samples with unequal variances and sizes, was applied to the mtcars dataset to compare 4-cylinder, 6-cylinder and 8-cylinder cars MPG. The test revealed a significant difference in means, with a p-value of 0.000001271, rejecting the null hypothesis. The Satterthwaite approximation, yielding degrees of freedom at 18, provided a more accurate assessment than the traditional t-test (DoF = 29), ensuring robust statistical inference under unequal variances. This method enhances reliability, especially with large samples, where normality is approached due to the Central Limit Theorem.\n\n\n\n\nDelacre, M., Leys, C., Mora, Y. L., & Lakens, D. (2019). Taking parametric assumptions seriously: Arguments for the use of welch’s f-test instead of the classical f-test in one-way ANOVA. International Review of Social Psychology, 32(1).\n\n\nDerrick, B., Toher, D., & White, P. (2016). Why welch’s test is type I error robust. The quantitative methods for. Psychology, 12(1), 30–38.\n\n\nHuang, H. (2016). On the Welch-Satterthwaite formula for uncertainty estimation: A paradox and its resolution. Cal Lab the International Journal of Metrology, 23(4), 20–28.\n\n\nKwak, S. G., & Kim, J. H. (2017). Central limit theorem: The cornerstone of modern statistics. Korean J. Anesthesiol., 70(2), 144–156.\n\n\nLabovitz, S. (2017). Criteria for selecting a significance level:: A note on the sacredness of. 05. In The significance test controversy (pp. 166–171).\n\n\nLiu, H. (2015a). Comparing welch ANOVA, a Kruskal-Wallis test, and traditional ANOVA in case of heterogeneity of variance.\n\n\nLiu, H. (2015b). Comparing welch ANOVA, a Kruskal-Wallis test, and traditional ANOVA in case of heterogeneity of variance.\n\n\nRazali, N. M., & Wah, Y. B. (2011). Power comparisons of shapiro-wilk, kolmogorov-smirnov, lilliefors and anderson-darling tests. Journal of Statistical Modeling and Analytics, 2(1), 21–33.\n\n\nRuxton, G. D. (2006). The unequal variance t-test is an underused alternative to student’s t-test and the mann–whitney u test. Behavioral Ecology, 17(4), 688–690. https://doi.org/10.1093/beheco/ark016",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Welch's Anova</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_anova_kruskal_wallis.html",
    "href": "lessons_original/04_anova_kruskal_wallis.html",
    "title": "24  Non-Parametric ANOVA: The Kruskal-Wallis Test",
    "section": "",
    "text": "24.1 Introduction\nThe Kruskal-Wallis test (H-test) is a hypothesis test for multiple independent samples, which is used when the assumptions for a one factor analysis of variance are violated. In other word, it is the non-parametric alternative to the One Way ANOVA. Non-parametric means that the data does not follow normal distribution. It is sometimes called the one-way ANOVA on ranks, as the ranks of the data values are used in the test rather than the actual data points.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Non-Parametric ANOVA: The Kruskal-Wallis Test</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_anova_kruskal_wallis.html#introduction",
    "href": "lessons_original/04_anova_kruskal_wallis.html#introduction",
    "title": "24  Non-Parametric ANOVA: The Kruskal-Wallis Test",
    "section": "",
    "text": "24.1.1 Mathematical Equation\n\\[H = \\frac{{n-1}}{{n}}\\sum^k_{i=1}\\frac{n_i({\\bar{R_i} - E})^2}{{σ^2_R}}\\]\nWhere:\n\n\\(H\\) is the Kruskal-Wallis test statistic,\n\\(n\\) is the total number of observations,\n\\(R_i\\) is the sum of ranks for each group,\n\\(E\\) is the expected value of the sum of ranks under the null hypothesis.\n\\(σ^2_R\\) is the square of standard deviation of Rank sum.\n\nEquation for Expected Rank\n\\[E = \\frac{{n+1}}{{2}}\\]\nWhere:\n-n represents total number of observations.\nEquation for Rank Mean for group i\n\\[R_i = \\frac{{\\sum{R}}}{{n_g}}\\]\nWhere:\n\nR_i represents mean rank for \\(i^{th}\\) group,\n\\(\\sum{R}\\) represents sum of ranks in \\(i^{th}\\) group,\n\\(n_g\\) represents number of observation in \\(i^{th}\\) group.\n\nExample\n\n\n\nAssigning ranks/ E and mean rank calculated/ ready for H calculation\n\n\n\n\n24.1.2 Assumptions\n1. Ordinal or Continuous Response Variable – the response variable should be an ordinal or continuous variable.\n2. Independence – the observations in each group need to be independent of each other.\n3. Sample Size and distribution – each group must have a sample size of 5 or more and the distributions in each group need to have a similar shape but groups does not follow normal distribution.\n\n\n24.1.3 Hypothesis\nThe test determines whether two or more independent groups have same central tendency.\n\nH0: population rank sum average are equal for independent group and therefore come from same population.\nH1: population rank sum average are significantly different for at-least two or more independent group and therefore come from different population.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Non-Parametric ANOVA: The Kruskal-Wallis Test</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_anova_kruskal_wallis.html#running-kruskal-wallis-in-r",
    "href": "lessons_original/04_anova_kruskal_wallis.html#running-kruskal-wallis-in-r",
    "title": "24  Non-Parametric ANOVA: The Kruskal-Wallis Test",
    "section": "24.2 Running Kruskal-Wallis in R",
    "text": "24.2 Running Kruskal-Wallis in R\n\n24.2.1 Packages\n\n# install.packages(\"FSA\") # Houses dunnTest for pair wise comparison\n# install.packages(\"ggpubr\")  # For density plot and for creating and customizing 'ggplot2'- based publication ready plots\n# install.packages(\"ggstatplot\") # Houses gbetweenstats() function that allows building a combination of box and violin plots along with                                        statistical details.\n# install.packages(\"tidyverse\") # For wrangling and tidying the data\n# install.packages(\"MultNonParam\")\n\nlibrary(MultNonParam)\n\nLoading required package: ICSNP\n\n\nLoading required package: mvtnorm\n\n\nLoading required package: ICS\n\nsuppressPackageStartupMessages(library(ggpubr))   \nsuppressPackageStartupMessages(library(ggstatsplot))\nsuppressPackageStartupMessages(library(FSA))       \nsuppressPackageStartupMessages(library(gtsummary))\nsuppressPackageStartupMessages(library(tidyr))     \nsuppressPackageStartupMessages(library(tidyverse)) \n\n\n\n24.2.2 Data\nAs an example we will manually create a data, details of which can be found Here.\nThe data represents antibody production after receiving a vaccine. A hospital administered one of three different vaccines - A, B, or C to 6 individuals per group and measured the antibody presence (\\(\\mu\\)g/mL) in their blood after a chosen time period. The data is as follows: The goal of this exercise will be to determine how the three vaccines performed compared to each other. Essentially, we are looking to determine if the antibody data for each vaccine originates from the same distribution. The sample size is small and normal distribution cannot be assumed. Therefore, we will be conducting the Kruskal-Wallis test.\nNull Hypothesis (H0): The vaccines induce equal amounts of antibody production. (all three groups originate from the same distribution and have the same median)\nAlternative Hypothesis (H1): At least one vaccine induces different amount of antibodies to be produced.(at least one group originates from a different distribution and has a different median)\n\n# Creating dataframe for antibodies produced (in $\\mu$g/mL$) by three different vaccines;\n\nA &lt;- c(1232, 751, 339, 848, 447, 542)\nB &lt;- c(302, 57, 521, 278, 176, 201)\nC &lt;- c(839, 342, 473, 1128, 242, 475)\n\ndf &lt;- data.frame(A, B, C)\n\ndf_tidy &lt;- pivot_longer(\n  data = df,\n  cols = c(\"A\", \"B\", \"C\"),\n  names_to = \"Vaccines\",\n  values_to = \"Antibody\"\n)\n\ndf_tidy_sorted &lt;- \n  df_tidy %&gt;% \n  arrange(Vaccines)\n\ndf_tidy_sorted\n\n# A tibble: 18 × 2\n   Vaccines Antibody\n   &lt;chr&gt;       &lt;dbl&gt;\n 1 A            1232\n 2 A             751\n 3 A             339\n 4 A             848\n 5 A             447\n 6 A             542\n 7 B             302\n 8 B              57\n 9 B             521\n10 B             278\n11 B             176\n12 B             201\n13 C             839\n14 C             342\n15 C             473\n16 C            1128\n17 C             242\n18 C             475\n\nvaccine_efficacy = df_tidy_sorted\n\n\nstr(vaccine_efficacy)\n\ntibble [18 × 2] (S3: tbl_df/tbl/data.frame)\n $ Vaccines: chr [1:18] \"A\" \"A\" \"A\" \"A\" ...\n $ Antibody: num [1:18] 1232 751 339 848 447 ...\n\n\n\nvaccine_efficacy$Vaccines &lt;- as_factor(vaccine_efficacy$Vaccines)\n\nstr(vaccine_efficacy)\n\ntibble [18 × 2] (S3: tbl_df/tbl/data.frame)\n $ Vaccines: Factor w/ 3 levels \"A\",\"B\",\"C\": 1 1 1 1 1 1 2 2 2 2 ...\n $ Antibody: num [1:18] 1232 751 339 848 447 ...\n\n\n\n\n24.2.3 Computing summary statistics by group\nThe first step is to inspect the data and calculate a summary of statistics. This can be done by using the summarise function.\n\ngroup_by(vaccine_efficacy, Vaccines) %&gt;%\n  summarise(\n    count = n(),\n    mean = mean(Antibody, na.rm = TRUE),\n    sd = sd(Antibody, na.rm = TRUE),\n    median = median(Antibody, na.rm = TRUE),\n    IQR = IQR(Antibody, na.rm = TRUE)\n  )\n\n# A tibble: 3 × 6\n  Vaccines count  mean    sd median   IQR\n  &lt;fct&gt;    &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 A            6  693.  325.   646.  353 \n2 B            6  256.  156.   240.  114.\n3 C            6  583.  335.   474   373.\n\n\n\n\n24.2.4 Box Plot\nThe next step will be to visualize the dataset using a box plot. This will allow us to estimate differences in distribution.\n\nvaccine_efficacy %&gt;% \n  ggplot(aes(Vaccines, Antibody)) + \n  geom_boxplot() +\n  ggtitle(\"Vaccine Efficacy\") +\n  xlab(\"Vaccines\") + ylab(\"Antibodies\")\n\n\n\n\n\n\n\n\nBased on the box plot, we see that there is similarity in distribution of A and C while B looks to be different. We can also add the individual data points and connect the boxes to visually see the density distribution and compare with normal distribution for each vaccines.\n\n24.2.4.1 Adding error bars: mean_se\n\nggline(vaccine_efficacy, x = \"Vaccines\", y = \"Antibody\",\n       add = c(\"mean_se\", \"jitter\"),\n       order = c(\"A\", \"B\", \"C\"),\n       ylab = \"Antibody\", xlab = \"Vaccines\")\n\n\n\n\n\n\n\n\n\n\n24.2.4.2 Density plot with overlaid normal plot\nNext, we want to create a density plot to further visualize the data and compare it to what a normal distribution of these data should look like. This can be done by using the ggdensity function as seen below.\n\n# Plot the distribution of the antibodies for vaccine \nggdensity(df$A, fill = \"lightgray\", title = \"Distribution plot of Antibody Production for Vaccine A\") +\n  scale_x_continuous() +\n  xlab(\"A\") +\n  stat_overlay_normal_density(color = \"red\", linetype = \"dashed\")\n\n\n\n\n\n\n\n\n\n# Plot the distribution of the antibodies for vaccine \nggdensity(df$B, fill = \"lightgray\", title = \"Distribution plot of Antibody Production for Vaccine B\") +\n  scale_x_continuous() +\n  xlab(\"B\") +\n  stat_overlay_normal_density(color = \"red\", linetype = \"dashed\")\n\n\n\n\n\n\n\n\n\n# Plot the distribution of the antibodies for vaccine \nggdensity(df$C, fill = \"lightgray\", title = \"Distribution plot of Antibody Production for Vaccine C\") +\n  scale_x_continuous() +\n  xlab(\"C\") +\n  stat_overlay_normal_density(color = \"red\", linetype = \"dashed\")\n\n\n\n\n\n\n\n\nFrom these density plots, we see that our data is not normally distributed and distribution shape for two vaccines looks similar while one vaccine deviates. As our data is not normally distributed and has small sample size, we will now perform Kruskal-Wallis test to find out whether there are any significant differences between the three vaccines in terms of their efficacy (antibodies production in the body).\n\n\n\n24.2.5 Kruskal-Wallis Test\nThe Kruskal-Wallis test can be done in R using the kruskal.test function as seen below.\n\nresult &lt;- kruskal.test(Antibody ~ Vaccines, data = vaccine_efficacy)\n\nprint(result)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  Antibody by Vaccines\nKruskal-Wallis chi-squared = 7.2982, df = 2, p-value = 0.02601\n\n\n\n\n24.2.6 Tabulating the result\n\ntable1 &lt;-   \n  tbl_summary(\n    vaccine_efficacy,\n    by = Vaccines,\n) %&gt;% \n  add_p() %&gt;%\n  modify_caption(\"Antibody Production of Different Vaccines\") %&gt;%\n  bold_labels()\n\n\ntable1\n\n\n\n\n\n\nAntibody Production of Different Vaccines\n\n\nCharacteristic\nA, N = 61\nB, N = 61\nC, N = 61\np-value2\n\n\n\n\nAntibody\n647 (471, 824)\n240 (182, 296)\n474 (375, 748)\n0.026\n\n\n\n1 Median (IQR)\n\n\n2 Kruskal-Wallis rank sum test\n\n\n\n\n\n\n\n\n\n\n\n24.2.7 Interpretation\nFrom the Kruskal-Wallis test, we get that our test statistic is 26.63 with p-value 0.026, which is smaller than our level of significance 0.05. This gives us enough evidence to reject the null hypothesis. Therefore, we conclude that there is a significant difference in the efficacy of at least two of the three vaccines.\n\n24.2.7.1 Post-hoc-Test\nThe Kruskal-Wallis test helps to determine whether at least two groups differ from each other but it does not specify where in which groups the significance lies. We need to conduct a post-hoc test for this. For this purpose, the Dunn test is the appropriate nonparametric test for the pairwise multiple comparison. We will use Holm adjustment method for multiple comparison. You can read about various adjustment methods for multiple comparison herechen2017?\n\npair_wise_compare &lt;- dunnTest(Antibody~Vaccines, \n                              data = vaccine_efficacy,\n                              method = \"holm\"\n  \n)\n\npair_wise_compare\n\nDunn (1964) Kruskal-Wallis multiple comparison\n\n\n  p-values adjusted with the Holm method.\n\n\n  Comparison          Z     P.unadj     P.adj\n1      A - B  2.5955427 0.009444166 0.0283325\n2      A - C  0.6488857 0.516412268 0.5164123\n3      B - C -1.9466571 0.051575864 0.1031517\n\n\nWhen looking at the adjusted p-values in the last column for each pairwise comparison, we can see that only the A-B vaccine comparison has a p-value that is less than our level of significance of 0.05. Therefore, we conclude that there is significant difference in vaccine A-B while there is no significant difference between vaccines A-C, and B-C.\n\n\n\n24.2.8 Alternative method\nA very good alternative for performing a Kruskal-Wallis and the post-hoc tests in R is with the ggbetweenstats() function from the {ggstatsplot} package: It provides a combination of box and violin plots along with jittered data points for between-subjects designs with statistical details included in the plot as a subtitle.\n\nggbetweenstats(\n  data = vaccine_efficacy,\n  x = Vaccines,\n  y = Antibody,\n  type = \"nonparametric\", # ANOVA or Kruskal-Wallis\n  plot.type = \"box\",\n  pairwise.comparisons = TRUE,\n  pairwise.display = \"significant\",\n  centrality.type = \"nonparametric\", # It displays median for non parametric data by default.\n  bf.message = FALSE # Logical that decides whether to display Bayes Factor in favor of the null hypothesis. This argument is relevant only for parametric test\n)\n\n\n\n\n\n\n\n\nThis method has the advantage that all necessary statistical results are displayed directly on the plot. It also provides a more efficient and concise code.\nThe results of the Kruskal-Wallis test are shown in the subtitle above the plot (the p-value is after p =). Moreover, the results of the post-hoc test are displayed between each group via accolades, and the boxplots allow to visualize the distribution for each species.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Non-Parametric ANOVA: The Kruskal-Wallis Test</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_anova_kruskal_wallis.html#conclusion",
    "href": "lessons_original/04_anova_kruskal_wallis.html#conclusion",
    "title": "24  Non-Parametric ANOVA: The Kruskal-Wallis Test",
    "section": "24.3 Conclusion",
    "text": "24.3 Conclusion\nIn conclusion, the Kruskal-Wallace test is a non-parametric hypothesis test that can be used to determine if there are significant differences between two or more groups using the ranks of the data values. The first step involves visualizing the data to confirm it violates the rules of normality. Next, you conduct the Kruskal-Wallis test to determine if there are significant differences. Finally, you run a post-hoc test to calculate pairwise comparisons and determine which specific groups are significantly different.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Non-Parametric ANOVA: The Kruskal-Wallis Test</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_anova_kruskal_wallis.html#references",
    "href": "lessons_original/04_anova_kruskal_wallis.html#references",
    "title": "24  Non-Parametric ANOVA: The Kruskal-Wallis Test",
    "section": "References",
    "text": "References",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Non-Parametric ANOVA: The Kruskal-Wallis Test</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_random_intercept.html",
    "href": "lessons/04_anova_random_intercept.html",
    "title": "\n25  Repeated Measures ANOVA\n",
    "section": "",
    "text": "25.1 Introduction\nRepeated measures ANOVA is used when you have the same measure that participants were rated on at more than two time points. With only two time points a paired \\(t\\)-test will be sufficient, but for more times a repeated measures ANOVA is required. (2013-) There are many complex designs that can make use of repeated measures, but throughout this guide, we will be referring to the most simple case, that of a one-way repeated measures ANOVA. This particular test requires one independent variable and one dependent variable. The dependent variable needs to be continuous (interval or ratio) and the independent variable categorical (either nominal or ordinal). (2018-)",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Repeated Measures ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_random_intercept.html#neccessary-packages",
    "href": "lessons/04_anova_random_intercept.html#neccessary-packages",
    "title": "\n25  Repeated Measures ANOVA\n",
    "section": "\n25.2 Neccessary packages",
    "text": "25.2 Neccessary packages\nMake sure that you have installed the following R packages:\n\n\ntidyverse for data manipulation and visualization.\n\nggpubr for creating easily publication ready plots.\n\nrstatix provides pipe-friendly R functions for easy statistical analyses.(2018-)\n\n\ndatarium contains required data sets for this chapter.\n\nStart by loading the following R packages\n\n# Install packages first and then load the libraries. \n# install.packages(\"datarium\")\n\nlibrary(tidyverse)\nlibrary(ggpubr)\nlibrary(rstatix)",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Repeated Measures ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_random_intercept.html#data-source-and-description",
    "href": "lessons/04_anova_random_intercept.html#data-source-and-description",
    "title": "\n25  Repeated Measures ANOVA\n",
    "section": "\n25.3 Data source and description",
    "text": "25.3 Data source and description\nFor this example we will be using this dataset from the datarium package that contains 10 individuals’ self-esteem score on three time points during a specific diet to determine whether their self-esteem improved.\nOne-way repeated measures ANOVA can be performed in order to determine the effect of time on the self-esteem score.\n\n# Data preparation; wide format\ndata(\"selfesteem\", package = \"datarium\")\nselfesteem\n\n# A tibble: 10 × 4\n      id    t1    t2    t3\n   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     1  4.01  5.18  7.11\n 2     2  2.56  6.91  6.31\n 3     3  3.24  4.44  9.78\n 4     4  3.42  4.71  8.35\n 5     5  2.87  3.91  6.46\n 6     6  2.05  5.34  6.65\n 7     7  3.53  5.58  6.84\n 8     8  3.18  4.37  7.82\n 9     9  3.51  4.40  8.47\n10    10  3.04  4.49  8.58\n\n\nNow we “gather” columns t1, t2, and t3 into “long” format, then convert id and time into factor variables.\n\nselfesteem_df &lt;- \n  selfesteem %&gt;%\n  gather(key = \"time\", value = \"score\", t1, t2, t3) %&gt;%\n  convert_as_factor(id, time)\n\nselfesteem_df\n\n# A tibble: 30 × 3\n   id    time  score\n   &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt;\n 1 1     t1     4.01\n 2 2     t1     2.56\n 3 3     t1     3.24\n 4 4     t1     3.42\n 5 5     t1     2.87\n 6 6     t1     2.05\n 7 7     t1     3.53\n 8 8     t1     3.18\n 9 9     t1     3.51\n10 10    t1     3.04\n# ℹ 20 more rows\n\n\nThe one-way repeated measures ANOVA can be used to determine whether the means self-esteem scores are significantly different between the three time points.\nNote: Whilst the repeated measures ANOVA is used when you have just “one” independent variable, if you have “two” independent variables (e.g., you measured time and condition), you will need to use a two-way repeated measures ANOVA. Two and Three-way Repeated Measures ANOVA examples with this data can be found here.\n\n25.3.1 Summary statistics\nCompute some summary statistics of the self-esteem score by groups (time): mean and sd (standard deviation)\n\n# Statistics-summary\nselfesteem_df %&gt;%\n  group_by(time) %&gt;%\n  get_summary_stats(score, type = \"mean_sd\")\n\n# A tibble: 3 × 5\n  time  variable     n  mean    sd\n  &lt;fct&gt; &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 t1    score       10  3.14 0.552\n2 t2    score       10  4.93 0.863\n3 t3    score       10  7.64 1.14 \n\n\n\n25.3.2 Visualization\nCreate a box plot and add points corresponding to individual values:\n\nbxp &lt;- ggboxplot(selfesteem_df, x = \"time\", y = \"score\", add = \"point\")\nbxp\n\n\n\nVisualization of DATA",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Repeated Measures ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_random_intercept.html#test-assumptions",
    "href": "lessons/04_anova_random_intercept.html#test-assumptions",
    "title": "\n25  Repeated Measures ANOVA\n",
    "section": "\n25.4 Test Assumptions",
    "text": "25.4 Test Assumptions\nBefore computing repeated measures ANOVA test, you need to perform some preliminary tests to check if the assumptions are met.\n\n25.4.1 Outiliers\nOutliers can be easily identified using box plot methods, implemented in the R function identify_outliers() inside the rstatix package.\n\nselfesteem_df %&gt;%\n  group_by(time) %&gt;%\n  identify_outliers(score)\n\n# A tibble: 2 × 5\n  time  id    score is.outlier is.extreme\n  &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;lgl&gt;      &lt;lgl&gt;     \n1 t1    6      2.05 TRUE       FALSE     \n2 t2    2      6.91 TRUE       FALSE     \n\n\nThere were no extreme outliers. In the situation where we have extreme outliers, we can include the outlier in the analysis anyway if we do not believe the result will be substantially affected. This can be evaluated by comparing the result of the ANOVA with and without the outlier. It’s also possible to keep the outliers in the data and perform robust ANOVA test using the WRS2 package. WRS2 Package\n\n25.4.2 Normality Assumption\nThe outcome (or dependent) variable should be approximately normally distributed in each cell of the design. This can be checked using the Shapiro-Wilk normality test (shapiro_test() in rstatix package) or by visual inspection using QQ plot (ggqqplot() in the ggpubr package). If the data is normally distributed, the \\(p\\)-value should be greater than 0.05.\n\nselfesteem_df %&gt;%\n  group_by(time) %&gt;%\n  shapiro_test(score)\n\n# A tibble: 3 × 4\n  time  variable statistic     p\n  &lt;fct&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;\n1 t1    score        0.967 0.859\n2 t2    score        0.876 0.117\n3 t3    score        0.923 0.380\n\n\nThe self-esteem score was normally distributed at each time point, as assessed by Shapiro-Wilk’s test (\\(p &gt; 0.05\\)).\nNote that, if your sample size is greater than 50, the normal QQ plot is preferred because at larger sample sizes the Shapiro-Wilk test becomes very sensitive even to a minor deviation from normality. QQ plot draws the correlation between a given data and the normal distribution. Create QQ plots for each time point:\n\nggqqplot(selfesteem_df, \"score\", facet.by = \"time\")\n\n\n\nQQ Plot\n\n\n\nFrom the plot above, as all the points fall approximately along the reference line, we can assume normality.\n\n25.4.3 Assumption of Sphericity\nThe variance of the differences between groups should be equal. This can be checked using the Mauchly’s test of sphericity. This assumption will be automatically checked during the computation of the ANOVA test using the R function anova_test() in rstatix package. The Mauchly’s test is internally used to assess the sphericity assumption. Click HERE to know more about the Assumption of Sphericity and the Mauchly’s Test and to understand why is important.\nBy using the function get_anova_table() to extract the ANOVA table, the Greenhouse-Geisser sphericity correction is automatically applied to factors violating the sphericity assumption.\n\nres.aov &lt;- anova_test(\n  data = selfesteem_df, \n  # Selfesteem variable\n  dv = score,\n  # Sample individuals\n  wid = id, \n  # Independent variable time \n  within = time\n)\n\n# Get table\nget_anova_table(res.aov)\n\nANOVA Table (type III tests)\n\n  Effect DFn DFd      F        p p&lt;.05   ges\n1   time   2  18 55.469 2.01e-08     * 0.829\n\n\nThe self-esteem score was statistically significantly different at the different time points during the diet, \\(F_{(2, 18)} = 55.5\\), \\(p &lt; 0.0001\\), \\(\\eta^2_g = 0.83\\). where,\n\n\nF Indicates that we are comparing to an \\(F\\)-distribution (\\(F\\)-test),\n\n(2, 18) indicates the degrees of freedom in the numerator (DFn) and the denominator (DFd), respectively,\n\n55.5 indicates the obtained \\(F\\)-statistic value;\n\np specifies the \\(p\\)-value, and\n\n\\(\\eta^2_g\\) is the generalized effect size (amount of variability due to the within-subjects factor).\n\n25.4.4 Post-hoc test\nYou can perform multiple pairwise paired \\(t\\)-tests between the levels of the within-subjects factor (here time). We adjust \\(p\\)-values using the Bonferroni multiple testing correction method.\n\n# pairwise comparisons\npwc &lt;- pairwise_t_test(\n  data = selfesteem_df,\n  formula = score ~ time,\n  paired = TRUE,\n  p.adjust.method = \"bonferroni\"\n)\n\npwc\n\n# A tibble: 3 × 10\n  .y.   group1 group2    n1    n2 statistic    df           p p.adj p.adj.signif\n* &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;       \n1 score t1     t2        10    10     -4.97     9 0.000772     2e-3 **          \n2 score t1     t3        10    10    -13.2      9 0.000000334  1e-6 ****        \n3 score t2     t3        10    10     -4.87     9 0.000886     3e-3 **          \n\n\nAll the pairwise differences are statistically significant.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Repeated Measures ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_random_intercept.html#results",
    "href": "lessons/04_anova_random_intercept.html#results",
    "title": "\n25  Repeated Measures ANOVA\n",
    "section": "\n25.5 Results",
    "text": "25.5 Results\nWe could report the results of the post-hoc test as follows: post-hoc analyses with a Bonferroni adjustment revealed that all the pairwise differences, between time points, were statistically significantly different (\\(p &lt; 0.05\\)).\n\npwc &lt;- pwc %&gt;% add_xy_position(x = \"time\")\n\nbxp + \n  stat_pvalue_manual(pwc) +\n  labs(\n    subtitle = get_test_label(res.aov, detailed = TRUE),\n    caption = get_pwc_label(pwc)\n  )\n\n\n\nVisualization With Results",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Repeated Measures ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_random_intercept.html#conclusion",
    "href": "lessons/04_anova_random_intercept.html#conclusion",
    "title": "\n25  Repeated Measures ANOVA\n",
    "section": "\n25.6 Conclusion",
    "text": "25.6 Conclusion\nThis chapter describes how to compute, interpret and report repeated measures ANOVA in R, specifically one-way repeated measures ANOVA. We also explain the assumptions made by one-way repeated measures ANOVA tests and provide practical examples of R codes to check whether the test assumptions are met.\n\n\n\n\n“ANOVA for Repeated Measures Designs Related Designs (Rationale for One-Way Repeated Measures ANOVA); Between Subjects and Between Conditions Variation; Data Assumptions for Repeated Measures ANOVA; Two-Way Related Design; ANOVA Mixed Design  One Repeated Measure and One Unrelated Factor ; More Complex ANOVA Designs; Effect Size and Power; A Non- Parametric Equivalent  the Friedman Test for Correlated Samples; SPSS Procedures for Repeated Measures ANOVA and Friedman.” 2013. In, 528–49. Routledge. https://doi.org/10.4324/9780203769669-23.\n\n\n“ANOVA Repeated Measures.” 2018. In, 222–50. SAGE Publications, Inc. https://doi.org/10.4135/9781071802625.n9.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Repeated Measures ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_corr_cov.html",
    "href": "lessons/04_corr_cov.html",
    "title": "\n26  Correlation and Covariance Matrices\n",
    "section": "",
    "text": "26.1 Introduction to Correlation and Covariance Matrices\nA prominent theme in statistical analysis is evaluating whether or not a relationship exists between two or more variables, and the degree to which that relationship exists.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Correlation and Covariance Matrices</span>"
    ]
  },
  {
    "objectID": "lessons/04_corr_cov.html#introduction-to-correlation-and-covariance-matrices",
    "href": "lessons/04_corr_cov.html#introduction-to-correlation-and-covariance-matrices",
    "title": "\n26  Correlation and Covariance Matrices\n",
    "section": "",
    "text": "Covariance measures the degree to which the deviation of one variable (\\(X\\)) from its mean changes in relation to the deviation of another variable (\\(Y\\)) from its mean. In other words, covariance measures the joint variability of two random variables or how these increase or decrease in relation with each other.\n\nFor instance, if greater values of one variable tend to correspond with greater values of another variable, this suggests a positive covariance.\nCovariance can have both positive and negative values.\nA covariance of zero indicates that the variables are independent of each other, meaning that there is no linear relationship between them.\n\n\nA Covariance Matrix shows the covariance between different variables of a data set. Covariance matrices are helpful for:\n\nCharacterizing multivariate normal distributions.\nDimensionality reduction techniques, such as Principal Component Analysis, where the matrix is used to calculate the principal components (i.e., linear combinations of the original variables that capture the maximum variance in the data).\nMachine learning models, like Gaussian mixture models, where they are used to estimate the parameters of the model.\n\n\n\nCorrelation tells us both the strength and the direction of the relationship between two variables by listing the Correlation Coefficient or \\(r\\) (“Pearson”, “Spearman”, or “Kendall”) for the pair as measure of association.\n\nCorrelation differs from covariance in that it is standardized between -1 and 1, making it easier to interpret.\nA correlation of -1 indicates a perfect negative linear relationship between two variables, and a correlation of 1 indicates a perfect positive linear relationship between two variables.\nA correlation of 0 indicates no correlation.\nThe magnitude of the correlation coefficient indicates the strength of the association:\n\n.1 &lt;  \\(r\\) &lt; .3 (small / weak correlation).\n.3 &lt;  \\(r\\) &lt; .5 (medium / moderate correlation).\n.5 &lt;  \\(r\\) (large / strong correlation).\n\n\nCorrelation is helpful to test for multicollinarity in regression models.\n\n\nA Correlation matrix allows for the exploration of the correlation among the multiple variables in a data set simultaneously. It provides this information through a table listing the correlation coefficients (\\(r\\)) for each pair of variables in the data set.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Correlation and Covariance Matrices</span>"
    ]
  },
  {
    "objectID": "lessons/04_corr_cov.html#mathematical-definition",
    "href": "lessons/04_corr_cov.html#mathematical-definition",
    "title": "\n26  Correlation and Covariance Matrices\n",
    "section": "\n26.2 Mathematical Definition",
    "text": "26.2 Mathematical Definition\n\n26.2.1 Variance\nThe equation for the variance of a single variable is as follows: \\[\nvar(x) = \\frac{1}{n-1}{\\sum_{i = 1}^{n}{(x_i - \\bar{x})^2}}\n\\]\n\n26.2.2 Covariance\nFor a sample of \\(n\\) data points, the sample covariance is calculated as: \\[\nCov(x,y) = \\frac{1}{n-1}{\\sum_{i = 1}^{n}{(x_i - \\bar{x})(y_i-\\bar{y})}}\n\\] Where,\n\n\n\\(x_i\\) denotes all the possible values of x.\n\n\\(y_i\\) denotes all the possible values of y.\n\n\\(\\bar{x}\\) denotes the mean of variable x.\n\n\\(\\bar{y}\\) denotes the mean of variable y.\n\n\\(n\\) denotes the sample size.\n\n\n26.2.2.1 Covariance Matrix\nThe covariance matrix, \\(\\sum\\), can be represented as: \\[\n\\sum = \\begin{bmatrix} Var(x_{1}) & ... & Cov(x_{1},x_{n})\\\\ : &: & :\\\\ :& :& :\\\\ Cov(x_{n},x_{1}) & ... & Var(x_{n}) \\end{bmatrix}.\n\\] Where,\n\n\n\\(x_1\\) is the first variable of interest in the data set.\nUp to the last variable of interest in the data set, \\(x_n\\).\n\n26.2.3 Correlation\nHere is a simplified equation for the Pearson Correlation Coefficient, \\(r\\) that allows us to see the relationship between correlation and covariance: \\[\nr = \\frac {Cov(x,y)}{(\\sqrt{var(x)})(\\sqrt{var(y)})}\n\\] This video explains the standardization and mathematical properties of \\(r\\):\n\n\n26.2.3.1 Correlation Matrix\nHere is an example of a correlation matrix:\n\n\n\nVariable 1\nVariable 2\n…\nVariable n\n\n\n\nVariable 1\n1.00\n\\(r_{12}\\)\n…\n\\(r_{1n}\\)\n\n\nVariable 2\n\\(r_{21}\\)\n1.00\n…\n\\(r_{2n}\\)\n\n\n…\n…\n…\n…\n…\n\n\nVariable n\n\\(r_{n1}\\)\n\\(r_{n2}\\)\n…\n1.00\n\n\n\nWhere,\n\nThe diagonal elements are always 1, because it represents the correlation of each variable with itself.\nThe off-diagonal elements represent the correlation coefficients between pairs of variables. For example \\(r_{12}\\) represents the correlation between Variable 1 and Variable 2.\nThe correlation is symmetric, meaning that the correlation between Variable 1 and Variable 2 is the same as the correlation between Variable 2 and Variable 1.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Correlation and Covariance Matrices</span>"
    ]
  },
  {
    "objectID": "lessons/04_corr_cov.html#data-description",
    "href": "lessons/04_corr_cov.html#data-description",
    "title": "\n26  Correlation and Covariance Matrices\n",
    "section": "\n26.3 Data Description",
    "text": "26.3 Data Description\nWe utilized the R data set mtcars to provide examples for developing a Covariance Matrix and checking the assumptions for/development of a Correlation Matrix using the pearson correlation coefficient.\nmtcars is a data set that belongs to base R that contains data about the various models of cars. It contains measurement from 32 different automobiles (1,973,074 models). The variable in the mtcars data set are:\n\n\nmpg: Miles/(US) gallon.\n\ncyl: Number of cylinders.\n\ndisp: Displacement (cu.in.).\n\nhp: Gross horsepower.\n\ndrat: Rear axle ratio.\n\nwt: Weight (1000 lbs).\n\nqsec: 1/4 mile time.\n\nvs: V/S (0 = V-shaped engine, 1 = straight engine).\n\nam: Transmission (0 = automatic, 1 = manual).\n\ngear: Number of forward gears.\n\ncarb: Number of carburetors.\n\n\n# Load data\ndata(\"mtcars\")\n# Print sample\nhead(mtcars) %&gt;% \n  kable(\n    format = \"markdown\",\n    digits = 2,\n    caption = \"The `mtcars` data set\"\n  )\n\n\nThe mtcars data set\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\nMazda RX4\n21.0\n6\n160\n110\n3.90\n2.62\n16.46\n0\n1\n4\n4\n\n\nMazda RX4 Wag\n21.0\n6\n160\n110\n3.90\n2.88\n17.02\n0\n1\n4\n4\n\n\nDatsun 710\n22.8\n4\n108\n93\n3.85\n2.32\n18.61\n1\n1\n4\n1\n\n\nHornet 4 Drive\n21.4\n6\n258\n110\n3.08\n3.21\n19.44\n1\n0\n3\n1\n\n\nHornet Sportabout\n18.7\n8\n360\n175\n3.15\n3.44\n17.02\n0\n0\n3\n2\n\n\nValiant\n18.1\n6\n225\n105\n2.76\n3.46\n20.22\n1\n0\n3\n1",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Correlation and Covariance Matrices</span>"
    ]
  },
  {
    "objectID": "lessons/04_corr_cov.html#assumptions",
    "href": "lessons/04_corr_cov.html#assumptions",
    "title": "\n26  Correlation and Covariance Matrices\n",
    "section": "\n26.4 Assumptions",
    "text": "26.4 Assumptions\n\n26.4.1 Covariance Assumptions\nBecause the covariance measures the degree to which two variables change together, the following assumptions should be met:\n\n\nLinearity: The relationship between the two variables should be linear.\n\nScale: Covariance is sensitive to the units of measurement, so variables should be measured on interval or ratio scale if possible.\n\nAn interval scale means that there are equal intervals between points on the scale (e.g., temperature in Celsius and Fahrenheit).\nA ratio scale is an interval scale with a true/absolute zero point (e.g., time in minutes, height and weight).\n\n\n\nMean centered data: Covariance is based on deviations from the mean, so for accurate calculations the data should be mean centered.\n\nOutliers: There should be no outliers, or outliers should be handled prior to interpreting results.\n\n26.4.2 Correlation Assumptions\nCorrelation examines the strength and direction of a linear relationship between 2 variables. As such, the following assumptions should be met:\n\n\nLinearity: Correlation can underestimate the strength of a relationship if the relationship between variables is non-linear.\n\nScale: Like covariance, correlation assumes interval or ratio scale for valid results.\n\nHomoscedasticity: The range or spread of one variable should be consistent across the range of the second variable.\n\nThis assumption is only relevant for Pearson Correlation. Spearman Rank Correlation and Kendall’s Tau do not assume homoscedasticity.\n\n\n\nNormality: For hypothesis testing, the variables should be approximately normally distributed.\n\nThis assumption is only relevant for Pearson Correlation. Spearman Rank Correlation and Kendall’s Tau do not assume normality.\n\n\n\nIndependence: Each pair of observations/variables should be independent of other pairs (e.g., you should not have repeated measures, clustered data, or time series data).\n\nOutliers: There should be no outliers, or outliers should be handled prior to interpreting results.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Correlation and Covariance Matrices</span>"
    ]
  },
  {
    "objectID": "lessons/04_corr_cov.html#checking-the-assumptions",
    "href": "lessons/04_corr_cov.html#checking-the-assumptions",
    "title": "\n26  Correlation and Covariance Matrices\n",
    "section": "\n26.5 Checking the Assumptions",
    "text": "26.5 Checking the Assumptions\n\n26.5.1 Linearity\nWe can check this assumption with scatterplots of the continuous data.\n\n# The pairs function creates a scatterplot matrix for all continuous variables\npairs(\n  # Notice that all continuous variables come after the tilde (~)\n  ~ mpg + disp + hp + drat + wt + qsec,\n  data = mtcars,\n  main = \"Scatterplot Matrix\"\n)\n\n\n\n\n\n\n\nFrom this visualization, it appears that the continuous variables have a linear relationship (whether negative or positive).\n\n26.5.2 Scale\nUsing the skimr package along with the help (?) function, we can quickly get information about all of the variables in mtcars to understand what kind of scale the variables are on.\n\nskim(mtcars)\n\n\nData summary\n\n\nName\nmtcars\n\n\nNumber of rows\n32\n\n\nNumber of columns\n11\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n11\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nmpg\n0\n1\n20.09\n6.03\n10.40\n15.43\n19.20\n22.80\n33.90\n▃▇▅▁▂\n\n\ncyl\n0\n1\n6.19\n1.79\n4.00\n4.00\n6.00\n8.00\n8.00\n▆▁▃▁▇\n\n\ndisp\n0\n1\n230.72\n123.94\n71.10\n120.83\n196.30\n326.00\n472.00\n▇▃▃▃▂\n\n\nhp\n0\n1\n146.69\n68.56\n52.00\n96.50\n123.00\n180.00\n335.00\n▇▇▆▃▁\n\n\ndrat\n0\n1\n3.60\n0.53\n2.76\n3.08\n3.70\n3.92\n4.93\n▇▃▇▅▁\n\n\nwt\n0\n1\n3.22\n0.98\n1.51\n2.58\n3.33\n3.61\n5.42\n▃▃▇▁▂\n\n\nqsec\n0\n1\n17.85\n1.79\n14.50\n16.89\n17.71\n18.90\n22.90\n▃▇▇▂▁\n\n\nvs\n0\n1\n0.44\n0.50\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▆\n\n\nam\n0\n1\n0.41\n0.50\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▆\n\n\ngear\n0\n1\n3.69\n0.74\n3.00\n3.00\n4.00\n4.00\n5.00\n▇▁▆▁▂\n\n\ncarb\n0\n1\n2.81\n1.62\n1.00\n2.00\n2.00\n4.00\n8.00\n▇▂▅▁▁\n\n\n\n\n# # Checking the help file for mtcars\n# ?mtcars\n\nAs we mentioned previously in the description of the data, only 6 variables (mpg, disp, hp, drat, wt, and qsec) are interval or ratio values.\n\n26.5.3 Outliers\nTo check for outliers in our variables of interest, we can use a box plot to visually examine the data.\n\nbox_mpg &lt;- ggplot(mtcars) +\n  aes(y = mpg) +\n  geom_boxplot()\n\nbox_disp &lt;- ggplot(mtcars) +\n  aes(y = disp) +\n  geom_boxplot()\n\nbox_hp &lt;- ggplot(mtcars) +\n  aes(y = hp) +\n  geom_boxplot()\n\nbox_drat &lt;- ggplot(mtcars) +\n  aes(y = drat) +\n  geom_boxplot()\n\nbox_wt &lt;- ggplot(mtcars) +\n  aes(y = wt) +\n  geom_boxplot()\n\nbox_qsec &lt;- ggplot(mtcars) +\n  aes(y = qsec) +\n  geom_boxplot()\n\n\nbox_mpg + box_disp + box_hp + box_drat + box_wt + box_qsec\n\n\n\n\n\n\nFigure 26.1\n\n\n\n\nmpg, hp, wt, and qsec all appear to have some outliers.\n\n26.5.4 Homoscedasticity\nTo check the homoscedasticity (or homogeneity of variance) assumption for correlations, we can examine resdiual plots from linear models after conducting hypothesis testing. We can also look back at the box plots (Figure 26.1) to check if the interquartile range of the variables looks approximately the same. All of the variables of interest, except for disp appear to have similar variances.\n\n26.5.5 Normality\nTo check this assumption, we can examine if the data are approximately normally distributed using the Q-Q plot or histograms or with the Shapiro-Wilk test.\nFirst, we will look at the Q-Q plot:\n\n# Normality assumption: Q-Q plot\nqq_mpg &lt;- ggplot(mtcars) +\n  aes(sample = mpg) +\n  stat_qq() +\n  stat_qq_line() +\n  ggtitle(\"mpg\")\n\nqq_disp &lt;- ggplot(mtcars) +\n  aes(sample = disp) +\n  stat_qq() +\n  stat_qq_line() +\n  ggtitle(\"disp\")\n\nqq_hp &lt;- ggplot(mtcars) +\n  aes(sample = hp) +\n  stat_qq() +\n  stat_qq_line() +\n  ggtitle(\"hp\")\n\nqq_drat &lt;- ggplot(mtcars) +\n  aes(sample = drat) +\n  stat_qq() +\n  stat_qq_line() +\n  ggtitle(\"drat\")\n\nqq_wt &lt;- ggplot(mtcars) +\n  aes(sample = wt) +\n  stat_qq() +\n  stat_qq_line() +\n  ggtitle(\"wt\")\n\nqq_qsec &lt;- ggplot(mtcars) +\n  aes(sample = qsec) +\n  stat_qq() +\n  stat_qq_line() +\n  ggtitle(\"qsec\")\n\n\nqq_mpg + qq_disp + qq_hp + qq_drat + qq_wt + qq_qsec\n\n\n\n\n\n\nFigure 26.2\n\n\n\n\nAll of the variables seem to be approximately normally distributed.\nNext, we will look at histograms:\n\n# Normality assumption: Histograms\nhistogram_mpg &lt;- ggplot(mtcars) +\n  aes(x = mpg) +\n  geom_histogram(binwidth = 0.5)\n\nhistogram_disp &lt;- ggplot(mtcars) +\n  aes(x = disp) +\n  geom_histogram(binwidth = 1)\n\nhistogram_hp &lt;- ggplot(mtcars) +\n  aes(x = hp) +\n  geom_histogram(binwidth = 1)\n\nhistogram_drat &lt;- ggplot(mtcars) +\n  aes(x = drat) +\n  geom_histogram(binwidth = 0.5)\n\nhistogram_wt &lt;- ggplot(mtcars) +\n  aes(x = wt) +\n  geom_histogram(binwidth = 0.5)\n\nhistogram_qsec &lt;- ggplot(mtcars) +\n  aes(x = qsec) +\n  geom_histogram(binwidth = 0.5)\n\nhistogram_mpg + histogram_disp + histogram_hp + histogram_drat + \n  histogram_wt + histogram_qsec\n\n\n\n\n\n\nFigure 26.3\n\n\n\n\nHere, it appears that wt and possibly qsec, disp, and hp appear to be approximately normally distributed.\nLastly, we can use the Shapiro-Wilk test to test if the data is approximately normally distributed. P-values greater than 0.05 indicate that the data is likely approximately normally distributed.\n\n# Normality: Shapiro Wilks\nshapiro_mpg &lt;- mtcars %&gt;%\n  summarise(\n    statistic = shapiro.test(mpg)$statistic,\n    p.value = shapiro.test(mpg)$p.value\n  )\n\nshapiro_mpg\n\n  statistic   p.value\n1 0.9475647 0.1228814\n\nshapiro_disp &lt;- mtcars %&gt;%\n  summarise(\n    statistic = shapiro.test(disp)$statistic,\n    p.value = shapiro.test(disp)$p.value\n  )\n\nshapiro_disp\n\n  statistic    p.value\n1 0.9200127 0.02080657\n\nshapiro_hp &lt;- mtcars %&gt;%\n  summarise(\n    statistic = shapiro.test(hp)$statistic,\n    p.value = shapiro.test(hp)$p.value\n  )\n\nshapiro_hp \n\n  statistic    p.value\n1 0.9334193 0.04880824\n\nshapiro_drat &lt;- mtcars %&gt;%\n  summarise(\n    statistic = shapiro.test(drat)$statistic,\n    p.value = shapiro.test(drat)$p.value\n  )\n\nshapiro_drat \n\n  statistic   p.value\n1 0.9458839 0.1100608\n\nshapiro_wt &lt;- mtcars %&gt;%\n  summarise(\n    statistic = shapiro.test(wt)$statistic,\n    p.value = shapiro.test(wt)$p.value\n  )\n\nshapiro_wt \n\n  statistic    p.value\n1 0.9432577 0.09265499\n\nshapiro_qsec &lt;- mtcars %&gt;%\n  summarise(\n    statistic = shapiro.test(qsec)$statistic,\n    p.value = shapiro.test(qsec)$p.value\n  )\n\nshapiro_qsec\n\n  statistic   p.value\n1 0.9732509 0.5935176\n\n\nFrom these outputs, we can see that disp and hp are the only variables with a p-value less than 0.05, indicating that they are likely not approximately normally distributed.\n\n\n\n\n\n\nIf the normality assumption is not satisfied, it is recommended to use non-parametric correlation, including Spearman Rank Correlation and Kendall’s Tau tests, which will be discussed later.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Correlation and Covariance Matrices</span>"
    ]
  },
  {
    "objectID": "lessons/04_corr_cov.html#code-to-run",
    "href": "lessons/04_corr_cov.html#code-to-run",
    "title": "\n26  Correlation and Covariance Matrices\n",
    "section": "\n26.6 Code to Run",
    "text": "26.6 Code to Run\n\n26.6.1 Covariance\nAs with correlation, we can use the cov() to calculate covariance, where the three same methods are available dependent on the validity of assumptions.\n\n# cov(dataName$variable1, dataName$variable2)\n\ncov_result &lt;- cov(mtcars$mpg, mtcars$drat)\n\nprint(cov_result)\n\n[1] 2.195064\n\n\nThe resulting covariance from mpg and drat is 2.195064. We can also use cov() to create a matrix. However, the data must be manipulated to become a square matrix.\n\n# We select the six continuous variables within the `mtcars` data set and save \n# into our matrix object.\ndataMatrix &lt;- mtcars[, c(1, 3:7)]\n\n# We save the output to an object for later.\ncovOutput &lt;- cov(dataMatrix)\n\nprint(covOutput)\n\n             mpg        disp         hp         drat          wt         qsec\nmpg    36.324103  -633.09721 -320.73206   2.19506351  -5.1166847   4.50914919\ndisp -633.097208 15360.79983 6721.15867 -47.06401915 107.6842040 -96.05168145\nhp   -320.732056  6721.15867 4700.86694 -16.45110887  44.1926613 -86.77008065\ndrat    2.195064   -47.06402  -16.45111   0.28588135  -0.3727207   0.08714073\nwt     -5.116685   107.68420   44.19266  -0.37272073   0.9573790  -0.30548161\nqsec    4.509149   -96.05168  -86.77008   0.08714073  -0.3054816   3.19316613\n\n\nThe resulting output is the calculated covariances among all the variables specified. The Covariance can take any value from -\\(\\infty\\) to \\(\\infty\\).\nWe can also scale the covariance matrix into a corresponding correlation matrix.\n\ncov2cor(covOutput)\n\n            mpg       disp         hp        drat         wt        qsec\nmpg   1.0000000 -0.8475514 -0.7761684  0.68117191 -0.8676594  0.41868403\ndisp -0.8475514  1.0000000  0.7909486 -0.71021393  0.8879799 -0.43369788\nhp   -0.7761684  0.7909486  1.0000000 -0.44875912  0.6587479 -0.70822339\ndrat  0.6811719 -0.7102139 -0.4487591  1.00000000 -0.7124406  0.09120476\nwt   -0.8676594  0.8879799  0.6587479 -0.71244065  1.0000000 -0.17471588\nqsec  0.4186840 -0.4336979 -0.7082234  0.09120476 -0.1747159  1.00000000\n\n\nThis defaults to Pearson Correlation so should be interpreted with caution in the case of variables that do not meet the proper assumptions.\n\n26.6.2 Correlation\nCorrelation can be calculated using: cor(), which calculates the correlation coefficient or cor.test(), which tests for the association (or correlation) between paired samples.\nThree different methods are available when using cor(), either pearson (which is default if none is specified), kendall, or spearman. Let’s run cor() on the variables that satisfied our assumptions: mpg, drat, wt, and qsec.\n\n# cor(dataName$variable1, dataName$variable2, method = \"methodName\")\n\n# Pearson is the default, and does not need to be specified. However, for \n# completeness is specified below.\ncorrPearson &lt;- cor.test(mtcars$mpg, mtcars$drat, method = \"pearson\")\n\nprint(corrPearson)\n\n\n    Pearson's product-moment correlation\n\ndata:  mtcars$mpg and mtcars$drat\nt = 5.096, df = 30, p-value = 1.776e-05\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.4360484 0.8322010\nsample estimates:\n      cor \n0.6811719 \n\n\nIn the output, the following is included:\n\n\nCorrelation Method: Pearson's Product-Moment Correlation\n\n\nData: mtcars$mpg and mtcars$drat\n\n\nt, which represents the t-test statistic: 5.096.\n\ndf, which represents the degrees of freedom: 30 (n - 2).\n\nalternative hypothesis, where true correlation is not equal to 0. Therefore, the null hypothesis states true correlation is equal to 0.\n\np-value, which is the significance level of the t-test (1.776e-05) and the probability of this correlation if the null hypothesis were true.\n\n95% confidence interval or conf.int, where we are 95% confidence that the true correlation coefficient lies between [0.4360484, 0.8322010].\n\nsample estimates provides the calculated value of the correlation coefficient: 0.6811719.\n\nThe following will illustrate the output from a cor.test with kendall specified as the method, or known as the Kendall Rank Correlation Coefficient. This is typically used if the data does not satisfy the normality assumption, so we will utilize the variables: mpg and disp, where disp did not satisfy our normality assumption as indicated by the Shapiro-Wilks test.\n\ncorrKendall &lt;- cor.test(mtcars$mpg, mtcars$disp, method = \"kendall\")\n\nWarning in cor.test.default(mtcars$mpg, mtcars$disp, method = \"kendall\"):\nCannot compute exact p-value with ties\n\nprint(corrKendall)\n\n\n    Kendall's rank correlation tau\n\ndata:  mtcars$mpg and mtcars$disp\nz = -6.1083, p-value = 1.007e-09\nalternative hypothesis: true tau is not equal to 0\nsample estimates:\n       tau \n-0.7681311 \n\n\nSimilar information is provided as with the Pearson method, where the type of correlation test and alternative hypothesis is specified. However, in this non-parametric test, we are provided the z-statistic accompanied by the resulting p-value (1.007e-09). The sample estimates also provides us with the Kendall correlation coefficient (also known as tau): -0.7681311.\n\ncorrSpearman &lt;- cor.test(mtcars$mpg, mtcars$disp, method = \"spearman\")\n\nWarning in cor.test.default(mtcars$mpg, mtcars$disp, method = \"spearman\"):\nCannot compute exact p-value with ties\n\nprint(corrSpearman)\n\n\n    Spearman's rank correlation rho\n\ndata:  mtcars$mpg and mtcars$disp\nS = 10415, p-value = 6.37e-13\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n       rho \n-0.9088824 \n\n\nSimilar to both outputs above, the output from Spearman’s rank method displays the type of correlation test, the variables being tested, alternative hypothesis, as well as the S test statistic and associated p-value (6.37e-13). The sample estimate also provides us with the Spearman’s correlation coefficient (also known as rho): -0.9088824.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Correlation and Covariance Matrices</span>"
    ]
  },
  {
    "objectID": "lessons/04_corr_cov.html#brief-interpretation-of-the-output",
    "href": "lessons/04_corr_cov.html#brief-interpretation-of-the-output",
    "title": "\n26  Correlation and Covariance Matrices\n",
    "section": "\n26.7 Brief Interpretation of the Output",
    "text": "26.7 Brief Interpretation of the Output\n\n26.7.1 Interpreting the Covariance\nFrom our Covariance Matrix, the covariance between mpg and drat is 2.195064. A positive covariance indicates that when mpg is high, drat also tends to be high. The covariance between mpg and disp is -633.097208. A negative covariance indicates that when mpg is high, disp tends to be low (vice versa).\n\n26.7.2 Interpreting Correlation Coefficients\n\n\n\n\n\n\n\n-1 indicates a strong negative correlation: Each time x increases, y decreases.\n0 means that there is no association between the two variables (x and y).\n+1 indicates a strong positive correlation: Each time x increases, y increases.\n\n\n\n\nFor the Pearson correlation, the resulting p-value was 1.776e-05, which is less than our significance level alpha of 0.05. Therefore, the data provides sufficient evidence to suggest that mpg and drat are significantly correlated (0.6811719). Each time mpg increases, drat increases.\nFor the Kendall’s Tau test, the resulting p-value was 1.007e-09, which is less than our significance level alpha of 0.05. Therefore, the data provides sufficient evidence to suggest that mpg and disp are significantly correlated (-0.7681311). Each time mpg increases, disp decreases.\nFor the Spearman Rank correlation, the resulting p-value was 6.37e-13, which is less than our significance level alpha of 0.05. Therefore, the data provides sufficient evidence to suggest that mpg and disp are significantly correlated (-0.9088824). Each time mpg increases, disp decreases significantly (illustrates a strong relationship as it is close to -1).",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Correlation and Covariance Matrices</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_regression_ols.html",
    "href": "lessons_original/04_regression_ols.html",
    "title": "\n27  Ordinary Least Squares Regression\n",
    "section": "",
    "text": "27.1 Ordinary Least Squares (OLS) Regression\nOLS is a “method that allows to find a line that best describes the relationship between one or more predictor variables and a response variable”howtop?, with our end result being:\n\\[\n\\hat{y} = b_0 + b_1x\n\\]\nThe best fitting line is typically calculated utilizing the least squares, which can be visually described as the deviation in the vertical direction.\nNow, we will examine the relationship between a vehicle’s weight (WT) and its miles per gallon or fuel efficiency (MPG).\nIn a previous lesson, we found a Pearson’s correlation coefficient of r(30) = -0.868, p&lt;0.05. Based on this information, we concluded that there is a strong negative relationship between MPG and WT. Where, heavier vehicles are associated with lower miles per gallon. Essentially this means that heavier vehicles are less fuel efficient. We will use the interpretation of this correlation as the basis of building an OLS regression to predict the value of MPG for a vehicle based on its weight. An OLS regression could be described as a common method used in regression analysis due to its efficiency in fitting the best straight line through a set of points. Thus, an OLS regression model gives best approximate of true population regression line as it minimizes the total distance from all of the points to the line.\nThe OLS model could be expressed as: \\[\\hat{y}_i = \\beta_0 + \\beta_1x_i\\]\nThen the OLS regression model line for our example is:\n\\[\\widehat{MPG_i} = \\beta_0 +\\beta_1*WT_i\\]",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Ordinary Least Squares Regression</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_regression_ols.html#ordinary-least-squares-ols-regression",
    "href": "lessons_original/04_regression_ols.html#ordinary-least-squares-ols-regression",
    "title": "\n27  Ordinary Least Squares Regression\n",
    "section": "",
    "text": "The line has the following properties:\n\nThe intercept (\\(\\beta_0\\)), its measure is defined by the units in Y. In our case, the units used in MPG. It is the predicted value of Y (MPG) when X (WT) is zero\n\n\nThe slope (\\(\\beta_1\\)), is the predicted change in Y for a one-unit increase in X. Like the correlation coefficient, it provides information on the relationship between X and Y. But unlike the correlation coefficient (unitless), it highlights the relationship in real terms of units. In our example this would look at how miles per gallon increase or decrease for a one unit increase in a vehicle’s weight (according to the R docummentation for the mtcars data set, weight is provided as a measure per every 1,000 pounds and miles per gallon are provided as Miles/(US) gallon), therefore the units are defined by the Y (MPG) and the X (WT).\n\n\n\nFor example, the data for a vehicle that weighs 2,000 pounds the unit is given as “2”\nFor example, a vehicle that spends one gallon of fuel per every 19 miles is given as “19”\n\n\n\n\n\n27.1.1 How to develop the best fitting line?\n\n\nSample residual/error terms plot\n\nThe best fitting line is one that minimizes errors in prediction or one with the Minimum sum of squared residuals (SSR). For more details, you can watch this video:khanacademy2018?\n\\[\nSSR = \\sum_{i = 1}^{n}{(y_i - \\hat{y_i})^2}\n\\]\n\\(residual_i = y_i - \\hat{y_i}\\)\nIt is important to note that prior to calculating the residuals, we must visualize and examine the data, which was done in the previous example. Then, we must run the regression line. We can utilize lm() to perform the OLS regression which will provide us with the model summary, including the following:\n\nPr(&gt;|t|) Multiple R-Squared Adjusted R-Squared Residual Standard Error F-statistic P-value\n\nOnce the model summary is given, we can then move on to creating the residual plots. When performing this step, we have to check the assumptions of homoscedasticity and normality.\n* Residuals = error terms\n* $$ Residual = observed value - predicted value $$\n* The larger the error term in absolute value, the worse the prediction\n* Squaring residuals solve issues arising from some residuals being negative and some positive.\n\n\nAssumptions\n\nLinearity: Linear relationship between the dependent variable and the independent variables.\nIndependence: The observations must be independent of each other.\nHomoscedasticity: The variance of the residuals should be constant across all levels of the independent variables.\nNormality: The residuals / errors should be normally distributed.\nNo multicollinearity: In the case of multiple regression (2+ independent variables), the independent variables should not be highly correlated with each other.\n\n\n\n\n\n\n\n\n\nBe careful about outliers\n\n\n\nOutliers can influence the estimates of the relationship.\n\n\n\n27.1.2 Example of an OLS regression in R\nIn R, the lm function command allows us to develop an OLS regression.\n\n# model predicting mpg (fuel efficiency) using wt (weight)\nMPGReg &lt;- lm(mpg ~ wt, data = mtcars)\nsummary(MPGReg)\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n\nThe OLS regression model is then:\n\\(\\widehat{MPG_i} = 37.285 - 5.344*WT_i\\)\n\n\nInterpretation: \\(\\beta_0\\)\n\nThe model predicts that vehicles with no weight will have 37.285 miles per gallon, on average.\n\nThis is not a very meaningful intercept as vehicles with “0” weight do not exist. A meaningful intercept can be created by subtracting a constant from the x variable to move the intercept.In R as part of the lm command, this can be done by surrounding the independent variable with I() which applies the function inside and treats it as a new variable. For our example we used the rounded lowest weight of the data (1.5) to predict miles per gallon.\n\nThis procedure does not change the slope of the line\n\n\n\n\n\n\n# model predicting mpg (fuel efficiency) using wt (weight)\nMPGReg2 &lt;- lm(mpg ~ I(wt-1.5), data = mtcars)\nsummary(MPGReg2)\n\n\nCall:\nlm(formula = mpg ~ I(wt - 1.5), data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  29.2684     1.1008  26.589  &lt; 2e-16 ***\nI(wt - 1.5)  -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n\n\nThen, the meaningful intercept model predicts that vehicles with a weight of 1500 pounds have 29.268 miles per gallon, on average.\n\nInterpretation: \\(\\beta_1\\)\n\nThe model predicts that on average, an increase of 1,000 pounds in the weight of a vehicle is associated with a decrease of 5.344 miles per gallon.\n\n\n\n\n# define residuals \nres &lt;- resid(MPGReg)\n\n# produce residual vs. fitted plot \nplot(fitted(MPGReg), res)\n\n# add a horizontal line at 0\nabline(0,0)\n\n\n\n\n\n\n# create Q-Q- plot for residuals\nqqnorm(res)\n\n# add a straight diagonal line to the plot\nqqline(res)\n\n\n\n\n\n\n\nBased on the graph above, it is visually clear that normality may not be met due to some outliers. This means that we must explore our data even deeper as it is possible that transformation of our data utilizing one of the following methods must take place:\n\nLog transformation Square Root Transformation Cube Root Transformation\n\nOnce the data is transformed, we can run the residual plot over again in order to achieve normality. For the sake of this presentation, we are only using an example with known limitations such as non-normality.\n\n27.1.3 Hypothesis testing in OLS regression\nThe null hypothesis in this case would be that the slope is zero indicating no relationship between x and y. Or in our example, we can state that there is no relationship between a vehicle’s weight and its miles per gallon. The alternative hypothesis is then that the slope is not zero.\n\\[\nH_0: \\beta_1 = 0\n\\]\n\\[\nH_1: \\beta_1 ≠ 0\n\\]\nWe can test this hypothesis by using the lm summary printout which provides the p-value for the wt coefficient. This indicates that there is indeed a significant relationship between the weight of the car and its efficiency (miles per gallon used). R provides a t-value for the ‘wt’ coefficient which has a p-value of p &lt; 0.000 as seen below:\n                  Estimate        Std. Error       t value      Pr(&gt;|t|)   \n    wt           -5.3445             0.5591       -9.559       1.29e-10\n\n27.1.4 R-Squared Value\nR-squared is “a measure of how much of the variation in the dependent variable is explained by the independent variables in the model. It ranges from 0 to 1, with higher values indicating a better fit”ordinary?.\nAdjusted R-squared is “similar to R-squared, but it takes into account the number of independent variables in the model. It is a more conservative estimate of the model’s fit, as it penalizes the addition of variables that do not improve the model’s performance”ordinary?.\n\n27.1.5 F-Statistic\nThe F-statistic “tests the overall significance of the model by comparing the variation in the dependent variable explained by the model to the variation not explained by the model. A large F-statistic indicates that the model as a whole is significant”interpre?.\n\n27.1.6 Visual representation\nThe visual representation of this model using ggplot is the following:\n\nggplot(mtcars, aes(x = wt, y = mpg))+\n  geom_point()+\n  geom_smooth(method = \"lm\", se = FALSE)+ #se is option for coinfidence bar\n  labs(x= \"Weight (per 1,000 pounds)\",\n       y = \"Miles per gallon\")+\n  theme_bw()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nAs shown in the figure above, we see the summary statistics represented in a visual manner with the line of best fit. As indicated previously, we see a steep negative correlation between weight of the car and the miles per gallon (efficiency) utilized.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Ordinary Least Squares Regression</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_regression_ols.html#references",
    "href": "lessons_original/04_regression_ols.html#references",
    "title": "\n27  Ordinary Least Squares Regression\n",
    "section": "\n27.2 References",
    "text": "27.2 References",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Ordinary Least Squares Regression</span>"
    ]
  },
  {
    "objectID": "lessons/04_regression_mls.html",
    "href": "lessons/04_regression_mls.html",
    "title": "\n28  Multiple Linear Regression\n",
    "section": "",
    "text": "28.1 Multiple Linear Regression\nMultiple Linear Regression (MLR) is a statistical technique used to understand the relationship between one dependent variable and two or more independent variables. The relationship is explained by by modeling the observed relationship using a mathematical representation or approximation.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "lessons/04_regression_mls.html#multiple-linear-regression",
    "href": "lessons/04_regression_mls.html#multiple-linear-regression",
    "title": "\n28  Multiple Linear Regression\n",
    "section": "",
    "text": "28.1.1 Key Components in MLR:\n\nDependent Variable (Y): The outcome variable or the variable that is potentially going to change due to influencing factors.\nIndependent Variables (X1, X2, …, Xn): Predictor or influencing variables used to predict the dependent variable.\nRegression Coefficients (β0, β1, …, βn): Parameters that represent the relationship between each independent variable and the dependent variable where β0 is the intercept, β1 to βn are the slopes for each independent variable.\nError Term (ε): Represents the random variability in the dependent variable that might perhaps not be explained by the independent variables.\n\n28.1.2 Model Equation:\n\\[\nY = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\ldots + \\beta_pX_p + \\varepsilon\n\\]",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "lessons/04_regression_mls.html#assumptions",
    "href": "lessons/04_regression_mls.html#assumptions",
    "title": "\n28  Multiple Linear Regression\n",
    "section": "\n28.2 Assumptions:",
    "text": "28.2 Assumptions:\n\n\nLinearity: it is assumed that the relationship between the dependent and independent variables is linear.\n\nIndependence: Observations are independent of each other.\n\nHomoscedasticity: The variance of the residuals (errors) is constant across all levels of the independent variables.\n\nNormality: Residuals are normally distributed.\n\nNo Multicollinearity: Independent variables are not highly correlated with each other.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "lessons/04_regression_mls.html#purpose",
    "href": "lessons/04_regression_mls.html#purpose",
    "title": "\n28  Multiple Linear Regression\n",
    "section": "\n28.3 Purpose:",
    "text": "28.3 Purpose:\nTo predict the value of the dependent variable based on the values of the independent variables and to understand the strength and type of relationships between the dependent variable and multiple independent variables.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "lessons/04_regression_mls.html#steps-in-conducting-mlr",
    "href": "lessons/04_regression_mls.html#steps-in-conducting-mlr",
    "title": "\n28  Multiple Linear Regression\n",
    "section": "\n28.4 Steps in Conducting MLR:",
    "text": "28.4 Steps in Conducting MLR:\n\n\nData Collection: Gather data for the dependent and independent variables.\n\nModel Specification: Define the model equation with the dependent variable and chosen independent variables.\n\nEstimation of Coefficients: Use statistical software to estimate the regression coefficients.\n\nModel Evaluation: Assess the model’s goodness-of-fit using R-squared, adjusted R-squared, and other metrics.\n\nDiagnostic Checking: Check the assumptions of MLR (linearity, independence, homoscedasticity, normality, no multicollinearity).",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "lessons/04_regression_mls.html#model-evaluation-metrics",
    "href": "lessons/04_regression_mls.html#model-evaluation-metrics",
    "title": "\n28  Multiple Linear Regression\n",
    "section": "\n28.5 Model Evaluation Metrics:",
    "text": "28.5 Model Evaluation Metrics:\n\n\nR-squared (R²): Measures the proportion of variance in the dependent variable explained by the independent variables.\n\nAdjusted R-squared: Adjusted version of R² that accounts for the number of predictors in the model.\n\nF-statistic: Tests the overall significance of the model.\n\np-values: Test the significance of individual regression coefficients.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "lessons/04_regression_mls.html#demostration",
    "href": "lessons/04_regression_mls.html#demostration",
    "title": "\n28  Multiple Linear Regression\n",
    "section": "\n28.6 Demostration",
    "text": "28.6 Demostration\nWe perform a multiple linear regression analysis on a dataset of medical insurance costs. This dataset includes variables such as age, sex, BMI, number of children, smoker status, and region. Our goal is to understand the relationship between these variables and to predict insurance charges based on the other factors.\nThe data of insurance could be found from Kagglelink\n\n# Load the data\ndata &lt;- read_csv(\"../data/04_insurance.csv\")\n\nRows: 1338 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): sex, smoker, region\ndbl (4): age, bmi, children, charges\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Preview the data\nhead(data)\n\n# A tibble: 6 × 7\n    age sex      bmi children smoker region    charges\n  &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;       &lt;dbl&gt;\n1    19 female  27.9        0 yes    southwest  16885.\n2    18 male    33.8        1 no     southeast   1726.\n3    28 male    33          3 no     southeast   4449.\n4    33 male    22.7        0 no     northwest  21984.\n5    32 male    28.9        0 no     northwest   3867.\n6    31 female  25.7        0 no     southeast   3757.\n\n\n\n28.6.1 Summary Statistics\nWe begin by examining the summary statistics of the dataset to understand its structure and the distribution of variables.\n\nsummary(data)\n\n      age            sex                 bmi           children    \n Min.   :18.00   Length:1338        Min.   :15.96   Min.   :0.000  \n 1st Qu.:27.00   Class :character   1st Qu.:26.30   1st Qu.:0.000  \n Median :39.00   Mode  :character   Median :30.40   Median :1.000  \n Mean   :39.21                      Mean   :30.66   Mean   :1.095  \n 3rd Qu.:51.00                      3rd Qu.:34.69   3rd Qu.:2.000  \n Max.   :64.00                      Max.   :53.13   Max.   :5.000  \n    smoker             region             charges     \n Length:1338        Length:1338        Min.   : 1122  \n Class :character   Class :character   1st Qu.: 4740  \n Mode  :character   Mode  :character   Median : 9382  \n                                       Mean   :13270  \n                                       3rd Qu.:16640  \n                                       Max.   :63770  \n\n\nwe will first log-transform the charges variable. This is often done to stabilize variance and make the data more normally distributed, which can help improve the performance and interpretation of regression models.\n\n# Log-transform the charges variable\ndata$log_charges &lt;- log(data$charges)\n\nBefore fitting the model, we need to ensure that our categorical variables are correctly encoded.\n\n# Convert categorical variables to factors\ndata$sex &lt;- as.factor(data$sex)\ndata$smoker &lt;- as.factor(data$smoker)\ndata$region &lt;- as.factor(data$region)\n\n\nBefore converting to factors:  sex: Character values  “female”, “male”.  smoker: Character values  “no”, “yes”.  region: Character values  “northeast” “northwest” “southeast” “southwest”. \nAfter converting to factors:  sex:  Factor with levels  1 = “female”,  2 = “male”.  smoker:  Factor with levels  1 = “no”,  2 = “yes”.  region: Factor with levels  1 = “northeast”, 2 = “northwest”, 3 = “southeast”, 4 = “southwest”.\n\n\n28.6.2 Model Fitting\nWe fit a multiple linear regression model to predict insurance charges based on the other variables in the dataset.\n\n# Fit the multiple linear regression model\nmodel &lt;- lm(log_charges ~ age + sex + bmi + children + smoker + region,\n  data = data)\n\n# Summary of the model\nsummary(model)\n\n\nCall:\nlm(formula = log_charges ~ age + sex + bmi + children + smoker + \n    region, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.07186 -0.19835 -0.04917  0.06598  2.16636 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      7.0305581  0.0723960  97.112  &lt; 2e-16 ***\nage              0.0345816  0.0008721  39.655  &lt; 2e-16 ***\nsexmale         -0.0754164  0.0244012  -3.091 0.002038 ** \nbmi              0.0133748  0.0020960   6.381 2.42e-10 ***\nchildren         0.1018568  0.0100995  10.085  &lt; 2e-16 ***\nsmokeryes        1.5543228  0.0302795  51.333  &lt; 2e-16 ***\nregionnorthwest -0.0637876  0.0349057  -1.827 0.067860 .  \nregionsoutheast -0.1571967  0.0350828  -4.481 8.08e-06 ***\nregionsouthwest -0.1289522  0.0350271  -3.681 0.000241 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4443 on 1329 degrees of freedom\nMultiple R-squared:  0.7679,    Adjusted R-squared:  0.7666 \nF-statistic: 549.8 on 8 and 1329 DF,  p-value: &lt; 2.2e-16\n\n\nCoefficients and Interpretation: Intercept: The intercept of the model is 7.0305581, which represents the expected log of charges when all predictors are zero. This value is highly significant with a p-value less than 2e-16. Age: For each additional year of age, the log of charges increases by 0.0345816. This effect is highly significant with a p-value less than 2e-16. Sex (male): Being male decreases the log of charges by 0.0754164 compared to being female. This effect is significant with a p-value of 0.002038. BMI: Each unit increase in BMI results in an increase in the log of charges by 0.0133748. This effect is highly significant with a p-value of 2.42e-10. Number of children: Each additional child increases the log of charges by 0.1018568. This effect is highly significant with a p-value less than 2e-16. Smoking status (yes): Being a smoker increases the log of charges by 1.5543288. This effect is highly significant with a p-value less than 2e-16. Region (northwest):Living in the northwest region decreases the log of charges by 0.0637876 compared to the baseline region. This effect is marginally significant with a p-value of 0.067860. Region (southeast): Living in the southeast region decreases the log of charges by 0.1571967 compared to the baseline region. This effect is highly significant with a p-value of 8.08e-06. Region (southwest): Living in the southwest region decreases the log of charges by 0.1289522 compared to the baseline region. This effect is significant with a p-value of 0.000241. The model explains approximately 76.79% of the variance in the log of charges, as indicated by the multiple R-squared value of 0.7679 and the adjusted R-squared value of 0.7666.  The overall model is highly significant, as indicated by the F-statistic of 549.8 with a p-value less than 2.2e-16.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "lessons/04_regression_mls.html#conclusion",
    "href": "lessons/04_regression_mls.html#conclusion",
    "title": "\n28  Multiple Linear Regression\n",
    "section": "\n28.7 Conclusion",
    "text": "28.7 Conclusion\nOur multiple linear regression model suggests that age, sex, BMI, number of children, smoking status, and region are significant predictors of log-transformed insurance charges.  Specifically, older age, higher BMI, more children, and being a smoker are associated with higher log-transformed insurance charges. In contrast, being male and residing in the northwest, southeast, or southwest regions tends to be associated with lower log-transformed insurance charges compared to their respective reference categories.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "lessons/04_regression_polynomial.html",
    "href": "lessons/04_regression_polynomial.html",
    "title": "\n29  Introduction to Polynomial Regression\n",
    "section": "",
    "text": "29.1 What is a Polynomial Regression?\nPolynomial regression is a type of regression analysis that models the non-linear relationship between the predictor variable(s) and response variable. It is an extension of simple linear regression that allows for more complex relationships between predictor and response variables. Field A, 2013",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Introduction to Polynomial Regression</span>"
    ]
  },
  {
    "objectID": "lessons/04_regression_polynomial.html#what-is-a-polynomial-regression",
    "href": "lessons/04_regression_polynomial.html#what-is-a-polynomial-regression",
    "title": "\n29  Introduction to Polynomial Regression\n",
    "section": "",
    "text": "29.1.1 When is a Polynomial Regression Used?\nPolynomial regression is useful when the relationship between the independent and dependent variables is nonlinear. It can capture more complex relationships than linear regression, making it suitable for cases where the data exhibits curvature.\n\n29.1.2 Assumptions of Polynomial Regression\n\n\nLinearity: There is a curvilinear relationship between the independent variable(s) and the dependent variable.\n\nHomoscedasticity: The variance of the errors should be constant across all levels of the independent variable(s).\n\nNormality: The errors should be normally distributed with mean zero and a constant variance.\n\nIndependence:The predictor variables are independent of each other.\n\n29.1.3 Mathematical Equation\nConsider independent samples \\(i = 1, \\ldots, n\\). The general formula for a polynomial regression representing the relationship between the response variable (\\(y\\)) and the predictor variable (\\(x\\)) as a polynomial function of degree \\(d\\) is:\n\\[\ny_i = \\beta_0 + \\beta_1x_i + \\beta_2x_i^2 + \\beta_3x_i^3 + ... \\beta_dx_i^d + \\epsilon_i,\n\\]\nwhere:\n\n\n\\(y_i\\) represents the response variable,\n\n\\(x_i\\) represents the predictor variable,\n\n\\(\\beta_0,\\ \\beta_1,\\ \\ldots,\\ \\beta_d\\) are the coefficients to be estimated, and\n\n\\(\\epsilon_i\\) represents the errors.\n\nFor large degree \\(d\\), polynomial regression allows us to produce an extremely non-linear curve. Therefore, it is not common to use \\(d &gt; 4\\) because the larger value of \\(d\\), the more overly flexible polynomial curve becomes, which can lead to overfitting the model to the data. Jackson SE, 2024\nThe coefficients in polynomial function can be estimated using least square linear regression because it can be viewed as a standard linear model with predictors \\(x_i, \\,x_i^2, \\,x_i^3, ..., x_i^d\\). Hence, polynomial regression is also known as polynomial linear regression.\n\n29.1.4 Performing a Polynomial Regression in R\n\nStep 0: Load required packages\nStep 1: Load and inspect the data\nStep 2: Visualize the data\nStep 3: Fit the model\nStep 4: Assess Assumptions\nStep 5: Describe model output",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Introduction to Polynomial Regression</span>"
    ]
  },
  {
    "objectID": "lessons/04_regression_polynomial.html#lets-practice",
    "href": "lessons/04_regression_polynomial.html#lets-practice",
    "title": "\n29  Introduction to Polynomial Regression\n",
    "section": "\n29.2 Let’s Practice!",
    "text": "29.2 Let’s Practice!\nNow let’s go through the steps to perform a polynomial regression in R. We will be using the lm() function to fit the polynomial regression model. This function comes standard in base R.\nFor this example, we will use the built-in mtcars dataset (from the standard R package datasets) which is publicly available and contains information about various car models.\n\n29.2.1 Hypotheses\nFor this example, we are investigating the following:\n\n\nResearch Question: Is there a significant quadratic relationship between the weight of a car (wt) and its miles per gallon (mpg) in the mtcars dataset?\n\nNull hypothesis (\\(H_0\\)): There is no significant relationship between the weight of a car (wt) and its miles per gallon (mpg).\n\nAlternative hypothesis (\\(H_A\\)): There is a significant relationship between the weight of a car (wt) and its miles per gallon (mpg).\n\nIn this case, the null hypothesis assumes that the coefficients of the quadratic polynomial terms are zero, indicating no relationship between the weight of the car and miles per gallon. The alternative hypothesis, on the other hand, suggests that at least one of the quadratic polynomial terms is non-zero, indicating a significant relationship between the weight of the car and miles per gallon.\nBy performing the polynomial regression analysis and examining the model summary and coefficients, we can evaluate the statistical significance of the relationship and determine whether to reject or fail to reject the null hypothesis.\n\n29.2.2 Step 0: Install and load required package\nAs we want to visualize our data after fitting the model, we will be loading the ggplot2 package.\n\n# For data visualization purposes\n# install.packages(\"ggplot2\")\nlibrary(ggplot2)\n\n\n29.2.3 Step 1: Load and inspect the data\n\n# Load mtcars dataset\ndata(mtcars)\n\n\n# Print the first few rows\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\n\n29.2.4 Step 2: Visualize the data\nBefore fitting a polynomial regression model, it’s helpful to visualize the data to identify any non-linear patterns. For our example, we will use a scatter plot to visualize the relationship between the independent and dependent variables:\n\n# Scatter plot of mpg (dependent variable) vs. wt (independent variable)\nggplot(mtcars) +\n  theme_minimal() +\n  aes(x = wt, y = mpg) + \n  labs(x = \"Weight (lbs/1000)\", y = \"Miles per Gallon\") +\n  geom_point()\n\n\n\n\n\n\n\n\n29.2.5 Step 3: Fit Models\nLet us create a function so we can build multiple models. We will fit a standard linear (degree = 1) and a quadratic polynomial (degree = 2) to the mtcars dataset.\n\n# Function to fit and evaluate polynomial regression models\nfit_poly_regression &lt;- function(degree) { \n  #argument specifies the degree of the polynomial for the regression\n  formula &lt;- as.formula(paste(\"mpg ~ poly(wt, \", degree, \", raw = TRUE)\")) \n  #paste concatenates the components into a single string eg:`mpg ~ poly(wt, 2)`\n  #poly returns an orthogonal polynomials as default\n  #(If `raw = TRUE` calculates raw polynomial instead of orthogonal polynomial)  \n  #as.formula converts the constructed string into a formula object \n  lm(formula, data = mtcars) #fitting the model\n}\n\n# Fit polynomial regression models with degrees 1 to 2\nmodel_1 &lt;- fit_poly_regression(1)\nmodel_2 &lt;- fit_poly_regression(2)\n\n*Note 1: To learn more about orthogonal polynomial regression, follow: a. Orthogonal polynomial equation and explanation b. Stackoverflow - difference between raw vs orthogonal polynomial c. StackExchange - Interpreting coefficients from raw vs orthogonal polynomial\n*Note 2: Using orthogonal polynomial regression would have also resulted in same plots based on which the assumptions are assessed. Moreover, it produces exact same p-value for the quadratic term but the p-value for linear term and the estimates for intercept, linear and quadratic term would be different than using raw polynomial regression.\n\n29.2.6 Step 4: Assess Assumptions\nBefore we can interpret the model, we have to check the assumptions. We will check these assumptions via plots:\n\nResiduals vs. Fitted values (used to check the linearity assumption),\na Q-Q plot of the Residuals (used to check the normality of the residuals),\na Scale-Location plot (used to check for heteroskedasticity), and\nResiduals vs. Leverage values (identifies overly influential values, if any exist).\n\n\npar(mfrow = c(2, 2)) #sets graphical parameters\nplot(model_1, which = c(1, 2, 3, 5))\n\n\n\n\n\n\nplot(model_2, which = c(1, 2, 3, 5))\n\n\n\n\n\n\npar(mfrow = c(1, 1))\n\nThe residuals vs fitted graph in the linear model has a U-shape, which suggests that we need a quadratic polynomial component. The Q-Q plot shows that the residuals are not normally distributed, so we should take additional steps to transform the response feature (such as via a square root or log transformation, or something similar).\n\n29.2.7 Conducting cube root transformation\nWe will cube root transform the response variable to build the models again.\n\n#cube root transformation of response variable (mpg)\nlibrary(dplyr)\nmtcars &lt;- mtcars %&gt;% mutate(mpg_cuberoot = mpg^(1/3))\n\n#Running model after cube root transformation\nfit_poly_regression_2 &lt;- function(degree) { \n  #argument specifies the degree of the polynomial for the regression\n  formula_2 &lt;- as.formula(paste(\"mpg_cuberoot ~ poly(wt, \", degree, \", raw = TRUE)\")) \n  #paste concatenates the components into a single string eg:`mpg ~ poly(wt, 2)`\n  #poly returns an orthogonal polynomials as default\n  #(If `raw = TRUE` calculates raw polynomial instead of orthogonal polynomial)  \n  #as.formula converts the constructed string into a formula object \n  lm(formula_2, data = mtcars) #fitting the model\n}\n\n# Fit polynomial regression models with degrees 1 to 2\nmodel_1_b &lt;- fit_poly_regression_2(1)\nmodel_2_b &lt;- fit_poly_regression_2(2)\n\n\n29.2.8 Assessing assumptions on cube root transformed data\nWe will assess the assumption of the polynomial regression with the plots similar to non-transformed model.\n\npar(mfrow = c(2, 2)) #sets graphical parameters\nplot(model_1_b, which = c(1, 2, 3, 5))\n\n\n\n\n\n\nplot(model_2_b, which = c(1, 2, 3, 5))\n\n\n\n\n\n\npar(mfrow = c(1, 1))\n\nAfter cube root transformation of the response variable, the residuals are seem to approximately normally distributed. Zoom in to see the difference between the Q-Q plots in the first quadratic model (model_2) and cube root transformed quadratic model (model_2_b). You can also draw the Q-Q plot again to see the difference more distinctly.\n\nqqnorm(residuals(model_2))\nqqline(residuals(model_2))\n\n\n\n\n\n\nqqnorm(residuals(model_2_b))\nqqline(residuals(model_2_b))\n\n\n\n\n\n\n\n\n29.2.9 Step 5. Describe Model Output\nAs the residuals of the cube root transformed model seem to be approximately normal, here is an example of how to interpret this output.\n\nsummary(model_2)\n\n\nCall:\nlm(formula = formula, data = mtcars)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.483 -1.998 -0.773  1.462  6.238 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               49.9308     4.2113  11.856 1.21e-12 ***\npoly(wt, 2, raw = TRUE)1 -13.3803     2.5140  -5.322 1.04e-05 ***\npoly(wt, 2, raw = TRUE)2   1.1711     0.3594   3.258  0.00286 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.651 on 29 degrees of freedom\nMultiple R-squared:  0.8191,    Adjusted R-squared:  0.8066 \nF-statistic: 65.64 on 2 and 29 DF,  p-value: 1.715e-11\n\nsummary(model_2_b)\n\n\nCall:\nlm(formula = formula_2, data = mtcars)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.16361 -0.08607 -0.02612  0.07303  0.23272 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               3.84775    0.18784  20.484  &lt; 2e-16 ***\npoly(wt, 2, raw = TRUE)1 -0.48063    0.11214  -4.286 0.000183 ***\npoly(wt, 2, raw = TRUE)2  0.03471    0.01603   2.165 0.038777 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1182 on 29 degrees of freedom\nMultiple R-squared:  0.8171,    Adjusted R-squared:  0.8044 \nF-statistic: 64.76 on 2 and 29 DF,  p-value: 2.012e-11\n\n\nThe model could be said as being highly significant as indicated by the p-value. The significant negative linear term and significant positive quadratic term imply that mpg decreases with weight initially, but the rate of decrease slows as weight increases.\nThe purpose of checking the transformed response is to confirm that the confidence intervals are valid and the signs of the regression coefficients are going are correct (i.e. in the same direction) as the original model (non-transformed quadratic model).\n\n29.2.10 Bonus Step: Visualize the Final Model\nFinally, we will build the scatter plot with the polynomial regression line to visualize the fit:\n\n# Create a data frame with data points and predictions \nplot_data &lt;- data.frame(\n  wt = mtcars$wt,\n  mpg = mtcars$mpg, \n  mpg_predicted = predict(model_2, newdata = mtcars)\n)\n\n# Scatter plot with the polynomial regression line\nggplot(plot_data) +\n  theme_minimal() + \n  aes(x = wt, y = mpg) + \n  labs(\n    title = \"Scatter Plot with Polynomial Regression Line\",\n    x = \"Weight (wt)\",\n    y = \"Miles per Gallon (mpg)\"\n  ) +\n  geom_point() +\n  geom_line(aes(y = mpg_predicted), color = \"red\", size = 1)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\nplot_data_b &lt;- data.frame(\n  wt = mtcars$wt,\n  mpg_cuberoot = mtcars$mpg_cuberoot, \n  mpg_predicted = predict(model_2_b, newdata = mtcars)\n)\n\n# Scatter plot with the polynomial regression line\nggplot(plot_data_b) +\n  theme_minimal() + \n  aes(x = wt, y = mpg_cuberoot) + \n  labs(\n    title = \"Scatter Plot with Polynomial Regression Line (Cube transformed)\",\n    x = \"Weight (wt)\",\n    y = \"Cube transfomred Miles per Gallon (mpg)\"\n  ) +\n  geom_point() +\n  geom_line(aes(y = mpg_predicted), color = \"red\", size = 1)",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Introduction to Polynomial Regression</span>"
    ]
  },
  {
    "objectID": "lessons/04_regression_polynomial.html#references",
    "href": "lessons/04_regression_polynomial.html#references",
    "title": "\n29  Introduction to Polynomial Regression\n",
    "section": "\n29.3 References",
    "text": "29.3 References\n\nField, A. (2013). Discovering Statistics Using IBM SPSS Statistics. (4th ed.). Sage Publications.\nhttps://bookdown.org/ssjackson300/Machine-Learning-Lecture-Notes/#welcome\nhttps://stats.libretexts.org/Bookshelves/Advanced_Statistics/Analysis_of_Variance_and_Design_of_Experiments/10%3A_ANCOVA_Part_II/10.02%3A_Quantitative_Predictors_-_Orthogonal_Polynomials#",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Introduction to Polynomial Regression</span>"
    ]
  },
  {
    "objectID": "lessons/04_regression_mixed_effects.html",
    "href": "lessons/04_regression_mixed_effects.html",
    "title": "30  Mixed Effect Regression",
    "section": "",
    "text": "30.1 Introduction",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Mixed Effect Regression</span>"
    ]
  },
  {
    "objectID": "lessons/04_regression_mixed_effects.html#introduction",
    "href": "lessons/04_regression_mixed_effects.html#introduction",
    "title": "30  Mixed Effect Regression",
    "section": "",
    "text": "30.1.1 What is Mixed Effect Model?\nMixed Effects Regression is a method for analyzing data that are non-independent, multilevel/hierarchical, longitudinal, repeated or correlated. It allows both fixed and random effects, and are particularly used when there is non independence in the data, such as arises from a hierarchical structure. For example, students sampled from within schools, or patients from within hospitals.\n\n\n30.1.2 Notes on the content\nThroughout this lesson, we added references of some YouTube videos which we found helpful. Most of the content here is from those videos, courtesy of the Quant Psych channel. If you find the content of this discussion difficult, we highly recommend you to go watch those videos.\n\n\n30.1.3 Terminology:\nOther terms that are used for Mixed Effect Models include:\n\nHierarchical Leaner Model (HLM)\nMulti-level model (MLM)\nLinear mixed-effect model (LMM)\nMixed model\nRepeated measures linear regression\nRandom effects model\nVarying coefficients model\n\n Focus on what the model is trying to do instead of what the model is called.\n\n\n30.1.4 An Example\nFirst figure shows a positive correlation: the proportion of survival increases as the the severity of symptom increases.\n\nHowever, in real, if we color-code these data, we can see that three different clusters of hospitals data are shown.\n\nIn mixed effect models, they fit a separate regression line for each and every cluster. Then it estimates an average slope between Y (Proportion of survival) and X (severity of symptoms) across all clusters (all hospitals).\n \nHere, the black line is the fixed effects and the colored lines are the random effects. So, all the participants across the three hospitals cannot be treated as independent data. Every cluster has its own regression line.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Mixed Effect Regression</span>"
    ]
  },
  {
    "objectID": "lessons/04_regression_mixed_effects.html#mathematical-formulation-of-the-model",
    "href": "lessons/04_regression_mixed_effects.html#mathematical-formulation-of-the-model",
    "title": "30  Mixed Effect Regression",
    "section": "30.2 Mathematical Formulation of the Model",
    "text": "30.2 Mathematical Formulation of the Model\nFixed effects represent the systematic or population-level effects. These are parameters that apply to the entire population. In mathematical notation, we denote fixed effects as \\(\\beta\\) and the fixed effects part of the model can be expressed as:\n\\[\nY = \\textbf{X}\\beta+e\n\\] Where,\n\n\\(Y\\) is response variable\n\\(\\textbf{X}\\) is a matrix of predictor variables\n\n\\(e\\) represents the random error.\n\nRandom effects account for variability due to specific groups or clusters within the data. These effects are specific to certain levels of a grouping variable (e.g., subjects, schools, or regions). In mathematical notation, we denote random effects as \\(\\textbf{Z}a\\), where,\n\n\\(\\textbf{Z}\\) is a fixed matrix\n\\(a\\) is an unobserved random effect vector\n\nThe full model incorporating both fixed and random effects is:\n\\[\nY = \\textbf{X}\\beta + \\textbf{Z}a+ e.\n\\] For example: to study the effect of different doses (predictor variable) of an anti-hypertensive drug on blood pressure (response variable) across multiple subjects, the model might look like this:\n\\[\nY_{ij} =\n  \\overbrace{\\beta_0 + \\beta_1Dose_{ij}}^{\\text{fixed part}} +\n  \\underbrace{a_i + e_{ij}}_{\\text{random part}}\n\\]\nWhere,\n\n\\(Y_{ij}\\) is blood pressure for subject \\(i\\) at dose level \\(j\\)\n\\(\\beta_0\\) is fixed effect parameter (intercept), represents baseline blood pressure when the dose was zero.\n\\(\\beta_1\\) is another fixed effect parameter, represents effect of the dose levels on the blood pressure\n\\(\\text{Dose}_{ij}\\) is dose level for subject \\(i\\) at level \\(j\\)\n\\(a_i\\) is random effect for subject \\(i\\), that captures subject-specific variability not explained by fixed effects. These random effects account for individual differences.\n\\(e_{ij}\\) is random error term, represents unexplained variability or noise in the model.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Mixed Effect Regression</span>"
    ]
  },
  {
    "objectID": "lessons/04_regression_mixed_effects.html#regression-models-used-in-analysis",
    "href": "lessons/04_regression_mixed_effects.html#regression-models-used-in-analysis",
    "title": "30  Mixed Effect Regression",
    "section": "30.3 Regression Models used in analysis",
    "text": "30.3 Regression Models used in analysis\n\nFixed effects model (when all model effects are fixed effects) Same intercept and same slope\nMixed effects model (when both fixed effect(s) and random effect(s) are present) Different intercepts same slope OR same intercept different slopes\nRandom effects model (when all model effects are random effects) Different intercepts different slopes\n\nDepending on the data type and research question, we can use different types of models with random effects.\n\n\n\nExamples: Top Left: no group effects; Top Right: temperature/time (3 different unit of AC); Bottom Left: blood sugar level/hour (3 different diabetic-drug); Bottom Right: weight/week (3 different diets)",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Mixed Effect Regression</span>"
    ]
  },
  {
    "objectID": "lessons/04_regression_mixed_effects.html#choosing-the-right-model",
    "href": "lessons/04_regression_mixed_effects.html#choosing-the-right-model",
    "title": "30  Mixed Effect Regression",
    "section": "30.4 Choosing the Right Model",
    "text": "30.4 Choosing the Right Model\nHow should we decide which variable should be considered as fixed, mixed, or random? We have three strategies:\n\n30.4.1 Strategy 1: Look into the data set\n\nExample: Math dataset: Math achievement data for different schools (clusters)\nVariables: School, Minority Status, Sex, Socio-Economic Status (SES), Math achievement and mean SES,\n\n\n\nDecision: If all the values in a variable (MEANSES) does not vary within a cluster variable (School), it cannot be fitted as random effect. This is because there is no slope as the MEANSES variable doesn’t vary within the SCHOOL cluster.).\n\n\n\n30.4.2 Strategy 2: Use theoretical knowledge to guide you\nAsk yourself or an expert to know more about the relation between two variables, if that relation changes within the cluster or not. For example, according to historic nutrition models, if we control the age, the relationship between calorie consumption and weight loss is exactly same for all individuals. However, in many cases using theory to decide if the slope will be fixed or random may not be possible.\n\n\n30.4.3 Strategy 3: Model comparison\nTo explicitly test whether those effects should be fixed or random, we can compare the models with and without using the specific variable of interest as full model and reduced model. For example, in full model below, SES varies by school (random effect), but in reduced model SES did not vary by school (fixed effect).\n Here are some decision criteria based on this model comparison output:\n\nDecision 1: Because \\(p &gt; 0.05\\), there is no statistically significant difference between the full and reduced models, so we can use the reduced model.\nDecision 2: Because the Bayes Factor \\(&gt; 10\\) for reduced model, we can choose the reduced model.\nDecision 3: The AIC and BIC give mixed results. The reduced model has lower BIC but higher AIC when compared to the full model. However, considering the “decisions” we made based on the \\(p\\)-value and Bayes Factor, we still choose the reduced model.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Mixed Effect Regression</span>"
    ]
  },
  {
    "objectID": "lessons/04_regression_mixed_effects.html#how-to-do-the-analysis",
    "href": "lessons/04_regression_mixed_effects.html#how-to-do-the-analysis",
    "title": "30  Mixed Effect Regression",
    "section": "30.5 How to do the analysis?",
    "text": "30.5 How to do the analysis?\n\n30.5.1 Packages for the following Lesson\n\n# install.packages(\"Matrix\")\n# install.packages(\"MASS\") \nlibrary(gt)        # to make table outputs presentation/publication ready. \nlibrary(broom)     # clean output.\nlibrary(knitr)     # to make tables. \nlibrary(gtsummary) # extension to gt\nlibrary(lattice)   # It is a powerful data visualization for multivariate data\nlibrary(lme4)      # Fit linear and generalized linear mixed-effects models\nlibrary(arm)       # Data Analysis Using Regression and Multilevel models\nlibrary(tidyverse) # for cleaning wrangling data\n\n\n\n30.5.2 Data Source and Description\nLet’s do the analysis using the Planktonic larval duration (PLD) example! This example is from O’Connor et al (2007). As a brief intro on this study, temperature is important for the development of organisms. In marine species, temperature is very important and linked to growth. We want to see the association between time of survival (duration/PLD) and the temperature.\n\nPLD &lt;- read_table(\"../data/04_PLD.txt\")\n\nLet’s check the structure of this data and how it briefly looks.\n\n#strcuture \nstr(PLD)\n\nspc_tbl_ [214 × 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ phylum : chr [1:214] \"Annelida\" \"Annelida\" \"Annelida\" \"Annelida\" ...\n $ species: chr [1:214] \"Circeus.spirillum\" \"Circeus.spirillum\" \"Circeus.spirillum\" \"Hydroides.elegans\" ...\n $ D.mode : chr [1:214] \"L\" \"L\" \"L\" \"P\" ...\n $ temp   : num [1:214] 5 10 15 15 20 25 30 5 10 17 ...\n $ pld    : num [1:214] 16 16 4 8 6.5 5 3.2 15 9.5 4.5 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   phylum = col_character(),\n  ..   species = col_character(),\n  ..   D.mode = col_character(),\n  ..   temp = col_double(),\n  ..   pld = col_double()\n  .. )\n\n# just the top - seeing how it looks\nhead(PLD)\n\n# A tibble: 6 × 5\n  phylum   species           D.mode  temp   pld\n  &lt;chr&gt;    &lt;chr&gt;             &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 Annelida Circeus.spirillum L          5  16  \n2 Annelida Circeus.spirillum L         10  16  \n3 Annelida Circeus.spirillum L         15   4  \n4 Annelida Hydroides.elegans P         15   8  \n5 Annelida Hydroides.elegans P         20   6.5\n6 Annelida Hydroides.elegans P         25   5  \n\n# brief summary\nsummary(PLD)\n\n    phylum            species             D.mode               temp      \n Length:214         Length:214         Length:214         Min.   : 2.50  \n Class :character   Class :character   Class :character   1st Qu.:12.28  \n Mode  :character   Mode  :character   Mode  :character   Median :20.00  \n                                                          Mean   :18.59  \n                                                          3rd Qu.:25.00  \n                                                          Max.   :32.00  \n      pld        \n Min.   :  1.00  \n 1st Qu.: 11.00  \n Median : 19.30  \n Mean   : 26.28  \n 3rd Qu.: 33.45  \n Max.   :129.00  \n\n\nLet’s check how this would look if we plot the variable pld or planktonic larvae duration and the temperature. So we can see how the temperature is associated with their survival duration.\n\n# how to plot in base R \nplot(pld ~ temp, data = PLD,\n     xlab = \"Temperature (C)\",\n     ylab = \"PLD survival (Days)\",\n     pch = 16)\n# add lm line in base R\nabline(lm(pld ~ temp, data = PLD))\n\n\n\n\n\n\n\n\n\n\n30.5.3 Fit first model: Linear model\nLet’s first fit a linear model and check any assumptions. Why are we fitting a linear model first? It might be important to check the standard error of this model and compare to the next model, that might be a better fit later on.\n\n# fitting linear model first \n\nLinearModel_1 &lt;- lm(pld ~ temp, data = PLD)\n\nsummary(LinearModel_1)\n\n\nCall:\nlm(formula = pld ~ temp, data = PLD)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-44.158 -11.351  -0.430   7.684  93.884 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  54.8390     3.7954  14.449  &lt; 2e-16 ***\ntemp         -1.5361     0.1899  -8.087 4.65e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 20.36 on 212 degrees of freedom\nMultiple R-squared:  0.2358,    Adjusted R-squared:  0.2322 \nF-statistic:  65.4 on 1 and 212 DF,  p-value: 4.652e-14\n\n\nWe can fit this output in a gtsummary to make it nicer looking:\n\nLinearModel_1 |&gt; \n  tbl_regression()\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI1\np-value\n\n\n\n\ntemp\n-1.5\n-1.9, -1.2\n&lt;0.001\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\n\nWe are interested in checking out visually, the equal variance (homoscedasticity) of residuals. So, we plot a base residual graph:\n\n# better to do a scatter plot \nLinearModel_res &lt;- resid(LinearModel_1)\n\n# plot residual \nplot(PLD$temp, LinearModel_res,\n     ylab = \"Residuals\",\n     xlab = \"Temperature (C)\",\n     main = \"Residual graph\"\n     )\nabline(0,0)\n\n\n\n\n\n\n\n\nJust upon visual observation, it seems like this assumption may be violated, so we think it might be important to do some transformations.\n\n\n30.5.4 Log transformation\n\nLinearMode_2Log &lt;- lm(log(pld) ~ temp, data = PLD)\n\nsummary(LinearMode_2Log)\n\n\nCall:\nlm(formula = log(pld) ~ temp, data = PLD)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.1670 -0.4327  0.1948  0.5704  1.9141 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.743473   0.153517  24.385  &lt; 2e-16 ***\ntemp        -0.044345   0.007683  -5.772 2.76e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8236 on 212 degrees of freedom\nMultiple R-squared:  0.1358,    Adjusted R-squared:  0.1317 \nF-statistic: 33.31 on 1 and 212 DF,  p-value: 2.759e-08\n\n\n\n\n30.5.5 Residual of new log transformed graph\n\n# better to do a scatter plot \nLinearModel2_res &lt;- resid(LinearMode_2Log)\n\n# plot residual \nplot(PLD$temp, LinearModel2_res,\n     ylab = \"residuals\",\n     xlab = \"temp\",\n     main = \"Residual graph (log transformation)\"\n     )\nabline(0,0)\n\n\n\n\n\n\n\n\nA bit better! Now we want to see the original plot we plotted with PLD and temperature:\n\nplot(log(pld) ~ temp, data = PLD,\n     xlab = \"Temperature in C\",\n     ylab = \"PLD in days\")\nabline(LinearMode_2Log)\n\n\n\n\n\n\n\n\n\n\n30.5.6 Check distribution by phylum\nIn ggplot you can use the facet_wrap() function to separate by phylum:\n\nggplot(data = PLD) +\n  aes(x = temp, y = log(pld)) +\n  geom_point() +\n  labs(x = \"temperature, C\",\n       y = \"Log(PLD)\") + \n  stat_smooth(method = \"lm\", formula = \"y ~ x\", se = FALSE, fullrange = TRUE) +\n  theme_classic() +\n  facet_wrap(~ phylum)\n\n\n\n\n\n\n\n\n\n\n30.5.7 Fitting the mixed model\nWe can use the library lme4 to fit a log-transformed linear regression model with random intercepts and fixed slope\n\n# creating log -transformed variables \nPLD$log_pld &lt;- log(PLD$pld)\n\n# mixed model with random intercepts and fixed slope\nMixEfcModel &lt;- lmer(log_pld ~ temp + (1 | phylum), data = PLD)\n\nsummary(MixEfcModel)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: log_pld ~ temp + (1 | phylum)\n   Data: PLD\n\nREML criterion at convergence: 493.1\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.6049 -0.5235  0.1978  0.6963  2.2825 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n phylum   (Intercept) 0.3128   0.5593  \n Residual             0.5278   0.7265  \nNumber of obs: 214, groups:  phylum, 6\n\nFixed effects:\n             Estimate Std. Error t value\n(Intercept)  3.544039   0.271814  13.038\ntemp        -0.056631   0.007217  -7.846\n\nCorrelation of Fixed Effects:\n     (Intr)\ntemp -0.446\n\n\nInterpretation: For one unit increase in the degrees of temperature, there is a 0.057 unit decrease in Planktonic larval duration. (or, as the temperature increase, the plankton duration is lower.) We now check the coefficients for random intercepts and fixed slope of phylum cluster:\n\ncoef(MixEfcModel)$phylum\n\n              (Intercept)        temp\nAnnelida         3.102042 -0.05663111\nArthropoda       4.244984 -0.05663111\nBrachiopoda      2.865529 -0.05663111\nChordata         3.355257 -0.05663111\nEchinodermata    3.829580 -0.05663111\nMollusca         3.866841 -0.05663111\n\n\n\n\n30.5.8 Add Mixed Effect Predictions\nWe first need to create a new data frame with these random intercepts and fixed slope. We create a new data frame modelCoef_df:\n\nmodelCoef_df &lt;- \n  coef(MixEfcModel)$phylum %&gt;% \n  # to convert row names between an explicit column\n  rownames_to_column(var = \"phylum\") %&gt;% \n  rename(intercept = `(Intercept)`, slope = temp)\n\nmodelCoef_df\n\n         phylum intercept       slope\n1      Annelida  3.102042 -0.05663111\n2    Arthropoda  4.244984 -0.05663111\n3   Brachiopoda  2.865529 -0.05663111\n4      Chordata  3.355257 -0.05663111\n5 Echinodermata  3.829580 -0.05663111\n6      Mollusca  3.866841 -0.05663111\n\n\nNow we create a prediction model data frame by adding the modelCoef_df with the prediction model to calculate the predicted values:\n\npld_predicted_df &lt;- \n  PLD %&gt;% \n  left_join(y = modelCoef_df, by = \"phylum\") %&gt;% \n  mutate(\n    log_pld_pred = intercept + temp * slope\n  )\n\n\n\n30.5.9 Plotting the prediction models\n\nggplot(data = pld_predicted_df) +\n  theme_classic() +\n  aes(x = temp) +\n  labs(\n    x = \"temperature, C\",\n    y = \"Log(PLD)\"\n  ) + \n  geom_point(aes(y = log_pld)) +\n  geom_line(aes(y = log_pld_pred), color = \"blue\", size = 1) +\n  facet_wrap(~ phylum)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Mixed Effect Regression</span>"
    ]
  },
  {
    "objectID": "lessons/04_regression_mixed_effects.html#interpretation",
    "href": "lessons/04_regression_mixed_effects.html#interpretation",
    "title": "30  Mixed Effect Regression",
    "section": "30.6 Interpretation",
    "text": "30.6 Interpretation\nWe can see in the predicted model, all slopes for within all phylum clusters are in the same line, showing it has fixed effect and random intercepts. You can compare it with the linear model figure and can see the differences of the slopes.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Mixed Effect Regression</span>"
    ]
  },
  {
    "objectID": "lessons/04_regression_mixed_effects.html#conclusion",
    "href": "lessons/04_regression_mixed_effects.html#conclusion",
    "title": "30  Mixed Effect Regression",
    "section": "30.7 Conclusion",
    "text": "30.7 Conclusion\nIn this lecture you learned about the definition and types of mixed effect model, when it is appropriate to use a random, fixed or mixed effect model, the difference between an ordinary single-level model, and a mixed effect model, the assumptions of the random intercept model, hypothesis testing for the variation, testing coefficient, fitting prediction model and finally, how to interpret results from the fixed part and the random part of a random intercept model.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Mixed Effect Regression</span>"
    ]
  },
  {
    "objectID": "lessons/04_regression_mixed_effects.html#references",
    "href": "lessons/04_regression_mixed_effects.html#references",
    "title": "30  Mixed Effect Regression",
    "section": "30.8 References",
    "text": "30.8 References\n\nHarrison XA, Donaldson L, Correa-Cano ME, Evans J, Fisher DN, Goodwin CED, Robinson BS, Hodgson DJ, Inger R. A brief introduction to mixed effects modelling and multi-model inference in ecology. PeerJ. 2018 May 23;6:e4794. doi: 10.7717/peerj.4794. PMID: 29844961; PMCID: PMC5970551.\nhttps://m-clark.github.io/mixed-models-with-R/random_intercepts.html\nhttps://medium.com/(marc.jacobs012/introduction-to-mixed-models-in-r-9c017fd83a63?)\nhttps://www.youtube.com/watch?v=c_tYZxQLoDA&list=PL8F480DgtpW9_IT7xN1XeRF_dglZmK0nM\nhttps://www.youtube.com/watch?v=eVuQlGDZxHs&list=PL8F480DgtpW9_IT7xN1XeRF_dglZmK0nM&index=2\nsupplementary material for Tom Snijders and Roel Bosker textbook - Shjders, T Bosker R, 1999. Multilevel analysis: an introduction to basic and advanced mltilevel modeling, London, Sage, including updates and corrections data set examples http://stat.gamma.rug.nl/multilevel.htm\nUniversity of Bristol, Random Intercept model, (2018). http://www.bristol.ac.uk/cmm/learning/videos/random-intercepts.html\nMidway, S. (2019). “Data Analysis in R.” https://bookdown.org/steve_midway/DAR/random-effects.html",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Mixed Effect Regression</span>"
    ]
  },
  {
    "objectID": "05_header_generalized-linear-models.html",
    "href": "05_header_generalized-linear-models.html",
    "title": "Generalized Linear Models",
    "section": "",
    "text": "Text Outline\nThis text covers the basics of:",
    "crumbs": [
      "Generalized Linear Models"
    ]
  },
  {
    "objectID": "05_header_generalized-linear-models.html#text-outline",
    "href": "05_header_generalized-linear-models.html#text-outline",
    "title": "Generalized Linear Models",
    "section": "",
    "text": "Exploring data\nOne-sample tests\nTwo-sample tests\nANOVA and linear regression\nGeneralized linear models\nSome special topics\nMaybe a bit about power calculations…",
    "crumbs": [
      "Generalized Linear Models"
    ]
  },
  {
    "objectID": "05_header_generalized-linear-models.html#part-outline",
    "href": "05_header_generalized-linear-models.html#part-outline",
    "title": "Generalized Linear Models",
    "section": "Part Outline",
    "text": "Part Outline\nThis part of the text contains the chapters on various different models within the GLM family:\n\nGeneralized Linear Models: Binary\nGeneralized Linear Models: Ordered\nGeneralized Linear Models: Count (Poisson)\nGeneralized Linear Models: Count (Negative Binomial)",
    "crumbs": [
      "Generalized Linear Models"
    ]
  },
  {
    "objectID": "lessons/05_glm_logistic.html",
    "href": "lessons/05_glm_logistic.html",
    "title": "\n31  Generalized Linear Models for Binary Response\n",
    "section": "",
    "text": "31.1 Generalized Liner models(binary) or Logistic Regression\nA type of Generalized Linear Model (GLM) used to model binary outcomes.",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Generalized Linear Models for Binary Response</span>"
    ]
  },
  {
    "objectID": "lessons/05_glm_logistic.html#generalized-liner-modelsbinary-or-logistic-regression",
    "href": "lessons/05_glm_logistic.html#generalized-liner-modelsbinary-or-logistic-regression",
    "title": "\n31  Generalized Linear Models for Binary Response\n",
    "section": "",
    "text": "Dependent Variable: Binary (e.g., 0 or 1).\nIndependent Variables: One or more predictor variables that can be continuous or categorical.",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Generalized Linear Models for Binary Response</span>"
    ]
  },
  {
    "objectID": "lessons/05_glm_logistic.html#purpose",
    "href": "lessons/05_glm_logistic.html#purpose",
    "title": "\n31  Generalized Linear Models for Binary Response\n",
    "section": "\n31.2 Purpose:",
    "text": "31.2 Purpose:\nThis regression models the relationship between a set of predictor variables and a binary response variable. Commonly used for classification problems where the outcome is categorical with two possible values (e.g., yes/no, success/failure).",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Generalized Linear Models for Binary Response</span>"
    ]
  },
  {
    "objectID": "lessons/05_glm_logistic.html#understanding-the-function",
    "href": "lessons/05_glm_logistic.html#understanding-the-function",
    "title": "\n31  Generalized Linear Models for Binary Response\n",
    "section": "\n31.3 Understanding the function",
    "text": "31.3 Understanding the function\nLogit Function: Links the linear combination of predictors to the probability of the outcome. The logit function is defined as:\n\\[\n\\text{logit}(p) := \\log\\left(\\frac{p}{1 - p}\\right).\n\\]\nSo our regression equation is thus written as:\n\\[\n\\log\\left(\\frac{p}{1 - p}\\right) =\n  \\beta_0 + \\beta_1X_1 + \\ldots + \\beta_pX_p.\n\\]\nThe probability of the outcome being 1 (success) is given by:\n\\[\np = \\frac{1}{1 + \\exp\\left[-(\\beta_0 + \\beta_1X_1 + \\ldots + \\beta_pX_p)\\right]}.\n\\]\n\n31.3.1 Interpret and Coefficients:\nIntercept (\\(\\beta_0\\)): The log-odds of the outcome when all predictors are zero.\nCoefficients (\\(\\beta_i\\)): The change in log-odds of the outcome for a one-unit increase in the predictor\n\n31.3.2 Odds and Odds Ratio:\nOdds: The ratio of the probability of the event occurring to the probability of it not occurring.\nOdds Ratio: The ratio of the odds of the outcome occurring for different values of a predictor.",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Generalized Linear Models for Binary Response</span>"
    ]
  },
  {
    "objectID": "lessons/05_glm_logistic.html#assumptions",
    "href": "lessons/05_glm_logistic.html#assumptions",
    "title": "\n31  Generalized Linear Models for Binary Response\n",
    "section": "\n31.4 Assumptions:",
    "text": "31.4 Assumptions:\nParameters are estimated using Maximum Likelihood Estimation (MLE). The goal is to find the values of the coefficients that maximize the likelihood of observing the given data. We assume that:\n\n\nObservations are independent: There is a linear relationship between the logit of the outcome and the predictors.\n\nNo multicollinearity among the predictors. The sample size is sufficiently large.",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Generalized Linear Models for Binary Response</span>"
    ]
  },
  {
    "objectID": "lessons/05_glm_logistic.html#data-analysis",
    "href": "lessons/05_glm_logistic.html#data-analysis",
    "title": "\n31  Generalized Linear Models for Binary Response\n",
    "section": "\n31.5 Data Analysis",
    "text": "31.5 Data Analysis\nWe will use the train.csv dataset from Kaggle’s: Machine Learning from Disaster competition.link\n\n# Load the Titanic train dataset\ntitanic_train_data &lt;- read_csv(\"../data/05_titanic_train.csv\")\n\nRows: 891 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): Name, Sex, Ticket, Cabin, Embarked\ndbl (7): PassengerId, Survived, Pclass, Age, SibSp, Parch, Fare\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Display the first few rows of the dataset\nhead(titanic_train_data)\n\n# A tibble: 6 × 12\n  PassengerId Survived Pclass Name    Sex     Age SibSp Parch Ticket  Fare Cabin\n        &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;\n1           1        0      3 Braund… male     22     1     0 A/5 2…  7.25 &lt;NA&gt; \n2           2        1      1 Cuming… fema…    38     1     0 PC 17… 71.3  C85  \n3           3        1      3 Heikki… fema…    26     0     0 STON/…  7.92 &lt;NA&gt; \n4           4        1      1 Futrel… fema…    35     1     0 113803 53.1  C123 \n5           5        0      3 Allen,… male     35     0     0 373450  8.05 &lt;NA&gt; \n6           6        0      3 Moran,… male     NA     0     0 330877  8.46 &lt;NA&gt; \n# ℹ 1 more variable: Embarked &lt;chr&gt;\n\n\n\n# Handle missing values by removing rows with NA values in 'Age' and 'Embarked' columns\ntitanic_train_data &lt;- \n  titanic_train_data %&gt;%\n  filter(!is.na(Age) & !is.na(Embarked))\n\n# Convert necessary columns to factors\ntitanic_train_data$Pclass &lt;- factor(titanic_train_data$Pclass)\ntitanic_train_data$Sex &lt;- factor(titanic_train_data$Sex)\ntitanic_train_data$Embarked &lt;- factor(titanic_train_data$Embarked)\ntitanic_train_data$Survived &lt;- factor(titanic_train_data$Survived, levels = c(0, 1))\n\n\n31.5.1 Fit the Logistic Regression Model\nFit a logistic regression model using Survived as the response variable and Pclass, Age, Sex, and Embarked as predictors.\n\n# Fit the logistic regression model\nmodel &lt;- glm(\n  Survived ~ Pclass + Age + Sex + Embarked, \n  data = titanic_train_data, \n  family = binomial\n)\n\n# Display the summary of the model\nsummary(model)\n\n\nCall:\nglm(formula = Survived ~ Pclass + Age + Sex + Embarked, family = binomial, \n    data = titanic_train_data)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  4.036825   0.430758   9.371  &lt; 2e-16 ***\nPclass2     -1.144614   0.290678  -3.938 8.23e-05 ***\nPclass3     -2.409565   0.291179  -8.275  &lt; 2e-16 ***\nAge         -0.036082   0.007715  -4.677 2.92e-06 ***\nSexmale     -2.515793   0.209293 -12.020  &lt; 2e-16 ***\nEmbarkedQ   -0.814190   0.567903  -1.434   0.1517    \nEmbarkedS   -0.493651   0.266886  -1.850   0.0644 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 960.90  on 711  degrees of freedom\nResidual deviance: 642.68  on 705  degrees of freedom\nAIC: 656.68\n\nNumber of Fisher Scoring iterations: 5\n\n\nModel Summary Interpretation:\nIntercept: The estimated intercept is 4.036825 with a standard error of 0.430758. This represents the log-odds of survival for a baseline group (first-class female passengers embarked from port ‘C’ with age = 0).\nPclass2: The coefficient is -1.144614 with a standard error of 0.290678. This indicates that being in the second class decreases the log-odds of survival by 1.144614 compared to being in the first class. The p-value (8.23e-05) is less than 0.05, indicating statistical significance.\nPclass3: The coefficient is -2.409565 with a standard error of 0.291179. This indicates that being in the third class decreases the log-odds of survival by 2.409565 compared to being in the first class. The p-value (&lt; 2e-16) is very small, indicating strong statistical significance.\nAge: The coefficient is -0.036082 with a standard error of 0.007715. This indicates that each additional year of age decreases the log-odds of survival by 0.036082. The p-value (2.92e-06) is less than 0.05, indicating statistical significance.\nSexmale: The coefficient is -2.515793 with a standard error of 0.209293. This indicates that being male decreases the log-odds of survival by 2.515793 compared to being female. The p-value (&lt; 2e-16) is very small, indicating strong statistical significance.\nEmbarkedQ: The coefficient is -0.814190 with a standard error of 0.567903. This indicates that embarking from port ‘Q’ decreases the log-odds of survival by 0.814190 compared to embarking from port ‘C’. The p-value (0.1517) is greater than 0.05, indicating that this effect is not statistically significant.\nEmbarkedS: The coefficient is -0.493651 with a standard error of 0.266886. This indicates that embarking from port ‘S’ decreases the log-odds of survival by 0.493651 compared to embarking from port ‘C’. The p-value (0.0644) is slightly greater than 0.05, indicating marginal significance.\n*Pclass, Age, and Sex are statistically significant in predicting survival.\nWe perform the Paris Plot to shows the relationship between the predicted probabilities and the empirical probabilities.\n\n# Predict probabilities\npredicted_probs &lt;- predict(model, type = \"response\")\n\n# Create the Paris plot\nempirical_probs &lt;- ecdf(predicted_probs)\nsorted_probs &lt;- sort(predicted_probs)\nplot(sorted_probs, empirical_probs(sorted_probs), type = \"l\", \n     xlab = \"Predicted probability\", ylab = \"Empirical probability\", \n     main = \"Paris Plot for Logistic Regression\")\ngrid()\n\n\n\n\n\n\n\nThe Paris plot shows that the logistic regression model is generally effective in predicting survival on the Titanic dataset. The predicted probabilities closely follow the empirical probabilities, especially at the extremes (very low and very high predicted probabilities).  There may be some room for improvement in the middle range of predicted probabilities, where the line is not as steep. This could indicate that the model might benefit from additional predictors or different modeling techniques to improve accuracy.\n\n31.5.2 Evaluate the model performance\nWe can visualize the ROC curve and calculate the AUC to evaluate the model’s performance.\n\n# Compute the ROC curve\nroc_curve &lt;- roc(titanic_train_data$Survived, predict(model, type = \"response\"))\n\nSetting levels: control = 0, case = 1\n\n\nSetting direction: controls &lt; cases\n\n# Plot the ROC curve\nplot(roc_curve, main = \"ROC Curve for Logistic Regression Model\")\n\n\n\n\n\n\n# Calculate AUC\nauc(roc_curve)\n\nArea under the curve: 0.8544\n\n\nThe ROC curve is well above the diagonal line and closer to the top-left corner, indicating that the model has good discriminatory power. This means the model is effective in distinguishing between survivors and non-survivors.\nAn AUC of 0.8544 falls within the “excellent” range. This means this logistic regression model has a high ability to distinguish between survivors and non-survivors on the Titanic dataset. &gt;AUC Value Range:\n\nAUC value: AUC ranges from 0.5 to 1.0.  value of 0.5 indicating that the test is no better than chance at distinguishing between diseased and nondiseased individuals.  A value of 1.0 indicates perfect discrimination.  AUC values above 0.80 are generally consideredclinically useful.  AUC values below 0.80 are considered of limited clinical utility.  When interpreting AUC values, it is important to consider the 95% confidence interval. The confidence interval reflects the uncertainty around the AUC value.A narrow confidence interval indicates that the AUC value is likely accurate, while a wide confidence interval indicates that the AUC value is less reliable.Çorbacıoğlu ŞK,etc. 2023",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Generalized Linear Models for Binary Response</span>"
    ]
  },
  {
    "objectID": "lessons/05_glm_ordinal_logistic.html",
    "href": "lessons/05_glm_ordinal_logistic.html",
    "title": "\n32  Ordinal Logistic Regression\n",
    "section": "",
    "text": "32.1 Introduction\nSo you’ve found yourself thinking about regression again, haven’t you? I can relate. At this point, you’re probably familiar with the fact that logistic regression means the logit (log odds) of a binary response is linearly related to the independent variable(s). Well, Proportional Odds Logistic Regression is an extension of Logistic Regression and that’s likely how you’ve found your way here – that, or you really, really enjoy the dark corners of the internet! Welcome, make yourself at home.",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Ordinal Logistic Regression</span>"
    ]
  },
  {
    "objectID": "lessons/05_glm_ordinal_logistic.html#mathematical-definition-of-method",
    "href": "lessons/05_glm_ordinal_logistic.html#mathematical-definition-of-method",
    "title": "\n32  Ordinal Logistic Regression\n",
    "section": "\n32.2 Mathematical Definition of Method",
    "text": "32.2 Mathematical Definition of Method\nNow, let’s get down to business. Proportional Odds Logistic Regression (POLR), which is also known as Ordinal Logistic Regression, is a method of statistical analysis which may be used to model the relationship between an ordinal response, and one or more independent (or explanatory) variables. These explanatory variables may be categorical or continuous.\nLet’s establish some context: Have you ever been filling out a survey and it’s asked you how much you agree with something, how often you do something, or how important something is to you? While this is not an extensive list, these Likert-type responses should give you a general idea of common ordinal responses you can expect to encounter when using POLR.\nSome more specific examples where Ordinal Logistic Regression can be applied are:\n\nLevel of agreement: In a survey the responses to the outcome variable is categorized in multiple levels such as, Strongly Disagree, Disagree, Agree, Strongly Agree.\nSatisfaction level: Measuring satisfaction level of a service on a scale like, “very dissatisfied,” “dissatisfied,” “neutral,” “satisfied,” and “very satisfied.”\nPain Intensity: Patients participating in medical research may be asked to rate the intensity of their pain on a scale ranging from “no pain” to “mild pain,” “moderate pain,” and “severe pain.”",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Ordinal Logistic Regression</span>"
    ]
  },
  {
    "objectID": "lessons/05_glm_ordinal_logistic.html#mathematical-formulation-of-an-ordinal-model",
    "href": "lessons/05_glm_ordinal_logistic.html#mathematical-formulation-of-an-ordinal-model",
    "title": "\n32  Ordinal Logistic Regression\n",
    "section": "\n32.3 Mathematical Formulation of an Ordinal Model",
    "text": "32.3 Mathematical Formulation of an Ordinal Model\nWell, you’ve probably reached the point where you’re about to check out because we see formulas, lots of them. This is understandable, but be not afraid! UCLA’s Advanced Research Computing Center is here to help in ways I cannot.\nTo start… let’s establish some notation and review the concepts involved in ordinal logistic regression. Let 𝑌 be an ordinal outcome with 𝐽 categories. Then 𝑃(𝑌≤𝑗) is the cumulative probability of 𝑌 less than or equal to a specific category 𝑗= 1,⋯,𝐽−1. Note that 𝑃(𝑌≤𝐽)=1. The odds of being less than or equal a particular category can be defined as\n\\[ \\frac{P(Y \\le j)}{P(Y&gt;j)} \\]\nFor 𝑗=1,⋯,𝐽−1 since 𝑃(𝑌&gt;𝐽)=0 and dividing by zero is undefined. Alternatively, you can write 𝑃(𝑌&gt;𝑗)=1–𝑃(𝑌≤𝑗). The log odds is also known as the logit, so that\n\\[ log \\frac{P(Y \\le j)}{P(Y&gt;j)} = logit (P(Y \\le j)) \\]\nThe proportional odds logistic regression model can be defined as\n\\[logit (P(Y \\le j)) = \\beta_{j0} – \\eta_{1}x_1 – \\cdots – \\eta_{p} x_p \\]\nI think that’s enough formulas for now. Let’s keep the party going by actually putting the pedal to the metal, or should we say, aPpLyiNg wHaT wE LeArNeD.",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Ordinal Logistic Regression</span>"
    ]
  },
  {
    "objectID": "lessons/05_glm_ordinal_logistic.html#data-source-and-description",
    "href": "lessons/05_glm_ordinal_logistic.html#data-source-and-description",
    "title": "\n32  Ordinal Logistic Regression\n",
    "section": "\n32.4 Data Source and Description",
    "text": "32.4 Data Source and Description\nThe data we will be using is the Behavioral Risk Factor Surveillance System (BRFSS) data from the year 2013. Why are we using this dataset? I thought you’d never ask! The BRFSS is the actually nation’s premier system of health-related telephone surveys that collect state data about U.S. residents regarding their health-related risk behaviors, chronic health conditions, and use of preventive services. Established in 1984 with 15 states, BRFSS now collects data in all 50 states as well as the District of Columbia and three U.S. territories.\nBRFSS completes more than 400,000 adult interviews each year, making it the largest continuously conducted health survey system in the world. Impressive right? More information can be found here. The codebook for the 2013 data can be found here.\nIn this data, there is a variable called “General Health,” which has posed the following questions to participants: “Would you say that in general your health is…” — 1) Excellent 2) Very good 3) Good 4) Fair or 5) Poor. This is a great example of Likert scale responses mentioned earlier.",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Ordinal Logistic Regression</span>"
    ]
  },
  {
    "objectID": "lessons/05_glm_ordinal_logistic.html#cleaning-the-data-to-create-a-model-frame",
    "href": "lessons/05_glm_ordinal_logistic.html#cleaning-the-data-to-create-a-model-frame",
    "title": "\n32  Ordinal Logistic Regression\n",
    "section": "\n32.5 Cleaning the Data to Create a Model Frame",
    "text": "32.5 Cleaning the Data to Create a Model Frame\n\nBRFSS_df1 &lt;- read_csv(\"../data/05_brfss_subset.csv\")\n\nAll things considered, the original is a somewhat large dataset, with 330 variables and having taken Microsoft Excel 2013 about three minutes to load the original csv. Patience is key!\nFor this example, we are interested in the following variables:\nIndependent Variables:\n\nParticipant ID\nSex\nRace\nMarital Status\nHighest Level of Schooling Completed\n\nDependent Variable:\n\nGeneral Health\n\nThe research question is straightforward: we want to know to what extent certain sociodemographic characteristics are associated with better self-reported health.\n\nBRFSS_df2 &lt;- \n  BRFSS_df1 %&gt;%\n  as_tibble() %&gt;% \n  drop_na(genhlth) %&gt;% \n  mutate(\n    Health = factor(genhlth, ordered = TRUE),\n    Sex = factor(\n      sex,\n      levels = c(\"Male\", \"Female\"),\n      labels = c(\"Male\", \"Female\")\n    )\n  ) %&gt;%\n  mutate(Race = as_factor(X_racegr3)) %&gt;% \n  mutate(MaritalStatus = as_factor(marital)) %&gt;% \n  mutate(\n    Education = factor(\n      educa,\n      levels = c(\n        \"Never attended school or only kindergarten\", \n        \"Grades 1 through 8 (Elementary)\", \n        \"Grades 9 though 11 (Some high school)\", \n        \"Grade 12 or GED (High school graduate)\", \n        \"College 1 year to 3 years (Some college or technical school)\",\n        \"College 4 years or more (College graduate)\"\n      ),\n      labels = c(\n        \"Never attended school or only kindergarten\", \n        \"Grades 1 through 8 (Elementary)\", \n        \"Grades 9 through 11 (Some high school)\", \n        \"Grade 12 or GED (High school graduate)\", \n        \"College 1 year to 3 years (Some college or technical school)\",\n        \"College 4 years or more (College graduate)\"\n      ),\n      ordered = TRUE\n    )\n  ) %&gt;% \n  mutate(Employment = as_factor(employ1)) %&gt;% \n  mutate(\n    Income = factor(\n    income2,\n    levels = c(\"Less than $10,000\",\n               \"Less than $15,000\",\n               \"Less than $20,000\", \n               \"Less than $25,000\", \n               \"Less than $35,000\", \n               \"Less than $50,000\",\n               \"Less than $75,000\",\n               \"$75,000 or more\"),\n    labels = c(\"Less than $10,000\",\n               \"Less than $15,000\",\n               \"Less than $20,000\", \n               \"Less than $25,000\", \n               \"Less than $35,000\", \n               \"Less than $50,000\",\n               \"Less than $75,000\",\n               \"$75,000 or more\"),\n    ordered = TRUE\n    )\n  ) %&gt;%\n  select(\n    \"Health\",\"Sex\", \"MaritalStatus\", \"Education\", \"Income\", \"Employment\", \"Race\"\n  )",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Ordinal Logistic Regression</span>"
    ]
  },
  {
    "objectID": "lessons/05_glm_ordinal_logistic.html#assumptions-of-the-method",
    "href": "lessons/05_glm_ordinal_logistic.html#assumptions-of-the-method",
    "title": "\n32  Ordinal Logistic Regression\n",
    "section": "\n32.6 Assumptions of the Method",
    "text": "32.6 Assumptions of the Method\nThe key assumptions of Ordinal logistic Regression which ensures the validity of the model are as follows:\n\nThe outcome variable is ordered.\nThe predictor variables are either continuous, categorical, or ordinal.\nThere is no multicollinearity among the predictors.\nProportional odds.\n\nIn proportional odds logistic regression, the assumption is that the odds of being at or below any particular level of the ordinal dependent variable relative to being above that level is constant across levels of the independent variables. In other words, the relationship between the independent variables and the dependent variable is assumed to be proportional across all levels of the dependent variable. The model estimates coefficients for each independent variable, indicating how they influence the odds of being in a lower category versus a higher category of the ordinal dependent variable. The interpretation of these coefficients is similar to that in binary logistic regression, but applies to each threshold between categories of the ordinal dependent variable.\nOne of the assumptions underlying ordinal logistic (and ordinal probit) regression is that the relationship between each pair of outcome groups is the same. In other words, ordinal logistic regression assumes that the coefficients that describe the relationship between, say, the lowest versus all higher categories of the response variable are the same as those that describe the relationship between the next lowest category and all higher categories, etc. This is called the proportional odds assumption or the parallel regression assumption. Because the relationship between all pairs of groups is the same, there is only one set of coefficients.",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Ordinal Logistic Regression</span>"
    ]
  },
  {
    "objectID": "lessons/05_glm_ordinal_logistic.html#data-exploration",
    "href": "lessons/05_glm_ordinal_logistic.html#data-exploration",
    "title": "\n32  Ordinal Logistic Regression\n",
    "section": "\n32.7 Data Exploration",
    "text": "32.7 Data Exploration\nLets take a look at the data we are working with.\n\n# Skim Data\nskim(BRFSS_df2)\n\n\nData summary\n\n\nName\nBRFSS_df2\n\n\nNumber of rows\n489790\n\n\nNumber of columns\n7\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n7\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nHealth\n0\n1.00\nTRUE\n5\nVer: 159076, Goo: 150555, Exc: 85482, Fai: 66726\n\n\nSex\n2\n1.00\nFALSE\n2\nFem: 289319, Mal: 200469\n\n\nMaritalStatus\n3356\n0.99\nFALSE\n6\nMar: 252510, Nev: 74828, Div: 70053, Wid: 65322\n\n\nEducation\n2228\n1.00\nTRUE\n6\nCol: 169667, Gra: 142371, Col: 133740, Gra: 27905\n\n\nIncome\n70870\n0.86\nTRUE\n8\n$75: 115659, Les: 65102, Les: 61319, Les: 48687\n\n\nEmployment\n3337\n0.99\nFALSE\n8\nEmp: 201700, Ret: 137566, Sel: 39693, Una: 37135\n\n\nRace\n8472\n0.98\nFALSE\n5\nWhi: 374955, Bla: 39121, His: 36823, Oth: 21337\n\n\n\n\nBRFSS_df2 %&gt;% \n  tbl_summary(by = Health) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nExcellent, N = 85,4821\n\n\nFair, N = 66,7261\n\n\nGood, N = 150,5551\n\n\nPoor, N = 27,9511\n\n\nVery good, N = 159,0761\n\n\n\n\nSex\n\n\n\n\n\n\n\n    Male\n35,741 (42%)\n25,882 (39%)\n62,998 (42%)\n10,713 (38%)\n65,135 (41%)\n\n\n    Female\n49,740 (58%)\n40,844 (61%)\n87,557 (58%)\n17,238 (62%)\n93,940 (59%)\n\n\n    Unknown\n1\n0\n0\n0\n1\n\n\nMaritalStatus\n\n\n\n\n\n\n\n    Divorced\n10,049 (12%)\n12,402 (19%)\n21,736 (15%)\n6,528 (24%)\n19,338 (12%)\n\n\n    Married\n49,682 (59%)\n27,645 (42%)\n74,911 (50%)\n10,098 (36%)\n90,174 (57%)\n\n\n    Widowed\n7,007 (8.3%)\n12,744 (19%)\n21,933 (15%)\n6,148 (22%)\n17,490 (11%)\n\n\n    Never married\n14,419 (17%)\n9,295 (14%)\n23,398 (16%)\n3,179 (11%)\n24,537 (16%)\n\n\n    Separated\n1,221 (1.4%)\n2,438 (3.7%)\n3,366 (2.3%)\n1,287 (4.6%)\n2,280 (1.4%)\n\n\n    A member of an unmarried couple\n2,364 (2.8%)\n1,807 (2.7%)\n4,177 (2.8%)\n529 (1.9%)\n4,252 (2.7%)\n\n\n    Unknown\n740\n395\n1,034\n182\n1,005\n\n\nEducation\n\n\n\n\n\n\n\n    Never attended school or only kindergarten\n56 (&lt;0.1%)\n199 (0.3%)\n201 (0.1%)\n118 (0.4%)\n82 (&lt;0.1%)\n\n\n    Grades 1 through 8 (Elementary)\n900 (1.1%)\n4,383 (6.6%)\n3,972 (2.7%)\n2,516 (9.1%)\n1,452 (0.9%)\n\n\n    Grades 9 through 11 (Some high school)\n2,281 (2.7%)\n7,555 (11%)\n9,298 (6.2%)\n4,065 (15%)\n4,706 (3.0%)\n\n\n    Grade 12 or GED (High school graduate)\n17,552 (21%)\n24,614 (37%)\n50,213 (34%)\n10,175 (37%)\n39,817 (25%)\n\n\n    College 1 year to 3 years (Some college or technical school)\n21,347 (25%)\n17,642 (27%)\n42,828 (29%)\n7,061 (25%)\n44,862 (28%)\n\n\n    College 4 years or more (College graduate)\n42,912 (50%)\n11,997 (18%)\n43,324 (29%)\n3,853 (14%)\n67,581 (43%)\n\n\n    Unknown\n434\n336\n719\n163\n576\n\n\nIncome\n\n\n\n\n\n\n\n    Less than $10,000\n2,319 (3.1%)\n6,929 (12%)\n7,515 (5.9%)\n4,513 (20%)\n3,976 (2.9%)\n\n\n    Less than $15,000\n2,128 (2.9%)\n7,187 (13%)\n8,440 (6.6%)\n4,311 (19%)\n4,567 (3.3%)\n\n\n    Less than $20,000\n3,551 (4.8%)\n8,053 (14%)\n11,948 (9.4%)\n3,674 (16%)\n7,479 (5.4%)\n\n\n    Less than $25,000\n4,719 (6.4%)\n7,845 (14%)\n14,931 (12%)\n3,268 (14%)\n10,800 (7.8%)\n\n\n    Less than $35,000\n6,521 (8.8%)\n7,564 (14%)\n17,287 (14%)\n2,575 (11%)\n14,740 (11%)\n\n\n    Less than $50,000\n9,972 (13%)\n7,153 (13%)\n20,468 (16%)\n2,058 (8.9%)\n21,668 (16%)\n\n\n    Less than $75,000\n12,701 (17%)\n5,226 (9.4%)\n19,710 (15%)\n1,421 (6.1%)\n26,044 (19%)\n\n\n    $75,000 or more\n32,343 (44%)\n5,645 (10%)\n27,402 (21%)\n1,296 (5.6%)\n48,973 (35%)\n\n\n    Unknown\n11,228\n11,124\n22,854\n4,835\n20,829\n\n\nEmployment\n\n\n\n\n\n\n\n    Retired\n17,110 (20%)\n23,602 (36%)\n46,701 (31%)\n8,937 (32%)\n41,216 (26%)\n\n\n    Employed for wages\n43,990 (52%)\n15,825 (24%)\n60,875 (41%)\n2,476 (8.9%)\n78,534 (50%)\n\n\n    A homemaker\n5,663 (6.7%)\n4,811 (7.3%)\n9,937 (6.7%)\n1,318 (4.8%)\n9,786 (6.2%)\n\n\n    Out of work for less than 1 year\n1,971 (2.3%)\n1,859 (2.8%)\n4,263 (2.9%)\n501 (1.8%)\n3,588 (2.3%)\n\n\n    Unable to work\n955 (1.1%)\n13,050 (20%)\n8,090 (5.4%)\n12,510 (45%)\n2,530 (1.6%)\n\n\n    Self-employed\n10,232 (12%)\n3,198 (4.8%)\n11,283 (7.6%)\n709 (2.6%)\n14,271 (9.0%)\n\n\n    Out of work for 1 year or more\n1,636 (1.9%)\n3,006 (4.5%)\n4,862 (3.3%)\n1,172 (4.2%)\n3,332 (2.1%)\n\n\n    A student\n3,291 (3.9%)\n861 (1.3%)\n3,400 (2.3%)\n124 (0.4%)\n4,978 (3.1%)\n\n\n    Unknown\n634\n514\n1,144\n204\n841\n\n\nRace\n\n\n\n\n\n\n\n    Black only, Non-Hispanic\n5,089 (6.1%)\n7,830 (12%)\n13,843 (9.4%)\n2,865 (10%)\n9,494 (6.1%)\n\n\n    White only, Non-Hispanic\n68,211 (81%)\n44,948 (69%)\n110,669 (75%)\n19,972 (73%)\n131,155 (84%)\n\n\n    Other race only, Non-Hispanic\n3,804 (4.5%)\n2,843 (4.3%)\n7,475 (5.1%)\n1,295 (4.7%)\n5,920 (3.8%)\n\n\n    Hispanic\n5,496 (6.5%)\n8,377 (13%)\n12,778 (8.6%)\n2,407 (8.8%)\n7,765 (5.0%)\n\n\n    Multiracial, Non-Hispanic\n1,377 (1.6%)\n1,486 (2.3%)\n3,061 (2.1%)\n770 (2.8%)\n2,388 (1.5%)\n\n\n    Unknown\n1,505\n1,242\n2,729\n642\n2,354\n\n\n\n\n1 n (%)",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Ordinal Logistic Regression</span>"
    ]
  },
  {
    "objectID": "lessons/05_glm_ordinal_logistic.html#plotting-outcome-variable",
    "href": "lessons/05_glm_ordinal_logistic.html#plotting-outcome-variable",
    "title": "\n32  Ordinal Logistic Regression\n",
    "section": "\n32.8 Plotting Outcome Variable",
    "text": "32.8 Plotting Outcome Variable\n\nBRFSS_df2 %&gt;% \n  count(Health) %&gt;% \n  mutate(prop = n / sum(n)) %&gt;% \n  rename(Health = Health) %&gt;% \n  ggplot() + \n  aes(x = Health, y = prop) +\n  labs(\n    x = \"Self Reported Health\", \n    y = \"Relative Frequencies (w Obs. Count)\"\n  ) +\n  scale_y_continuous(labels = scales::percent) +\n  geom_col() +\n  geom_text(aes(label = n), vjust = 1.5, color = \"white\")\n\n\n\n\n\n\n# Reorder due to outcome variable being ordered\nBRFSS_df2 %&gt;%\n  ggplot(aes(x = fct_relevel(Health, c(\"Poor\", \"Fair\", \"Good\", \"Very good\", \"Excellent\"))))+\n  geom_bar() +\n  labs(x = \"Health\")\n\n\n\n\n\n\n\n\n# To plot this dataset, take a small sample (2000), due to it having a very large sample size\nGGally::ggpairs(BRFSS_df2[sample(1:489790, size = 2000),])\n\n\n\n\n\n\n\n\nBRFSS_df3 &lt;- \n  BRFSS_df2 %&gt;% \n  select(-Income, -Employment) %&gt;% \n  mutate(\n    EducationLevel = case_when(\n      Education == \"Never attended school or only kindergarten\" ~ 1,\n      Education == \"Grades 1 through 8 (Elementary)\" ~ 1,\n      Education == \"Grades 9 through 11 (Some high school)\" ~ 1,\n      Education == \"Grade 12 or GED (High school graduate)\" ~ 2,\n      Education == \"College 1 year to 3 years (Some college or technical school)\" ~ 3,\n      Education == \"College 4 years or more (College graduate)\" ~ 4\n    )\n  ) %&gt;% \n  mutate(\n    EducationLevel = factor(\n      EducationLevel, \n      levels = 1:4, \n      labels = c(\n        \"Less than HS\", \"HS/GED\", \"Some College\", \"College Grad\"\n      ), \n      ordered = TRUE\n    )\n  ) %&gt;% \n  select(-Education)\n\nAt this point, we ought to consider the relationship between education, income, and employment, and whether or not it is best to choose one of these variables as a predictor, as opposed to all three. Considering a majority of the literature highlights education as one of the most important predictors of health, we will move forward with education only (out of the three aforementioned variables).",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Ordinal Logistic Regression</span>"
    ]
  },
  {
    "objectID": "lessons/05_glm_ordinal_logistic.html#fitting-the-model",
    "href": "lessons/05_glm_ordinal_logistic.html#fitting-the-model",
    "title": "\n32  Ordinal Logistic Regression\n",
    "section": "\n32.9 Fitting the Model",
    "text": "32.9 Fitting the Model\nUsing the polr function, we can fit the Proportional Odds Logistic Regression model to the data. By default, the Hess option is turned off, but you can turn it on if you’d like to be able to calculate the odds ratios later.\n\nolr_fit &lt;- MASS::polr(Health ~ ., data = BRFSS_df3, Hess = TRUE)\n\nThe output is a bit messy, so it is wise to clean it up using the pander() function (from the pander:: and knitr:: packages).\n\npander(summary(olr_fit))\n\nCall: MASS::polr(formula = Health ~ ., data = BRFSS_df3, Hess = TRUE)\n\nCoeficients\n\n\n\n\n\n\n\n \nValue\nStd. Error\nt value\n\n\n\nSexFemale\n0.01283\n0.005411\n2.372\n\n\nMaritalStatusMarried\n0.08766\n0.007668\n11.43\n\n\nMaritalStatusWidowed\n0.04205\n0.009621\n4.37\n\n\nMaritalStatusNever married\n0.04758\n0.009511\n5.003\n\n\nMaritalStatusSeparated\n-0.01979\n0.01806\n-1.096\n\n\nMaritalStatusA member of an unmarried couple\n0.07227\n0.01727\n4.186\n\n\nRaceWhite only, Non-Hispanic\n0.1817\n0.009417\n19.29\n\n\nRaceOther race only, Non-Hispanic\n0.01742\n0.01504\n1.158\n\n\nRaceHispanic\n-0.1347\n0.01267\n-10.63\n\n\nRaceMultiracial, Non-Hispanic\n0.03694\n0.02036\n1.814\n\n\nEducationLevel.L\n0.1511\n0.006744\n22.4\n\n\nEducationLevel.Q\n-0.09438\n0.005861\n-16.1\n\n\nEducationLevel.C\n-0.005021\n0.005036\n-0.997\n\n\n\n\nIntercepts\n\n\n\n\n\n\n\n \nValue\nStd. Error\nt value\n\n\n\nExcellent|Fair\n-1.338\n0.0116\n-115.4\n\n\nFair|Good\n-0.5809\n0.01139\n-51\n\n\nGood|Poor\n0.7042\n0.01142\n61.66\n\n\nPoor|Very good\n0.9546\n0.01147\n83.23\n\n\n\nResidual Deviance: 1397152.94\nAIC: 1397186.94\n(12464 observations deleted due to missingness)",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Ordinal Logistic Regression</span>"
    ]
  },
  {
    "objectID": "lessons/05_glm_ordinal_logistic.html#interpreting-the-regression-coefficients",
    "href": "lessons/05_glm_ordinal_logistic.html#interpreting-the-regression-coefficients",
    "title": "\n32  Ordinal Logistic Regression\n",
    "section": "\n32.10 Interpreting the Regression Coefficients",
    "text": "32.10 Interpreting the Regression Coefficients\nSex: Females were more likely than males to report higher self-reported health compared to males.\nRace: Those who reported being white were significantly more likely to report better health when compared to black participants. Hispanics were more likely to report worse levels of health than those who were black. Those who reported being multiracial did not have a statistically significant difference from black participants.\nMarital Status: Compared to being divorced, those who reported being married and never having been married had greater levels of self-reported health. There was no statistical difference between those who reported being divorced and those who reported being separated.\nEducation: The most notable statistically significant positive effect for greater self-reported health was present in the category where there was a change between those who fell under the category of having less than a high school education and having completed high school. For every increase in this particular educational category/level, the log odds of reporting a higher level of self-reported health increase by .15.",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Ordinal Logistic Regression</span>"
    ]
  },
  {
    "objectID": "lessons/05_glm_ordinal_logistic.html#generating-p-values",
    "href": "lessons/05_glm_ordinal_logistic.html#generating-p-values",
    "title": "\n32  Ordinal Logistic Regression\n",
    "section": "\n32.11 Generating P Values",
    "text": "32.11 Generating P Values\nOftentimes, not having a p-value makes us itch with discomfort! One way to calculate a p-value here is by comparing the t-value against the standard normal distribution, like a z test. Note: this is only true with infinite degrees of freedom, but is reasonably approximated by large samples, becoming increasingly biased as sample size decreases.\nIt is an easy and straightforward process: store the coefficient table, then calculate the p-values and combine back with the table.\n\n(ctable &lt;- coef(summary(olr_fit)))\n\n                                                    Value  Std. Error\nSexFemale                                     0.012833165 0.005410530\nMaritalStatusMarried                          0.087662642 0.007667744\nMaritalStatusWidowed                          0.042046687 0.009621344\nMaritalStatusNever married                    0.047583341 0.009511314\nMaritalStatusSeparated                       -0.019790594 0.018055829\nMaritalStatusA member of an unmarried couple  0.072266730 0.017265077\nRaceWhite only, Non-Hispanic                  0.181677664 0.009416588\nRaceOther race only, Non-Hispanic             0.017416505 0.015044601\nRaceHispanic                                 -0.134730457 0.012668599\nRaceMultiracial, Non-Hispanic                 0.036936068 0.020360856\nEducationLevel.L                              0.151050979 0.006743830\nEducationLevel.Q                             -0.094380889 0.005861344\nEducationLevel.C                             -0.005021198 0.005036294\nExcellent|Fair                               -1.338156462 0.011597785\nFair|Good                                    -0.580928226 0.011391590\nGood|Poor                                     0.704240760 0.011420730\nPoor|Very good                                0.954568453 0.011469520\n                                                  t value\nSexFemale                                       2.3718869\nMaritalStatusMarried                           11.4326505\nMaritalStatusWidowed                            4.3701469\nMaritalStatusNever married                      5.0028145\nMaritalStatusSeparated                         -1.0960778\nMaritalStatusA member of an unmarried couple    4.1857171\nRaceWhite only, Non-Hispanic                   19.2933651\nRaceOther race only, Non-Hispanic               1.1576581\nRaceHispanic                                  -10.6349929\nRaceMultiracial, Non-Hispanic                   1.8140725\nEducationLevel.L                               22.3983971\nEducationLevel.Q                              -16.1022602\nEducationLevel.C                               -0.9970026\nExcellent|Fair                               -115.3803424\nFair|Good                                     -50.9962367\nGood|Poor                                      61.6633742\nPoor|Very good                                 83.2265402\n\np &lt;- pnorm(abs(ctable[, \"t value\"]), lower.tail = FALSE) * 2\neBeta &lt;- exp(ctable[, \"Value\"])\n\n# Combine Table\nctable &lt;- cbind(ctable, \"expBeta\" = eBeta)\n(ctable &lt;- cbind(ctable, \"p value\" = p))\n\n                                                    Value  Std. Error\nSexFemale                                     0.012833165 0.005410530\nMaritalStatusMarried                          0.087662642 0.007667744\nMaritalStatusWidowed                          0.042046687 0.009621344\nMaritalStatusNever married                    0.047583341 0.009511314\nMaritalStatusSeparated                       -0.019790594 0.018055829\nMaritalStatusA member of an unmarried couple  0.072266730 0.017265077\nRaceWhite only, Non-Hispanic                  0.181677664 0.009416588\nRaceOther race only, Non-Hispanic             0.017416505 0.015044601\nRaceHispanic                                 -0.134730457 0.012668599\nRaceMultiracial, Non-Hispanic                 0.036936068 0.020360856\nEducationLevel.L                              0.151050979 0.006743830\nEducationLevel.Q                             -0.094380889 0.005861344\nEducationLevel.C                             -0.005021198 0.005036294\nExcellent|Fair                               -1.338156462 0.011597785\nFair|Good                                    -0.580928226 0.011391590\nGood|Poor                                     0.704240760 0.011420730\nPoor|Very good                                0.954568453 0.011469520\n                                                  t value   expBeta\nSexFemale                                       2.3718869 1.0129159\nMaritalStatusMarried                           11.4326505 1.0916198\nMaritalStatusWidowed                            4.3701469 1.0429432\nMaritalStatusNever married                      5.0028145 1.0487336\nMaritalStatusSeparated                         -1.0960778 0.9804040\nMaritalStatusA member of an unmarried couple    4.1857171 1.0749420\nRaceWhite only, Non-Hispanic                   19.2933651 1.1992276\nRaceOther race only, Non-Hispanic               1.1576581 1.0175691\nRaceHispanic                                  -10.6349929 0.8739514\nRaceMultiracial, Non-Hispanic                   1.8140725 1.0376267\nEducationLevel.L                               22.3983971 1.1630559\nEducationLevel.Q                              -16.1022602 0.9099361\nEducationLevel.C                               -0.9970026 0.9949914\nExcellent|Fair                               -115.3803424 0.2623288\nFair|Good                                     -50.9962367 0.5593789\nGood|Poor                                      61.6633742 2.0223107\nPoor|Very good                                 83.2265402 2.5975494\n                                                   p value\nSexFemale                                     1.769751e-02\nMaritalStatusMarried                          2.872062e-30\nMaritalStatusWidowed                          1.241630e-05\nMaritalStatusNever married                    5.649930e-07\nMaritalStatusSeparated                        2.730447e-01\nMaritalStatusA member of an unmarried couple  2.842672e-05\nRaceWhite only, Non-Hispanic                  6.106620e-83\nRaceOther race only, Non-Hispanic             2.470036e-01\nRaceHispanic                                  2.048444e-26\nRaceMultiracial, Non-Hispanic                 6.966656e-02\nEducationLevel.L                             4.079955e-111\nEducationLevel.Q                              2.459537e-58\nEducationLevel.C                              3.187632e-01\nExcellent|Fair                                0.000000e+00\nFair|Good                                     0.000000e+00\nGood|Poor                                     0.000000e+00\nPoor|Very good                                0.000000e+00",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Ordinal Logistic Regression</span>"
    ]
  },
  {
    "objectID": "lessons/05_glm_ordinal_logistic.html#conclusion",
    "href": "lessons/05_glm_ordinal_logistic.html#conclusion",
    "title": "\n32  Ordinal Logistic Regression\n",
    "section": "\n32.12 Conclusion",
    "text": "32.12 Conclusion\nAfter running a statistical analysis consisting of an Ordinal Logistic Regression for a dataset including an ordered outcome variable of self-reported health, we are able to see that being female, white, and being married all indicated a greater likelihood of reporting higher levels of self-reported health. Additionally, certain changes in categories of education indicated greater odds of reporting greater self-reported health.",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Ordinal Logistic Regression</span>"
    ]
  },
  {
    "objectID": "lessons/05_glm_poisson.html",
    "href": "lessons/05_glm_poisson.html",
    "title": "33  Poisson Regression Model",
    "section": "",
    "text": "33.1 Libraries for this lesson\n# Installing Required Packages\n# intsall.packages(readxl)\n# install.packages(skimr)\n# intsall.packages(gtsummary)\n# intsall.packages(GGally)\n# install.packages(epiDisplay)\n# install.packages(broom)\n# install.packages(tidyverse)\n\n# Loading Required Packages\nlibrary(readxl)\nlibrary(skimr)\nlibrary(gtsummary)\nlibrary(GGally)\nlibrary(epiDisplay)\nlibrary(broom)\nlibrary(tidyverse)",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Poisson Regression Model</span>"
    ]
  },
  {
    "objectID": "lessons/05_glm_poisson.html#introduction-to-poisson-regression-model",
    "href": "lessons/05_glm_poisson.html#introduction-to-poisson-regression-model",
    "title": "33  Poisson Regression Model",
    "section": "33.2 Introduction to Poisson Regression Model",
    "text": "33.2 Introduction to Poisson Regression Model\nThe Poisson regression model (PRM) is an appropriate model for studying counts response variable, which follows the Poisson distribution. Thus, the values of the response variable are non-negative integers. It is a type of Generalized linear models (GLM) whenever the outcome is count. It also accommodates rate data as we will see shortly. Although count and rate data are very common in medical and health sciences. For instance, how the colony counts of bacteria are associated with different environmental conditions and dilutions. Another example related to vital statistics, which is related to infant mortality or cancer incidence among groups with different demographics. In such scenarios, the benchmark model PRM is more appropriate than the linear regression model (LRM).\nBasically, Poisson regression models the linear relationship between:\n\noutcome: count variable (e.g. the number of hospital admissions, parity, cancerous lesions, asthmatic attacks). This is transformed into the natural log scale.\npredictors/independent variables: numerical variables (e.g. age, blood pressure, income) and categorical variables (e.g. gender, race, education level).\n\nFor example, we might be interested in knowing the relationship between the number of asthmatic attacks in the past one year with sociodemographic factors. This relationship can be explored by a Poisson regression analysis.\nWe know that logistic regression allows us to obtain the odds ratio, which is approximately the relative risk given a predictor. For Poisson regression, by taking the exponent of the coefficient, we obtain the rate ratio RR (also known as incidence rate ratio IRR),\n\\[\nRR = \\exp(b_p)\n\\]\nfor the coefficient \\(b_p\\) of the p’s predictor. This is interpreted in similar way to the odds ratio for logistic regression, which is approximately the relative risk given a predictor.",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Poisson Regression Model</span>"
    ]
  },
  {
    "objectID": "lessons/05_glm_poisson.html#mathematical-formulation-of-the-model",
    "href": "lessons/05_glm_poisson.html#mathematical-formulation-of-the-model",
    "title": "33  Poisson Regression Model",
    "section": "33.3 Mathematical Formulation of the Model",
    "text": "33.3 Mathematical Formulation of the Model\nThe Poisson distribution for a random variable Y has the following probability mass function for a given value Y = y \\[\nP(Y|y=\\lambda) = \\frac{e^{-\\lambda}\\lambda^y}{y!},\n\\] for \\(y = 0, 1, 2, \\dots\\). The Poisson distribution is characterized by the single parameter \\(\\lambda\\), which is the mean rate of occurrence for the event being measured. For the Poisson distribution, it is assumed that large counts (with respect to the value of \\(\\lambda\\)) are rare. And the rate \\(\\lambda\\) is determined by a set of \\(k\\) predictors \\(X = (X_1,\\dots, X_k)\\). The expression relating these quantities is \\[\n\\lambda = \\exp(\\beta)\n\\]\nThus, for observation i the simple model model for Poisson rate parameter \\(\\lambda_i\\) is given by\n\\[\n\\log \\lambda_i = \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip}\n\\]\nor equivalently\n\\[\n\\lambda_i = e^{\\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip}}\n\\]\nTogether with the distributional assumption \\(Y_i ∼ Poisson(\\lambda_i)\\), this is also called the Poisson log-linear model,\nIn Generalized Linear Model, response variable usually originates in the form of pdf which generally fits to the exponential family (EF) of distribution. Since Poisson distribution is the member of EF. So, we define the density function of EF as\n\\[\nf(y_i, \\theta_i, \\lambda) =\n  \\exp\\left[\\frac{y_i\\theta_i - b(\\theta_i)}{\\alpha(\\lambda)} + c(y_i, \\lambda)\\right], i = 1, 2, \\dots,n\n\\] Here,\n\n\\(\\theta_i\\) represents the link function\n\\(b(\\theta_i)\\) is the cumulant\n\\(\\alpha(\\lambda)\\) is the dispersion parameter\n\\(c(y_i, \\lambda)\\) is the normalization term\n\nSince the value of dispersion parameter for the Poisson distribution is one. The PRM is generally applied in the situations, when the response \\(y_i\\) is in the form of counts, that is, \\(y_i = 0, 1, 2, \\dots\\) and distributed as \\(P(\\mu_i)\\), where \\(\\mu_i = exp ( x^T_i \\beta)\\) and \\(log (\\mu_i) = x^T_i \\beta\\), while \\(x_i\\) denotes the \\(i\\)th row of design matrix \\(X\\) having order \\(n \\times r\\) while \\(\\beta\\) is a coefficient vector of order \\(r \\times 1\\), where \\(r = p + 1\\) are the explanatory variables.",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Poisson Regression Model</span>"
    ]
  },
  {
    "objectID": "lessons/05_glm_poisson.html#example",
    "href": "lessons/05_glm_poisson.html#example",
    "title": "33  Poisson Regression Model",
    "section": "33.4 Example",
    "text": "33.4 Example\nTo demonstrate the method of PRM, we consider the asthma attack data set(https://github.com/drkamarul/multivar_data_analysis/tree/main/data). The data on the number of asthmatic attacks per year among a sample of 120 patients and the associated factors are given in 05_asthma.csv.\nThe dataset contains four variables:\n\ngender: Gender of the subjects (categorical) {male, female}.\nres_inf: Recurrent respiratory infection (categorical) {no, yes}.\nghq12: General Health Questionnare 12 (GHQ-12) score of psychological well being (numerical) {0 to 36}.\nattack: Number of athmatic attack per year (count).\n\n\n33.4.1 Exploring the data\nLet’s begin by looking at the data.\n\nasthma &lt;- \n  read_csv(\"../data/05_asthma.csv\")\n\nRows: 120 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): gender, res_inf\ndbl (2): ghq12, attack\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nasthma %&gt;%\n  head(n = 10)\n\n# A tibble: 10 × 4\n   gender res_inf ghq12 attack\n   &lt;chr&gt;  &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1 female yes        21      6\n 2 male   no         17      4\n 3 male   yes        30      8\n 4 female yes        22      5\n 5 male   yes        27      2\n 6 male   yes        33      3\n 7 female yes        24      2\n 8 female yes        23      1\n 9 female yes        25      2\n10 male   no         28      2\n\n\n\n33.4.1.1 Structure of the dataset\n\nstr(asthma)\n\nspc_tbl_ [120 × 4] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ gender : chr [1:120] \"female\" \"male\" \"male\" \"female\" ...\n $ res_inf: chr [1:120] \"yes\" \"no\" \"yes\" \"yes\" ...\n $ ghq12  : num [1:120] 21 17 30 22 27 33 24 23 25 28 ...\n $ attack : num [1:120] 6 4 8 5 2 3 2 1 2 2 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   gender = col_character(),\n  ..   res_inf = col_character(),\n  ..   ghq12 = col_double(),\n  ..   attack = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\n\n\n33.4.1.2 Summary\n\nskim(asthma)\n\n\nData summary\n\n\nName\nasthma\n\n\nNumber of rows\n120\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ngender\n0\n1\n4\n6\n0\n2\n0\n\n\nres_inf\n0\n1\n2\n3\n0\n2\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nghq12\n0\n1\n16.34\n9.81\n0\n7\n19\n25\n33\n▆▅▃▇▅\n\n\nattack\n0\n1\n2.46\n2.01\n0\n1\n2\n4\n9\n▇▇▅▁▁\n\n\n\n\n\n\n\n33.4.1.3 Descriptives\n\nasthma %&gt;%\n  tbl_summary()\n\n\n\nTable 33.1: Asthma Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nN = 1201\n\n\n\n\ngender\n\n\n\n\n    female\n67 (56%)\n\n\n    male\n53 (44%)\n\n\nres_inf\n69 (58%)\n\n\nghq12\n19 (7, 25)\n\n\nattack\n2.00 (1.00, 4.00)\n\n\n\n1 n (%); Median (IQR)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n33.4.1.4 Pairs\n\nggpairs(asthma)\n\n\n\n\n\n\n\n\n\n\n\n33.4.2 Fitting the Poisson Regression Model\nUsing the glm() function to fit a Poisson regression model. The model formula specified attack as the response variable and gender(gender), recurrent respiratory infection (res_inf) and, GHQ12 (ghq12) as predictor variables.\n\n33.4.2.1 Univariate Analysis\n\npois_attack1 &lt;-  glm(attack ~ gender, data = asthma, family = \"poisson\")\nsummary(pois_attack1)\n\n\nCall:\nglm(formula = attack ~ gender, family = \"poisson\", data = asthma)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  1.02105    0.07332  13.925   &lt;2e-16 ***\ngendermale  -0.30000    0.12063  -2.487   0.0129 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 229.56  on 119  degrees of freedom\nResidual deviance: 223.23  on 118  degrees of freedom\nAIC: 500.3\n\nNumber of Fisher Scoring iterations: 5\n\n\n\npois_attack2 &lt;- glm(attack ~ res_inf, data = asthma, family = \"poisson\")\nsummary(pois_attack2)\n\n\nCall:\nglm(formula = attack ~ res_inf, family = \"poisson\", data = asthma)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   0.2877     0.1213   2.372   0.0177 *  \nres_infyes    0.9032     0.1382   6.533 6.44e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 229.56  on 119  degrees of freedom\nResidual deviance: 180.49  on 118  degrees of freedom\nAIC: 457.56\n\nNumber of Fisher Scoring iterations: 5\n\n\n\npois_attack3 &lt;- glm(attack ~ ghq12, data = asthma, family = \"poisson\")\nsummary(pois_attack3)\n\n\nCall:\nglm(formula = attack ~ ghq12, family = \"poisson\", data = asthma)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -0.230923   0.159128  -1.451    0.147    \nghq12        0.059500   0.006919   8.599   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 229.56  on 119  degrees of freedom\nResidual deviance: 145.13  on 118  degrees of freedom\nAIC: 422.2\n\nNumber of Fisher Scoring iterations: 5\n\n\nFrom the outputs, all variables are important with p &lt; .05. These variables are the candidates for inclusion in the multivariable analysis. However, as a reminder, in the context of confirmatory research, the variables that we want to include must consider expert judgement.\n\n\n33.4.2.2 Multivariate Analysis\nFor the multivariable analysis, we included all variables as predictors of attack. Here we use dot . as a shortcut for all variables when specifying the right-hand side of the formula of the glm.\n\npois_attack_all &lt;- glm(attack ~ ., data = asthma, family = \"poisson\")\nsummary(pois_attack_all)\n\n\nCall:\nglm(formula = attack ~ ., family = \"poisson\", data = asthma)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -0.315387   0.183500  -1.719  0.08566 .  \ngendermale  -0.041905   0.122469  -0.342  0.73222    \nres_infyes   0.426431   0.152859   2.790  0.00528 ** \nghq12        0.049508   0.007878   6.285 3.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 229.56  on 119  degrees of freedom\nResidual deviance: 136.68  on 116  degrees of freedom\nAIC: 417.75\n\nNumber of Fisher Scoring iterations: 5\n\n\nFrom the output, we noted that gender is not significant with P &gt; 0.05, although it was significant at the univariable analysis.\nFrom the above pairs graph, we can see that there is a relation ship between varibale gender and ghq12. The left side of the graph shows that high density between 20 and 30 but when change the gender and go to the right side, we can see that there is high density between 0 and 10. So both variables are related here and we can keep only one and R choose the variable ghq12 and remove the gender variable.\nNow, we fit a model excluding gender,\n\npois_attack_reduced &lt;- glm(\n  attack ~ res_inf + ghq12, data = asthma, family = \"poisson\"\n)\nsummary(pois_attack_reduced)\n\n\nCall:\nglm(formula = attack ~ res_inf + ghq12, family = \"poisson\", data = asthma)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -0.34051    0.16823  -2.024  0.04296 *  \nres_infyes   0.42816    0.15282   2.802  0.00508 ** \nghq12        0.04989    0.00779   6.404 1.51e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 229.56  on 119  degrees of freedom\nResidual deviance: 136.80  on 117  degrees of freedom\nAIC: 415.86\n\nNumber of Fisher Scoring iterations: 5\n\n\nFrom the output, both variables are significant predictors of asthmatic attack (or more accurately the natural log of the count of asthmatic attack). This serves as our preliminary model.\n\n\n\n33.4.3 Model Fit Assessment\nFor Poisson regression, we assess the model fit by chi-square goodness-of-fit test, model-to-model AIC comparison and scaled Pearson chi-square statistic. We also assess the regression diagnostics using standardized residuals.\n\n33.4.3.1 Chi-square goodness-of-fit\nChi-square goodness-of-fit test can be performed using poisgof() function in epiDisplay package. Note that, instead of using Pearson chi-square statistic, it utilizes residual deviance with its respective degrees of freedom (df) (e.g. from the output of summary(pois_attack_reduced) above). A p-value &gt; 0.05 indicates good model fit.\n\npoisgof(pois_attack_reduced)\n\n$results\n[1] \"Goodness-of-fit test for Poisson assumption\"\n\n$chisq\n[1] 136.7964\n\n$df\n[1] 117\n\n$p.value\n[1] 0.101934\n\n\n\n\n33.4.3.2 Model-to-model AIC comparison\nWe may also compare the models that we fit so far by Akaike information criterion (AIC). Recall that R uses AIC for stepwise automatic variable selection, which was explained in Linear Regression chapter.\n\nAIC(\n  pois_attack1, pois_attack2, pois_attack3,\n  pois_attack_all, pois_attack_reduced\n)\n\n                    df      AIC\npois_attack1         2 500.3009\npois_attack2         2 457.5555\npois_attack3         2 422.1997\npois_attack_all      4 417.7474\npois_attack_reduced  3 415.8649\n\n\nThe best model is the one with the lowest AIC, which is the model model with the multivariate analysis without gender.\n\n\n\n33.4.4 Interpreting the Results\nAfter all these assumption check points, we decide on the final model and rename the model for easier reference.\n\npois_attack_final &lt;- pois_attack_reduced\n\nWe use tbl_regression() to come up with a table for the results. Here, for interpretation, we exponent the coefficients to obtain the incidence rate ratio, IRR.\n\ntbl_regression(pois_attack_final, exponentiate = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nIRR1\n95% CI1\np-value\n\n\n\n\nres_inf\n\n\n\n\n\n\n\n\n    no\n—\n—\n\n\n\n\n    yes\n1.53\n1.14, 2.08\n0.005\n\n\nghq12\n1.05\n1.04, 1.07\n&lt;0.001\n\n\n\n1 IRR = Incidence Rate Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n\n\nBased on this table, we may interpret the results as follows:\n\nThose with recurrent respiratory infection are at higher risk of having an asthmatic attack with an IRR of 1.53 (95% CI: 1.14, 2.08), while controlling for the effect of GHQ-12 score.\nAn increase in GHQ-12 score by one mark increases the risk of having an asthmatic attack by 1.05 (95% CI: 1.04, 1.07), while controlling for the effect of recurrent respiratory infection.\n\nWe can also view and save the output in a format suitable for exporting to the spreadsheet format for later use. We use tidy() function for the job,\n\ntib_pois_attack &lt;- tidy(\n  pois_attack_final, exponentiate = TRUE, conf.int = TRUE\n)\ntib_pois_attack\n\n# A tibble: 3 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    0.711   0.168       -2.02 4.30e- 2    0.505     0.978\n2 res_infyes     1.53    0.153        2.80 5.08e- 3    1.14      2.08 \n3 ghq12          1.05    0.00779      6.40 1.51e-10    1.04      1.07 \n\n\nThen, we display the coefficients (i.e. without the exponent) and transfer the values into an equation,\n\nround(summary(pois_attack_final)$coefficients, 2)\n\n            Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)    -0.34       0.17   -2.02     0.04\nres_infyes      0.43       0.15    2.80     0.01\nghq12           0.05       0.01    6.40     0.00\n\n\n\\[\n\\ln(\\text{attack}) = - 0.34 + 0.43 \\text{res} + 0.05 \\text{ghq12}\n\\]\n\nintercept: when all predictors are zero, the expected count of the response variable is \\(\\exp(-0.34) \\approx 0.711\\).\nres_inf: when variable changes (holding ghq12 constant), the expected count of the response variable is multiplied by \\(\\exp(0.43) \\approx 1.53\\). This means that the presence of res_inf when (res_inf = Yes) increases the expected count by approximately 53% compared to when res_inf is absent (when res_inf = No).\nghq12: For each one unit increase in ghq12 (holding res_inf constant), the expected count of the response variable is multiplied by \\(\\exp(0.05) \\approx 1.05\\). This indicates that each unit increase in ghq12 is associated with a 5% increase in the expected count.\n\n\n\n33.4.5 Prediction\nWe can use the final model above for prediction. Relevant to our data set, we may want to know the expected number of asthmatic attacks per year for a patient with recurrent respiratory infection and GHQ-12 score of 8,\n\npred &lt;- predict(\n  pois_attack_final, list(res_inf = \"yes\", ghq12 = 8), type = \"response\"\n)\nround(pred, 1)\n\n  1 \n1.6 \n\n\nNow, let’s say we want to know the expected number of asthmatic attacks per year for those with and without recurrent respiratory infection for each 12-mark increase in GHQ-12 score.\n\nnew_data &lt;- tibble(\n  res_inf = rep(c(\"yes\", \"no\"), each = 4),\n  ghq12   = rep(c(0, 12, 24, 36), 2)\n)\nnew_data$attack_pred &lt;- round(\n  predict(pois_attack_final, new_data, type = \"response\"),\n  digits = 1\n)\nnew_data\n\n# A tibble: 8 × 3\n  res_inf ghq12 attack_pred\n  &lt;chr&gt;   &lt;dbl&gt;       &lt;dbl&gt;\n1 yes         0         1.1\n2 yes        12         2  \n3 yes        24         3.6\n4 yes        36         6.6\n5 no          0         0.7\n6 no         12         1.3\n7 no         24         2.4\n8 no         36         4.3\n\n\nWe can also check using the histogram,\n\nlambda_fun &lt;- function(ghq12, currRespInf = c(0, 1)){\n  exp(-0.34 + 0.43 * currRespInf + 0.05 * ghq12)\n}\n\npar(mfrow = c(1, 2))\nhist(\n  rpois(n = 10000, lambda = lambda_fun(ghq12 = 6, currRespInf = 0)),\n  main = \"Dist of Pred Asthma Attacks for Healthy Participant\"\n)\n\nhist(\n  rpois(n = 10000, lambda = lambda_fun(ghq12 = 28, currRespInf = 1)),\n  main = \"Dist of Pred Asthma Attacks for Sick Participant\"\n)\n\n\n\n\n\n\n\npar(mfrow = c(1, 1))\n\nWe can see that the histogram supports the relationship we saw in the pairs grpah above.\nWe can also predict the probability of number of ashma attacks.\n\n# Pr(nAttacks &gt;= 3|healthy)\n1 - ppois(q = 2, lambda_fun(ghq12 = 6, currRespInf = 0))\n\n[1] 0.07323225\n\n\nA healthy person has approximately 7% chance of experiencing 3 or more asthma attacks in the specified time interval.\n\n# Pr(nAttacks &gt;= 3|sick)\n1 - ppois(q = 2, lambda_fun(ghq12 = 28, currRespInf = 1))\n\n[1] 0.8192219\n\n\nA sick person has approximately 82% chance of experiencing 3 or more asthma attacks in the specified time interval.",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Poisson Regression Model</span>"
    ]
  },
  {
    "objectID": "lessons/05_glm_poisson.html#summary-1",
    "href": "lessons/05_glm_poisson.html#summary-1",
    "title": "33  Poisson Regression Model",
    "section": "33.5 Summary",
    "text": "33.5 Summary\nIn this lecture, we went through the basics about Poisson regression for count data. We performed the analysis for each and learned how to assess the model fit for the regression models. We learned how to nicely present and interpret the results. In addition, we also learned how to utilize the model for prediction.",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Poisson Regression Model</span>"
    ]
  },
  {
    "objectID": "lessons/05_glm_negative_binomial.html",
    "href": "lessons/05_glm_negative_binomial.html",
    "title": "\n34  Generalized Linear Models: Negative Binomial\n",
    "section": "",
    "text": "34.1 What is Negative Binomial Regression?\nPoisson regression is commonly used when modeling count data. In this type of GLM, the counts outcome is required to follow a Poisson distribution wherein its mean is equal to its variance. A Poisson distribution, as we have learned, describes the probability of a given number of events occurring within a fixed interval. Poisson regression models are accurate but inefficient for data that are under-dispersed or over-dispersed, that is, the variance is smaller or larger than the mean. Standard error estimates are thus underestimated by Poisson regression, which results in an inflated chance of type 1 error (incorrectly rejecting a true null hypothesis). In real count data, the variance is often greater than the mean.\nInstead, negative binomial regression is used to model count data when the data are over-dispersed. A binomial distribution is a distribution of the number of successes (x) in a fixed number (n) of independent Bernoulli trials. The negative binomial distribution, on the other hand, is the distribution of the number of trials (x) needed to get a fixed number of successes (x). The negative binomial regression model has a dispersion parameter, \\(\\theta\\), to represent random error. This allows it more flexibility to model distributions that cannot follow a Poisson distribution.\nFurthermore, a distribution of count data that have an attainable maximum value can not follow a Poisson distribution, whose support ranges from zero to infinity. The negative binomial distribution, with its extra \\(\\theta\\) parameter, provides a better fit for these types of count data. If the largest actual observed values are relatively close to the largest possible value of the outcome, a negative binomial distribution should be used.\nFor an in-depth explanation of the theory behind the negative binomial distribution and regression model, along with examples, please read this chapter by NCSS Statistical Software.",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Generalized Linear Models: Negative Binomial</span>"
    ]
  },
  {
    "objectID": "lessons/05_glm_negative_binomial.html#what-is-negative-binomial-regression",
    "href": "lessons/05_glm_negative_binomial.html#what-is-negative-binomial-regression",
    "title": "\n34  Generalized Linear Models: Negative Binomial\n",
    "section": "",
    "text": "34.1.1 Negative Binomial Regression Formula\nThe negative binomial regression model formula is as follows:\n\\[\n\\log(\\mu_i) = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\ldots + \\beta_k X_{ik}\n\\]\nwhere:\n\n\n\\(\\mu_i\\) is the mean of the response variable for the \\(i\\)-th observation,\n\n\\(\\beta_0\\) is the intercept,\n\n\\(\\beta_1, \\beta_2, \\ldots, \\beta_k\\) are the regression coefficients, and\n\n\\(X_{i1}, X_{i2}, \\ldots, X_{ik}\\) are the predictor variables.\n\nThe variance of the response variable \\(Y_i\\) is given by:\n\\[\n\\text{Var}(Y_i) = \\mu_i + \\frac{\\mu_i^2}{\\theta}\n\\]\nwhere: \\(\\theta\\) is the dispersion parameter. It measures the extra variation, or dispersion, in the data that is not explained by the Poisson distribution. A small theta indicates the variance is much larger than \\(\\mu\\). As theta increases, the variance approaches \\(\\mu\\).\n\n34.1.2 Assumptions of Negative Binomial GLM\n\n\nIndependence: Observations should be independent of one another.\n\nLinearity: the relationship between the dependent variable and independent variables should be linear.\n\nNo Multicolinearity: Independent variables should not be highly correlated with one another. \n\n\n34.1.3 Other GLMs for Count Data\nA zero-inflated Poisson regression is used to model discrete (count) data that has an excess amount of zero outcomes. This model may be used to model data that are over-dispersed due to the presence of excessive zeros that are generated by a separate process from the count values.\nZero-truncated Poisson regression is used to model count data that can not contain a zero value. This model should be used if the data generating process does not allow for a zero value to occur - for example, when measuring the number of days patients spent at a hospital.\nCensored regression models, such as the Tobit model, are used to model data with right- or left-censoring in the dependent variable. Censoring from the right occurs when the actual values at or above the maximum threshold are recorded as the value of that threshold. In left-censoring, values below a threshold take on the value of that threshold.",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Generalized Linear Models: Negative Binomial</span>"
    ]
  },
  {
    "objectID": "lessons/05_glm_negative_binomial.html#mental-illness-example",
    "href": "lessons/05_glm_negative_binomial.html#mental-illness-example",
    "title": "\n34  Generalized Linear Models: Negative Binomial\n",
    "section": "\n34.2 Mental Illness Example",
    "text": "34.2 Mental Illness Example\nTo explore a possible relationship between mental illness and illicit drug use, we will create a negative binomial GLM using participant data from a clinical trial to predict the number of comorbid mental illnesses that clinical trial participants have been diagnosed with in addition to opioid use disorder (OUD). The data were collected during the clinical trial “CTN-0094: Individual Level Predictive Modeling of Opioid Use Disorder Treatment Outcome” and provided by the public.ctn0094data and public.ctn0094extra packages. These packages contain demographic, clinical, and drug use data on trial participants seeking treatment for OUD.\n\n34.2.1 Install Packages\n\n34.2.2 Data Cleaning and Exploration\n\npsychDx_df &lt;- \n  psychiatric %&gt;% \n  # select Identifier column `who` and disease state columns\n  dplyr::select(\"who\", starts_with(\"has_\")) %&gt;% \n  dplyr::select(!ends_with(\"_dx\")) %&gt;% \n  drop_na()\n\nhead(psychDx_df)\n\n# A tibble: 6 × 7\n    who has_schizophrenia has_major_dep has_bipolar has_anx_pan has_brain_damage\n  &lt;int&gt; &lt;fct&gt;             &lt;fct&gt;         &lt;fct&gt;       &lt;fct&gt;       &lt;fct&gt;           \n1     1 No                No            No          No          No              \n2     2 No                No            No          No          Yes             \n3     3 No                No            No          No          No              \n4     4 No                Yes           No          Yes         No              \n5     6 No                No            No          No          No              \n6     7 No                No            No          Yes         No              \n# ℹ 1 more variable: has_epilepsy &lt;fct&gt;\n\ncountComorbidDx_int &lt;- \n  psychDx_df %&gt;% \n  dplyr::select(-who) %&gt;% \n  # count number of psychiatric disorders per ID\n  mutate(\n    across(\n      # create data frame containing `TRUE` if the value in each cell of the \n      #   columns equals \"Yes\" and `FALSE` otherwise\n      .cols = has_schizophrenia:has_epilepsy,\n      .fns = ~ `==`(.x, \"Yes\")\n    )\n  ) %&gt;% \n  # `rowSums(.)` sums the number of `TRUE` values in this data frame for each\n  #   row. Result is converted to integers for our new column, `nComorbidDx`.\n  mutate(nComorbidDx = as.integer(rowSums(.))) %&gt;% \n  # extract column as vector\n  pull(nComorbidDx)\n\n# add vector of count of psychiatric disorders to dataset as a column\npsychDx_df$nComorbidDx &lt;- countComorbidDx_int\n\n# visualize distribution of psych disorder counts (correctly, using a bar chart)\nggplot(psychDx_df) +\n  aes(nComorbidDx) +\n  labs(\n    title = \"Bar Chart of Comorbid Dx Distribution\",\n    x = \"Number of Comorbid Diagnoses\",\n    y = \"Count\"\n  ) +\n  geom_bar()\n\n\n\n\n\n\n# visualize with a histogram (inappropriate for our count data)\nhist(\n  countComorbidDx_int, \n  main = \"Histogram (WRONG) of Comorbid Dx Distribution\",\n  xlab = \"Number of Comorbid Diagnoses\",\n  ylab = \"Count\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\nBar Chart vs. Histogram\n\n\n\nWhen graphing count data, use a bar chart, not a histogram. Histograms treat data as continuous and will graph count data as if it may contain decimals.\n\n\nThis distribution supports using negative binomial regression because our count cannot go past 6. A normal (Poisson) binomial regression support, however, ranges from 0 to infinity, and would therefore be an inappropriate distribution to model the data with. The negative binomial model has more flexibility to model count data that has a maximum value.\nWe will now join our dataset with the demographics and rbs datasets from the Clinical Trials Network study CTN-0094 by the subject ID, \"who\", to get demographic and substance use data for each subject. \n\n# inspect demographics, qol, and rbs datasets from CTN and join to PsychDx\ndata(\"demographics\")\ndata(\"rbs\")\n\n# `rbs` is in long format, so pivot wider\nrbs_wide &lt;- rbs %&gt;% \n   pivot_wider(\n    id_cols = who,\n    names_from = what,\n    values_from = days\n  )\n\n# left join by `who` to retain rows from `demographics` for all matching `who`s\n#   in psychDx_df\ndxPred_df &lt;- left_join(psychDx_df, demographics, by = \"who\") %&gt;% \n  left_join(rbs_wide, by = \"who\")\n\n\n# the diagnosis columns for each disease and `who` column are no longer needed\n#   after the join and thus are removed\ndxPred2_df &lt;- dxPred_df %&gt;% \n  dplyr::select(\n    -(has_schizophrenia:has_epilepsy),\n    -who\n  )\n\nprint(dxPred2_df)\n\n# A tibble: 3,080 × 14\n   nComorbidDx   age is_hispanic race  job    is_living_stable education marital\n         &lt;int&gt; &lt;dbl&gt; &lt;fct&gt;       &lt;fct&gt; &lt;fct&gt;  &lt;fct&gt;            &lt;fct&gt;     &lt;fct&gt;  \n 1           0    43 No          White Full … Yes              More tha… Marrie…\n 2           1    30 No          White Full … Yes              More tha… Never …\n 3           0    23 No          Black Full … Yes              More tha… Never …\n 4           2    19 No          White Stude… Yes              More tha… Never …\n 5           0    43 No          White Full … Yes              HS/GED    Marrie…\n 6           1    33 No          White Part … Yes              More tha… Never …\n 7           0    25 Yes         Black Part … Yes              HS/GED    Never …\n 8           4    29 No          Other Part … Yes              More tha… Never …\n 9           2    40 No          White &lt;NA&gt;   &lt;NA&gt;             &lt;NA&gt;      &lt;NA&gt;   \n10           0    19 No          White &lt;NA&gt;   &lt;NA&gt;             &lt;NA&gt;      &lt;NA&gt;   \n# ℹ 3,070 more rows\n# ℹ 6 more variables: is_male &lt;fct&gt;, cocaine &lt;dbl&gt;, heroin &lt;dbl&gt;,\n#   speedball &lt;dbl&gt;, opioid &lt;dbl&gt;, speed &lt;dbl&gt;\n\n\n\n34.2.3 Predictor Selection\nWe will now check to see how much data are missing. A negative binomial GLM can only be built using complete rows, meaning rows with no missing values. Predictors with a high proportion of data should not be used as they would negatively impact the GLM’s fit to the data. If we use all features, we will be left with 567 out of 3080 rows with complete cases. The number of usable rows can be increased by removing incomplete predictor columns.\n\n# check for features with lots of missing data and remove them.\n\nskim(dxPred2_df)\n\n\nData summary\n\n\nName\ndxPred2_df\n\n\nNumber of rows\n3080\n\n\nNumber of columns\n14\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n7\n\n\nnumeric\n7\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nis_hispanic\n10\n1.00\nFALSE\n2\nNo: 2634, Yes: 436\n\n\nrace\n0\n1.00\nFALSE\n4\nWhi: 2318, Oth: 438, Bla: 295, Ref: 29\n\n\njob\n1512\n0.51\nFALSE\n5\nFul: 842, Une: 330, Par: 299, Oth: 56\n\n\nis_living_stable\n1519\n0.51\nFALSE\n2\nYes: 1500, No: 61\n\n\neducation\n1455\n0.53\nFALSE\n3\nMor: 676, HS/: 634, Les: 315\n\n\nmarital\n1463\n0.52\nFALSE\n3\nNev: 937, Sep: 368, Mar: 312\n\n\nis_male\n1\n1.00\nFALSE\n2\nYes: 2041, No: 1038\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nnComorbidDx\n0\n1.00\n0.95\n1.08\n0\n0\n1\n2\n6\n▇▂▁▁▁\n\n\nage\n97\n0.97\n35.90\n10.86\n18\n27\n34\n45\n77\n▇▇▆▂▁\n\n\ncocaine\n303\n0.90\n2.95\n6.55\n0\n0\n0\n2\n30\n▇▁▁▁▁\n\n\nheroin\n295\n0.90\n17.51\n13.68\n0\n0\n26\n30\n30\n▆▁▁▁▇\n\n\nspeedball\n856\n0.72\n1.42\n5.05\n0\n0\n0\n0\n30\n▇▁▁▁▁\n\n\nopioid\n524\n0.83\n13.38\n13.39\n0\n0\n5\n30\n30\n▇▁▁▁▆\n\n\nspeed\n852\n0.72\n1.18\n4.00\n0\n0\n0\n0\n30\n▇▁▁▁▁\n\n\n\n\n# count rows missing any data on substance use\nsum(is.na(dxPred2_df$cocaine) | is.na(dxPred2_df$heroin) | is.na(dxPred2_df$speedball) | is.na(dxPred2_df$opioid) | is.na(dxPred2_df$speed))\n\n[1] 1198\n\n\nIf I use all features, I will be left with 567 out of 3080 rows with complete cases. I can increase the number of complete rows by removing incomplete predictor columns. The features job, is_living_stable, education, and marital from the demographics dataset are all missing data on about half of the participants (N = 3080) and should therefore be removed from the model. Furthermore, about one-third of participants are missing data on their drug usage from the rbs dataset. I will also remove speedball and speed as they are 27% missing.\nWe will use cor() to create a correlation matrix of the continuous independent variables to investigate possible multicolinearity.\n\n# remove features missing &gt; 20% of observations\ndxPred3_df &lt;- dxPred2_df %&gt;% \n  dplyr::select(\n    -(job:marital),\n    - speed,\n    - speedball\n  )\n\nprint(dxPred3_df)\n\n# A tibble: 3,080 × 8\n   nComorbidDx   age is_hispanic race  is_male cocaine heroin opioid\n         &lt;int&gt; &lt;dbl&gt; &lt;fct&gt;       &lt;fct&gt; &lt;fct&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1           0    43 No          White Yes           0      0     30\n 2           1    30 No          White No            0      0     30\n 3           0    23 No          Black No            4     30      4\n 4           2    19 No          White Yes           0      4     30\n 5           0    43 No          White Yes           0      0     30\n 6           1    33 No          White No            4      4     30\n 7           0    25 Yes         Black No            4      4      4\n 8           4    29 No          Other No           30     30      3\n 9           2    40 No          White No            2     30      1\n10           0    19 No          White Yes           1     30      0\n# ℹ 3,070 more rows\n\n# calculate Spearman correlation matrix for the continuous predictors\nround(\n  cor(\n    dxPred3_df[, c(\"age\", \"cocaine\", \"heroin\", \"opioid\")],\n    # use only pairwise complete observations for calculating correlations\n    use = \"pairwise.complete.obs\",\n    method = \"spearman\"\n  ),\n  # round to 2 digits for easy interpretation\n  digits = 2\n)\n\n          age cocaine heroin opioid\nage      1.00    0.04   0.13  -0.10\ncocaine  0.04    1.00   0.29  -0.22\nheroin   0.13    0.29   1.00  -0.75\nopioid  -0.10   -0.22  -0.75   1.00\n\n\nI will also remove opioid because it is highly correlated with heroin, which I chose to retain over opioid simply because it contains more data.\nThe next step is to create a pairs plot to investigate the remaining predictors and determine whether any pairs are highly correlated.\n\n\n\n\n\n\n\n\nThe remaining variables exhibit no strong correlations or unexpected behavior, so they will all be used in the model.\n\n34.2.4 Negative Binomial GLM\n\n# create negative binomial regression to predict number of comorbid diagnoses\nnb_mod &lt;- MASS::glm.nb(\n  nComorbidDx ~\n    age +\n    is_hispanic +\n    relevel(race, ref = \"White\") +\n    is_male +\n    cocaine * heroin,\n  data = dxPred4_df\n) \n\nsummary(nb_mod)\n\n\nCall:\nMASS::glm.nb(formula = nComorbidDx ~ age + is_hispanic + relevel(race, \n    ref = \"White\") + is_male + cocaine * heroin, data = dxPred4_df, \n    init.theta = 5.828534372, link = log)\n\nCoefficients:\n                                              Estimate Std. Error z value\n(Intercept)                                  0.2523885  0.0816969   3.089\nage                                          0.0018235  0.0021112   0.864\nis_hispanicYes                              -0.0025841  0.0795171  -0.032\nrelevel(race, ref = \"White\")Black           -0.3616542  0.0876751  -4.125\nrelevel(race, ref = \"White\")Other           -0.1348193  0.0796699  -1.692\nrelevel(race, ref = \"White\")Refused/missing  0.0457402  0.3635756   0.126\nis_maleYes                                  -0.5167470  0.0444520 -11.625\ncocaine                                      0.0233846  0.0072177   3.240\nheroin                                      -0.0026203  0.0017605  -1.488\ncocaine:heroin                              -0.0006089  0.0002821  -2.159\n                                            Pr(&gt;|z|)    \n(Intercept)                                  0.00201 ** \nage                                          0.38774    \nis_hispanicYes                               0.97408    \nrelevel(race, ref = \"White\")Black           3.71e-05 ***\nrelevel(race, ref = \"White\")Other            0.09060 .  \nrelevel(race, ref = \"White\")Refused/missing  0.89988    \nis_maleYes                                   &lt; 2e-16 ***\ncocaine                                      0.00120 ** \nheroin                                       0.13665    \ncocaine:heroin                               0.03087 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(5.8285) family taken to be 1)\n\n    Null deviance: 3143.0  on 2629  degrees of freedom\nResidual deviance: 2957.1  on 2620  degrees of freedom\n  (450 observations deleted due to missingness)\nAIC: 6781.2\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  5.83 \n          Std. Err.:  1.32 \n\n 2 x log-likelihood:  -6759.229 \n\n\nThe beta coefficients indicate the log change in the expected count of comorbid diagnoses for a one-unit change in the predictor variable. For continuous predictors like age, cocaine, heroin, and the cocaine:heroin interaction, a positive coefficient means that as the predictor increases, the expected count increases. For categorical predictors like is_hispanic, is_male, and race, the coefficient indicates the change in the log count relative to the reference category.\nWe can compare the null deviance to the residual deviance to determine whether our predictors are useful for predicting the dependent variable. The null deviance is a measure of how well nComorbidDx is predicted by the null model, which consists of the intercept with no predictors, whereas the residual deviance quantifies how well the nComorbidDx can be predicted by the model with the predictors. A residual deviance that is much higher than the degrees of freedom indicates data are over-dispersed. The residual deviance is smaller than the null deviance of this model, meaning the predictors are predicting.\nAfter removing insignificant predictors, we are left with this model, where cocaine and heroin are “the number of days in the past 30 days where the substance was used”:\n\\[\nn_{\\text{Dx}} = e^{\\left( 0.252 - 0.362 \\cdot \\text{race}_{\\text{Black}} - 0.517 \\cdot \\text{gender}_{\\text{Male}} + 0.023 \\cdot \\text{cocaine} - 0.001 \\cdot \\text{cocaine} \\cdot \\text{heroin} \\right)}\n\\]\n\n34.2.5 Interpretation\nWe must exponentiate the beta coefficients to calculate the multiplicative effect they have on an individual’s expected number of comorbid diagnoses.\n\n# exponentiate intercept\nexp(coef(nb_mod)[1])\n\n(Intercept) \n   1.287096 \n\n# exponentiate beta for race = Black group\nexp(coef(nb_mod)[4])\n\nrelevel(race, ref = \"White\")Black \n                        0.6965232 \n\n# exponentiate beta for male = Yes group\nexp(coef(nb_mod)[7])\n\nis_maleYes \n 0.5964577 \n\n# exponentiate beta for cocaine use\nexp(coef(nb_mod)[8])\n\ncocaine \n1.02366 \n\n# exponentiate beta for cocaine:heroin interaction\nexp(coef(nb_mod)[10])\n\ncocaine:heroin \n     0.9993912 \n\n\nBased on this model, the expected number of comorbid mental health diagnoses for a white, non-hispanic female who does not use cocaine alone and does not use both cocaine and heroin together is 1.29 (on average, at least one other diagnosis in addition to OUD). We expect black individuals to have 69.7% fewer comorbid diagnoses than individuals of other racial groups when all other predictors are zero (p &lt; 0.0001); that is, the expected number of comorbid mental health diagnoses for a black, non-hispanic female who does not use cocaine alone and does not use both cocaine and heroin together is 0.896. Men are also expected to have 59.6% fewer comorbid diagnoses compared to women when all other predictors are zero (p &lt; 0.0001); that is, the expected number of comorbid mental health diagnoses for a white, non-hispanic male who does not use cocaine alone and does not use both cocaine and heroin together is 0.768.\nFurthermore, an individual’s number of comorbid diagnoses increased by 2.37% for every 1-day increase in days of cocaine use per month (p = 0.001). Interestingly, heroin moderated this relationship: for every 1-day increase in both heroin and cocaine usage per month, an individual’s number of comorbid diagnoses was expected to decrease by 0.060% (p = 0.031).\n\n34.2.6 Goodness-of-Fit Testing\nTo test whether the model fits the data well, I will overlay our distribution of comorbid diagnoses over a simulated negative binomial distribution that was generated using the parameters of the empirical distribution following this example.\n\nset.seed(20220625)\n\n# generate a true negative binomial distribution with the same, n, number of \n#   successes, and probability of success\n\n# create df of the Dx distribution containing only values for which data are \n#   complete (and thus used in the regression, nb_mod)\nxx &lt;- dxPred4_df[complete.cases(dxPred4_df), \"nComorbidDx\"]\n\n# xx must be a numeric vector for fitdistr; put column data in vector\nxx_fit &lt;- as.numeric(xx$nComorbidDx)\n\n# fit parameters of Dx distribution\nfit &lt;- MASS::fitdistr(xx_fit, densfun = \"negative binomial\")\n\nWarning in densfun(x, parm[1], parm[2], ...): NaNs produced\n\n# create example negative binomial data frame (is this supposed to replace \n#   the simulated dist, or should I name it something else?)\nyy &lt;- data.frame(negative_binomial = 0:45)\n\n# calculate density for each observation given mu and size from simulated\n#   negative binomial distribution\nyy$density &lt;- dnbinom(\n  yy$negative_binomial,\n  mu = fit$estimate[\"mu\"],\n  size = fit$estimate[\"size\"]\n)\n\n# convert to factor to make x-axis discrete\nxx$nComorbidDx &lt;- as.factor(xx$nComorbidDx)\n\n# graph Dx distribution with blue bars\nggplot(data = xx, aes(x = nComorbidDx)) +\n  geom_bar(\n    # set y = the density (frequency) of each Dx. `..density..` is a variable\n    #  created by `geom_histogram()`.\n    aes(y = after_stat(count/sum(count))),\n    color = \"black\",\n    fill = \"lightblue\"\n  ) +\n  # graph red lines to represent the simulated neg binomial distribution with \n  #   the same parameters as the Dx distribution\n  labs(\n    title = \"Comorbid Mental Illness Dx Distribution vs. A Negative Binomial Distribution\",\n    x = \"Number of Comorbid Diagnoses\",\n    y = \"Density\"\n  ) +\n  # Add red lines to represent theoretical negative binomial distribution\n  geom_linerange(\n    data = yy,\n    aes(x = as.factor(negative_binomial), ymin = 0, ymax = density),\n    color = \"red\") +\n  scale_x_discrete(limits = factor(0:10))\n\nWarning: Removed 35 rows containing missing values or values outside the scale range\n(`geom_segment()`).\n\n\n\n\n\n\n\n\nThe theoretical negative binomial distribution fits the empirical data well. It returns small probabilities of greater than six comorbid diagnoses occurring, but is a better fit to the data overall than a Poisson distribution. To illustrate this point, I will also compare the empirical distribution to a simulated Poisson distribution using the parameters of the empirical distribution.\n\n# Fit parameters of Dx distribution to a Poisson distribution\nfit2 &lt;- MASS::fitdistr(xx_fit, densfun = \"Poisson\")\n\n# Create example Poisson data frame\nyy2 &lt;- data.frame(poisson = 0:45)\n\n# Calculate density for each observation given lambda from the Poisson\n#   distribution\nyy2$density &lt;- dpois(yy2$poisson, lambda = fit2$estimate[\"lambda\"])\n\n# Graph Dx distribution with blue bars\nggplot(data = xx, aes(x = nComorbidDx)) +\n  geom_bar(\n    # Set y = the density (frequency) of each Dx. `..density..` is a variable \n    #   created by `geom_histogram()`.\n    aes(y = after_stat(count/sum(count))),\n    color = \"black\",\n    fill = \"lightblue\"\n  ) +\n  # Add labels\n  labs(\n    title = \"Comorbid Mental Illness Dx Distribution vs. A Poisson Distribution\",\n    x = \"Number of Comorbid Diagnoses\",\n    y = \"Density\"\n  ) +\n  # Add red lines to represent the theoretical Poisson distribution\n  geom_linerange(\n    data = yy2,\n    aes(x = as.factor(poisson), ymin = 0, ymax = density),\n    color = \"red\"\n  ) +\n  scale_x_discrete(limits = factor(0:10))\n\nWarning: Removed 35 rows containing missing values or values outside the scale range\n(`geom_segment()`).\n\n\n\n\n\n\n\n\nNotice the Poisson distribution models the density of zero and one values as roughly equal, thereby underestimating the density of zero values and overestimating the density of one values in the empirical dataset. Although the differences in fit between the two examples may seem insignificant, remember that precision is key when making decisions that impact people’s health. (pro tip: if the graphs don’t support my point, just keep re-rendering them until they do!)",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Generalized Linear Models: Negative Binomial</span>"
    ]
  },
  {
    "objectID": "06_header_special-topics.html",
    "href": "06_header_special-topics.html",
    "title": "Special Topics in Statistical Modelling",
    "section": "",
    "text": "Text Outline\nThis text covers the basics of:",
    "crumbs": [
      "Special Topics in Statistical Modelling"
    ]
  },
  {
    "objectID": "06_header_special-topics.html#text-outline",
    "href": "06_header_special-topics.html#text-outline",
    "title": "Special Topics in Statistical Modelling",
    "section": "",
    "text": "Exploring data\nOne-sample tests\nTwo-sample tests\nANOVA and linear regression\nGeneralized linear models\nSome special topics\nMaybe a bit about power calculations…",
    "crumbs": [
      "Special Topics in Statistical Modelling"
    ]
  },
  {
    "objectID": "06_header_special-topics.html#part-outline",
    "href": "06_header_special-topics.html#part-outline",
    "title": "Special Topics in Statistical Modelling",
    "section": "Part Outline",
    "text": "Part Outline\nThis part of the text contains the chapters on various different special statistical models:\n\nLinear Mixed Effects Models\nStructural Equation Models\nCox Proportional Hazards Regression\n(TBD) Multivariate Methods for Genetics/Genomics\nRidge, LASSO, and Elastic Net Regression",
    "crumbs": [
      "Special Topics in Statistical Modelling"
    ]
  },
  {
    "objectID": "lessons/06_elastic_net_regression.html",
    "href": "lessons/06_elastic_net_regression.html",
    "title": "\n35  Elastic Net Regression Model\n",
    "section": "",
    "text": "35.1 Libraries for the lessons\nFor this chapter, we will be using the following packages\nThese are loaded as follows using the function library(),\n# Installing Required Packages\n# intsall.packages(\"foreign\")\n# intsall.packages(\"skimr\")\n# intsall.packages(\"gtsummary\")\n# intsall.packages(\"GGally\")\n# install.packages(\"glmnet\")\n# install.packages(\"car\")\n# intsall.packages(tidyverse)\n\n# Loading Required Packages\nlibrary(foreign)\nlibrary(skimr)\nlibrary(gtsummary)\nlibrary(GGally)\nlibrary(glmnet)\nlibrary(car)\nlibrary(tidyverse)",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Elastic Net Regression Model</span>"
    ]
  },
  {
    "objectID": "lessons/06_elastic_net_regression.html#libraries-for-the-lessons",
    "href": "lessons/06_elastic_net_regression.html#libraries-for-the-lessons",
    "title": "\n35  Elastic Net Regression Model\n",
    "section": "",
    "text": "foreign: for reading SPSS and STATA datasets\n\nskimr: for summaries the datasets\n\ngtsummary: for coming up with nice tables for results and plotting the graphs\n\nGGally: for plotting the pairs graphs\n\nglmnet: ridge regression model\n\ncar: for finding vif\n\ntidyverse: a general and powerful package for data transformation",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Elastic Net Regression Model</span>"
    ]
  },
  {
    "objectID": "lessons/06_elastic_net_regression.html#introduction-to-elastic-net-regression",
    "href": "lessons/06_elastic_net_regression.html#introduction-to-elastic-net-regression",
    "title": "\n35  Elastic Net Regression Model\n",
    "section": "\n35.2 Introduction to Elastic Net Regression",
    "text": "35.2 Introduction to Elastic Net Regression\nIn multiple linear regression analysis, it is not uncommon for specific problems to arise during the analysis. One of them is the problem of multicollinearity. Multicollinearity is a condition that appears in multiple regression analysis when one independent variable is correlated with another independent variable. Multicollinearity can create inaccurate estimates of the regression coefficients, inflate the standard errors of the regression coefficients, deflate the partial t-tests for theregression coefficients, give false, nonsignificant, p-values, and degrade the predictability of the model. Multicollinearity is a serious problem, where in cases of high multicollinearity, it results in making inaccurate decisions or increasing the chance of accepting the wrong hypothesis. Therefore it is very important to find the most suitable method to deal with multicollinearity. There are several ways to detect the presence of multicollinearity including looking at the correlation between independent variables and using the Variance Inflation Factor (VIF). As for the method to overcome the problem of multicollinearity, one way is by shrinking the estimated coefficients. The shrinkage method is often referred to as the regularization method. The regularization method can shrink the parameters to near zero relative to the least squares estimate. The regularization methods that are often used are Regression Ridge, Least Absolute Shrinkage and Selection Operator (LASSO), and Elastic-Net. Ridge Regression(RR) is a technique to stabilize the value of the regression coefficient due to multicollinearity problems. By adding a degree of bias to the regression estimate, RR reduces the standard error and obtains a more accurate estimate of the regression coefficient than the OLS. Meanwhile, LASSO and Elastic-Net overcome the problem of multicollinearity by reducing the regression coefficients of the independent variables that have a high correlation close to zero or exactly zero.",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Elastic Net Regression Model</span>"
    ]
  },
  {
    "objectID": "lessons/06_elastic_net_regression.html#mathematical-formulation-of-the-model",
    "href": "lessons/06_elastic_net_regression.html#mathematical-formulation-of-the-model",
    "title": "\n35  Elastic Net Regression Model\n",
    "section": "\n35.3 Mathematical Formulation of the Model",
    "text": "35.3 Mathematical Formulation of the Model\nOne method that can be used to estimate parameters is Ordinary Least Squares (OLS). This method requires the absence of multicollinearity between independent variables. If the independent variable has multicollinearity, the estimate of the regression coefficient may be imprecise. This method is used to estimate \\(\\beta\\) by minimizing the sum of squared errors. If the data consists of n observations \\([{y_i, x_i}]^n_{i=1}\\) and each observation i includes a scalar response \\(y_i\\) and a vector of p predictors (regressors) \\(x_{ij}\\) for j = 1, …, m, a multiple linear regression model can be written as n the matrix form the model as \\(Y = X \\beta + \\epsilon\\) where \\(Y_{n \\times 1}\\) is the vector dependent variable, \\(X_{n \\times m}\\) represents the explanatory variables, \\(\\beta_{m \\times 1}\\) is the regression coefficients to be estimated, and \\(\\epsilon_{m \\times 1}\\) represents the errors or residuals. \\(\\hat\\beta_{OLS} = (X^T X)^{-1} X^T Y\\) is estimated regression coefficients using OLS by minimizing the squared distances between the observed and the predicted dependent variable. To have unbiased OLS estimation of the model, some assumptions should be satisfied. Those assumptions are that the errors have an expected value of zero, that the independent variables are non-random, that the independent variables are linearly independent (non-multicollinearity), that the disturbance are homoscedastic and not autocorrelated. If the independent variables have multicollinearity the estimates of coefficient regression may be imprecise.\n\n35.3.1 Ridge Regression\nRidge regression introduced by Horel (1962) is one method for deal with multicollinearity problems. The ridge regression technique is based on addition the ridge parameter (\\(\\lambda\\)) to the diagonal of the \\(X^T X\\) matrix forms a new matrix \\((X^T X + \\lambda I)\\). is called ridge regression because diagonal one in the correlation matrix can be described as ridge Hoerl and Kennard (1970). The ridge regression coefficients estimator is \\[\n\\hat\\beta_R = (X^T X + \\lambda I)^{-1} X^T Y, \\lambda \\geq 0\n\\] when \\(\\lambda = 0\\), the ridge estimator become as the OLS. If \\(\\lambda &gt; 0\\) the ridge estimator will be biased against the \\(\\hat\\beta_{OLS}\\) but tends to be more accurate than the least squares estimator. Ridge regression can also be written in Lagrangian form: \\[\n\\hat\\beta_{\\text{RIDGE}} = \\arg \\min_{\\beta}||y - X\\beta||^2_2 + \\lambda||\\beta||^2_2\n\\] where \\(||y - X\\beta||^2_2 = \\sum^n_{i=1}(y_i - x^T_i\\beta)^2\\) is norm loss function (i.e. residual sum of squares), \\(x^T_i\\) is the i-th row of X, \\(||\\beta||^2_2 = \\sum^p_{j=1}\\beta^2_j\\) is the penalty on \\(\\beta\\), and \\(\\lambda \\geq 0\\) is the tuning parameter which regulates the strength of the penalty by determining the relative importance of the data-dependet empirical error and penalty term. Ridge regression has the ability to solve problems multicollinearity by limiting the estimated coefficients, hence, it reduces the estimator’s variance but introduces some bias.\n\n35.3.2 Least Absolute Shrinkage and Selection Operator\nLeast Absolute Shrinkage and Selection Operator (LASSO) introduced by Tibshirani (1996) is a method that aims to reduce the regression coefficients of independent variables that have a high correlation with errors to exactly zero or close to zero. LASSO regression can also be written in Lagrangian form: \\[\n\\hat\\beta_{\\text{LASSO}} = \\arg \\min_{\\beta}||y - X\\beta||^2_2 + \\lambda||\\beta||_1\n\\] where \\(||\\beta||_1 = \\sum^p_{j=1}|\\beta_j|\\) is the penalty on \\(\\beta\\). with the condition \\(||\\beta||_1 \\leq \\lambda\\), where \\(\\lambda\\) is a tuning parameter that controls the shrinkage of the LASSO coefficient with \\(\\lambda \\geq 0\\). If \\(\\lambda &lt; \\lambda_0\\) with \\(\\lambda_0 = ||\\hat\\beta_j||_1\\) it will cause the shrinkage coefficient to approach zero or exactly zero, so LASSO helps as a variable selection. Like ridge, the absolute value penalty of the LASSO coefficient introduces shrinkage towards zero. However, on ridge regression, some of the coefficients are not shrinks to exactly zero.\n\n35.3.3 Elastic Net\nAccording to Zou and Hastie (2005), the Elastic-Net method can shrink the regression coefficient exactly to zero, besides that this method can also perform variable selection simultaneously with Elastic-Net penalties which are written as follows: \\[\n\\sum^p_{j=1}[\\alpha |\\beta_j| + (1 - \\alpha)\\beta^2_j]\n\\] with \\(\\alpha = \\frac{\\lambda_1}{\\lambda_1 + \\lambda_2},0 \\leq \\alpha \\leq 1\\). The coefficient estimator on Elastic-Net can be written as follows: \\[\n\\hat\\beta_{\\text{Elastic-net}} = \\arg \\min_{\\beta}||y - X\\beta||^2_2 + \\lambda_2||\\beta||^2_2 + \\lambda_1||\\beta||_1\n\\] Elastic-Net can be used to solve problems from LASSO. Where the LASSO Regression has disadvantages include; when p &gt; n then LASSO only chooses n variables included in the model, if there is a set of variables with high correlation, then LASSO only tends to choose one variable from the group and doesn’t care which one is selected, and when p &lt; n, LASSO performance is dominated by Ridge Regression. Multicollinearity is the existence of a linear relationship between independent variables, where multicollinearity can occur in either some or all of the independent variables in the multiple linear regression model. One way to detect multicollinearity is to use the Variation Inflation Factor (VIF). VIF value can be calculated by the following formula: \\[\n\\text{VIF}_j = \\frac{1}{1 - R^2_j}\n\\] if the VIF value &gt; 10, it can be concluded significantly that there is multicollinearity between the independent variables and one way to overcome multicollinearity is using the Ridge Regression, LASSO and Elastic-Net.",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Elastic Net Regression Model</span>"
    ]
  },
  {
    "objectID": "lessons/06_elastic_net_regression.html#example",
    "href": "lessons/06_elastic_net_regression.html#example",
    "title": "\n35  Elastic Net Regression Model\n",
    "section": "\n35.4 Example",
    "text": "35.4 Example\nTo demostrate the Elastic net regression, we will use coronary.dta (https://github.com/drkamarul/multivar_data_analysis/tree/main/data) dataset in STATA format. The dataset contains the total cholesterol level, their individual characteristics and intervention groups in a hypothetical clinical trial. The dataset contains 200 observations for nine variables:\n\n\nid: Subjects’ ID.\n\ncad: Coronary artery disease status (categorical) {no cad, cad}.\n\nsbp : Systolic blood pressure in mmHg (numerical).\n\ndbp : Diastolic blood pressure in mmHg (numerical).\n\nchol: Total cholesterol level in mmol/L (numerical).\n\nage: Age in years (numerical).\n\nbmi: Body mass index (numerical).\n\nrace: Race of the subjects (categorical) {malay, chinese, indian}.\n\ngender: Gender of the subjects (categorical) {woman, man}.\n\n\n35.4.1 Exploring the data\nThe dataset is loaded as follows,\n\ncoronary &lt;- foreign::read.dta(\"../data/06_coronary.dta\")\nhead(coronary, n = 10)\n\n    id    cad sbp dbp   chol age      bmi    race gender\n1    1 no cad 106  68 6.5725  60 38.90000  indian  woman\n2   14 no cad 130  78 6.3250  34 37.80000   malay  woman\n3   56 no cad 136  84 5.9675  36 40.50000   malay  woman\n4   61 no cad 138 100 7.0400  45 37.60000   malay  woman\n5   62 no cad 115  85 6.6550  53 40.30000  indian    man\n6   64 no cad 124  72 5.9675  43 37.60000   malay    man\n7   69    cad 110  80 4.4825  44 34.28411   malay    man\n8  108 no cad 112  70 5.4725  50 40.90000 chinese  woman\n9  112 no cad 138  85 7.4525  43 41.20000 chinese  woman\n10 134 no cad 104  70 6.4350  48 41.00000 chinese    man\n\n\n\n35.4.1.1 Structure of the dataset\n\nstr(coronary)\n\n'data.frame':   200 obs. of  9 variables:\n $ id    : num  1 14 56 61 62 64 69 108 112 134 ...\n $ cad   : Factor w/ 2 levels \"no cad\",\"cad\": 1 1 1 1 1 1 2 1 1 1 ...\n $ sbp   : num  106 130 136 138 115 124 110 112 138 104 ...\n $ dbp   : num  68 78 84 100 85 72 80 70 85 70 ...\n $ chol  : num  6.57 6.33 5.97 7.04 6.66 ...\n $ age   : num  60 34 36 45 53 43 44 50 43 48 ...\n $ bmi   : num  38.9 37.8 40.5 37.6 40.3 ...\n $ race  : Factor w/ 3 levels \"malay\",\"chinese\",..: 3 1 1 1 3 1 1 2 2 2 ...\n $ gender: Factor w/ 2 levels \"woman\",\"man\": 1 1 1 1 2 2 2 1 1 2 ...\n - attr(*, \"datalabel\")= chr \"Written by R.              \"\n - attr(*, \"time.stamp\")= chr \"\"\n - attr(*, \"formats\")= chr [1:9] \"%9.0g\" \"%9.0g\" \"%9.0g\" \"%9.0g\" ...\n - attr(*, \"types\")= int [1:9] 100 108 100 100 100 100 100 108 108\n - attr(*, \"val.labels\")= chr [1:9] \"\" \"cad\" \"\" \"\" ...\n - attr(*, \"var.labels\")= chr [1:9] \"id\" \"cad\" \"sbp\" \"dbp\" ...\n - attr(*, \"version\")= int 7\n - attr(*, \"label.table\")=List of 3\n  ..$ cad   : Named int [1:2] 1 2\n  .. ..- attr(*, \"names\")= chr [1:2] \"no cad\" \"cad\"\n  ..$ race  : Named int [1:3] 1 2 3\n  .. ..- attr(*, \"names\")= chr [1:3] \"malay\" \"chinese\" \"indian\"\n  ..$ gender: Named int [1:2] 1 2\n  .. ..- attr(*, \"names\")= chr [1:2] \"woman\" \"man\"\n\n\n\n35.4.1.2 Summary\n\nskim(coronary)\n\n\nData summary\n\n\nName\ncoronary\n\n\nNumber of rows\n200\n\n\nNumber of columns\n9\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\ncad\n0\n1\nFALSE\n2\nno : 163, cad: 37\n\n\nrace\n0\n1\nFALSE\n3\nmal: 73, chi: 64, ind: 63\n\n\ngender\n0\n1\nFALSE\n2\nwom: 100, man: 100\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nid\n0\n1\n2218.28\n1411.69\n1.00\n901.50\n2243.50\n3346.75\n4696.00\n▇▅▇▆▅\n\n\nsbp\n0\n1\n130.18\n19.81\n88.00\n115.00\n126.00\n144.00\n187.00\n▂▇▅▃▁\n\n\ndbp\n0\n1\n82.31\n12.90\n56.00\n72.00\n80.00\n92.00\n120.00\n▂▇▆▃▁\n\n\nchol\n0\n1\n6.20\n1.18\n4.00\n5.39\n6.19\n6.89\n9.35\n▅▇▇▃▂\n\n\nage\n0\n1\n47.33\n7.34\n32.00\n42.00\n47.00\n53.00\n62.00\n▃▇▇▆▃\n\n\nbmi\n0\n1\n37.45\n2.68\n28.99\n36.10\n37.80\n39.20\n45.03\n▁▂▇▆▁\n\n\n\n\n\n\n35.4.1.3 Descriptives\n\ncoronary %&gt;%\n  tbl_summary()\n\n\nTable 35.1: Coronary Data\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nN = 2001\n\n\n\n\nid\n2,244 (902, 3,347)\n\n\ncad\n\n\n\n    no cad\n163 (82%)\n\n\n    cad\n37 (19%)\n\n\nsbp\n126 (115, 144)\n\n\ndbp\n80 (72, 92)\n\n\nchol\n6.19 (5.39, 6.89)\n\n\nage\n47 (42, 53)\n\n\nbmi\n37.80 (36.10, 39.20)\n\n\nrace\n\n\n\n    malay\n73 (37%)\n\n\n    chinese\n64 (32%)\n\n\n    indian\n63 (32%)\n\n\ngender\n\n\n\n    woman\n100 (50%)\n\n\n    man\n100 (50%)\n\n\n\n\n1 Median (IQR); n (%)\n\n\n\n\n\n\n\n\n\n\n35.4.1.4 Pairs\n\ncoronary %&gt;%\n  select(-id) %&gt;% \n  ggpairs()\n\n\n\n\n\n\n\n\n35.4.1.5 Variance Inflation Factor (VIF)\nThe most common way to detect multicollinearity is by using the variance inflation factor (VIF), which measures the correlation and strength of correlation between the predictor variables in a regression model.\nThe value for VIF starts at 1 and has no upper limit. A general rule of thumb for interpreting VIFs is as follows:\n\nA value of 1 indicates there is no correlation between a given predictor variable and any other predictor variables in the model.\nA value between 1 and 5 indicates moderate correlation between a given predictor variable and other predictor variables in the model, but this is often not severe enough to require attention.\nA value greater than 5 indicates potentially severe correlation between a given predictor variable and other predictor variables in the model. In this case, the coefficient estimates and p-values in the regression output are likely unreliable.\n\n\nmodel &lt;- lm(chol ~ sbp + dbp + age + bmi, data = coronary)\nvif(model)\n\n     sbp      dbp      age      bmi \n3.395821 3.222173 1.215033 1.036817",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Elastic Net Regression Model</span>"
    ]
  },
  {
    "objectID": "lessons/06_elastic_net_regression.html#fitting-the-regression-mode",
    "href": "lessons/06_elastic_net_regression.html#fitting-the-regression-mode",
    "title": "\n35  Elastic Net Regression Model\n",
    "section": "\n35.5 Fitting the regression mode",
    "text": "35.5 Fitting the regression mode\nWe are interested in knowing the relationship between blood pressure (SBP and DBP), age, and BMI as the predictors and the cholesterol level (outcome).\n\n35.5.1 Ridge regression model\nIn R, we can use ridge regression using several packages, with glmnet being one of the most popular.\n\n# X and y datasets\nX &lt;- coronary %&gt;%\n  select(-id, -cad, -chol, -race, -gender) %&gt;%\n  as.matrix()\nX_train &lt;- X[1:150,]\nX_test &lt;- X[151:200,]\n\ny &lt;- coronary %&gt;%\n  select(chol) %&gt;%\n  as.matrix()\ny_train &lt;- y[1:150,]\ny_test &lt;- y[151:200,]\n\n# Fit the model\nridge_model &lt;- glmnet(X_train, y_train, alpha = 0)\n\n# Cross-validation\ncv_ridge &lt;- cv.glmnet(X_train, y_train, alpha = 0)\n\n# Optimal lambda\nridge_best_lambda &lt;- cv_ridge$lambda.min\n\n# Refit the model with the best lambda\nridge_model_best &lt;- glmnet(X_test, y_test, alpha = 0, lambda = ridge_best_lambda)\n\n# Coefficients\ncoef(ridge_model_best)\n\n5 x 1 sparse Matrix of class \"dgCMatrix\"\n                      s0\n(Intercept)  4.833796427\nsbp          0.016509767\ndbp          0.010086680\nage         -0.002821509\nbmi         -0.043286358\n\n# Predict values using the fitted model\nridge_pred &lt;- predict(ridge_model_best, newx = X_test)\n\n# Calculate RMSE\nridge_rmse &lt;- sqrt(mean((ridge_pred - y_test)^2))\nprint(ridge_rmse)\n\n[1] 1.00202\n\n# Manually predict values using the fitted model and RMSE\nridge_pred_manual &lt;- as.matrix(cbind(1, X_test)) %*% coef(ridge_model_best)\nsqrt(mean((y_test - ridge_pred_manual)^2))\n\n[1] 1.00202\n\n\n\n35.5.2 LASSO Regression\n\n# Fit the lasso regression model\nlasso_model &lt;- glmnet(X_train, y_train, alpha = 1) # alpha = 1 for lasso regression\n\n# Cross-validation\ncv_lasso &lt;- cv.glmnet(X_train, y_train, alpha = 1)\n\n# Optimal lambda\nlasso_best_lambda &lt;- cv_lasso$lambda.min # Lambda value with miniumum cross-validated error\n\n# Refit the model with the best lambda\nlasso_model_best &lt;- glmnet(X_test, y_test, alpha = 1, lambda = lasso_best_lambda)\n\ncoef(lasso_model_best)\n\n5 x 1 sparse Matrix of class \"dgCMatrix\"\n                      s0\n(Intercept)  4.451313138\nsbp          0.019033872\ndbp          0.005244015\nage          .          \nbmi         -0.034847216\n\n# Predict using the best lambda\nlasso_pred &lt;- predict(lasso_model_best, newx = X_test)\n\n# Calculate RMSE\nlasso_rmse &lt;- sqrt(mean((lasso_pred - y_test)^2))\nprint(lasso_rmse)\n\n[1] 1.002444\n\n# Manually predict values using the fitted model and RMSE\nlasso_pred_manual &lt;- as.matrix(cbind(1, X_test)) %*% coef(lasso_model_best)\nsqrt(mean((y_test - lasso_pred_manual)^2))\n\n[1] 1.002444\n\n\n\n35.5.3 Elastic Net Regresssion\n\n# Fit the elastic net regression model\nelastic_net_model &lt;- glmnet(X_train, y_train, alpha = 0.5) # alpha = 0.5 for elastic net\n\ncv_elastic &lt;-  cv.glmnet(X_train, y_train, alpha = 0.5)\n\nelastic_best_lambda &lt;-  cv_elastic$lambda.min\n\nelastic_model_best &lt;- glmnet(X_test, y_test, alpha = 0.5, lambda = elastic_best_lambda)\n\ncoef(elastic_model_best)\n\n5 x 1 sparse Matrix of class \"dgCMatrix\"\n                       s0\n(Intercept)  4.5572244811\nsbp          0.0186750459\ndbp          0.0065594280\nage         -0.0002822666\nbmi         -0.0389448093\n\n# Predict using the best lambda\nelastic_pred &lt;- predict(elastic_model_best, newx = X_test)\n\n# Calculate RMSE\nelastic_rmse &lt;- sqrt(mean((elastic_pred - y_test)^2))\nprint(elastic_rmse)\n\n[1] 1.001745\n\n# Manually predict values using the fitted model and RMSE\nelastic_pred_manual &lt;- as.matrix(cbind(1, X_test)) %*% coef(elastic_model_best)\nsqrt(mean((y_test - elastic_pred_manual)^2))\n\n[1] 1.001745",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Elastic Net Regression Model</span>"
    ]
  },
  {
    "objectID": "lessons/06_elastic_net_regression.html#results",
    "href": "lessons/06_elastic_net_regression.html#results",
    "title": "\n35  Elastic Net Regression Model\n",
    "section": "\n35.6 Results",
    "text": "35.6 Results\n\n35.6.1 Coefficients\n\nc1 &lt;- coef(ridge_model_best) %&gt;% as.matrix()\nc2 &lt;- coef(lasso_model_best) %&gt;% as.matrix()\nc3 &lt;- coef(elastic_model_best) %&gt;% as.matrix()\n\ncoef &lt;- cbind(c1, c2, c3)\ncolnames(coef) &lt;- c(\"Ridge\", \"LASSO\", \"Elastic Net\")\nprint(coef)\n\n                   Ridge        LASSO   Elastic Net\n(Intercept)  4.833796427  4.451313138  4.5572244811\nsbp          0.016509767  0.019033872  0.0186750459\ndbp          0.010086680  0.005244015  0.0065594280\nage         -0.002821509  0.000000000 -0.0002822666\nbmi         -0.043286358 -0.034847216 -0.0389448093\n\n\n\n35.6.2 RMSE\n\nrmse &lt;- cbind(\"RMSE\" = c(ridge_rmse, lasso_rmse, elastic_rmse)) \nrownames(rmse) &lt;- c(\"Ridge\", \"LASSO\", \"Elastic Net\")\nprint(rmse)\n\n                RMSE\nRidge       1.002020\nLASSO       1.002444\nElastic Net 1.001745",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Elastic Net Regression Model</span>"
    ]
  },
  {
    "objectID": "lessons/06_elastic_net_regression.html#summary-1",
    "href": "lessons/06_elastic_net_regression.html#summary-1",
    "title": "\n35  Elastic Net Regression Model\n",
    "section": "\n35.7 Summary",
    "text": "35.7 Summary\nIn this lecture, we went through the basic multicollinearity problem of liner regression model and trying yo solve it. We discussed different methods about this. We can solve the multicollinearity problem using ridge, LASSO and Elastic-net. We can also compare between them using both simulated data and real data based on different criteria. And, based on those we can say that which one can perform better than others.\n\n\n\n\nHoerl, Arthur E, and Robert W Kennard. 1970. “Ridge Regression: Biased Estimation for Nonorthogonal Problems.” Technometrics 12 (1): 55–67.\n\n\nHorel, AE. 1962. “Application of Ridge Analysis to Regression Problems.” Chemical Engineering Progress 58: 54–59.\n\n\nTibshirani, Robert. 1996. “Regression Shrinkage and Selection via the Lasso.” Journal of the Royal Statistical Society Series B: Statistical Methodology 58 (1): 267–88.\n\n\nZou, Hui, and Trevor Hastie. 2005. “Regularization and Variable Selection via the Elastic Net.” Journal of the Royal Statistical Society Series B: Statistical Methodology 67 (2): 301–20.",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Elastic Net Regression Model</span>"
    ]
  },
  {
    "objectID": "lessons/06_survival_analysis.html",
    "href": "lessons/06_survival_analysis.html",
    "title": "\n36  Survival Analysis in R\n",
    "section": "",
    "text": "36.1 What is survival analysis\nSurvival analysis is a statistical method for analyzing survival data (longitudinal time to event data). Now, before discussing the statistical terms, lets think about what survives, and what does it mean by survival. The answer that comes to our mind is, it’s the living being that survives, and the survival time means life span such that time between ones’s birth to death. That’s true, survival analysis was originally used solely for investigations of mortality and morbidity on vital registration statistics. Xian Lui (2012) noted the history of arithmetic analysis of survival process can be traced back to the 17th century when English statistician John Graunt first published the life table in 1662. Since the outcome occurs over the course of time, the data should be longitudinal data.Since then the survival analysis was widely used in clinical trials.\nOver the past 50 years, the literature notes the expanded applicability of survival analysis to the diverse fields like domain of biological science, biomedical science, engineering, and public health. Survival time therefore does not necessarily mean life span of living organisms, it can be start of drug to the first remission of the disease, diagnosis of the disease to the incidence of comorbidities, construction of a building to the collapse of the building, marriage to divorce, unemployment to the start of addiction drugs, occupational careers etc. In survival analysis the latter outcome is called an event, and the outcome of interest is the time to event.",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Survival Analysis in R</span>"
    ]
  },
  {
    "objectID": "lessons/06_survival_analysis.html#general-features-of-survival-data-structure",
    "href": "lessons/06_survival_analysis.html#general-features-of-survival-data-structure",
    "title": "\n36  Survival Analysis in R\n",
    "section": "\n36.2 General features of survival data structure",
    "text": "36.2 General features of survival data structure\n1.Survival process: The primary feature of the survival data is the description of a change in status from specified original status as the underlying outcome measure. For example, start of cancer drug to remission of the cancer. The survival probability is the probability that an individual survives from the time origin (e.g., diagnosis of diabetes) to a specified future time t.\n2.Time to event: It is calculated subtracting the specified starting time of the original status from the time of the occurrence of a particular event. It varies for different observations.\n3.Censoring: Censoring is defined as assigning an specific value to any observation whose information on specified event/outcome is missing. Any study has a specified time or survival data generally are collected for a particular interval in which the occurrence of a particular event is observed. Researchers therefore can only observe those events that occur within a surveillance window between two-time limits. Many observations may not encounter the event in the given time frame, or some observations may get lost before the specified time. Such observations whose information on specified event are missing are censored which indicates that event did not occur for those observations in the given time. Censoring may occur for various reasons. In clinical trials, patients may be lost to follow-up due to migration or health problems, in longitudinal survey, some baseline respondents may lose interest in participating etc. Since censoring frequently occurs, most of the survival analysis literally deals with incomplete survival data, and accordingly scientists have found ways to use such limited information for correctly analyzing the incomplete survival data based on some restrictive assumptions on the distribution of censored survival times. Given the importance of handling censoring in survival analysis, a variety of censoring types are possible as below:\na)Right censoring: The observations that are lost to follow-up or that do not encounter event during the specified study period, the actual event for such observations is placed somewhere to the right of the censored time along the time axis. This type of censoring is called right censoring. This type of censoring occurs most frequently in survival data. The basic assumption of this type of censoring is that the individual’s censored time is independent of the actual survival time, thereby making right censoring non-informative.\nb)Left censoring\nc)Interval censoring",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Survival Analysis in R</span>"
    ]
  },
  {
    "objectID": "lessons/06_survival_analysis.html#statistical-methods-used-in-survival-analysis",
    "href": "lessons/06_survival_analysis.html#statistical-methods-used-in-survival-analysis",
    "title": "\n36  Survival Analysis in R\n",
    "section": "\n36.3 Statistical methods used in survival analysis",
    "text": "36.3 Statistical methods used in survival analysis\n1) Kaplan-Meier (product limit)\nThe Kaplan–Meier (KM) method explores the survival of a population under investigation and/or tests differences in the crude cumulative survival between exposure groups, with a graphical representation of the endpoint occurrence as a function of time. It is a survival probability estimation method for non-parametric data. As events are assumed to occur independently of one another, the probabilities of surviving from one interval to the next may be multiplied together to give the cumulative survival probability. The KM survival curve, a plot of the KM survival probability against time, provides a useful summary of the data that can be used to estimate measures such as median survival time. The large skew encountered in the distribution of most survival data is the reason that the mean is not often used. It is called product limit approach because it estimates the survival probability each time an event occurs. (meaning it does not consider time as an interval/range (e.g., 5-10 years of age) but as a specific time (e.g., 5 years, 6 years, etc).\nImportant limitations of the KM method are\n\nIt does not provide an effect estimate (i.e., a relative risk) or the related confidence interval to compare the survival in different patient groups.\nIt does not permit the adjustment of confounders in etiological research or predictors in prognostic research.\nIt requires data categorization, so calculation of the incremental increase (or decrease) in the relative risk of a given event associated with one unit (or any number of units) increase in the candidate risk factor is not possible. These limitations can be approached by Cox regression analysis,\n\n2) Log rank test\nIt is a method of comparing survival function among groups (non-parametric test). It test the following hypothesis;\nHo: In terms of survivability, there is no difference between two groups.\nH1: There is a survival differential between the two groups.\nWe can reject the null hypothesis and infer that there is enough evidence to claim there is a difference in survival between the two groups if the p-value of the test is less than specified p-value which is generally 0.05 (95% confidence level).\n3) Cox proportional hazard regression\nThe Cox model is a regression technique for performing survival analyses. This model estimates the hazard ratio (HR) of a given endpoint associated with a specific risk factor which can be continous or categorical variable. The hazard is the probability that an individual who is under observation at a time t has an event at that time. It represents the instantaneous event rate for an individual who has already survived to time t. The hazard function for a particular time interval gives the probability that the subject will fail in that interval, given that the subject has not failed up to that point in time. In regression models for survival analysis, we attempt to estimate parameters which describe the relationship between our predictors and the hazard rate. It is called the proportional hazards model because it assumes that the effects of different variables on survival are constant over time and additive over a particular scale. When the risk factor is a continuous variable, the Cox model provides the HR of the study endpoint associated with a predefined unit of increase in the independent variable and when the risk factor is categorical variable, the Cox model provides HR of one group compared to another reference group.\nMathematical equation for Cox model\nHazard equation\nHazard = Probability of the event happening at time t given it hasn’t happened up until time t\n\\[\nH(t) = \\frac{p(\\text{Event} \\in [t, t + \\Delta t) | \\text{Event} &gt; t)}{\\Delta t}\n\\]\nCox Model Equation\n\\[\nH(t) = H_0(t)\\exp[B_1X_1 + B_2X_2 + ....B_kX_k]\n\\]\nWhere\n\n\n\\(X_1, \\ldots, X_k\\) represents the predictor variables and\n\n\\(H_0(t)\\) is the baseline hazard at time t, which is the hazard of an individual having the predictors set to zero.\n\n\\(B_1, \\ldots, B_k\\) represent regression coeffiecient\n\nBy computing the exponential of the regression coefficient \\(B_1, \\ldots, B_k\\) (directly provided by the software), we can calculate the HR of a given risk factor or predictor in the model. For example, if the risk factor \\(X_1\\) is dichotomous and it is codified “1” if present (exposed) and “0” if absent (unexposed), the expression \\(e^{B_i}\\) (where exp = 2.7183) can be interpreted as the estimated increase in the HR of the event in patients with the risk factor compared to those without the same risk factor; this is applied by assuming exposed and unexposed patients are similar for all the other covariates included in the model. If the risk factor is a continuous variable and it is directly related to the incidence rate of a given event (e.g., age in years as a risk factor for mortality), the HR will be interpreted as an increase in the hazard rate of death due to a 1-year increase in age.\nAssumption of Cox model\nHazard may fluctuate as a function of time, but the hazardous effects of different variables on survival are constant over time and additive over a particular scale.\n\\[\n\\frac{H(t)}{H_0} = \\text{constant}\n\\]\nWhere,\n\\(H(t)\\) = Increased hazard as a result of exposure.\n\\(H_0\\) = Baseline hazard in non-exposed",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Survival Analysis in R</span>"
    ]
  },
  {
    "objectID": "lessons/06_survival_analysis.html#packages",
    "href": "lessons/06_survival_analysis.html#packages",
    "title": "\n36  Survival Analysis in R\n",
    "section": "\n36.4 Packages",
    "text": "36.4 Packages\nThe packages required for conducting survival analysis can be installed automatically using the ctv packages. Following are the packages that will be functioning in this survival analysis project\nsurvival The survival package is the cornerstone of the entire R survival analysis. Not only is the package itself rich in features, but the object created by the Surv() function, which contains failure time and censoring information, is the basic survival analysis data structure in R.\nggfortify ggfortify enables producing handsome, one-line survival plots with ggplot2::autoplot",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Survival Analysis in R</span>"
    ]
  },
  {
    "objectID": "lessons/06_survival_analysis.html#load-the-data",
    "href": "lessons/06_survival_analysis.html#load-the-data",
    "title": "\n36  Survival Analysis in R\n",
    "section": "\n36.5 Load the data",
    "text": "36.5 Load the data\nThis project uses the veterans dataset contained in the survival package. Veteran dataset contains data from a two-treatment, randomized trial for lung cancer. We load the data with required packages into our library as follows:\n\nlibrary(survival)\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggfortify)",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Survival Analysis in R</span>"
    ]
  },
  {
    "objectID": "lessons/06_survival_analysis.html#checking-the-veteran-data",
    "href": "lessons/06_survival_analysis.html#checking-the-veteran-data",
    "title": "\n36  Survival Analysis in R\n",
    "section": "\n36.6 Checking the veteran data",
    "text": "36.6 Checking the veteran data\n\ndata(veteran)\n\nWarning in data(veteran): data set 'veteran' not found\n\nhead(veteran)\n\n  trt celltype time status karno diagtime age prior\n1   1 squamous   72      1    60        7  69     0\n2   1 squamous  411      1    70        5  64    10\n3   1 squamous  228      1    60        3  38     0\n4   1 squamous  126      1    60        9  63    10\n5   1 squamous  118      1    70       11  65    10\n6   1 squamous   10      1    20        5  49     0\n\n\nVeteran data contains following variables\ntrt: 1=standard 2=test\ncelltype: 1=squamous, 2=small cell, 3=adeno, 4=large\ntime: survival time in days after randomization\nstatus: censoring status. 1 = dead, 0 = censored\nkarno: Karnofsky performance score (100=good). For proper pronounciation and meaning of this score click here\ndiagtime: months from diagnosis to randomization\nage: in years\nprior: prior therapy 0 = no, 1 = yes",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Survival Analysis in R</span>"
    ]
  },
  {
    "objectID": "lessons/06_survival_analysis.html#kaplan-meier-analysis",
    "href": "lessons/06_survival_analysis.html#kaplan-meier-analysis",
    "title": "\n36  Survival Analysis in R\n",
    "section": "\n36.7 Kaplan Meier Analysis",
    "text": "36.7 Kaplan Meier Analysis\nWe first need to use Surv() to build the standard survival object. Note: a “+” after the time in the print out of the output indicates censored observation.\n\n# Kaplan Meier Survival curve\n\nkm &lt;- with(veteran, Surv(time, status))\nhead(km, 80)\n\n [1]  72  411  228  126  118   10   82  110  314  100+  42    8  144   25+  11 \n[16]  30  384    4   54   13  123+  97+ 153   59  117   16  151   22   56   21 \n[31]  18  139   20   31   52  287   18   51  122   27   54    7   63  392   10 \n[46]   8   92   35  117  132   12  162    3   95  177  162  216  553  278   12 \n[61] 260  200  156  182+ 143  105  103  250  100  999  112   87+ 231+ 242  991 \n[76] 111    1  587  389   33 \n\n\nNow to begin our analysis, we use the formula Surv() and the Surfit() function to produce the Kaplan-Meier estimates of the probability of survival over time. The times parameter of the summary() function gives some control over which times to print. Here, it is set to print the estimates for 1, 30, 60 and 90 days, and then every 90 days thereafter. This is the simplest possible model. It only takes three lines of R code to fit it, and produce numerical and graphical summaries.\n\nkm_fit &lt;- survfit(Surv(time, status) ~ 1, data=veteran)\nsummary(km_fit, times = c(1,30,60,90*(1:10)))\n\nCall: survfit(formula = Surv(time, status) ~ 1, data = veteran)\n\n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    1    137       2    0.985  0.0102      0.96552       1.0000\n   30     97      39    0.700  0.0392      0.62774       0.7816\n   60     73      22    0.538  0.0427      0.46070       0.6288\n   90     62      10    0.464  0.0428      0.38731       0.5560\n  180     27      30    0.222  0.0369      0.16066       0.3079\n  270     16       9    0.144  0.0319      0.09338       0.2223\n  360     10       6    0.090  0.0265      0.05061       0.1602\n  450      5       5    0.045  0.0194      0.01931       0.1049\n  540      4       1    0.036  0.0175      0.01389       0.0934\n  630      2       2    0.018  0.0126      0.00459       0.0707\n  720      2       0    0.018  0.0126      0.00459       0.0707\n  810      2       0    0.018  0.0126      0.00459       0.0707\n  900      2       0    0.018  0.0126      0.00459       0.0707\n\n#plot(km_fit, xlab=\"Days\", main = 'Kaplan Meyer Plot') #base graphics is always ready\nautoplot(km_fit)",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Survival Analysis in R</span>"
    ]
  },
  {
    "objectID": "lessons/06_survival_analysis.html#survival-curves-by-treatment",
    "href": "lessons/06_survival_analysis.html#survival-curves-by-treatment",
    "title": "\n36  Survival Analysis in R\n",
    "section": "\n36.8 Survival curves by treatment",
    "text": "36.8 Survival curves by treatment\n\nkm_trt_fit &lt;- survfit(Surv(time, status) ~ trt, data=veteran)\nautoplot(km_trt_fit)\n\n\n\n\n\n\n\nNow to show one more small exploratory plot, we will create a new data frame to look at survival by age. First, we will create a new data frame with a categorical variable AG that has values LT60 and GT60, which respectively describe veterans younger and older than sixty. And we make trt and prior into factor variables.\n\nvet &lt;- mutate(\n  veteran,\n  AG = ifelse((age &lt; 60), \"LT60\", \"OV60\"),\n  AG = factor(AG),\n  trt = factor(trt,labels=c(\"standard\",\"test\")),\n  prior = factor(prior,labels=c(\"N0\",\"Yes\"))\n)\n\nkm_AG_fit &lt;- survfit(Surv(time, status) ~ AG, data=vet)\nautoplot(km_AG_fit)\n\n\n\n\n\n\n\nAlthough the two curves appear to overlap in the first fifty days, younger patients clearly have a better chance of surviving more than a year.",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Survival Analysis in R</span>"
    ]
  },
  {
    "objectID": "lessons/06_survival_analysis.html#conducting-log-rank-test",
    "href": "lessons/06_survival_analysis.html#conducting-log-rank-test",
    "title": "\n36  Survival Analysis in R\n",
    "section": "\n36.9 Conducting log-rank test",
    "text": "36.9 Conducting log-rank test\n\nlibrary(\"survival\")\nlibrary(\"survminer\")\n\nLoading required package: ggpubr\n\n\n\nAttaching package: 'survminer'\n\n\nThe following object is masked from 'package:survival':\n\n    myeloma\n\nlibrary(\"Rcpp\")\n\nFirst lets look at the summary table of the survival curve\n\nsummary(km_trt_fit)$table\n\n      records n.max n.start events    rmean se(rmean) median 0.95LCL 0.95UCL\ntrt=1      69    69      69     64 123.9282  14.84352  103.0      59     132\ntrt=2      68    68      68     64 142.0613  26.81071   52.5      44      95\n\n\nNow for better visualization with p-value, we are using following plot\n\nggsurvplot(\n  km_trt_fit,\n  pval = TRUE, conf.int = TRUE,\n  risk.table = TRUE, # Add risk table\n  risk.table.col = \"strata\", # Change risk table color by groups\n  linetype = \"strata\", # Change line type by groups\n  surv.median.line = \"hv\", # Specify median survival\n  ggtheme = theme_bw(), # Change ggplot2 theme\n  palette = c(\"#E7B800\", \"#2E9FDF\")\n)\n\nWarning in geom_segment(aes(x = 0, y = max(y2), xend = max(x1), yend = max(y2)), : All aesthetics have length 1, but the data has 2 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 2 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 2 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 2 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\nP-value is 0.93 which indicates that there is no significant difference between treatment 1 and 2 for survivability outcome.\nThe survival chance is 1.0 at time zero (or 100 percent of the participants are alive).\nAt time 250, the chances of survival for both trt=1 and tr2=2 are about 0.13 (or 13 percent).\nThe median survival time for trt=2 is about 60 days and for trt=1 is about 100 days, indicating that trt=1 has a better survival rate than trt=2, however the difference is not statistically significant\nThe following code shows how to perform a log-rank test to determine if there is a difference in survival between trt groups who received different treatments:\n\nsurv_diff &lt;- survdiff(Surv(time, status) ~ trt, data = veteran)\nsurv_diff\n\nCall:\nsurvdiff(formula = Surv(time, status) ~ trt, data = veteran)\n\n       N Observed Expected (O-E)^2/E (O-E)^2/V\ntrt=1 69       64     64.5   0.00388   0.00823\ntrt=2 68       64     63.5   0.00394   0.00823\n\n Chisq= 0  on 1 degrees of freedom, p= 0.9 \n\n\nThe Chi-Squared test statistic is 0 with 1 degree of freedom and the corresponding p-value is 0.9. Since this p-value is greater than 0.05, we cannot reject the null hypothesis.\nIn other words, we do not have sufficient evidence to say that there is a statistically significant difference in survival between the two treatment groups.",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Survival Analysis in R</span>"
    ]
  },
  {
    "objectID": "lessons/06_survival_analysis.html#cox-proportional-hazard-models",
    "href": "lessons/06_survival_analysis.html#cox-proportional-hazard-models",
    "title": "\n36  Survival Analysis in R\n",
    "section": "\n36.10 Cox Proportional Hazard Models",
    "text": "36.10 Cox Proportional Hazard Models\nNow we will fit a Cox Proportional Hazards model that makes use of all of the covariates in the data set.\n\n# Fit the Cox Model\ncox &lt;- coxph(\n  Surv(time, status) ~ trt + celltype + karno + diagtime + age + prior,\n  data = vet\n)\nsummary(cox)\n\nCall:\ncoxph(formula = Surv(time, status) ~ trt + celltype + karno + \n    diagtime + age + prior, data = vet)\n\n  n= 137, number of events= 128 \n\n                        coef  exp(coef)   se(coef)      z Pr(&gt;|z|)    \ntrttest            2.946e-01  1.343e+00  2.075e-01  1.419  0.15577    \ncelltypesmallcell  8.616e-01  2.367e+00  2.753e-01  3.130  0.00175 ** \ncelltypeadeno      1.196e+00  3.307e+00  3.009e-01  3.975 7.05e-05 ***\ncelltypelarge      4.013e-01  1.494e+00  2.827e-01  1.420  0.15574    \nkarno             -3.282e-02  9.677e-01  5.508e-03 -5.958 2.55e-09 ***\ndiagtime           8.132e-05  1.000e+00  9.136e-03  0.009  0.99290    \nage               -8.706e-03  9.913e-01  9.300e-03 -0.936  0.34920    \npriorYes           7.159e-02  1.074e+00  2.323e-01  0.308  0.75794    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n                  exp(coef) exp(-coef) lower .95 upper .95\ntrttest              1.3426     0.7448    0.8939    2.0166\ncelltypesmallcell    2.3669     0.4225    1.3799    4.0597\ncelltypeadeno        3.3071     0.3024    1.8336    5.9647\ncelltypelarge        1.4938     0.6695    0.8583    2.5996\nkarno                0.9677     1.0334    0.9573    0.9782\ndiagtime             1.0001     0.9999    0.9823    1.0182\nage                  0.9913     1.0087    0.9734    1.0096\npriorYes             1.0742     0.9309    0.6813    1.6937\n\nConcordance= 0.736  (se = 0.021 )\nLikelihood ratio test= 62.1  on 8 df,   p=2e-10\nWald test            = 62.37  on 8 df,   p=2e-10\nScore (logrank) test = 66.74  on 8 df,   p=2e-11",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Survival Analysis in R</span>"
    ]
  },
  {
    "objectID": "lessons/06_survival_analysis.html#plot-the-cox-model",
    "href": "lessons/06_survival_analysis.html#plot-the-cox-model",
    "title": "\n36  Survival Analysis in R\n",
    "section": "\n36.11 Plot the Cox model",
    "text": "36.11 Plot the Cox model\n\ncox_fit &lt;- survfit(cox)\n#plot(cox_fit, main = \"cph model\", xlab=\"Days\")\nautoplot(cox_fit)\n\n\n\n\n\n\n\nNote that the model flags small cell type, adeno cell type and karno as significant. However, some caution needs to be exercised in interpreting these results. While the Cox Proportional Hazard’s model is thought to be “robust”, a careful analysis would check the assumptions underlying the model. For example, the Cox model assumes that the covariates do not vary with time. In a vignette that accompanies the Survival package Therneau, Crowson and Atkinson demonstrate that the Karnofsky score (karno) is, in fact, time-dependent so the assumptions for the Cox model are not met. The vignette authors have presented a strategy for dealing with time dependent covariates.\nData scientists who are accustomed to computing ROC curves to assess model performance should be interested in the Concordance statistic. The documentation for the survConcordance() function in the Survival package defines concordance as “the probability of agreement for any two randomly chosen observations, where in this case agreement means that the observation with the shorter survival time of the two also has the larger risk score. The predictor (or risk score) will often be the result of a Cox model or other regression” and notes that: “For continuous covariates concordance is equivalent to Kendall’s tau, and for logistic regression it is equivalent to the area under the ROC curve.”.\nTo demonstrate using the Survival package, along with ggplot2 and ggfortify, here we will fit Aalen’s additive regression model for censored data to the veteran data. To further understand AA regression please click here. The documentation states: The Aalen model assumes that the cumulative hazard H(t) for a subject can be expressed as\na(t) + X B(t), where\na(t) is a time-dependent intercept term, X is the vector of covariates for the subject (possibly time-dependent), and B(t) is a time-dependent matrix of coefficients.”\nThe plots show how the effects of the covariates change over time. We can see the steep slope and then abrupt change in slope of karno.\n\naa_fit &lt;- aareg(\n  Surv(time, status) ~ trt + celltype + karno + diagtime + age + prior, \n  data = vet\n)\naa_fit\n\nCall:\naareg(formula = Surv(time, status) ~ trt + celltype + karno + \n    diagtime + age + prior, data = vet)\n\n  n= 137 \n    75 out of 97 unique event times used\n\n                      slope      coef se(coef)      z        p\nIntercept          0.083400  3.81e-02 1.09e-02  3.490 4.79e-04\ntrttest            0.006730  2.49e-03 2.58e-03  0.967 3.34e-01\ncelltypesmallcell  0.015000  7.30e-03 3.38e-03  2.160 3.09e-02\ncelltypeadeno      0.018400  1.03e-02 4.20e-03  2.450 1.42e-02\ncelltypelarge     -0.001090 -6.21e-04 2.71e-03 -0.229 8.19e-01\nkarno             -0.001180 -4.37e-04 8.77e-05 -4.980 6.28e-07\ndiagtime          -0.000243 -4.92e-05 1.64e-04 -0.300 7.65e-01\nage               -0.000246 -6.27e-05 1.28e-04 -0.491 6.23e-01\npriorYes           0.003300  1.54e-03 2.86e-03  0.539 5.90e-01\n\nChisq=41.62 on 8 df, p=1.6e-06; test weights=aalen",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Survival Analysis in R</span>"
    ]
  },
  {
    "objectID": "lessons/06_survival_analysis.html#plot-the-aalens-addictive-regression-model",
    "href": "lessons/06_survival_analysis.html#plot-the-aalens-addictive-regression-model",
    "title": "\n36  Survival Analysis in R\n",
    "section": "\n36.12 Plot the Aalen’s Addictive Regression model",
    "text": "36.12 Plot the Aalen’s Addictive Regression model\n\n#summary(aa_fit)  # provides a more complete summary of results\nautoplot(aa_fit)\n\n\n\n\n\n\n\n\n36.12.1 Interpretation of AA model graph\nLooking at this plot we can see that only karno is the time dependent variable because the graph is steeply below 0. Now with this knowledge, we can re-fit our model.",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Survival Analysis in R</span>"
    ]
  },
  {
    "objectID": "lessons/06_survival_analysis.html#references",
    "href": "lessons/06_survival_analysis.html#references",
    "title": "\n36  Survival Analysis in R\n",
    "section": "\n36.13 References",
    "text": "36.13 References\nLiu, X. (Xian X. ). (2012). Survival analysis models and applications. Wiley/Higher Education Press.\nAbd ElHafeez, S., D’Arrigo, G., Leonardis, D., Fusaro, M., Tripepi, G., & Roumeliotis, S. (2021). Methods to Analyze Time-to-Event Data: The Cox Regression Analysis. Oxidative medicine and cellular longevity, 2021, 1302811. https://doi.org/10.1155/2021/1302811\nAzzato, E., Greenberg, D., Shah, M. et al. Prevalent cases in observational studies of cancer survival: do they bias hazard ratio estimates?. Br J Cancer 100, 1806–1811 (2009). https://doi.org/10.1038/sj.bjc.6605062",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Survival Analysis in R</span>"
    ]
  },
  {
    "objectID": "lessons/06_structural_equation_models.html",
    "href": "lessons/06_structural_equation_models.html",
    "title": "\n37  Introduction to Structural Equation Modeling\n",
    "section": "",
    "text": "37.1 What is SEM?\nStructural Equation Modeling (SEM) is a robust statistical approach that allows researchers to examine complex relationships between variables. It provides a theoretical framework that effectively combine series of statistical analyses techniques, including regression analysis, path analysis, and factor analysis, to test hypotheses of complex relationships in our data.",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Introduction to Structural Equation Modeling</span>"
    ]
  },
  {
    "objectID": "lessons/06_structural_equation_models.html#what-is-sem",
    "href": "lessons/06_structural_equation_models.html#what-is-sem",
    "title": "\n37  Introduction to Structural Equation Modeling\n",
    "section": "",
    "text": "37.1.1 Advantages of using SEM\n\nSimultaneous Analysis: SEM can analyze multiple variables and their relationships simultaneously, unlike traditional regression, which typically handles one dependent variable at a time.\nLatent Variables: SEM allows the inclusion of latent variables in the analysis. Testing the hypotheses using the latent variables rather than the observed sum-scores of the indicator, frees the analysis of measurement error because the error are estimated and removed. SEM has the ability to test construct-level hypotheses at a construct level.\nFlexibility: It can model complex and multidimensional relationships, including mediation and moderation effects.\nImproved statistical estimation and remove/reduces random error.\n\n37.1.2 Terminology in SEM\nBefore we proceed with the presentation, let’s briefly define some of the terms typically used in SEM context.\n\nobserved/measured/indicator/manifest variable: a variable that exists in the data. Example, items in the questionnaire for measuring the latent variable.\nlatent/factor variable: variable that are not directly measured. They are constructed and does not exist in the data.\nexogenous variable: This is the independent variable. It is either observed or latent variable that influence the endogenous variable.\nendogenous variable: analogous to dependent variables used in traditional analyses, representing outcomes or effects. It either observed or latent variable that has a causal path leading to it.\nDisturbances: refers to residuals",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Introduction to Structural Equation Modeling</span>"
    ]
  },
  {
    "objectID": "lessons/06_structural_equation_models.html#fundamental-mathematical-components-of-sem-model",
    "href": "lessons/06_structural_equation_models.html#fundamental-mathematical-components-of-sem-model",
    "title": "\n37  Introduction to Structural Equation Modeling\n",
    "section": "\n37.2 Fundamental Mathematical Components of SEM Model",
    "text": "37.2 Fundamental Mathematical Components of SEM Model\n\nMeasurement Model: The measurement model is the part of SEM that specifies the relationship between the indicator/measured/observed variable and the latent/unobserved construct. It is typically validated using Confirmatory Factor Analysis (CFA).It is the bases for evaluating the adequacy of the measurement quality of the underlying construct.\nStructural Model: The structural model specifies the causal relationships between the latent variables or other variables. It represents the hypothesized directions of influence and the strength of these relationships.\n\n\n37.2.1 Exogenous measurement variable:\nExogenous measurement variable: $ $\n\n\n\\(\\mathbf{x} =(x_1, \\cdots, x_q)’\\): Vector of exogenous indicators.\n\n\\(\\mathbf{\\tau_x}\\): vector of intercepts for exogenous indicators\n\n\\(\\mathbf{\\Lambda_x}\\): Matrix of factor loadings corresponding to the latent exogenous variables.\n\n\\(\\mathbf{\\xi}\\): Vector of latent exogenous variables.\n\n\\(\\mathbf{\\delta}= ( \\delta_1, \\cdots, \\delta_q)’\\): Vector of residuals for exogenous variables. #- \\(\\mathbf{\\theta_{\\delta}}\\): Variance or covariance of residuals for exogenous indicators.#\n\n37.2.2 Endogenous measurement variable:\nEndogenous measurement variable: $ $\n\n\n\\(\\mathbf{y} = (y_1, \\cdots, y_p)’\\): Vector of endogenous indicators.\n\n\\(\\mathbf{\\tau_y}\\): vector of intercepts for endogenous indicators\n\n\\(\\mathbf{\\Lambda_y}\\): Matrix of factor loadings corresponding to the latent endogenous variables.\n\n\\(\\mathbf{\\eta}\\): Vector of latent endogenous variables.\n\n\\(\\mathbf{\\epsilon}= ( \\epsilon_1, \\cdots,\\epsilon_p)’\\): Vector of residuals for endogenous variables.\n\n\\(\\mathbf{\\theta_{\\epsilon}}\\): Variance or covariance of residuals for endogenous indicators.\n\n37.2.3 Structural equation definition\nStructural variable: $ $\n\n\n\\(\\mathbf{\\alpha}\\): a vector of intercepts.\n\n\\(B\\): Matrix of regression coefficients for the endogenous variables.\n\n\\(\\Gamma\\): Matrix of regression coefficients for the exogenous variables.\n\n\\(ξ\\): Vector of latent exogenous variables.#\n\n\\(\\zeta= ( \\zeta_1, \\cdots, \\zeta_m)’\\): Vector of disturbances (residuals) for endogenous variables.\n\nNote: The structural regression links the measurement and structural models",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Introduction to Structural Equation Modeling</span>"
    ]
  },
  {
    "objectID": "lessons/06_structural_equation_models.html#assumptions-in-sem",
    "href": "lessons/06_structural_equation_models.html#assumptions-in-sem",
    "title": "\n37  Introduction to Structural Equation Modeling\n",
    "section": "\n37.3 Assumptions in SEM",
    "text": "37.3 Assumptions in SEM\n\nLinearity: SEM assumes that the relationship between endogenous (dependent) and exogenous (independent) variables are linear. This can be assessed using scatter plots and residual analysis.\nNormality: Indicators for latent variables follow a normal distribution. This can be checked by estimating skewness and kurtosis (skewness ≤ 2 and kurtosis ≤ 7).\nMulticollinearity: SEM assumes that Predictors are not perfectly correlated. We can Assess multicollinearity among the indicators using Variance Inflation Factor (VIF).\nMissing Data: variables in study should be complete. Examine and handle missing data.",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Introduction to Structural Equation Modeling</span>"
    ]
  },
  {
    "objectID": "lessons/06_structural_equation_models.html#intalling-required-packages",
    "href": "lessons/06_structural_equation_models.html#intalling-required-packages",
    "title": "\n37  Introduction to Structural Equation Modeling\n",
    "section": "\n37.4 Intalling required packages",
    "text": "37.4 Intalling required packages\nFirst I will begin the SEM analysis by installing and loading the R packages that I will utilize in the example.\n\n#Install.packages(\"lavaan\") #to estimate the SEM model\n#install.packages(\"semPlot\") #for plotting the path diagram\n#install.packages(\"seminr\") # contains the dataset for the analysis\n#install.packages(\"car\") #use to compute Variance Inflation Factors (VIF) for multicollinearity check\n#install.packages(\"MVN\") # for multivariate normality check\n#install.packages(\"tidyverse\")\nlibrary(lavaan)\nlibrary(seminr)\nlibrary(semPlot)\nlibrary(ggplot2) \nlibrary(MVN)\nlibrary(car)\n#library(corrplot)\nlibrary(tidyverse)",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Introduction to Structural Equation Modeling</span>"
    ]
  },
  {
    "objectID": "lessons/06_structural_equation_models.html#data-source-and-description",
    "href": "lessons/06_structural_equation_models.html#data-source-and-description",
    "title": "\n37  Introduction to Structural Equation Modeling\n",
    "section": "\n37.5 Data source and description",
    "text": "37.5 Data source and description\nFor our demonstration, we will be using the mobi dataframe found in the seminr package. The dataset is used as measurement instrument for the European Customer Satisfaction Index (ECSI) adapted to the mobile phone market. The data contain 250 observations of 24 latent variables. Our research question: Does customer’s perceived quality of services impact their satisfaction with the product?\nThe variables of interest for the analysis are:\nExogenous variable: Perceived Quality measured with a 7-item survey question.\n\nPERQ1 - Overall perceived quality\n\nPERQ2 - Technical quality of the network\n\nPERQ3 - Customer service and personal advice offered\n\nPERQ4 - Quality of the services you use\n\nPERQ5 - Range of services and products offered\n\nPERQ6 - Reliability and accuracy of the products and services provided\n\nPERQ7 - Clarity and transparency of information provided\n\n\nEndogenous variable: Satisfaction with phone service provider measured with a 3-item survey questions.\n\nCUSA1 - Overall satisfaction\n\nCUSA2 - Fulfillment of expectations\n\nCUSA3 - How well do you think \"your mobile phone provider\" compares with your ideal mobile phone provider?\n\n\n\n37.5.1 Conceptual diagram of the demonstration SEM model\nTo explain further on the relationship of the variables included in this SEM demonstration, a conceptual diagram of the model is shown below:\n\nFig.1: Observed/indicator/item/measured variables are depicted by rectangles, latent/unobserved variables are represented by ovals, exogenous variables sends a one-headed arrow to other variables, exogenous variable receives one-headed arrows from other variables, variance are depicted with two-headed arrow from the variable to itself and covariance (not specified in this diagram) are depicted with two-headed arrow from one variable to another.\n\n37.5.2 Steps in SEM\n\nModel Specification: Define the theoretical model you want to test, that is, the hypothesized relationships between variables (fig. 1)\n\nModel Identification: It involves checking if there is enough information with the available data to estimate the parameters. An identified model is one that is estimable. Your analysis will not run if your model is not identified. The degree of freedom of the model tells us if the model is under-identified, exactly/just-identified or over-identified.\nParameter Estimation: comparing actual and estimated covariance (i.e., maximum likelihood estimate)\nModel Evaluation: Assess how well the model fits the data using various goodness of fit indices (i.e., Chi-square test, Root Mean Square Error of Approximation (RMSEA), Comparative Fit Index (CFI), Tucker Lewis Index (TLI), e.t.c.)\n\n\n37.5.2.1 Notes on Model Identification\nThe goal is to maximize the degrees of freedom (df).\n\ndegrees of freedom \\(&lt; 0\\) (under-identified, bad)\ndegrees of freedom \\(= 0\\) (just identified or saturated, neither bad nor good)\ndegrees of freedom \\(&gt; 0\\) (over-identified, good)\n\nModel degrees of freedom (df) is calculated using the formula: \\[\n\\mbox{df} = \\mbox{number of known values } – \\mbox{ number of free parameters}\n\\] Where:\n\n\n\\(p\\) = number of observed variables (items in survey)\n\n\\(p(p+1)/2\\) is the number of known values\n\n\\(\\mbox{number of free parameters}\\) = number of (unique) model parameters minus the number of fixed parameters\n\nFor example, to calculate the degree of freedom of our SEM model:\n\nnumber of observed variables (\\(p\\)) = 10\nnumber of free parameters = \\(23 - 2 = 21\\)\n\n\nTherefore, \\[\n\\mbox{df} = 10(10+1)/2 – \\mbox{21} = 55 - 21 = 34.\n\\] Since \\[\n\\mbox{df} = 34 \\ ({df} &gt; 0),\n\\] our model is overidentified.\n\n37.5.2.2 Criteria for Model Fit Evaluation\nCutoff criteria of the common fit indices:\n\nModel chi-square (\\(\\chi^2\\)): We want to observe a non-significant chi-square \\(p &gt; .05\\). This indicates good fit. However, Model chi-square is highly sensitive sample size. So, it is often not considered as a reasonable measure of fit, especially with large sample.\nCFI and TLI: values greater than 0.90, conservatively 0.95 indicate good fit\nRMSEA:\n\n\n\\(\\le 0.05\\) : good-fit\n0.05 - 0.08 : reasonable approximate fit\n\n\\(&gt;= 0.10\\) : poor fit\n\n\nModel Modification: Post hoc model modification indexes gives suggestions about ways to adjust the model if necessary based on the fit indices and theoretical considerations.",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Introduction to Structural Equation Modeling</span>"
    ]
  },
  {
    "objectID": "lessons/06_structural_equation_models.html#performing-the-sem",
    "href": "lessons/06_structural_equation_models.html#performing-the-sem",
    "title": "\n37  Introduction to Structural Equation Modeling\n",
    "section": "\n37.6 Performing the SEM",
    "text": "37.6 Performing the SEM\n\n#| label: datasets \n#| message: false\n#| warning: false\n\n# retrieve the dataset from the seminr package\ndata(\"mobi\")\n\n# View documentation\n#help(mobi)\n\n#colnames(mobi)\n#head(mobi)\n\n#create a new dataset that contains only variable needed in for our analysis \n\nassum_dat &lt;- mobi %&gt;% \n  select(PERQ1:PERQ7, CUSA1:CUSA3)\n\n# Check for missing values in the entire data frame\nis_na &lt;- is.na(assum_dat)\nmissing_per_column &lt;- colSums(is_na)\nprint(missing_per_column)\n\nPERQ1 PERQ2 PERQ3 PERQ4 PERQ5 PERQ6 PERQ7 CUSA1 CUSA2 CUSA3 \n    0     0     0     0     0     0     0     0     0     0 \n\n# Specifying the model\nmodel1.fit &lt;- \"\n# measurement model\nQuality =~ PERQ1 + PERQ2 + PERQ3 + PERQ4 + PERQ5 + PERQ6 + PERQ7\nSatisfaction =~ CUSA1 + CUSA2 + CUSA3 \n\n#structural model\nSatisfaction ~ Quality\n\"\n# model estimation and identification of model fit\n fit_sem &lt;- sem(\n   model1.fit, \n   data=assum_dat\n )\n\n#use the summary function to produce the result summaries\n summary(fit_sem, \n         fit.measures = T, #include the fit indices\n         standardized=TRUE\n )\n\nlavaan 0.6.17 ended normally after 26 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        21\n\n  Number of observations                           250\n\nModel Test User Model:\n                                                      \n  Test statistic                               105.123\n  Degrees of freedom                                34\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              1286.661\n  Degrees of freedom                                45\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.943\n  Tucker-Lewis Index (TLI)                       0.924\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -4176.834\n  Loglikelihood unrestricted model (H1)      -4124.272\n                                                      \n  Akaike (AIC)                                8395.668\n  Bayesian (BIC)                              8469.619\n  Sample-size adjusted Bayesian (SABIC)       8403.047\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.091\n  90 Percent confidence interval - lower         0.072\n  90 Percent confidence interval - upper         0.112\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.840\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.045\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Quality =~                                                            \n    PERQ1             1.000                               1.093    0.770\n    PERQ2             0.995    0.109    9.103    0.000    1.087    0.576\n    PERQ3             1.244    0.102   12.209    0.000    1.359    0.747\n    PERQ4             1.084    0.093   11.670    0.000    1.185    0.719\n    PERQ5             0.905    0.082   10.983    0.000    0.989    0.682\n    PERQ6             1.054    0.092   11.468    0.000    1.151    0.708\n    PERQ7             1.278    0.103   12.429    0.000    1.396    0.759\n  Satisfaction =~                                                       \n    CUSA1             1.000                               0.865    0.702\n    CUSA2             1.548    0.141   10.983    0.000    1.339    0.760\n    CUSA3             1.510    0.139   10.839    0.000    1.306    0.749\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Satisfaction ~                                                        \n    Quality           0.759    0.070   10.838    0.000    0.959    0.959\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .PERQ1             0.819    0.086    9.542    0.000    0.819    0.407\n   .PERQ2             2.381    0.224   10.628    0.000    2.381    0.668\n   .PERQ3             1.460    0.150    9.761    0.000    1.460    0.442\n   .PERQ4             1.314    0.132    9.984    0.000    1.314    0.484\n   .PERQ5             1.126    0.110   10.211    0.000    1.126    0.535\n   .PERQ6             1.320    0.131   10.056    0.000    1.320    0.499\n   .PERQ7             1.436    0.149    9.655    0.000    1.436    0.424\n   .CUSA1             0.768    0.079    9.669    0.000    0.768    0.507\n   .CUSA2             1.312    0.146    8.962    0.000    1.312    0.423\n   .CUSA3             1.335    0.146    9.126    0.000    1.335    0.439\n    Quality           1.194    0.170    7.004    0.000    1.000    1.000\n   .Satisfaction      0.060    0.035    1.691    0.091    0.080    0.080",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Introduction to Structural Equation Modeling</span>"
    ]
  },
  {
    "objectID": "lessons/06_structural_equation_models.html#check-assumptions",
    "href": "lessons/06_structural_equation_models.html#check-assumptions",
    "title": "\n37  Introduction to Structural Equation Modeling\n",
    "section": "\n37.7 Check Assumptions",
    "text": "37.7 Check Assumptions\n\n#| label: datasets \n#| message: false\n#| warning: false\n\n# (1) Linearity:\n\n#First, we extract the factors scores of the latent variables  \n\n fac_scores &lt;- data.frame(\n   lavPredict(fit_sem)\n )\n\n#Plotting the scatter plot with the using extracted factor scores \n\n with(\n   fac_scores,\n   plot(Quality,Satisfaction)\n )\n\n\n\n\n\n\n#test linearity using ggplot\n\n  ggplot(fac_scores, \n    aes(Quality,Satisfaction)) + \n      geom_point(size = 1) +\n        geom_smooth(method=`loess`\n    )\n\n\n\n\n\n\n#2. # Check multivariate normality \n\n mvn(assum_dat, \n     mvnTest = \"mardia\")\n\n$multivariateNormality\n             Test        Statistic               p value Result\n1 Mardia Skewness 1208.97674897777 3.73215048046629e-136     NO\n2 Mardia Kurtosis 39.0584746716299                     0     NO\n3             MVN             &lt;NA&gt;                  &lt;NA&gt;     NO\n\n$univariateNormality\n               Test  Variable Statistic   p value Normality\n1  Anderson-Darling   PERQ1      7.1534  &lt;0.001      NO    \n2  Anderson-Darling   PERQ2      6.2090  &lt;0.001      NO    \n3  Anderson-Darling   PERQ3      6.6901  &lt;0.001      NO    \n4  Anderson-Darling   PERQ4      8.0496  &lt;0.001      NO    \n5  Anderson-Darling   PERQ5      6.7105  &lt;0.001      NO    \n6  Anderson-Darling   PERQ6      6.6768  &lt;0.001      NO    \n7  Anderson-Darling   PERQ7      6.7429  &lt;0.001      NO    \n8  Anderson-Darling   CUSA1     10.3268  &lt;0.001      NO    \n9  Anderson-Darling   CUSA2      4.7763  &lt;0.001      NO    \n10 Anderson-Darling   CUSA3      5.7320  &lt;0.001      NO    \n\n$Descriptives\n        n  Mean  Std.Dev Median Min Max 25th 75th       Skew  Kurtosis\nPERQ1 250 7.944 1.421600      8   2  10    7    9 -0.7203890 1.1786363\nPERQ2 250 7.192 1.891414      7   1  10    6    8 -0.8186657 0.7287376\nPERQ3 250 7.700 1.821888      8   1  10    7    9 -0.9064463 0.8310284\nPERQ4 250 7.916 1.651622      8   1  10    7    9 -1.0704382 1.7901560\nPERQ5 250 7.872 1.453294      8   3  10    7    9 -0.6770313 0.5583053\nPERQ6 250 7.776 1.629862      8   1  10    7    9 -0.9743906 1.7870540\nPERQ7 250 7.592 1.843674      8   1  10    7    9 -0.9593571 1.0528448\nCUSA1 250 7.988 1.233671      8   4  10    7    9 -0.2201919 0.1527890\nCUSA2 250 7.128 1.765242      7   1  10    6    8 -0.5471886 0.5579660\nCUSA3 250 7.316 1.747099      7   1  10    7    8 -0.6671235 0.9615690\n\n#3. Check Multicollinearity\n\nmodel_vif &lt;- lm(PERQ1 ~ PERQ2 + PERQ3 + PERQ4 + PERQ5 + PERQ6 + PERQ7 + CUSA1 + CUSA2 + CUSA3, data = assum_dat) \n\nvif(model_vif)\n\n   PERQ2    PERQ3    PERQ4    PERQ5    PERQ6    PERQ7    CUSA1    CUSA2 \n1.486854 2.227753 2.130806 1.794253 1.936840 2.362554 1.741678 2.045506 \n   CUSA3 \n2.123181",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Introduction to Structural Equation Modeling</span>"
    ]
  },
  {
    "objectID": "lessons/06_structural_equation_models.html#visualization-of-the-sem-path-diagram",
    "href": "lessons/06_structural_equation_models.html#visualization-of-the-sem-path-diagram",
    "title": "\n37  Introduction to Structural Equation Modeling\n",
    "section": "\n37.8 Visualization of the SEM path diagram",
    "text": "37.8 Visualization of the SEM path diagram\nPath diagram provides a graphical representation of the structural relationships, including causality, variances and covariances, between the observed and latent variables in our SEM model.\n\n# Plot SEM path diagram using semPaths in semPlot package\n semPaths(fit_sem, \n    what = \"par\",  #display edges in the path diagram as weighted.\n    whatLabels = \"par\", #display the unstandardized parameter coefficient\n    rotation = 2, \n    edge.label.cex = 0.7,\n    fontname =\"Helvetica\",\n )\n mtext(\"Fig.2\", side = 1, line = 4, at = 0.5, cex = 1) #Add caption \n\n\n\n\n\n\n\nFig. The structural equation models for the effect of customer percieved quality of service on customers’ satisfaction. Arrows represent the hypothesized causal relationships between the exogenous and endogenous latent variables. The arrow width indicates the strength of the relationship. The values next to the arrows are path coefficients (unstandardized regression coefficients). The broken path lines indicate that the first factor loading is fixed (the Lavaan package model-syntax was designed by default to set up the first factor loading as fixed to set the scale).\n\n# Plot using semPaths in semPlot package\n semPaths(fit_sem, \n          what = \"path\", #display edges in the path diagram as unweighted.\n          whatLabels = \"par\", \n          rotation = 2, \n          edge.label.cex = 0.7,\n          fontname =\"Helvetica\", \n          edgeOptions = list(color = \"black\")\n ) \n mtext(\"Fig.3\", side = 1, line = 4, at = 0.5, cex = 1)\n\n\n\n\n\n\n #semPaths(fit_sem, \n          #what = \"std\", #display edges in the path diagram as weighted.\n          #whatLabels = \"std\", #display the standardized parameter coefficient\n          #rotation = 2,\n          #edge.label.cex = 0.7,\n          #fontname =\"Helvetica\",\n #)\n #mtext(\"Fig.4\", side = 1, line = 4, at = 0.5, cex = 1)",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Introduction to Structural Equation Modeling</span>"
    ]
  },
  {
    "objectID": "lessons/06_structural_equation_models.html#interpretation",
    "href": "lessons/06_structural_equation_models.html#interpretation",
    "title": "\n37  Introduction to Structural Equation Modeling\n",
    "section": "\n37.9 Interpretation",
    "text": "37.9 Interpretation\n\nModel fit: Although the model chi-square was less than 0.05 and RMSEA = 0.091, the CFI = 0.943 and TLI = 0.943 indicate the model is a reasonable good fit.\nOur result is statistically significant given the p-value of the regression analysis (p &lt;.0001). The finding shows that the customer’s perceived quality of services positively predicts satisfaction, indicating that for every one unit increase in customer perceived quality scores, satisfaction is predicted to increase by 0.76 points.",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Introduction to Structural Equation Modeling</span>"
    ]
  },
  {
    "objectID": "lessons/06_structural_equation_models.html#conclusion",
    "href": "lessons/06_structural_equation_models.html#conclusion",
    "title": "\n37  Introduction to Structural Equation Modeling\n",
    "section": "\n37.10 Conclusion",
    "text": "37.10 Conclusion\n\nIn summary, we have used SEM, which is a powerful tool for testing theoretical models, to explore data to gain a deeper understanding of the complex relationships between variables. SEM requires careful specification, estimation, and evaluation to ensure accurate results. It is useful in various fields such as psychology, sociology, education, and business.",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Introduction to Structural Equation Modeling</span>"
    ]
  },
  {
    "objectID": "lessons/06_structural_equation_models.html#references",
    "href": "lessons/06_structural_equation_models.html#references",
    "title": "\n37  Introduction to Structural Equation Modeling\n",
    "section": "\n37.11 References",
    "text": "37.11 References\n\nIntroduction to Structural Equation Modeling (SEM) in R Lavaan. https://stats.oarc.ucla.edu/r/seminars/rsem/#\nLiu, X., Swenson, N. G., Lin, D., Mi, X., Umaña, M. N., Schmid, B., & Ma, K. (2016). Linking individual‐level functional traits to tree growth in a subtropical forest. Ecology, 97(9), 2396-2405. https://doi.org/10.1002/ecy.1445\nDonaldson, L. (2015). 1. The First Generation: Definition and Brief History of Structural Equation Modeling. Journal of Administrati e Sciences, 12, 182-94. https://www.stats.ox.ac.uk/~snijders/Encyclopedia_SEM_Kaplan.pdf\nStructural Equation Modeling https://www.statisticssolutions.com/free-resources/directory-of-statistical-analyses/structural-equation-modeling/\nSoumya Ray & Nicholas Danks. SEMinR. https://cran.r-project.org/web/packages/seminr/vignettes/SEMinR.html#data\nUllman, J. B., & Bentler, P. M. (2012). Structural equation modeling. Handbook of Psychology, Second Edition, 2. https://doi.org/10.1002/9781118133880.hop202023",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Introduction to Structural Equation Modeling</span>"
    ]
  },
  {
    "objectID": "lessons/06_pca.html",
    "href": "lessons/06_pca.html",
    "title": "\n38  Linear Dimensionality Reduction\n",
    "section": "",
    "text": "38.1 Introduction\nIn research, datasets with many features are called high-dimensional datasets. The growth and update speed of datasets are accelerating, and data is developing in a high-dimensional and unstructured direction. Big complex data contains a lot of useful information, but it also increases the difficulty of identifying this information, also known as the “curse of dimensionality” (Jia et al., 2022). Additionally, a significant amount of computing time and storage space is spent on processing the data making efficient robust information retrival resource a priority. We know effective information is submerged in complex data, making it difficult to discover the essential characteristics of the data. It takes considerable time and manpower to process the data, which also negatively impacts the accuracy of recognition (Jia et al., 2022).\nIn the figure below, Jia et al., illustrate the performance of a classifier (Jia et al., 2022). As demonstrated, when the data dimension increases, the performance of the classifier improves; however, when the data dimension continues to increase, the performance of the classifier deteriorates. Analyzing the vast amount of information and extracting useful information features from high-dimensional data, as well as eliminating the influence of related or repetitive factors, are problems that can be mitigated through dimension reduction. In short, the basic principle of feature dimensionality reduction is to map a data sample from a high-dimensional space to a relatively low-dimensional space. The primary objective is to find the mapping and obtain an effective low-dimensional structure hidden in high-dimensional observable data.",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Linear Dimensionality Reduction</span>"
    ]
  },
  {
    "objectID": "lessons/06_pca.html#introduction",
    "href": "lessons/06_pca.html#introduction",
    "title": "\n38  Linear Dimensionality Reduction\n",
    "section": "",
    "text": "Figure 38.1: Performance of the classifier as a function of data dimension",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Linear Dimensionality Reduction</span>"
    ]
  },
  {
    "objectID": "lessons/06_pca.html#principle-of-feature-dimensionality-reduction",
    "href": "lessons/06_pca.html#principle-of-feature-dimensionality-reduction",
    "title": "\n38  Linear Dimensionality Reduction\n",
    "section": "\n38.2 Principle of Feature Dimensionality Reduction",
    "text": "38.2 Principle of Feature Dimensionality Reduction\nDatasets with many characteristics are called high-dimensional data. There are often lots of redundant information in it, including related or duplicated factors. Dimensionality reduction aims to eliminate these interferences. Feature dimensionality reduction uses existing feature parameters to form a low-dimensional feature space and overcomes the effects of redundant or irrelevant information, mapping the effective information contained in the original features to fewer features (Jia et al., 2022).\nIn mathematical notation, suppose there is an \\(n\\)-dimensional vector:\n\\[\n\\mathbf{X} = [x_1, x_2, \\ldots, x_n]^T,\n\\] Note: \\(T\\) indicates that the vector \\([x_1, x_2, \\ldots, x_n]\\) is being transposed from a row vector to a column vector.\nThese features are mapped to an \\(m\\)-dimensional vector \\(Y\\) through a map \\(f\\), where:\n\\[\n\\mathbf{Y} = [y_1, y_2, \\ldots, y_m]^T,\n\\]\nand\n\\[\nm \\ll n.\n\\]\nThe vector \\(Y\\) should contain the main features of vector \\(X\\). Mathematically, the mapping function can be expressed as:\n\\[\n\\mathbf{Y} = f(\\mathbf{X}).\n\\]\nThis is the process of feature extraction and selection. It can also be called the “low loss reduction dimension” process of the original data. A low-dimensional vector resulting from dimension reduction can be applied to fields such as pattern recognition, data mining, and machine learning.\nThis mapping \\(f\\) is the algorithm we want to find for feature reduction. The choice of mapping \\(f\\) differs depending on the dataset and question being addressed.",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Linear Dimensionality Reduction</span>"
    ]
  },
  {
    "objectID": "lessons/06_pca.html#linear-transformations",
    "href": "lessons/06_pca.html#linear-transformations",
    "title": "\n38  Linear Dimensionality Reduction\n",
    "section": "\n38.3 Linear Transformations",
    "text": "38.3 Linear Transformations\nA specific way to achieve dimensionality reduction is through linear transformations (Worley et al., 2013). The goal of multivariate dimensionality reduction through linear transformations is to find a \\(K\\) x \\(P\\) matrix \\(A\\) that optimally transforms the data matrix \\(X\\) into a new matrix of \\(P\\)-dimensional scores given by \\(T\\):\n\\[\n\\mathbf{T} = \\mathbf{X}\\mathbf{A},\n\\]\nThus, each row of \\(T\\) is a transformation of the corresponding row of \\(X\\). Alternately, expressing the \\(i\\)-th row of \\(X\\) as a column vector \\(x_i\\) and the corresponding row of \\(T\\) as a column vector \\(t_i\\) shows that the so-called ‘weights’ matrix \\(A^T\\) defines a linear transformation from the input data space occupied by \\(X\\) to the output space of \\(T\\), termed the ‘scores’ space:\n\\[\n\\mathbf{t}_i = \\mathbf{A}^T \\mathbf{x}_i.\n\\]\nAnother key characteristic of high-dimensional datasets is having more observed variables \\(K\\) than the number of observations \\(N\\), which is known as the ‘large \\(K\\), small \\(N\\)’ problem in statistics (Worley et al., 2013). This issue makes traditional linear regression methods unworkable because \\(X\\) becomes non-invertible (i.e., singular) and no unique least-squares solution can be found. Therefore, multivariate analysis methods that can handle significant collinearity in \\(X\\) are needed. When \\(P\\) is less than \\(K\\), the dimensionality of the scores space will be reduced compared to the input data space, achieving dimensionality reduction. This is an essential feature of multivariate analysis across various fields. By combining feature dimensionality reduction principles with linear transformations, we can effectively reduce the dimensionality of high-dimensional data while preserving its essential features, leading to more efficient and accurate analysis (Worley et al., 2013).",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Linear Dimensionality Reduction</span>"
    ]
  },
  {
    "objectID": "lessons/06_pca.html#principal-component-analysis-pca",
    "href": "lessons/06_pca.html#principal-component-analysis-pca",
    "title": "\n38  Linear Dimensionality Reduction\n",
    "section": "\n38.4 Principal Component Analysis (PCA)",
    "text": "38.4 Principal Component Analysis (PCA)\nPrincipal Component Analysis (PCA) is a widely used method in multivariate analysis. The main goal of PCA is to find a linear transformation that captures as much of the variance in the original data as possible in the lower-dimensional output data (Worley et al., 2013). The transformation matrix \\(A\\) that achieves this is composed of the first \\(P\\) eigenvectors of the sample covariance matrix \\(S\\):\n\\[\n\\mathbf{S} = \\frac{1}{N-1} \\mathbf{X}^T \\mathbf{H} \\mathbf{X} = \\mathbf{Q} \\mathbf{\\Lambda} \\mathbf{Q}^{-1}\n\\]\nHere, \\(H\\) is the \\(N\\)x\\(N\\) centering matrix used to center each variable around its sample mean. The second equality represents the eigendecomposition of \\(S\\), where \\(Q\\) is the matrix of eigenvectors and \\(\\mathbf{\\Lambda}\\) is a diagonal matrix of the corresponding eigenvalues. If \\(X\\) is left unscaled, the eigenvalues in \\(\\mathbf{\\Lambda}\\) equal the variances of the transformed data in \\(T\\), allowing the calculation of the ratio of variance preserved during the transformation relative to the original variance:\n\\[\nR^2_i = \\frac{\\Lambda_{ii}}{\\sum_{j=1}^N S_{jj}}\n\\]\nwhere \\(R^2_i\\) represents the amount of variance in \\(X\\) preserved in the \\(i\\)-th principal component. Since \\(\\mathbf{\\Lambda_{ii}}\\) decreases monotonically with \\(i\\), each subsequent principal component preserves progressively less of the original data’s variance.\nThe following exercise will demonstrate PCA utilizing the following material:\n\nlibrary(VIM) # For KNN imputation\nlibrary(caret) # For preProcess function\nlibrary(factoextra)\nlibrary(viridis) # Load viridis for color palette\nlibrary(vegan) # Load vegan for PERMANOVA\nlibrary(knitr) # For creating tables\nlibrary(kableExtra) # For enhancing tables\nlibrary(tidyverse)",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Linear Dimensionality Reduction</span>"
    ]
  },
  {
    "objectID": "lessons/06_pca.html#pca-example",
    "href": "lessons/06_pca.html#pca-example",
    "title": "\n38  Linear Dimensionality Reduction\n",
    "section": "\n38.5 PCA Example",
    "text": "38.5 PCA Example\nWe will use the following steps:\n\n\nLoad the dataset:\n\nImport the data file.\nCheck for missing values.\n\n\n\nPreprocess the data:\n\nHandle missing values.\nNormalize the data.\n\n\n\nPerform dimensionality reduction:\n\nApply PCA.\nVisualize the principal components.\n\n\n\nAnalyze the results:\n\nInterpret the loadings.\nAssess the variance explained.\n\n\n\nInterpret the findings:\n\nRelate the findings to biological significance.\nDiscuss potential applications.\n\n\n\n\n38.5.1 Data Source and Description\nFor this exercise we will use a metabolomics dataset because metabolomic datasets are inherently high-dimensional, requiring dimensionality reduction. Metabolomics is described as the quantitative measurement of the complex metabolic response of living systems to various stimuli or genetic changes. This means that metabolomic studies provide insights in the field of systems biology, capturing the molecular-level responses of an organism’s genome, transcriptome, and proteome to specific changes or stimuli. Metabolites, which are the final products of cellular processes, directly reflect enzymatic and protein activity. Therefore, they are more closely related to an organism’s phenotype or disease state than genetic or proteomic data alone. This is because a change in a gene or protein expression does not always correspond directly to a change in protein activity, whereas changes in metabolites are directly linked to such variations. As a result, metabolomics is widely used to identify disease biomarkers, support drug discovery, and study a variety of areas including plants, bacteria, nutrition, and the environment, among many other applications.\nThe variables in the dataset are:\n\nSample\nGroup\nMetabolite Concentrations (1-550)\n\n38.5.2 Loading the Dataset\n\n# Read the data\npca_data &lt;- read_csv(\n  \"../data/06_pca_eg_metabolomics.csv\",\n  show_col_types = FALSE\n)\n\n# Show the first 6 rows and the first 4 columns of the data set\nprint(head(pca_data[, 1:4], 6))\n\n# A tibble: 6 × 4\n  Sample  Group Metabolite1 Metabolite2\n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1 Blood_1 A        1432047.     292764.\n2 Blood_2 A        1346463.      10518.\n3 Blood_3 A        1648569.     403325.\n4 Blood_4 A        3697730.    1478574.\n5 Blood_5 A        1124143.     598174.\n6 Blood_6 A        3694154.     421392.\n\n# Check for missing values\nmissing_values &lt;- sum(is.na(pca_data))\ncat(\"Number of missing values: \", missing_values, \"\\n\")\n\nNumber of missing values:  2 \n\n\n\n38.5.3 Preprocess the data:\n\n# KNN imputation for missing values\n# Convert data frame to matrix for KNN imputation\ndata_matrix &lt;- as.matrix(pca_data[ , -c(1, ncol(pca_data))]) # Exclude Sample and Group columns\ndata_matrix_imputed &lt;- kNN(data_matrix, k = 5, imp_var = FALSE)\n\n# Convert back to data frame and add the Sample and Group columns\npca_data_imputed &lt;- as.data.frame(data_matrix_imputed)\npca_data_imputed$Sample &lt;- pca_data$Sample\npca_data_imputed$Group &lt;- pca_data$Group\n\n# Pivot longer for ggplot\npca_data_long &lt;- pivot_longer(\n  data = pca_data_imputed,\n  cols = -c(Sample, Group),\n  names_to = \"Metabolite\",\n  values_to = \"Concentration\"\n)\n\n# Ensure Concentration column is numeric\npca_data_long$Concentration &lt;- as.numeric(pca_data_long$Concentration)\n\n\n# Set the maximum x-axis limit\nmax_x_limit &lt;- 2e6  # Adjust this value as needed\n\n# Create the histogram with a continuous scale and limited x-axis\nggplot(pca_data_long, aes(x = Concentration)) +\n  geom_histogram(bins = 100, fill = viridis(100)) +\n  scale_x_continuous(limits = c(0, max_x_limit)) +\n  theme_minimal() +\n  labs(\n    title = \"Histogram of Metabolite Concentrations\",\n    y = \"Frequency\",\n    x = \"Concentration\"\n  ) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\n\n\n\n\n\nFigure 38.2: Histogram of Metabolite Concentrations (Log Scale on X-Axis)\n\n\n\n\nNow we will apply Log base 10 Transformation\n\n# Log base 10 transform the Concentration column\npca_data_long &lt;- pca_data_long %&gt;%\n  # Adding 1 to avoid log(0)\n  mutate(Log_Concentration = log10(Concentration + 1)) \n\n\n# Plot histogram for the overall distribution of metabolite concentrations after log transformation\nggplot(pca_data_long, aes(x = Log_Concentration)) +\n  geom_histogram(bins = 30, fill = viridis(30)) +\n  theme_minimal() +\n  labs(\n    title = \"Histogram of Metabolite Concentrations (After Log Transformation)\",\n    y = \"Frequency\",\n    x = \"Log10(Concentration)\"\n  ) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\n\n\n\n\n\nFigure 38.3: Histogram of Metabolite Concentrations (After Log Transformation)\n\n\n\n\nNow we will apply Autoscaling\n\n# Autoscaling (standardization)\npreProcValues &lt;- preProcess(\n  pca_data_long[, \"Log_Concentration\", drop = FALSE],\n  method = c(\"center\", \"scale\")\n)\nscaled_values &lt;- predict(\n  preProcValues,\n  pca_data_long[, \"Log_Concentration\", drop = FALSE]\n)\n\n# Convert the scaled values from tibble to numeric vector\npca_data_long$Scaled_Log_Concentration &lt;- as.numeric(scaled_values$Log_Concentration)\n\n\n# Plot histogram for the overall distribution of metabolite concentrations\n#   after autoscaling\nggplot(pca_data_long, aes(x = Scaled_Log_Concentration)) +\n  geom_histogram(bins = 30, fill = viridis(30)) +\n  theme_minimal() +\n  labs(\n    title = \"Histogram of Metabolite Concentrations (After Autoscaling)\",\n    y = \"Frequency\",\n    x = \"Scaled Log10(Concentration)\"\n  ) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\n\n\n\n\n\nFigure 38.4: Histogram of Metabolite Concentrations (After Autoscaling)\n\n\n\n\n\n38.5.4 Perform Dimensionality Reduction\n\n# Pivot the data to wide format for PCA\npca_data_wide &lt;- pca_data_long %&gt;%\n  dplyr::select(Sample, Group, Metabolite, Scaled_Log_Concentration) %&gt;%\n  pivot_wider(names_from = Metabolite, values_from = Scaled_Log_Concentration)\n\n# Perform PCA on the scaled log concentration data\npca_result &lt;- prcomp(\n  pca_data_wide[, -c(1, 2)],\n  center = TRUE, scale. = TRUE,\n  # To return the metabolite data rotated into the PC vector space; this is the\n  #   matrix we will use to reduce the dimension\n  retx = TRUE\n)\n\n# Calculate the percentage of variance explained by each principal component\npercent_var &lt;- round(100 * pca_result$sdev^2 / sum(pca_result$sdev^2), 1)\npc1_label &lt;- paste0(\"PC1 (\", percent_var[1], \"%)\")\npc2_label &lt;- paste0(\"PC2 (\", percent_var[2], \"%)\")\n\n\n# Plot PCA biplot after preprocessing with ellipses (only individuals)\nfviz_pca_ind(\n  pca_result, \n  # Show points only (not text)\n  geom.ind = \"point\",      \n  # Color by group\n  col.ind = pca_data_wide$Group, \n  # Use viridis color palette\n  palette = viridis( length(unique(pca_data_wide$Group)) ), \n  # Add concentration ellipses\n  addEllipses = TRUE,      \n  legend.title = \"Groups\",\n  # Increase point size\n  pointsize = 4\n) +         \n  labs(title = \"PCA Biplot\", x = pc1_label, y = pc2_label) +\n  theme_minimal() +\n  theme(legend.position = \"right\")\n\n\n\n\n\n\nFigure 38.5: PCA Biplot\n\n\n\n\nDescription: This plot shows the distribution of samples across the first two principal components (PC1 and PC2), which explain 11.4% and 9.6% of the variance, respectively. Different groups (A, B, C, D) are represented by distinct shapes and colors.\n\n# Scree Plot\nfviz_eig(\n  pca_result,\n  addlabels = TRUE,\n  ylim = c(0, 50),\n  barfill = viridis(1, begin = 0.7, end = 0.7)\n) +\n  labs(\n    title = \"Scree Plot\",\n    x = \"Principal Components\",\n    y = \"Percentage of Variance Explained\"\n  ) +\n  theme_minimal() +\n  scale_color_viridis(discrete = TRUE)\n\n\n\n\n\n\nFigure 38.6: Scree Plot\n\n\n\n\nDescription: The scree plot shows the percentage of variance explained by each of the principal components.\n\nggplot(\n  data = pca_result %&gt;% \n    get_eig() %&gt;% \n    rownames_to_column(var = \"dimension\") %&gt;% \n    mutate(q = as.integer(str_remove(dimension, pattern = \"Dim.\")))\n) + \n  theme_bw() +\n  aes(x = q, y = cumulative.variance.percent) + \n  labs(\n    title = \"Cumulative Variance Explained\",\n    y = \"Percent Variance\",\n    x = \"First q Principal Components\"\n  ) +\n  scale_x_continuous(breaks = seq(0, 100, by = 10)) +\n  geom_line() + \n  geom_abline(intercept = 80, slope = 0, colour = \"darkgreen\")\n\n\n\n\n\n\nFigure 38.7: Cumulative Variance Explained by PCs\n\n\n\n\nDescription: This plot shows that we need the first 35 principal components to explain 80% of the information in the original 550 metabolites.\n\n# Calculate contributions of variables\nvar_contrib &lt;- get_pca_var(pca_result)$contrib\n\n# Find the top contributing variables\ntop_contributors &lt;- order(rowSums(var_contrib[, 1:2]), decreasing = TRUE)[1:20]\n\n# PCA Variance Plot focusing on top contributors\nfviz_pca_var(\n  pca_result, \n  # Select top 20 contributing variables\n  select.var = list(contrib = 20), \n  # Color by contributions to the PCs\n  col.var = \"contrib\", \n  # Color gradient\n  gradient.cols = viridis(3), \n  legend.title = \"Contributions\",\n  # Use repel to avoid text overlap\n  repel = TRUE\n) + \n  labs(title = \"PCA Variance Plot (Top 20 Contributors)\") +\n  theme_minimal() +\n  # Adjust text size\n  theme(text = element_text(size = 12)) \n\n\n\n\n\n\nFigure 38.8: PCA Variance Plot (Top 20 Contributors)\n\n\n\n\nDescription: This plot shows the top 20 metabolites contributing to the variance in the first two principal components (Dim1 and Dim2).",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Linear Dimensionality Reduction</span>"
    ]
  },
  {
    "objectID": "lessons/06_pca.html#analyze-the-results",
    "href": "lessons/06_pca.html#analyze-the-results",
    "title": "\n38  Linear Dimensionality Reduction\n",
    "section": "\n38.6 Analyze the Results",
    "text": "38.6 Analyze the Results\n\n38.6.1 Original 550 Metabolites\nWe first analyze the data without dimension reduction.\n\n# Prepare data for PERMANOVA\n# Use the scaled log concentrations and group information; make sure it's a\n#   data frame\nscaled_data &lt;- as.data.frame(pca_data_wide[, -c(1, 2)])\ngroups &lt;- pca_data_wide$Group\n\n# Calculate distance matrix\ndist_matrix &lt;- dist(scaled_data, method = \"euclidean\")\n\n# Perform PERMANOVA\npermanova_result &lt;- vegan::adonis2(\n  dist_matrix ~ groups,\n  data = pca_data_wide,\n  permutations = 999\n)\n\n# Print PERMANOVA results in a table format\npermanova_table &lt;- \n  permanova_result %&gt;%\n  as.data.frame() %&gt;% \n  rownames_to_column(\"Term\") %&gt;%\n  rename(\n    `Sum of Squares` = `SumOfSqs`,\n    `R-Squared` = `R2`,\n    `F-Value` = `F`,\n    `P-Value` = `Pr(&gt;F)`\n  )\n\npermanova_table %&gt;% \n  kable(format = \"html\") %&gt;%\n  kable_styling(\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\")\n  )\n\n\nTable 38.1: PERMANOVA results before PCA\n\n\n\n\n\n\nTerm\nDf\nSum of Squares\nR-Squared\nF-Value\nP-Value\n\n\n\ngroups\n3\n913.0741\n0.1418102\n5.122546\n0.001\n\n\nResidual\n93\n5525.6309\n0.8581898\nNA\nNA\n\n\nTotal\n96\n6438.7050\n1.0000000\nNA\nNA\n\n\n\n\n\n\n\n\n\n\n\n\nDescription: The PERMANOVA results provide statistical evidence of significant differences between the groups consistent with the PCA findings. However, the explained variance by the groups is relatively low.\n\n38.6.2 First 35 Principal Components\nNow we use this same method to analyse the differences between groups using only the first 35 principal components of the metabolite data. We use the data after it has been rotated into the new Principal Component vector space (the matrix x returned by the prcomp() function).\n\n# Prepare data for PERMANOVA\nreduced_data &lt;- as.data.frame(pca_result$x[, 1:35])\n\n# Calculate distance matrix\ndist_matrix_reduced &lt;- dist(reduced_data, method = \"euclidean\")\n\n# Perform PERMANOVA\npermanova_result_pca &lt;- vegan::adonis2(\n  dist_matrix_reduced ~ pca_data_wide$Group,\n  data = reduced_data,\n  permutations = 999\n)\n\n# Print PERMANOVA results for PCA in a table format\npermanova_table_pca &lt;- \n  permanova_result_pca %&gt;%\n  as.data.frame() %&gt;% \n  rownames_to_column(\"Term\") %&gt;%\n  rename(\n    `Sum of Squares` = `SumOfSqs`,\n    `R-Squared` = `R2`,\n    `F-Value` = `F`,\n    `P-Value` = `Pr(&gt;F)`\n  )\n\npermanova_table_pca %&gt;% \n  kable(format = \"html\") %&gt;%\n  kable_styling(\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\")\n  )\n\n\nTable 38.2: PERMANOVA results for PCA\n\n\n\n\n\n\nTerm\nDf\nSum of Squares\nR-Squared\nF-Value\nP-Value\n\n\n\npca_data_wide$Group\n3\n5594.861\n0.1323235\n4.7276\n0.001\n\n\nResidual\n93\n36686.843\n0.8676765\nNA\nNA\n\n\nTotal\n96\n42281.704\n1.0000000\nNA\nNA\n\n\n\n\n\n\n\n\n\n\n\n\nDescription: The PERMANOVA results provide nearly the same statistical evidence of significant differences between the groups consistent as above.\n\n38.6.3 Post-Hoc Tests\nNotice that our two PERMANOVA models had practically the same results. However, post-hoc analysis is tractable for the PCA-based solution: we have to test 35 independent PCs instead of 550 correlated metabolites.\n\nmap_dbl(\n  .x = reduced_data[, 1:35],\n  .f = ~{\n    aov(.x ~ pca_data_wide$Group) %&gt;% \n      summary() %&gt;% \n      # It's a list of length 1 for some stupid reason\n      `[[`(1) %&gt;% \n      # Get the first row of the p-value column\n      `[`(1, \"Pr(&gt;F)\")\n  }\n) %&gt;% \n  p.adjust(method = \"fdr\") %&gt;% \n  sort()\n\n         PC2          PC3          PC4         PC15          PC1         PC10 \n4.584910e-07 8.323208e-06 3.188619e-03 3.188619e-03 6.032935e-03 2.521421e-02 \n         PC9         PC11         PC23          PC6         PC26         PC13 \n4.818898e-02 4.818898e-02 1.859289e-01 1.887834e-01 2.584835e-01 3.167604e-01 \n         PC8         PC17         PC20         PC25         PC34          PC5 \n4.973156e-01 4.973156e-01 4.973156e-01 4.973156e-01 4.973156e-01 5.163382e-01 \n        PC12         PC22         PC16         PC18         PC29         PC21 \n5.585821e-01 5.585821e-01 5.665549e-01 5.665549e-01 5.665549e-01 6.308594e-01 \n         PC7         PC30         PC19         PC24         PC27         PC28 \n6.451604e-01 7.902657e-01 9.850381e-01 9.850381e-01 9.850381e-01 9.850381e-01 \n        PC31         PC32         PC33         PC35         PC14 \n9.850381e-01 9.850381e-01 9.850381e-01 9.850381e-01 9.997223e-01 \n\n\nWhat is interesting here is that the first principal component isn’t as strongly related to the group as second and third PC. Many times, we are often worried that the first PC (and potentially even the second) are driven by lab effects or batch effects. There is a modification to PCA, called principal variance component analysis which may help here, but that’s beyond the scope of this lesson. At this point though, we’d probably need to re-estimate which metabolites are loading the most onto these top most significant principal components.",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Linear Dimensionality Reduction</span>"
    ]
  },
  {
    "objectID": "lessons/06_pca.html#interpret-the-findings",
    "href": "lessons/06_pca.html#interpret-the-findings",
    "title": "\n38  Linear Dimensionality Reduction\n",
    "section": "\n38.7 Interpret the Findings",
    "text": "38.7 Interpret the Findings\n\n38.7.1 PCA Biplot (Figure 4)\n\n\nInterpretation: The separation of different groups in the PCA biplot indicates distinct metabolic profiles among these groups. For example, Group A (circles) and Group C (squares) show some overlap but are generally distinct from Group B (triangles) and Group D (crosses).\n\nBiological Significance: This suggests that the metabolic pathways or the concentration of metabolites differ among these groups. These differences could be due to varying biological conditions such as health status, environmental factors, or treatment effects.\n\n38.7.2 Scree Plot (Figure 5)\n\n\nInterpretation: The first few components explain a substantial amount of variance (e.g., PC1 = 11.4%, PC2 = 9.6%), and the explained variance decreases significantly after the first few components.\n\nBiological Significance: This indicates that the first few principal components capture the most critical metabolic variations in the data, suggesting that focusing on these components can provide valuable insights into the primary sources of variation in the metabolomic data.\n\n38.7.3 PCA Variance Plot (Figure 6)\n\n\nInterpretation: Metabolites like Metabolite55, Metabolite310, and Metabolite76 contribute significantly to the variance in Dim1, while others contribute to Dim2.\n\nBiological Significance: Identifying these key metabolites can help pinpoint specific biochemical pathways that are most influential in differentiating the sample groups. These metabolites could be biomarkers for certain conditions or targets for therapeutic intervention.",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Linear Dimensionality Reduction</span>"
    ]
  },
  {
    "objectID": "lessons/06_pca.html#conclusion-and-discussion",
    "href": "lessons/06_pca.html#conclusion-and-discussion",
    "title": "\n38  Linear Dimensionality Reduction\n",
    "section": "\n38.8 Conclusion and Discussion",
    "text": "38.8 Conclusion and Discussion\nThe combination of PCA and PERMANOVA in this study demonstrated the utility of dimensionality reduction techniques in metabolomics. PCA facilitated the visualization and interpretation of complex metabolic data, revealing significant group differences and identifying key metabolites. The PERMANOVA results reinforced these findings by statistically validating the observed group differences.\nOverall, dimensionality reduction techniques like PCA are invaluable tools in bioinformatics and systems biology. They enable researchers to distill complex data into actionable insights, paving the way for advances in diagnostics, therapeutics, and personalized medicine. By continuing to leverage these techniques, we can further our understanding of intricate biological systems and improve health outcomes.",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Linear Dimensionality Reduction</span>"
    ]
  },
  {
    "objectID": "lessons/06_pca.html#references",
    "href": "lessons/06_pca.html#references",
    "title": "\n38  Linear Dimensionality Reduction\n",
    "section": "\n38.9 References",
    "text": "38.9 References\n\nJia, W., Sun, M., Lian, J., & Hou, S. (2022). Feature dimensionality reduction: a review. Complex & Intelligent Systems, 8(3), 2663-2693.\nWorley, B., & Powers, R. (2013). Multivariate Analysis in Metabolomics. Current Metabolomics, 1(1), 92–107. https://doi.org/10.2174/2213235X11301010092\nChi, J., Shu, J., Li, M., Mudappathi, R., Jin, Y., Lewis, F., … & Gu, H. (2024). Artificial Intelligence in Metabolomics: A Current Review. TrAC Trends in Analytical Chemistry, 117852.",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Linear Dimensionality Reduction</span>"
    ]
  },
  {
    "objectID": "07_header_power.html",
    "href": "07_header_power.html",
    "title": "Statistical Power and Sample Size Determination",
    "section": "",
    "text": "Text Outline\nThis text covers the basics of:",
    "crumbs": [
      "Statistical Power and Sample Size Determination"
    ]
  },
  {
    "objectID": "07_header_power.html#text-outline",
    "href": "07_header_power.html#text-outline",
    "title": "Statistical Power and Sample Size Determination",
    "section": "",
    "text": "Exploring data\nOne-sample tests\nTwo-sample tests\nANOVA and linear regression\nGeneralized linear models\nSome special topics\nMaybe a bit about power calculations…",
    "crumbs": [
      "Statistical Power and Sample Size Determination"
    ]
  },
  {
    "objectID": "07_header_power.html#part-outline",
    "href": "07_header_power.html#part-outline",
    "title": "Statistical Power and Sample Size Determination",
    "section": "Part Outline",
    "text": "Part Outline\nThis part of the text will eventually contain some examples on statistical power calculations and sample size determination methods for some of the techniques covered in this text.",
    "crumbs": [
      "Statistical Power and Sample Size Determination"
    ]
  },
  {
    "objectID": "lessons_original/07_power_ols.html",
    "href": "lessons_original/07_power_ols.html",
    "title": "\n39  Power Analysis for OLS Regression\n",
    "section": "",
    "text": "39.1 Introduction\nThe power of a hypothesis test is the probability of correctly rejecting the null hypothesis or the probability that the test will correctly support the alternative hypothesis (detecting an effect when there actually is one)1. Then,\n\\[\nPower = 1-\\beta\n\\]\nWhere, \\(\\beta\\) = probability of committing a Type II Error (the probability that we would accept the null hypothesis even if the alternative hypothesis is actually true). Then, by decreasing \\(\\beta\\) power increases [@(pdf)ef].\nPower is mainly influenced by sample size, effect size, and significance level.",
    "crumbs": [
      "Statistical Power and Sample Size Determination",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Power Analysis for OLS Regression</span>"
    ]
  },
  {
    "objectID": "lessons_original/07_power_ols.html#introduction",
    "href": "lessons_original/07_power_ols.html#introduction",
    "title": "\n39  Power Analysis for OLS Regression\n",
    "section": "",
    "text": "High power: large chance of a test detecting a true effect.\nLow power: test only has a small chance of detecting a true effect or that the results are likely to be distorted by random and systematic error.\n\n\n\n\n\nVisual view of beta",
    "crumbs": [
      "Statistical Power and Sample Size Determination",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Power Analysis for OLS Regression</span>"
    ]
  },
  {
    "objectID": "lessons_original/07_power_ols.html#power-analysis-ols-regression",
    "href": "lessons_original/07_power_ols.html#power-analysis-ols-regression",
    "title": "\n39  Power Analysis for OLS Regression\n",
    "section": "\n39.2 Power Analysis: OLS Regression",
    "text": "39.2 Power Analysis: OLS Regression\nFor this power analysis we will use the univariate (simple) OLS regression example of our last presentation examining the relationship between a vehicle’s weight (WT) and its miles per gallon or fuel efficiency (MPG).\nWhen performed, the paired correlation provided us with a pearson’s correlation coefficient of r(30) = -.868, p&lt;0.05, (n = 32). When we ran this regression we got an (\\(R^2\\) = .75) Therefore for the r2 value (effect size) for a power analysis we will begin with an r2 value of .75 and an n = 32 to account for the observations already collected. However, the power analysis should occur before collecting samples so that we can have an appropriate number of observations required for our hypothesized effect size. In our example, we are also assuming that the variables are normally distributed. Based on our correlation analysis, weight likely needs a cubic transformation, This would mean that our model would have three coefficients of interest.\nFormula for a univariate Ordinary Least Squares (OLS) Regression:\n\\[\n\\hat{y}_i = \\beta_0 + \\beta_1x_i\n\\]\nThe OLS regression model line for our example is:\n\\[\n\\widehat{MPG_i} = \\beta_0 +\\beta_1*WT_i\n\\]\nUsing an alpha value of \\(\\alpha\\) = .05 (The probability of a type I error/rejecting a correct \\(H_0\\), we will identify the number of observations or sample size (n) necessary to obtain statistical power (80% or \\(\\beta\\) = 0.20) given various effect sizes. Statistical power in our example identifies the likelihood that a univariate OLS will detect an effect of a certain size if there is one.\nA power analysis is made up of four main components. We will provide estimates for any three of these, as the following functions in r calculate the fourth component.\nWe found three functions in r to conduct power analyses for an OLS regression:\n\nThe pwrss.f.reg function in the pwrss package\nThe pwr.f2.test function in the pwr package\nThe wp.regression function in the WebPower package\n\n\n39.2.1 The pwrss.f.reg function\nWe will start our power analysis using the The pwrss.f.reg function for one predictor in an OLS regression, with our given observations of n = 32 and \\(R^2\\) = .75. Given these values, we are expecting that one variable (WT) explains 75% of the variance in the outcome or Miles per gallon (R2=0.75 or r2 = 0.75 in the code)2.\n\nRegOne_lm &lt;- pwrss.f.reg(\n  r2 = 0.75,\n  k = 1,\n  n = 32,\n  power = NULL,\n  alpha = 0.05\n)\n\n Linear Regression (F test) \n R-squared Deviation from 0 (zero) \n H0: r2 = 0 \n HA: r2 &gt; 0 \n ------------------------------ \n  Statistical power = 1 \n  n = 32 \n ------------------------------ \n Numerator degrees of freedom = 1 \n Denominator degrees of freedom = 30 \n Non-centrality parameter = 96 \n Type I error rate = 0.05 \n Type II error rate = 0 \n\nRegOne_lm$power\n\n[1] 1\n\n\nGiven the information provided, we get 100% power. Our effect size of r2 = 0.75 is considered a large effect provided the following guidelines by Cohen (1988)3\n\\(f^2\\) = 0.02 indicates a small effect;\n\\(f^2\\) = 0.15 indicates a medium effect;\n\\(f^2\\) = 0.35 indicates a large effect.\nWe will use these guidelines to continue our exploration. We will concentrate on a fixed medium effect size. Where, the paired correlation is approximately r = .40 for a medium correlation and for an \\(f^2\\) or effect size of 0.15. using this fixed effect, we will look at various sample sizes to obtain power of 80% or greater given a medium effect size. In our sequence of possible sample sizes, the minimum n = 1 as n &gt; p(p+1)/2 = 1(2)/2 = 1\n\nOLSReg_df &lt;- tibble(n = seq.int(from = 2, to = 99 + 2))\n\nOLSReg_df$power &lt;- map_dbl(\n  .x = OLSReg_df$n,\n  .f = ~{\n    out_ls &lt;- pwrss.f.reg(\n      # \"Effect size\" of a linear model\n      r2 = 0.15,\n      # number of predictors\n      k = 1,\n      # sample size\n      n = .x,\n      power = NULL,\n      alpha = 0.05,\n      # Stop printing messages\n      verbose = FALSE\n    )\n    out_ls$power\n  }\n)\n\nWarning in qf(alpha, df1 = u, df2 = v, lower.tail = FALSE): NaNs produced\n\n\nThe following is the power curve for a fixed effect of f2 = 0.15\n\nggplot(data = OLSReg_df) +\n  theme_bw() +\n  aes(x = n, y = power) +\n  labs(\n    title = \"Omnibus (F-test) Power for Linear Model\",\n    subtitle = \"Fixed Effect Size R2 = 0.15, 1 Predictor\"\n  ) +\n  scale_y_continuous(limits = c(0, 1)) +\n  geom_point() +\n  geom_abline(slope = 0, intercept = 0.8, colour = \"gold\")\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nGiven the graph, we notice that we need an approximate sample size or n of close to 50 to detect a medium effect size in an OLS Regression.\nThe following is a power analysis for a univariate OLS regression given a fixed sample size. We will create a sequence of effect sizes that capture Cohen’s guidelines as well as the effect size of 0.75 of our sample regression. Our fixed n will be n = 32 as the sample.\n\nOLSRegN_df &lt;- tibble(R2 = seq(0, 0.75, length.out = 100))\n\nOLSRegN_df$power &lt;- map_dbl(\n  .x = OLSRegN_df$R2,\n  .f = ~{\n    out2_ls &lt;- pwrss.f.reg(\n      # \"Effect size\" of a linear model\n      r2 = .x,\n      # number of predictors\n      k = 1,\n      # sample size\n      n = 32,\n      power = NULL,\n      alpha = 0.05,\n      # Stop printing messages\n      verbose = FALSE\n    )\n    out2_ls$power\n  }\n)\n\nThe following is the power curve for a fixed sample size of n = 32\n\nggplot(data = OLSRegN_df) +\n  theme_bw() +\n  aes(x = R2, y = power) +\n  labs(\n    title = \"Omnibus (F-test) Power for Linear Model\",\n    subtitle = \"Fixed Sample Size = 32, 1 Predictor\"\n  ) +\n  scale_y_continuous(limits = c(0, 1)) +\n  geom_point() +\n  geom_abline(slope = 0, intercept = 0.8, colour = \"red\")\n\n\n\n\n\n\n\nGiven the graph, we notice that given n = 32, a power of 80% and higher is achieved when the effect size is at least approximately r2 = 0.20.\n\n39.2.2 The pwr.f2.test function\nPower analysis using the pwr.f2.test: where, u = 1, The F numerator degrees of freedom (u=1) or the number of coefficients(independent variables) in the model\nand we will use Cohen’s criteria for effect sizes and first provide analyses for a medium effect size of 0.15 [3]45\n\n# Using Cohen 1988 criteria, where, \n#f2 = 0.02 small effect;\n#f2 = 0.15 medium effect;\n#f2 = 0.35 indicates a large effect\n\n### Fixed Effect size f2 = 0.15###\n# n = 50\npwr.f2.test(\n  u = 1, \n  v = 50 - 1 - 1,\n  f2 = .15, \n  sig.level = 0.05, \n  power = NULL\n  )\n\n\n     Multiple regression power calculation \n\n              u = 1\n              v = 48\n             f2 = 0.15\n      sig.level = 0.05\n          power = 0.7653128\n\n# n = 25\npwr.f2.test(\n  u = 1, \n  v = 25 - 1 - 1,\n  f2 = 0.15, \n  sig.level = 0.05, \n  power = NULL\n  )\n\n\n     Multiple regression power calculation \n\n              u = 1\n              v = 23\n             f2 = 0.15\n      sig.level = 0.05\n          power = 0.4584646\n\n# n = 12\npwr.f2.test(\n  u = 1, \n  v = 12 - 1 - 1,\n  f2 = 0.15, \n  sig.level = 0.05, \n  power = NULL\n  )\n\n\n     Multiple regression power calculation \n\n              u = 1\n              v = 10\n             f2 = 0.15\n      sig.level = 0.05\n          power = 0.2289402\n\n\nNow, we will explore a fixed n = 32\n\n# ES = .02, r = .14\npwr.f2.test(\n  u = 1, \n  v = 32 - 1 - 1,\n  f2 = .02, \n  sig.level = 0.05, \n  power = NULL\n  )\n\n\n     Multiple regression power calculation \n\n              u = 1\n              v = 30\n             f2 = 0.02\n      sig.level = 0.05\n          power = 0.1210661\n\n# ES = 0.15, r = .39\npwr.f2.test(\n  u = 1, \n  v = 32 - 1 - 1,\n  f2 = 0.15, \n  sig.level = 0.05, \n  power = NULL\n  ) \n\n\n     Multiple regression power calculation \n\n              u = 1\n              v = 30\n             f2 = 0.15\n      sig.level = 0.05\n          power = 0.5637733\n\n# ES = .35, r = .59\npwr.f2.test(\n  u = 1, \n  v = 32 - 1 - 1,\n  f2 = 0.35, \n  sig.level = 0.05, \n  power = NULL\n  )\n\n\n     Multiple regression power calculation \n\n              u = 1\n              v = 30\n             f2 = 0.35\n      sig.level = 0.05\n          power = 0.8993357\n\n\nWe will now look at the 3 types of effect sizes given various sample sizes\n\neffect_sizes &lt;- c(0.02, 0.15, 0.35) \nsample_sizes = seq(20, 100, 20)\n\ninput_df &lt;- crossing(effect_sizes,sample_sizes)\nglimpse(input_df)\n\nRows: 15\nColumns: 2\n$ effect_sizes &lt;dbl&gt; 0.02, 0.02, 0.02, 0.02, 0.02, 0.15, 0.15, 0.15, 0.15, 0.1…\n$ sample_sizes &lt;dbl&gt; 20, 40, 60, 80, 100, 20, 40, 60, 80, 100, 20, 40, 60, 80,…\n\nget_power &lt;- function(df){\n  power_result &lt;- pwr.f2.test(\n    u = 1,\n    v = df$sample_sizes - 1 - 1, \n    f2 = df$effect_sizes,\n    )\n  df$power=power_result$power\n  return(df)\n}\n\n# run get_power for each combination of effect size \n# and sample size\n\npower_curves &lt;- input_df %&gt;%\n  do(get_power(.)) %&gt;%\n  mutate(effect_sizes = as.factor(effect_sizes)) \n\n\nggplot(power_curves, \n       aes(x=sample_sizes,\n           y=power, \n           color=effect_sizes)\n       ) + \n  geom_line() + \n  geom_hline(yintercept = 0.8, \n             linetype='dotdash',\n             color = \"purple\")\n\n\n\n\n\n\n\nBased on the graph, if we have an effect size of 0.15, we need approximately 50 or more observations (recall n = v + 1 + 1)\n\n39.2.3 The wp.regression function\nLastly, we use the wp.regression function to examine the appropriate sample size given an effect size of 0.15 to achieve a power of 80% or higher [6]7\n\n# Using webpower \n#p1 = 1\n### Fixed ES = 0.15 ###\nres &lt;- wp.regression(n = seq(20,100,20), \n                     p1 = 1, \n                     f2 = 0.15, \n                     alpha = 0.05, \n                     power = NULL\n                    )\nres\n\nPower for multiple regression\n\n      n p1 p2   f2 alpha     power\n     20  1  0 0.15  0.05 0.3745851\n     40  1  0 0.15  0.05 0.6654126\n     60  1  0 0.15  0.05 0.8389166\n     80  1  0 0.15  0.05 0.9280168\n    100  1  0 0.15  0.05 0.9695895\n\nURL: http://psychstat.org/regression\n\nplot(res,  main = \"Fixed Effect Size = 0.15\")+\nabline(a = .80, b = 0, col = 'steelblue', lwd = 3, lty = 2)\n\n\n\n\n\n\n\ninteger(0)\n\n\nThe results are similar to the previous functions. However, in this function, given an effect size of 0.15, we need an n of close to 60 to achieve 80% power.\n\n# Using webpower \n#p1 = 1\n### Fixed n = 50 ###\nres &lt;- wp.regression(n = 50, \n                     p1 = 1, \n                     f2 = seq(0.00, 0.35, 0.05), \n                     alpha = 0.05, \n                     power = NULL\n                    )\nres\n\nPower for multiple regression\n\n     n p1 p2   f2 alpha     power\n    50  1  0 0.00  0.05 0.0500000\n    50  1  0 0.05  0.05 0.3409707\n    50  1  0 0.10  0.05 0.5914439\n    50  1  0 0.15  0.05 0.7653128\n    50  1  0 0.20  0.05 0.8725329\n    50  1  0 0.25  0.05 0.9337077\n    50  1  0 0.30  0.05 0.9667049\n    50  1  0 0.35  0.05 0.9837529\n\nURL: http://psychstat.org/regression",
    "crumbs": [
      "Statistical Power and Sample Size Determination",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Power Analysis for OLS Regression</span>"
    ]
  },
  {
    "objectID": "lessons_original/07_power_ols.html#references",
    "href": "lessons_original/07_power_ols.html#references",
    "title": "\n39  Power Analysis for OLS Regression\n",
    "section": "References",
    "text": "References\n\n\n\n\n1. \nCohen J. Statistical power analysis for the behavioral sciences. 2nd ed. Hillsdale, N.J: L. Erlbaum Associates; 1988. \n\n\n2. \nA practical guide to statistical power and sample size calculations in r [Internet]. Available from: https://cran.r-project.org/web/packages/pwrss/vignettes/examples.html#3_Linear_Regression_(F_and_t_Tests)\n\n\n\n3. \nCohen J. Statistical power analysis for the behavioral sciences. 2nd ed. Hillsdale, N.J: L. Erlbaum Associates; 1988. \n\n\n4. \nPower in r | [Internet]. Available from: https://blogs.uoregon.edu/rclub/2015/11/10/power-in-r/\n\n\n\n5. \nStatistical power analysis - jacob cohen, 1992 [Internet]. Available from: https://journals.sagepub.com/doi/10.1111/1467-8721.ep10768783\n\n\n\n6. \nZhang Z, Wang L. Advanced statistics using r [Internet]. ISDSA Press; 2017. Available from: https://advstats.psychstat.org/\n\n\n\n7. \nWp.regression: Statistical power analysis for linear regression in WebPower: Basic and advanced statistical power analysis [Internet]. Available from: https://rdrr.io/cran/WebPower/man/wp.regression.html",
    "crumbs": [
      "Statistical Power and Sample Size Determination",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Power Analysis for OLS Regression</span>"
    ]
  }
]