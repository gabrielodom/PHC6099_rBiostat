[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "An R Cookbook for Public Health",
    "section": "",
    "text": "0.1 Source Code for PHC6099 Course Notes\nThis material is for the course “R Computing for Health Sciences”. The course notes are published here: https://gabrielodom.github.io/PHC6099_rBiostat/",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course Materials for PHC 6099: 'R Computing for Health Sciences'</span>"
    ]
  },
  {
    "objectID": "index.html#source-code-for-phc6099-course-notes",
    "href": "index.html#source-code-for-phc6099-course-notes",
    "title": "An R Cookbook for Public Health",
    "section": "",
    "text": "0.1.1 Topics\nThe chapters are:\n\nExploring Data\n\nggplot2:: mosaic plots, histograms, and violin plots\nggplot2:: scatterplots and facets \nskimr::\ntable1::\ngtsummary::\n\nOne-Sample Tests\n\n\\(Z\\)-test\nPaired \\(t\\)-test\nPaired Wilcoxon test\nTransformations to Normality\nMcNemar’s Test\nFisher’s Exact Test\nChi-Square Goodness of Fit\nBootstrapped Confidence Intervals\n\nTwo-Sample Tests\n\n\\(t\\)-test\nWelch’s \\(t\\)-test\nMann-Whitney \\(U\\) test\nCochran’s \\(Q\\) test\n\\(\\chi^2\\) Test for Independence\n\nANOVA and Linear Regression\n\nOne-Way ANOVA\nTwo-way ANOVA\nWelch’s ANOVA\nKruskal-Wallace Test\nTukey HSD Post-Hoc Test\nRepeated Measures ANOVA\nRandom Intercept Models\nCorrelation Matrices and Covariances\nMultiple Regression (linear)\nPolynomial regression\n\nGeneralized Linear Models\n\nGeneralized Linear Models: Binary\nGeneralized Linear Models: Ordered\nGeneralized Linear Models: Count (Poisson)\nGeneralized Linear Models: Count (Negative Binomial)\n\nSpecial Topics\n\nLinear Mixed Effects Models\nStructural Equation Models\nCox Proportional Hazards Regression\n(TBD) Multivariate Methods for Genetics/Genomics\nRidge, LASSO, and Elastic Net Regression\n\nPower Calculations (in progress)\n\n\n\n0.1.2 Lesson Outline\nThis is a shell of a lesson that can be copied and pasted for new lessons (or to edit and clean up existing lessons). If you copy this shell, then change all the headings from level 4 to 2. Replace &lt;the method&gt; with the name of your method, or its abbreviation. The file lessons/00_lesson_template.qmd has a .qmd template with these sections.\n\n0.1.2.1 Introduction to &lt;the method&gt;\n\n\n0.1.2.2 Mathematical definition of &lt;the method&gt;\n\n\n0.1.2.3 Data source and description\n\n\n0.1.2.4 Cleaning the data to create a model data frame\n\n\n0.1.2.5 Assumptions of &lt;the method&gt;\n\n\n0.1.2.6 Checking the assumptions with plots\n\n\n0.1.2.7 Code to run &lt;the method&gt;\n\n\n0.1.2.8 Code output\n\n\n0.1.2.9 Brief interpretation of the output",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course Materials for PHC 6099: 'R Computing for Health Sciences'</span>"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "2  About this Book",
    "section": "",
    "text": "2.1 About these Chapters\nThese are the written lecture materials for the class PHC 6099 at Florida International University’s Stempel College of Public Health. Each of the lessons were written by students, so we don’t guarantee that they are always mathematically/statistically accurate. You should use this material as a simple place to start to use these methods, but always read more about these methods when you use them to give yourself a better understanding of their theoretical foundations. This material should not be used to replace a traditional textbook in applied biostatistics. Here are some rather standard books on applied biostatistics (there are free/cheap versions on the internet for most of these texts, but I trust you to find them yourself):",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>About this Book</span>"
    ]
  },
  {
    "objectID": "about.html#about-these-chapters",
    "href": "about.html#about-these-chapters",
    "title": "2  About this Book",
    "section": "",
    "text": "Biostatistical Analysis. Jerrold H. Zar. https://www.pearson.com/en-us/subject-catalog/p/biostatistical-analysis/P200000006419/9780134995441.\nRegression Modelling Strategies. Frank E. Harrell, Jr. https://link.springer.com/book/10.1007/978-3-319-19425-7\nCategorical Data Analysis. Alan Agresti. https://onlinelibrary.wiley.com/doi/book/10.1002/0471249688",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>About this Book</span>"
    ]
  },
  {
    "objectID": "about.html#getting-help-in-r",
    "href": "about.html#getting-help-in-r",
    "title": "2  About this Book",
    "section": "2.2 Getting Help in R",
    "text": "2.2 Getting Help in R\nThis is the second semester of the “R” course sequence at FIU, so we spend very little time explaining the basics of the R language (the first semester is PHC6701; the text for that class is available here: https://gabrielodom.github.io/PHC6701_r4ds/). If you are new to R, please go back to the previous semester’s material and work through that first. If you want to see how we made this book, the source code and data sets for this book are available here: https://github.com/gabrielodom/PHC6099_rBiostat.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>About this Book</span>"
    ]
  },
  {
    "objectID": "01_header_EDA.html",
    "href": "01_header_EDA.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Text Outline\nThis text covers the basics of:",
    "crumbs": [
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "01_header_EDA.html#text-outline",
    "href": "01_header_EDA.html#text-outline",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Exploring data\nOne-sample tests\nTwo-sample tests\nLinear regression and ANOVA\nGeneralized linear models\nSome special topics\nMaybe a bit about power calculations…",
    "crumbs": [
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "01_header_EDA.html#part-outline",
    "href": "01_header_EDA.html#part-outline",
    "title": "Exploratory Data Analysis",
    "section": "Part Outline",
    "text": "Part Outline\nThis part of the text contains the chapters on the following packages which are useful for exploratory data analysis:\n\nggplot2:: mosaic plots, histograms, and violin plots\nggplot2:: scatterplots and facets\nskimr::\ntable1::\ngtsummary::",
    "crumbs": [
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "lessons/01_mosaic_violin.html",
    "href": "lessons/01_mosaic_violin.html",
    "title": "\n3  Mosaic & Box/Violin Plots\n",
    "section": "",
    "text": "3.1 Packages for this Lesson\n# Installing Required Packages\n# install.packages(\"public.ctn0094data\")\n# install.packages(\"tidyverse\")\n# install.packages(\"ggmosaic\")\n\n# Loading Required Packages\nlibrary(public.ctn0094data)\nlibrary(tidyverse)\nlibrary(ggmosaic)",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Mosaic & Box/Violin Plots</span>"
    ]
  },
  {
    "objectID": "lessons/01_mosaic_violin.html#introduction-to-mosaic-and-boxviolin-plots",
    "href": "lessons/01_mosaic_violin.html#introduction-to-mosaic-and-boxviolin-plots",
    "title": "\n3  Mosaic & Box/Violin Plots\n",
    "section": "\n3.2 Introduction to Mosaic and Box/Violin Plots",
    "text": "3.2 Introduction to Mosaic and Box/Violin Plots\nMosaic, box, and violin plots are useful for visualizing summary statistics.\nA mosaic plot is a special type of stacked bar chart used for two or more categorical variables. The width of the columns is proportional to the number of observations in each level of the variable plotted on the horizontal, or x-axis. The vertical length of the bars is proportional to the number of observations in the second variable within each level of the first variable.\nBox and violin plots are used for continuous variables by group. Box plots display six summary measures (the minimum, first quartile (Q1), median, third quartile (Q3), the interquartile range, and maximum). A violin plot illustrates the distribution of numerical data for one or more level of a categorical variable by combining summary statistics and density of each variable. Each curve corresponds to the respective frequency of data points within each region. A box plot is typically overlaid to provide additional information.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Mosaic & Box/Violin Plots</span>"
    ]
  },
  {
    "objectID": "lessons/01_mosaic_violin.html#data-source-and-description",
    "href": "lessons/01_mosaic_violin.html#data-source-and-description",
    "title": "\n3  Mosaic & Box/Violin Plots\n",
    "section": "\n3.3 Data Source and Description",
    "text": "3.3 Data Source and Description\nThe National Drug Abuse Treatment Clinical Trials Network (CTN) is a means by which medical and specialty treatment providers, treatment researchers, participating patients, and the National Institute on Drug Abuse cooperatively develop, validate, refine, and deliver new treatment options to patients. The CTN 094 demographics and everybody data sets from the public.ctn0094data package were utilized for the following visualizations. CTN 094 is a comprehensive, harmonized and normalized database of treatment data from CTN_0027, CTN_0030, and CTN_0051, where experiences of individuals with opioid use disorder (OUD) who seek care are described.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Mosaic & Box/Violin Plots</span>"
    ]
  },
  {
    "objectID": "lessons/01_mosaic_violin.html#cleaning-the-data-to-create-a-model-data-frame",
    "href": "lessons/01_mosaic_violin.html#cleaning-the-data-to-create-a-model-data-frame",
    "title": "\n3  Mosaic & Box/Violin Plots\n",
    "section": "\n3.4 Cleaning the Data to Create a Model Data Frame",
    "text": "3.4 Cleaning the Data to Create a Model Data Frame\nThe demographics and everybody data sets within the public.ctn0094data package were joined by ID (who variable). Race, age, is_male (gender), and project were selected features for the following visualizations.\n\n# Creating model data frame to include age, race, project, and is_male\n# from demographics and everybody data sets. Joined by subject ID (who)\ndemographics_df &lt;- demographics %&gt;% \n  left_join(everybody, by = \"who\")  %&gt;%\n  select(age, race, project, is_male)",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Mosaic & Box/Violin Plots</span>"
    ]
  },
  {
    "objectID": "lessons/01_mosaic_violin.html#assumptions-with-mosaic-boxviolin-plots",
    "href": "lessons/01_mosaic_violin.html#assumptions-with-mosaic-boxviolin-plots",
    "title": "\n3  Mosaic & Box/Violin Plots\n",
    "section": "\n3.5 Assumptions with Mosaic & Box/Violin Plots",
    "text": "3.5 Assumptions with Mosaic & Box/Violin Plots\nIn mosaic plots, two categorical variables are plotted along the horizontal (x) and vertical (y) axis. Each combination of categories forms a rectangle or tile within the plot.\nIn box and violin plots, a categorical variable is plotted along the horizontal or x-axis, while a continuous variable is plotted along the vertical or y-axis. Violin plots can be limiting if symmetry, skew, or other shape and variability characteristics are different between groups because precise comparison cannot be easily interpreted between density curves. For this reason, violin plots are typically rendered with another overlaid chart type, like box plot quartiles.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Mosaic & Box/Violin Plots</span>"
    ]
  },
  {
    "objectID": "lessons/01_mosaic_violin.html#code-to-run-mosaic-boxviolin-plots-output",
    "href": "lessons/01_mosaic_violin.html#code-to-run-mosaic-boxviolin-plots-output",
    "title": "\n3  Mosaic & Box/Violin Plots\n",
    "section": "\n3.6 Code to Run Mosaic & Box/Violin Plots & output",
    "text": "3.6 Code to Run Mosaic & Box/Violin Plots & output\n\n3.6.1 Mosaic Plots\nIn order to create a Mosaic plot, you must specify what data object you will be using within the ggplot() function. Then you will set aesthetic mapping options within the following geometric object layer: geom_mosaic().\nIn geom_mosaic(), the following aesthetics can be specified:\n\n\nweight: a weighting variable.\n\nx: categorical variable for the x-axis.\n\nSpecified as x = product(var1, var2, ...)\n\nThe product() function is used to extract the values from the categorical variable specified.\n\n\n\nalpha: a variable specifying transparency.\n\nIf the variable is not called in x:, then alpha: will be added in the first position.\n\n\n\nfill: a variable specifying fill color.\n\nIf the variable is not called in x:, then fill: will be added after the optional alpha: variable.\n\n\n\nconds: a variable specifying conditions.\n\nSpecified as conds = product(var1, var2, ...)\n\n\n\n\nThe ordering of the variables is vital as the product plot is created hierarchically.\n\n3.6.1.1 Basic Mosaic Plot\nIn the following example of a basic mosaic plot, we visualize the distribution of Race among CTN Projects 27, 30, and 51.\n\n# Basic Mosaic Plot\nmosaic_basic &lt;- demographics_df %&gt;% \n  ggplot() +\n  geom_mosaic(\n    aes(\n      # geom_mosaic() does not have one-to-one mapping between a variable and the x- \n      # or y-axis. So you must use the product() function when assigning a variable\n      # to the x-axis to account for the variable number of variables.\n      x = product(project),\n      fill = race\n    )\n  ) +\n  labs(\n    y = \"Race\",\n    x = \"Project\",\n    title = \"Mosaic Plot of Race by CTN Project\") +\n  # Specifies default `geom_mosaic` aesthetics, e.g white panel background, \n  # removes grid lines, adjusts widths and heights of rows and columns to \n  # reflect frequencies\n  theme_mosaic() +\n  # Removes legend illustrating Race and respective fill colors\n  theme(legend.position = \"None\")\n  \nmosaic_basic\n\n\n\n\n\n\nFigure 3.1\n\n\n\n\n\n3.6.1.2 More Advanced Mosaic Plot\nIn a more advanced version of a mosaic plot, we can visualize more than 2 categorical variables. The following example utilizes race, project, and ethnicity among CTN Projects 27, 30, and 51.\n\n# Advanced Mosaic Plot\nmosaic_advanced &lt;- demographics_df %&gt;% \n  ggplot() +\n  geom_mosaic(\n    aes(\n      x = product(race, project),\n      fill = is_male\n    )\n  ) +\n  labs(\n    y = \"Race\",\n    x = \"Project\",\n    title = \"Mosaic Plot of Race by Gender and CTN Project\",\n    fill = \"Gender\"\n  ) +\n  scale_fill_manual(\n    labels = c(\"No\" = \"Female\", \"Yes\" = \"Male\"),\n    values = c(\"darkseagreen2\", \"darkslategray3\", \"grey\")\n  ) +\n  theme_mosaic() +\n  # Adjust axis tick labels to 60 degrees and justification to the right\n  # with hjust (horizontal justification) and vjust (vertical justification)\n  theme(axis.text.x = element_text(angle = 60, hjust = 1, vjust = 1))\n  \nmosaic_advanced\n\n\n\n\n\n\nFigure 3.2\n\n\n\n\n\n3.6.2 Box Plots\nIn order to create a box plot, you must specify what data object you will be using within the ggplot() function. Then you will set aesthetic mapping options within the aes() or aesthetic layer. The geom_boxplot() layer specifies the box plot.\nThe following aesthetics are understood by geom_boxplot():\n\n\nx or y: Specifies the categorical variable along the x- or y-axis.\n\nlower or xlower: Specifies the 25th percentile/first quartile.\n\nupper or xupper: Specifies the 75th percentile/third quartile.\n\nmiddle or xmiddle: Specifies the 50th percentile/second quartile/median.\n\nymin or xmin: Specifies the y or x minimum for the plot.\n\nymax or xmax: Specifies the y or x maximum for the plot.\n\nalpha: Specifies a variable to determine transparency.\n\ncolor: Assigns an outline color to respective levels of a specified categorical variable.\n\nfill: Assigns a fill color to respective levels of a specified categorical variable.\n\ngroup: Partitions data by a discrete variable when no other grouping variable is specified, or grouping is incorrectly defaulted by R.\n\nlinetype: Specifies line type of box plot.\n\nlinewidth: Specifies line width of box plot.\n\nshape: Specifies the shape of the (outlier) points.\n\nsize: Specifies the size of the points and text.\n\nweight: Specifies a weight variable.\n\n\n3.6.2.1 Basic Box Plot\nThe following is a basic box plot showing the relationship between one continuous and one categorical variable.\n\n# Box Plot\nbox_basic &lt;- demographics_df %&gt;% \n  ggplot() +\n  aes(x = race, y = age, color = race) +\n  labs(\n    x = \"Race\",\n    y = \"Age\",\n    title = \"Box Plot of Race and Age\",\n    color = \"Race\"\n  ) +\n  # Using width to adjust the width of the boxes\n  geom_boxplot(width = 0.5) +\n  theme(legend.position = \"None\")\n\nbox_basic\n\n\n\n\n\n\nFigure 3.3\n\n\n\n\n\n3.6.2.2 More Advanced Box Plot\nWith geom_box(), you can also specify a additional categorical variable (different from your x and y variables) to break up your plot by that variable. For example, the following plot takes the previous plot of race and age and adds information side-by-side by gender (is_male).\n\n# Box Plot\nbox_advanced &lt;- demographics_df %&gt;% \n  ggplot() +\n  aes(x = race, y = age, color = is_male) +\n  # changing the labels for is_male, and specifying the colors we want\n  scale_color_manual(\n    labels = c(\"No\" = \"Female\", \"Yes\" = \"Male\"),\n    values = c(\"darkorchid4\", \"darkolivegreen4\")\n  ) +\n  labs(\n    x = \"Race\",\n    y = \"Age\",\n    title = \"Box Plot of Race and Age\",\n    color = \"Gender\"\n  ) +\n  # Using width to adjust the width of the boxes\n  geom_boxplot(width = 0.5)\n\nbox_advanced\n\n\n\n\n\n\nFigure 3.4\n\n\n\n\n\n3.6.3 Violin Plot\nIn order to create a Violin plot, you must specify what data object you will be using within the ggplot() function. Then you will set aesthetic mapping options within the aes() or aesthetic layer. The geom_violin() layer specifies the violin plot. An additional call for geom_boxplot() will overlay box quartiles on the violin plot to display summary statistics.\nThe following aesthetics are understood by geom_violin():\n\n\nx: Specifies the categorical variable along the x-axis.\n\ny: Specifies the continuous variable along the y-axis.\n\nalpha: Specifies a variable to determine transparency.\n\ncolor: Assigns an outline color to respective levels of a specified categorical variable.\n\nfill: Assigns a fill color to respective levels of a specified categorical variable.\n\ngroup: Partitions data by a discrete variable when no other grouping variable is specified, or grouping is incorrectly defaulted by R.\n\nlinetype: Specifies line type of violin plot.\n\nlinewidth: Specifies line width of violin plot.\n\nweight: Specifies a weight variable.\n\n\n# Violin Plot\nviolin_basic &lt;- demographics_df %&gt;% \n  ggplot() +\n  aes(x = race, y = age, color = race) +\n  scale_color_manual(\n    values = c(\"coral1\", \"darkgreen\", \"deepskyblue2\", \"darkorchid2\")\n  ) +\n  labs(\n    x = \"Race\",\n    y = \"Age\",\n    title = \"Violin Plot of Race and Age\",\n    subtitle = \"With Summary Information\",\n    color = \"Race\"\n  ) +\n  geom_violin() +\n  geom_boxplot(width = 0.1) +\n  theme(legend.position = \"None\")\n\nviolin_basic                        \n\n\n\n\n\n\nFigure 3.5",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Mosaic & Box/Violin Plots</span>"
    ]
  },
  {
    "objectID": "lessons/01_mosaic_violin.html#brief-interpretation",
    "href": "lessons/01_mosaic_violin.html#brief-interpretation",
    "title": "\n3  Mosaic & Box/Violin Plots\n",
    "section": "\n3.7 Brief Interpretation",
    "text": "3.7 Brief Interpretation\n\n3.7.1 Mosaic Plot\n\nCompared to Project 27 and Project 51, Project 30 had the highest proportion of participants who indicated that their race is ‘White’.\nCompared to Project 30 and Project 51, Project 27 had the highest proportion of participants who indicated that their race is ‘Other’.\nCompared to Project 27 and Project 51, Project 30 has the lowest proportion of participants who indicated that their race is ‘Other’.\n\n3.7.2 Box Plot\n\nParticipants who indicated that their race is ‘Black’ exhibited the highest median age of around 45 years old\nParticipants who indicated that their race is ‘White’ exhibited the lowest median age at approximately 31 years old.\n\n3.7.3 Violin Plot\nThis plot more clearly shows the bimodality of age by race among Black and ‘Other’ participants in CTN. It also shows the skewness of age in the White participants. Specifically:\n\nParticipants who indicated that their race is ‘White’ exhibited peak density around mid-20s compared to those who indicated that their race is ‘Black’, where peak density is exhibited around late-40s.\nParticipants who indicated that their race is ‘White’ had the lowest median age at approximately 31 years old, where participants who indicated that their race is ‘Black’ had the highest median age at approximately 45 years old.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Mosaic & Box/Violin Plots</span>"
    ]
  },
  {
    "objectID": "lessons/01_mosaic_violin.html#conclusion",
    "href": "lessons/01_mosaic_violin.html#conclusion",
    "title": "\n3  Mosaic & Box/Violin Plots\n",
    "section": "\n3.8 Conclusion",
    "text": "3.8 Conclusion\nThis lesson discusses three different plots for one-dimensional data: the Mosaic, Box, and Violin plots. Figure 3.1 is a basic mosaic plots that shows race by CTN project. In Figure 3.2, we added a third variable to the visualization: gender. The box plots, Figure 3.3 and Figure 3.4 we plotted age (continuous) by race and age by race and gender, respectively. Finally, Figure 3.5 shows a violin plot with an overlaid box plot for age by race in the CTN projects.\nMosaic plots are useful for proportionally visualizing the observations of two or more categorical variables. Box and violin plots can be used to visualize a continuous variable by one, or two in the case of box plots, categorical variables. Violin plots build on box plots in that they are able to provide quick information on the potential multimodal distribution(s) and skewness of a continuous variable across categories, as we saw in Figure 3.5.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Mosaic & Box/Violin Plots</span>"
    ]
  },
  {
    "objectID": "lessons/01_scatterplots.html",
    "href": "lessons/01_scatterplots.html",
    "title": "\n4  How to Create a Scatterplot\n",
    "section": "",
    "text": "4.1 Introduction to Scatterplots\nScatterplots display the relationship between two variables using dots to represent the values for each numeric variable. This presentation will examine the relationship between GDP per capita and Fertility over time using ggplot with facets.\nHypothesis: A negative relationship exists between GDP per capita and fertility i.e. as GDP per capita increases, fertility decreases.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>How to Create a Scatterplot</span>"
    ]
  },
  {
    "objectID": "lessons/01_scatterplots.html#gapminder-data-description",
    "href": "lessons/01_scatterplots.html#gapminder-data-description",
    "title": "\n4  How to Create a Scatterplot\n",
    "section": "\n4.2 Gapminder data description",
    "text": "4.2 Gapminder data description\nData was obtained from the dslabs package and comes from Gapminder a Swedish non-profit organization. The Gapmidner data set has health and income outcomes for 184 countries from 1960 to 2016. Gapminder aims to promote a fact-based worldview by providing accessible and understandable global development data. The dataset covers a wide range of variables, including economic, social, and health-related indicators like GDP, infant mortality, life expectancy, fertility, as well as population, making it a valuable resource for understanding global trends and patterns over time. Countries and territories with missing information were not excluded from the data set as the lack of information can also be looked into and shed light on why data was not collected or provided. To determine whether a country’s health and income outcomes are influenced by population sizes and GDP per capita, the data will be used to create a series of graphs to view different trends.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>How to Create a Scatterplot</span>"
    ]
  },
  {
    "objectID": "lessons/01_scatterplots.html#cleaning-the-data-to-create-a-model-data-frame",
    "href": "lessons/01_scatterplots.html#cleaning-the-data-to-create-a-model-data-frame",
    "title": "\n4  How to Create a Scatterplot\n",
    "section": "\n4.3 Cleaning the data to create a model data frame",
    "text": "4.3 Cleaning the data to create a model data frame\nA tibble was created from the gapminder dataset, and a new column was created to measure GDP per capita. Overall, using tibbles enhances the readability, usability, and compatibility of your code within the tidyverse ecosystem.\n\n# Creating gapminder dataset\\tibble\ngapminder_df &lt;-\n  as_tibble(gapminder) %&gt;%\n  mutate(gdp_per_capita = gdp / population)",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>How to Create a Scatterplot</span>"
    ]
  },
  {
    "objectID": "lessons/01_scatterplots.html#components-of-ggplot2",
    "href": "lessons/01_scatterplots.html#components-of-ggplot2",
    "title": "\n4  How to Create a Scatterplot\n",
    "section": "\n4.4 Components of ggplot2\n",
    "text": "4.4 Components of ggplot2\n\nggplot2 is a package used to create graphs and visualize data. The main three components of ggplot2 are the data, aesthetics and geom layers.\n\nThe data layer - states what data will be used to graph\nThe aesthetics layer - specifies the variables that are being mapped\nThe geom layer - specifies the type of graph to be produced",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>How to Create a Scatterplot</span>"
    ]
  },
  {
    "objectID": "lessons/01_scatterplots.html#code-to-run-interpretable-scatterplots-and-create-facets",
    "href": "lessons/01_scatterplots.html#code-to-run-interpretable-scatterplots-and-create-facets",
    "title": "\n4  How to Create a Scatterplot\n",
    "section": "\n4.5 Code to run interpretable scatterplots and create facets",
    "text": "4.5 Code to run interpretable scatterplots and create facets\nIn order to create a scatter-plot using ggplot, you must specify what data you will be using, state which variables will be mapped and how under aesthetics. What differentiates the scatter-plot from any other type of graph will be specified under the geom layer. For the scatter-plot, geom_point will be used.\nIn this example, we will analyze the relationship between fertility rates and gdp per capita for each country in 2011.\n\nfig_bubble_2011 &lt;-\n  ggplot(data = filter(gapminder_df, year == 2011)) +\n  aes(x = gdp_per_capita, y = fertility) +\n  geom_point()\n\nfig_bubble_2011 \n\n\n\n\n\n\nFigure 4.1: Association between fertility rates and gdp per capita for each country in 2011\n\n\n\n\nIn the example above, we have mapped out fertility as our y-axis and gdp per capita as our x-axis. However, at it’s very basic level, there is not enough information provided to accurately analyze the relationship between the two. For this reason, we can add additional layers that will provide more information to properly analyze the scatter-plot.\n\nfig_bubble_pretty_2011 &lt;-\n  ggplot(data = filter(gapminder_df, year == 2011)) +\n  aes(\n    x = gdp_per_capita,\n    y = fertility,\n    # will change the size of the point based on population size \n    size = population, \n    # will assign colors based on the continent the country is in \n    color = continent\n  ) +\n  # gives a range as to how big or small the points of population should be\n  scale_size(range = c(1, 20)) + \n  # removes N/A from the legend and titles it Continent \n  scale_colour_discrete(na.translate = F, name = \"Continent\") +\n  # removes population size from the legend \n  guides(size = \"none\") +\n  scale_x_continuous(\n    name = \"GDP per Capita\",\n    trans = \"log10\",\n    # transforms numbers from scientific notation to regular number \n    labels = scales::comma\n  ) +\n  labs(\n    title = \"Fertility rate descreases as GDP per capita increases in 2011\",\n    y = \"Fertility rates\",\n    caption = \"Source: Gapminder\"\n  ) +\n  # the ylim was set based on the fertility, lowest was near 1 & highest was above 7\n  ylim(1.2, 8.0) +\n  # alpha increases transparency of the points to ensure they can all be seen\n  geom_point(alpha = 0.5) \n\nfig_bubble_pretty_2011\n\n\n\n\n\n\nFigure 4.2: Association between fertility rates and gdp per capita for each country, grouped by continent, in 2011\n\n\n\n\nFigure 4.2 builds on the previous scatterplot of Fertility Rates (y axis) against GDP per capita (x axis) for 2011. The bubble size depicts respective country populations, and continents are coded by colors according to the key. This figure displays a negative relationship between GDP per capita and Fertility Rates. It supports the Hypothesis which states that as GDP per capita increases, Fertility Rates decreases. This trend can be confirmed for all continents, however, the degree to which fertility rates drop between continents varies. Most European country appear below a fertility rate of 2 babies per woman. The Americas appear to follow closely behind (under 4), followed by Oceania and Asia. A significant number of African countries still maintained higher fertility rates with lower GDP per capita for 2011.\nThis is an example of wanting to create four separate graphs to see the relationship between fertility rates and GDP per capita based on the years 1960, 1975, 1990 and 2005. In this example we omitted the facet argument.\n\nfig_bubble_multiple &lt;-\n  ggplot(data = filter(gapminder_df, year %in% c(1960, 1975, 1990, 2005))) +\n  aes(\n    x = gdp_per_capita,\n    y = fertility,\n    size = population,\n    color = continent\n  ) +\n  scale_x_continuous(\n    name = \"GDP per Capita\",\n    trans = \"log10\",\n    labels = scales::comma\n  ) +\n  scale_size(range = c(0, 20)) +\n  guides(size = \"none\") +\n  scale_colour_discrete(na.translate = FALSE, name = \"Continent\") +\n  labs(\n    title = \"Fertility continues to decrease as GDP per capita increases\",\n    caption = \"Source: Gapminder\",\n    y = \"Fertility rates\"\n  ) +\n  geom_point(alpha = 0.3) \n\nfig_bubble_multiple\n\n\n\n\n\n\nFigure 4.3: Association between fertility rates and GDP per capita based on the years 1960, 1975, 1990 and 2005\n\n\n\n\nWithout having used the facet argument, all points of all four years have been included into one graph. This graph does not provide us with the information we were looking for.\n\nfig_bubble_multiple_facet &lt;-\n  ggplot(data = filter(gapminder_df, year %in% c(1960, 1975, 1990, 2005))) +\n  aes(\n    x = gdp_per_capita,\n    y = fertility,\n    size = population,\n    color = continent\n  ) +\n  scale_x_continuous(\n    name = \"GDP per Capita\",\n    trans = \"log10\",\n    labels = scales::comma\n  ) +\n  scale_size(range = c(0, 20)) +\n  guides(size = \"none\") +\n  scale_colour_discrete(na.translate = FALSE , name = \"Continent\") +\n  labs(\n    title = \"Fertility continues to decrease as GDP per capita increases\",\n    caption = \"Source: Gapminder\",\n    y = \"Fertility rates\"\n  ) +\n  geom_point(alpha = 0.3) +\n  # specifiying we want the graphs split based on year\n  facet_wrap(~ year)\n\nfig_bubble_multiple_facet\n\n\n\n\n\n\nFigure 4.4: Association between fertility rates and GDP per capita based on the years 1960, 1975, 1990 and 2005, using facet\n\n\n\n\nNow that we’ve specified the facet argument, we now have four separate graphs that can be properly analysed. In Figure 4.4 we see an increasingly negative relationship between the two variables over time. This observation is congruent with the hypothesis that as GDP per capita increases, fertility decreases.\nThis global trend can be attributed to the increasing proportion of women in the workforce in the mid to late 20th century. As a result of World War II (1939-1945), women took on roles outside the home to compensate for men at war. Despite increased GDP per capita, this may have contributed to reduced fertility (babies per woman) over time. In 1960, a clear disparity among continents is seen. Most European countries’ fertility rates fell below 5, while their GDP per capita increased. Most African countries maintained high fertility rates above 5, but little change is seen in GDP per capita. The Asian continent shows the most variation among countries during that year. Some smaller Asian countries continued to maintain high fertility rates as GDP per capita increased in 1960. However, others displayed a drastic decrease in fertility rates by 1960. The Americas followed a steady decline over the years. By 2005, an overall negative relationship can be seen with most countries’ fertility rates below 5 babies per woman.\n\nfig_bubble_row_2011 &lt;-\n  ggplot(data = filter(gapminder_df, year == 2011)) +\n  aes(\n    x = gdp_per_capita,\n    y = fertility\n  ) +\n  geom_point(alpha = 0.5) +\n  facet_wrap(~ continent, nrow = 1)\n\nfig_bubble_row_2011 \n\n\n\n\n\n\nFigure 4.5: Association between fertility rates and GDP per capita based on continent\n\n\n\n\nIn the graph above, we see an example of separating the single graph into graphs based on continent. It has also been specified to have all graphs appear in one single row through the nrow argument. Very importantly however, this graph is unclear and cannot be used to compare the relationship between fertility and gdp per capita.\n\nfig_bubble_facet_2011 &lt;-\n  ggplot(data = filter(gapminder_df, year == 2011)) +\n  aes(\n    x = gdp_per_capita,\n    y = fertility\n  ) +\n  scale_x_continuous(\n    name = \"GDP per Capita\",\n    trans = \"log10\",\n    labels = scales::comma\n  )  +\n  geom_point(alpha = 0.5) +\n  facet_wrap(~ continent)\n\nfig_bubble_facet_2011 \n\n\n\n\n\n\nFigure 4.6: Association between fertility rates and GDP per capita based on continent not using nrow\n\n\n\n\nIn the next example above, we removed the nrow argument and the system automatically separated the graphs into three columns with two rows. Additionally, we changed the x-axis to a log scale to better interpret gdp per capita. There is a way to determine a relationship between fertility and gdp per capita by continent.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>How to Create a Scatterplot</span>"
    ]
  },
  {
    "objectID": "lessons/01_scatterplots.html#public-health-interpretation",
    "href": "lessons/01_scatterplots.html#public-health-interpretation",
    "title": "\n4  How to Create a Scatterplot\n",
    "section": "\n4.6 Public Health Interpretation",
    "text": "4.6 Public Health Interpretation\nA global negative trend is depicted between GDP per capita and fertility over time. Such changes were due to wars as well as social, cultural and economic changes that incentivize smaller families especially in Asian countries. Most European, American and Asian countries depicted significant decreases in fertility rates over time as GDP per capita increased. On the other hand, African countries remain in the top rank for fertility over the years. These differences are depicted in the population pyramid changes of developed vs developing countries. Public health policies can be tailored to incentivizing increased fertility in developed countries to ensure generation continuity, and effective family planning strategies in developing countries.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>How to Create a Scatterplot</span>"
    ]
  },
  {
    "objectID": "lessons/01_scatterplots.html#conclusion",
    "href": "lessons/01_scatterplots.html#conclusion",
    "title": "\n4  How to Create a Scatterplot\n",
    "section": "\n4.7 Conclusion",
    "text": "4.7 Conclusion\nIn this lesson, the basic functions of ggplot2 package were shown, which can create a scatterplot. There are three layers to the code to make a plot in R: data, aesthetic, and geometric. Within the aesthetic layer, functions can be added such as size and color to analyze more variables. Additionally, facets can split up graphs over a categorical variable, adding another potential variable to analyze in the plot.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>How to Create a Scatterplot</span>"
    ]
  },
  {
    "objectID": "lessons_original/01_skimr.html",
    "href": "lessons_original/01_skimr.html",
    "title": "\n5  Skimr Package\n",
    "section": "",
    "text": "5.1 Introduction\nSkimr is an R package designed to provide summary statistics about variables in data frames, tibbles, data tables and vectors. The function is modifiable where you can add additional variables, which are not a part of default summary function within R. Skimr allows us to quickly assess data quality by feature and type in a quick report. This is a critical step in Data Exploration, where Understanding our data helps us to generate a hypothesis and determine what data analysis are appropriate.\nThis presentation will cover the simplest and most effective ways to explore data in R.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Skimr Package</span>"
    ]
  },
  {
    "objectID": "lessons_original/01_skimr.html#introduction",
    "href": "lessons_original/01_skimr.html#introduction",
    "title": "\n5  Skimr Package\n",
    "section": "",
    "text": "5.1.1 Packages\nTo begin we will upload the packages necessary for the lesson, this includes the following:\n\n\nreadr() to import our data file\n\nknitr() that houses the kable() feature that allows us to construct and customize tables.\n\ntidyverse houses the dyplyrpackage that assists with data manipulation and visualization.\nTheskimrpackage provides a compact summary of the variables in a dataset.\n\n\n# install.packages(\"skimr\")\n# install.packages(\"knitr\")\n# install.packages(\"tidyverse\")\n\n# load all the packages we will need to analyze the data and use the skim\n#   function\nlibrary(skimr)\nlibrary(knitr)\nlibrary(readxl)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n5.1.2 Census Data\nFor this assignment we will be using the Census_2010 dataset. There is no code book associated with the data, making it difficult to provide an accurate description of the variables. The information recorded shows the United States population estimates from the years 2010-2015, as well as relevant variables like net population change, number of births, number of deaths, international and domestic migration. Within the dataframe, there are 3,193 observations and 100 variables.\nThe data can be imported into R from the following link: https://fiudit-my.sharepoint.com/:x:/g/personal/ssinc013_fiu_edu/ESK1A13PstVGtf7HUwNNt68Bnh1YPfH8L-hnvMUxjBuCVw?e=CCwQU9\n\n# import the data\n# census_2010 &lt;- read_csv(\"Data/census_2010.csv\")\ncensus_2010 &lt;- readxl::read_xlsx(\"../data/01_census_2010.xlsx\")\n\n# what are the variables\ncolnames(census_2010) %&gt;% \n  head(n = 10)\n\n [1] \"SUMLEV\"            \"REGION\"            \"DIVISION\"         \n [4] \"STATE\"             \"COUNTY\"            \"STNAME\"           \n [7] \"CTYNAME\"           \"CENSUS2010POP\"     \"ESTIMATESBASE2010\"\n[10] \"POPESTIMATE2010\"",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Skimr Package</span>"
    ]
  },
  {
    "objectID": "lessons_original/01_skimr.html#the-summary-function",
    "href": "lessons_original/01_skimr.html#the-summary-function",
    "title": "\n5  Skimr Package\n",
    "section": "\n5.2 The Summary() Function",
    "text": "5.2 The Summary() Function\nIn R, the most similar function is summary(). The summary() function in R can be used to quickly summarize the values in a data frame or vector.\nThis syntax shows examples of the summary function using both our data set, and a vector:\n\n#| label: Summary-syntax-with-data\n\n# Example using summary function with data\nsummary(census_2010$CENSUS2010POP)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n      82    11299    26424   193387    71404 37253956 \n\n# Example using summary function with vector\n# Define vector\nx &lt;- c(3, 4, 23, 5, 7, 8, 9, 12, 26, 15, 20, 21, NA)\n\n# Summarize values in vector\nsummary(x)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   3.00    6.50   10.50   12.75   20.25   26.00       1 \n\n\nThe summary() function automatically calculates: The minimum value, The value of the 1st quartile (25th percentile), The median value, The value of the 3rd quartile (75th percentile) and The maximum value. Any missing values (NA) in the vector, the summary() function will automatically exclude them when calculating the summary statistics.\nNow, let’s see how skim() compares.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Skimr Package</span>"
    ]
  },
  {
    "objectID": "lessons_original/01_skimr.html#skimr-package",
    "href": "lessons_original/01_skimr.html#skimr-package",
    "title": "\n5  Skimr Package\n",
    "section": "\n5.3 Skimr Package",
    "text": "5.3 Skimr Package\nThe skim() function will generate a summary of the variables in your dataset, including their data type, number of non-missing values, minimum and maximum values, median, mean, standard deviation, and more (Waring et al. 2022).\nThe following syntax ensures that the data is compatible with Skimr functions.\n\nCode# is the summary data a skimr dataframe\nskim(census_2010) %&gt;% \n  is_skim_df() # TRUE\n\n[1] TRUE\nattr(,\"message\")\ncharacter(0)\n\n\nWe can explore the data as a tibble:\n\nCode# use skim to get descriptive statistics of the data\nskim(census_2010) %&gt;% \n  head(n = 10)\n\n\nData summary\n\n\nName\ncensus_2010\n\n\nNumber of rows\n3193\n\n\nNumber of columns\n100\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n8\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\nSTNAME\n0\n1\n4\n20\n0\n51\n0\n\n\nCTYNAME\n0\n1\n4\n33\n0\n1927\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nSUMLEV\n0\n1\n49.84\n1.25\n40\n50\n50\n50\n50\n▁▁▁▁▇\n\n\nREGION\n0\n1\n2.67\n0.81\n1\n2\n3\n3\n4\n▁▆▁▇▂\n\n\nDIVISION\n0\n1\n5.19\n1.97\n1\n4\n5\n7\n9\n▂▇▅▆▃\n\n\nSTATE\n0\n1\n30.26\n15.15\n1\n18\n29\n45\n56\n▃▇▆▆▇\n\n\nCOUNTY\n0\n1\n101.92\n107.63\n0\n33\n77\n133\n840\n▇▁▁▁▁\n\n\nCENSUS2010POP\n0\n1\n193387.05\n1176201.45\n82\n11299\n26424\n71404\n37253956\n▇▁▁▁▁\n\n\nESTIMATESBASE2010\n0\n1\n193396.87\n1176244.25\n82\n11299\n26446\n71491\n37254503\n▇▁▁▁▁\n\n\nPOPESTIMATE2010\n0\n1\n193765.65\n1178710.28\n83\n11275\n26467\n71721\n37334079\n▇▁▁▁▁\n\n\n\n\n\nUsing skimr functions provides a cleaner and more detailed display of the results compared to the summary() function. In this example we are showing the first ten variables in our data set. The data summary tab shows the number of rows and columns, column type frequency and group variables. There is also additional descriptive information like missing values, unique characters.\nThis will be relevant for data cleaning as well as understanding the distribution. Both are critical to determine which statistical analysis would be most appropriate to use for a project.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Skimr Package</span>"
    ]
  },
  {
    "objectID": "lessons_original/01_skimr.html#other-skimr-features",
    "href": "lessons_original/01_skimr.html#other-skimr-features",
    "title": "\n5  Skimr Package\n",
    "section": "\n5.4 Other Skimr Features",
    "text": "5.4 Other Skimr Features\n\n5.4.1 Separate dataframes by type\nThe data frames produced by skim() are wide and sparse, filled with columns that are mostly NA. For that reason, it can be convenient to work with “by type” subsets of the original data frame. These smaller subsets have their NA columns removed.\nFeatures:\n\n\npartition() - Creates a list of smaller data frames. Each entry in the list is a data type from the original dataframe\n\nbind() - Takes the list and rebuilds the original dataframe.\n\nyank() - Extract a subtable from a dataframe with a particular type.\n\nThe following syntax is using partition() to separate the large census_df.\n\nCode# split the character and numeric data\nseparate_df &lt;- partition(skim(census_2010))\n# check only the character data\nseparate_df$character\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\nSTNAME\n0\n1\n4\n20\n0\n51\n0\n\n\nCTYNAME\n0\n1\n4\n33\n0\n1927\n0\n\n\n\n\nCode# create summary statistics for only numeric variables\nnumeric_separate_df &lt;- separate_df[2]\n# pull out the desired summary statistics in the nested list\nhead(numeric_separate_df$numeric[\"mean\"]) %&gt;% \n  kable(digits = 1) \n\n\n\nmean\n\n\n\n49.8\n\n\n2.7\n\n\n5.2\n\n\n30.3\n\n\n101.9\n\n\n193387.1\n\n\n\n\n\nThe following syntax is using bind() to combine the smaller character and numeric lists into the desired df.\n\nCode# combine the character and numeric data\nhead(bind(separate_df))\n\n\nData summary\n\n\nName\ncensus_2010\n\n\nNumber of rows\n3193\n\n\nNumber of columns\n100\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\nSTNAME\n0\n1\n4\n20\n0\n51\n0\n\n\nCTYNAME\n0\n1\n4\n33\n0\n1927\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nSUMLEV\n0\n1\n49.84\n1.25\n40\n50\n50\n50\n50\n▁▁▁▁▇\n\n\nREGION\n0\n1\n2.67\n0.81\n1\n2\n3\n3\n4\n▁▆▁▇▂\n\n\nDIVISION\n0\n1\n5.19\n1.97\n1\n4\n5\n7\n9\n▂▇▅▆▃\n\n\nSTATE\n0\n1\n30.26\n15.15\n1\n18\n29\n45\n56\n▃▇▆▆▇\n\n\n\n\nCode# confirm that the bound table is the same as the original skimmed table\nidentical(bind(separate_df), skim(census_2010)) \n\n[1] TRUE\n\n\nThe following syntax is using yank() to extract a specific table eg.character to examine.\n\nCode# Extract character data\nyank(skim(census_2010), \"character\")\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\nSTNAME\n0\n1\n4\n20\n0\n51\n0\n\n\nCTYNAME\n0\n1\n4\n33\n0\n1927\n0\n\n\n\n\n\n\n5.4.2 Skimr with Dplyr\nSkimr functions can be used in combination with Dplyr functions to examine specific variables within the census dataset.\nThe following example used skim() with filter() to display the variable CENSUS2010POP. The dataframe was further customized to display variable name and data type using select().\n\nCode# use dplyr functions on the statistics summary table\ncensus_filter &lt;- skim(census_2010) %&gt;% \n  filter(skim_variable == \"CENSUS2010POP\")\ncensus_filter\n\n\nData summary\n\n\nName\ncensus_2010\n\n\nNumber of rows\n3193\n\n\nNumber of columns\n100\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\nCENSUS2010POP\n0\n1\n193387\n1176201\n82\n11299\n26424\n71404\n37253956\n▇▁▁▁▁\n\n\n\nCodecensus_select &lt;- skim(census_2010) %&gt;% \n  select(skim_type, skim_variable)\nhead(census_select)\n\n# A tibble: 6 × 2\n  skim_type skim_variable\n  &lt;chr&gt;     &lt;chr&gt;        \n1 character STNAME       \n2 character CTYNAME      \n3 numeric   SUMLEV       \n4 numeric   REGION       \n5 numeric   DIVISION     \n6 numeric   STATE        \n\n\nYou can also customize the output of the skim() function by using various arguments. For example, you can use the numeric argument to specify which variables should be treated as numeric variables, or use the ranges argument to specify custom ranges for variables.\nUsing skim() in combination with mutate() we will compute a new variable to add to our skim dataframe.\n\nCode# create a new variable calculate the change in birth rate from 2010 to 2011\ncensus_2010 %&gt;% \n  # new variable\n  mutate(net_birth = BIRTHS2011 - BIRTHS2010) %&gt;% \n  # move the variable to the beginning of the dataset\n  relocate(net_birth, .after = CENSUS2010POP) %&gt;% \n  # summary statistics table\n  skim() %&gt;% \n  # only the first fifteen variables\n  head(n = 15) %&gt;% \n  # change the formatting \n  kable(digit = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_type\nskim_variable\nn_missing\ncomplete_rate\ncharacter.min\ncharacter.max\ncharacter.empty\ncharacter.n_unique\ncharacter.whitespace\nnumeric.mean\nnumeric.sd\nnumeric.p0\nnumeric.p25\nnumeric.p50\nnumeric.p75\nnumeric.p100\nnumeric.hist\n\n\n\ncharacter\nSTNAME\n0\n1\n4\n20\n0\n51\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\ncharacter\nCTYNAME\n0\n1\n4\n33\n0\n1927\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nnumeric\nSUMLEV\n0\n1\nNA\nNA\nNA\nNA\nNA\n49.84\n1.25\n40\n50\n50\n50\n50\n▁▁▁▁▇\n\n\nnumeric\nREGION\n0\n1\nNA\nNA\nNA\nNA\nNA\n2.67\n0.81\n1\n2\n3\n3\n4\n▁▆▁▇▂\n\n\nnumeric\nDIVISION\n0\n1\nNA\nNA\nNA\nNA\nNA\n5.19\n1.97\n1\n4\n5\n7\n9\n▂▇▅▆▃\n\n\nnumeric\nSTATE\n0\n1\nNA\nNA\nNA\nNA\nNA\n30.26\n15.15\n1\n18\n29\n45\n56\n▃▇▆▆▇\n\n\nnumeric\nCOUNTY\n0\n1\nNA\nNA\nNA\nNA\nNA\n101.92\n107.63\n0\n33\n77\n133\n840\n▇▁▁▁▁\n\n\nnumeric\nCENSUS2010POP\n0\n1\nNA\nNA\nNA\nNA\nNA\n193387.05\n1176201.45\n82\n11299\n26424\n71404\n37253956\n▇▁▁▁▁\n\n\nnumeric\nnet_birth\n0\n1\nNA\nNA\nNA\nNA\nNA\n1870.12\n11792.85\n-3\n96\n232\n639\n386443\n▇▁▁▁▁\n\n\nnumeric\nESTIMATESBASE2010\n0\n1\nNA\nNA\nNA\nNA\nNA\n193396.87\n1176244.25\n82\n11299\n26446\n71491\n37254503\n▇▁▁▁▁\n\n\nnumeric\nPOPESTIMATE2010\n0\n1\nNA\nNA\nNA\nNA\nNA\n193765.65\n1178710.28\n83\n11275\n26467\n71721\n37334079\n▇▁▁▁▁\n\n\nnumeric\nPOPESTIMATE2011\n0\n1\nNA\nNA\nNA\nNA\nNA\n195251.40\n1189647.76\n90\n11277\n26417\n72387\n37700034\n▇▁▁▁▁\n\n\nnumeric\nPOPESTIMATE2012\n0\n1\nNA\nNA\nNA\nNA\nNA\n196744.52\n1200508.37\n81\n11195\n26362\n72496\n38056055\n▇▁▁▁▁\n\n\nnumeric\nPOPESTIMATE2013\n0\n1\nNA\nNA\nNA\nNA\nNA\n198200.69\n1211123.45\n89\n11180\n26519\n72222\n38414128\n▇▁▁▁▁\n\n\nnumeric\nPOPESTIMATE2014\n0\n1\nNA\nNA\nNA\nNA\nNA\n199754.09\n1222669.36\n87\n11121\n26483\n72257\n38792291\n▇▁▁▁▁\n\n\n\n\n\n\n5.4.3 Adding Variables\n\nbase - An sfl that sets skimmers for all column types.\nappend - Whether the provided options should be in addition to the defaults already in skim. Default is TRUE.\n\nAs mentioned, skim() is designed to display default statistics, however you can use this function to change the summary statistics that it returns.\nskim_with() is type closure: a function that returns adds a new variable to the table. This lets you have several skimming functions in a single R session, but it also means that you need to assign the return of skim_with() before you can use it.\nYou assign values within skim_with() by using the sfl() helper (skimr function list). It identifies which skimming functions you want to remove, by setting them to NULL. Assign an sfl to each column type that you wish to modify.\nFor example, we will add the following variables to the dataframe: median, min, max, IQR, length.\n\nCodemy_skim &lt;- skim_with(\n  numeric = sfl(median, min, max, IQR),\n  character = sfl(length), \n  append = TRUE\n)\n\n# add new variables into the summary table\ncensus_2010 %&gt;% \n  my_skim() %&gt;% \n  head(n = 10)\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n3193\n\n\nNumber of columns\n100\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n8\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\nlength\n\n\n\nSTNAME\n0\n1\n4\n20\n0\n51\n0\n3193\n\n\nCTYNAME\n0\n1\n4\n33\n0\n1927\n0\n3193\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\nmedian\nmin\nmax\nIQR\n\n\n\nSUMLEV\n0\n1\n49.84\n1.25\n40\n50\n50\n50\n50\n▁▁▁▁▇\n50\n40\n50\n0\n\n\nREGION\n0\n1\n2.67\n0.81\n1\n2\n3\n3\n4\n▁▆▁▇▂\n3\n1\n4\n1\n\n\nDIVISION\n0\n1\n5.19\n1.97\n1\n4\n5\n7\n9\n▂▇▅▆▃\n5\n1\n9\n3\n\n\nSTATE\n0\n1\n30.26\n15.15\n1\n18\n29\n45\n56\n▃▇▆▆▇\n29\n1\n56\n27\n\n\nCOUNTY\n0\n1\n101.92\n107.63\n0\n33\n77\n133\n840\n▇▁▁▁▁\n77\n0\n840\n100\n\n\nCENSUS2010POP\n0\n1\n193387.05\n1176201.45\n82\n11299\n26424\n71404\n37253956\n▇▁▁▁▁\n26424\n82\n37253956\n60105\n\n\nESTIMATESBASE2010\n0\n1\n193396.87\n1176244.25\n82\n11299\n26446\n71491\n37254503\n▇▁▁▁▁\n26446\n82\n37254503\n60192\n\n\nPOPESTIMATE2010\n0\n1\n193765.65\n1178710.28\n83\n11275\n26467\n71721\n37334079\n▇▁▁▁▁\n26467\n83\n37334079\n60446",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Skimr Package</span>"
    ]
  },
  {
    "objectID": "lessons_original/01_skimr.html#conclusion",
    "href": "lessons_original/01_skimr.html#conclusion",
    "title": "\n5  Skimr Package\n",
    "section": "\n5.5 Conclusion",
    "text": "5.5 Conclusion\nOverall, Skimr is a useful package for quickly summarizing the variables in a dataset and gaining insights into its structure and content.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Skimr Package</span>"
    ]
  },
  {
    "objectID": "lessons_original/01_skimr.html#references",
    "href": "lessons_original/01_skimr.html#references",
    "title": "\n5  Skimr Package\n",
    "section": "\n5.6 References",
    "text": "5.6 References\n\n\n\n\nWaring, Elin, Michael Quinn, Amelia McNamara, Eduardo Arino de la Rubia, Hao Zhu, and Shannon Ellis. 2022. Skimr: Compact and Flexible Summaries of Data. https://docs.ropensci.org/skimr/.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Skimr Package</span>"
    ]
  },
  {
    "objectID": "lessons/01_table1.html",
    "href": "lessons/01_table1.html",
    "title": "\n6  Demographics Table With Table1\n",
    "section": "",
    "text": "6.1 Introduction\nIn most scientific research journals, the first included table is often referred to as Table1. It is a table that presents descriptive statistics of baseline characteristics of the study population stratified by exposure. This package makes it fairly straightforward to produce such a table using R. Table1 includes descriptive statistics for the total study sample, with the rows (explanatory variables) consisting of the key study variables that are often included in the final analysis1. Then within the columns (outcome of interest/response variable), you will find cells given as an (%) for categorical variables, whereas a mean, SD, or the median will be provided for continuous variables. Additionally, there will be a total column provided which can help in the assessment of the overall sample.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Demographics Table With Table1</span>"
    ]
  },
  {
    "objectID": "lessons/01_table1.html#necessary-packages",
    "href": "lessons/01_table1.html#necessary-packages",
    "title": "\n6  Demographics Table With Table1\n",
    "section": "\n6.2 Necessary Packages",
    "text": "6.2 Necessary Packages\nThe htmlTable package allows for the usage of the table1() function to create a table 1, while also making life easy when attempting to copy this table into a Word document.\nThe boot package was created to aid in performing bootstrapping analysis. With it comes numerous data sets, specifically clinical trial data sets to make this possible. However, there is no code book provided within the package when the data is downloaded as a csv file. This is a link on Github that explains and elaborates on every data within the package itself2.\n\n#install.packages(\"htmlTable\")\n#install.packages(\"boot\")\n\n# Load libraries\nlibrary(htmlTable)\nlibrary(table1)\nlibrary(boot)",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Demographics Table With Table1</span>"
    ]
  },
  {
    "objectID": "lessons/01_table1.html#data-source-and-description",
    "href": "lessons/01_table1.html#data-source-and-description",
    "title": "\n6  Demographics Table With Table1\n",
    "section": "\n6.3 Data source and description",
    "text": "6.3 Data source and description\nToday, we will be using the melanoma data set which consists of malignant melanoma measurements of patients. Each patient had their tumor surgically removed between the years of 1962 and 1977 at the Department of Plastic Surgery, University Hospital of Odense located in Denamrk. Each surgery consisted of the complete removal of the tumor with an additional removal of about 2.5cm of the surrounding skin. When this was completed, the thickness of the tumor was recorded along with the physical appearance of ulceration vs no ulceration, as it is an important prognostic indication of those with a thick/ulcerated tumor to have an increased chance of death as a consequence of melanoma.\n\ndata(melanoma, package = \"boot\")\nmelanoma_data &lt;- melanoma\n\n#Now that we loaded the raw data set, we will conduct a visual exploration before wrangling #the data and applying any functions, while also considering the requirements involved in #the construction of a table1.\n\nsummary(melanoma_data)\n\n      time          status          sex              age             year     \n Min.   :  10   Min.   :1.00   Min.   :0.0000   Min.   : 4.00   Min.   :1962  \n 1st Qu.:1525   1st Qu.:1.00   1st Qu.:0.0000   1st Qu.:42.00   1st Qu.:1968  \n Median :2005   Median :2.00   Median :0.0000   Median :54.00   Median :1970  \n Mean   :2153   Mean   :1.79   Mean   :0.3854   Mean   :52.46   Mean   :1970  \n 3rd Qu.:3042   3rd Qu.:2.00   3rd Qu.:1.0000   3rd Qu.:65.00   3rd Qu.:1972  \n Max.   :5565   Max.   :3.00   Max.   :1.0000   Max.   :95.00   Max.   :1977  \n   thickness         ulcer      \n Min.   : 0.10   Min.   :0.000  \n 1st Qu.: 0.97   1st Qu.:0.000  \n Median : 1.94   Median :0.000  \n Mean   : 2.92   Mean   :0.439  \n 3rd Qu.: 3.56   3rd Qu.:1.000  \n Max.   :17.42   Max.   :1.000",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Demographics Table With Table1</span>"
    ]
  },
  {
    "objectID": "lessons/01_table1.html#cleaning-the-data-to-create-a-model-data-frame",
    "href": "lessons/01_table1.html#cleaning-the-data-to-create-a-model-data-frame",
    "title": "\n6  Demographics Table With Table1\n",
    "section": "\n6.4 Cleaning the data to create a model data frame",
    "text": "6.4 Cleaning the data to create a model data frame\nLet us now explore the type of variables within the data set.\n\ntypeof(melanoma_data$status) \n\n[1] \"double\"\n\n\nWe will first provide a basic table1 to illustrate how the function works. Currently, all the variables are in numeric/double formats, however for the creation of a basic table1, it is of importance to convert the dependent/response variable of interest to reflect categories (factor).\nOur main variable of interest (dependent/response) is the status. According to the code book found in Github, status is coded into three levels that indicate the patients status at the end of the study. Level 1 indicates that they had died from melanoma, Level 2 indicates that they were still alive at the conclusion of the study, and Level 3 indicates that they had died from causes unrelated to their melanoma. As such, we will factor the “status” variable into three levels. With this in mind, let us go ahead and convert melanoma into a factor variable with three levels. For ease of analysis we will use 2 = “Alive” as the reference level. This can be done in two ways:\n\nAlthough more time consuming, it is highly recommended that beginners utilize the function as.factor() and then utilize the recode_factor() function to minimize the errors.\nWhen you become more skilled and are able to understand how the factor function works, it is possible to do everything in one step with the factor() function. In this function you can put levels and labels all in one function instead of having to break it up into more than one function.\n\nFor our example we will use as.factor then recode_factor() using 2 = “Alive” as our reference group.\n\nmelanoma_data$status &lt;-\n  as.factor(melanoma_data$status)\n\n# print the first six observations\nhead(melanoma_data$status)\n\n[1] 3 3 2 3 1 1\nLevels: 1 2 3\n\n# Recode\nmelanoma_data$status &lt;- recode_factor(\n  melanoma_data$status, \n  \"2\" = \"Alive\", # this is the reference group\n  \"1\" = \"Died from melanoma\",\n  \"3\" = \"Non-Melanoma death\"\n)\n\n# Print the first six observations\nhead(melanoma_data$status)\n\n[1] Non-Melanoma death Non-Melanoma death Alive              Non-Melanoma death\n[5] Died from melanoma Died from melanoma\nLevels: Alive Died from melanoma Non-Melanoma death\n\n\nAs you can see in the variable levels, “Alive” is the reference level. It is extremely important to pick a reference level to lay the foundation of the table along with highlighting the outcome of interest of your hypothesis. In summary, this lays the foundation of a well organized table.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Demographics Table With Table1</span>"
    ]
  },
  {
    "objectID": "lessons/01_table1.html#creation-of-basic-table-1",
    "href": "lessons/01_table1.html#creation-of-basic-table-1",
    "title": "\n6  Demographics Table With Table1\n",
    "section": "\n6.5 Creation of basic table 1",
    "text": "6.5 Creation of basic table 1\nNow that our main variable of interest is a factor with three levels, we will run a basic table1 with the independent/explanatory variables of interest: sex, age, ulcer, and thickness.\nRecall that the explanatory variables of interest are still in “double” formats. Conveniently, to analyze data before the independent variables are converted to factors and labeled, the table1 provides the ability to highlight level results. This only applies for independent variables that are in numeric/double formats in which each number represents a group. For instance 0 although is a number format we know it has a group meaning such as male.\nFor the independent variables, if they have factors in the front, it provides the number of cases (aka observations). If they are a continuous variable, we will get the mean, the SD, the minimum and the maximum amounts.\n\nbasic_table1 &lt;- table1( \n  ~ factor(sex) + age + factor(ulcer) + thickness | status, \n  data = melanoma_data\n)\n\nbasic_table1\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlive(N=134)\nDied from melanoma(N=57)\nNon-Melanoma death(N=14)\nOverall(N=205)\n\n\n\nfactor(sex)\n\n\n\n\n\n\n0\n91 (67.9%)\n28 (49.1%)\n7 (50.0%)\n126 (61.5%)\n\n\n1\n43 (32.1%)\n29 (50.9%)\n7 (50.0%)\n79 (38.5%)\n\n\nage\n\n\n\n\n\n\nMean (SD)\n50.0 (15.9)\n55.1 (17.9)\n65.3 (10.9)\n52.5 (16.7)\n\n\nMedian [Min, Max]\n52.0 [4.00, 84.0]\n56.0 [14.0, 95.0]\n65.0 [49.0, 86.0]\n54.0 [4.00, 95.0]\n\n\nfactor(ulcer)\n\n\n\n\n\n\n0\n92 (68.7%)\n16 (28.1%)\n7 (50.0%)\n115 (56.1%)\n\n\n1\n42 (31.3%)\n41 (71.9%)\n7 (50.0%)\n90 (43.9%)\n\n\nthickness\n\n\n\n\n\n\nMean (SD)\n2.24 (2.33)\n4.31 (3.57)\n3.72 (3.63)\n2.92 (2.96)\n\n\nMedian [Min, Max]\n1.36 [0.100, 12.9]\n3.54 [0.320, 17.4]\n2.26 [0.160, 12.6]\n1.94 [0.100, 17.4]\n\n\n\n\n\n\n\nNote that the table1 package uses a familiar formula interface, where the variables to include in the table are separated by ‘+’ symbols, the “stratification” variable (which creates the columns) appears to the right of a “conditioning” symbol ‘|’, and the data argument specifies a data.frame that contains the variables in the formula.\nIf we do not put factor for a grouped variable then the following will happen:\n\nwrong_table1 &lt;- table1(\n  ~ sex + age + ulcer + thickness | status, \n  data = melanoma_data\n)\n\nwrong_table1\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlive(N=134)\nDied from melanoma(N=57)\nNon-Melanoma death(N=14)\nOverall(N=205)\n\n\n\nsex\n\n\n\n\n\n\nMean (SD)\n0.321 (0.469)\n0.509 (0.504)\n0.500 (0.519)\n0.385 (0.488)\n\n\nMedian [Min, Max]\n0 [0, 1.00]\n1.00 [0, 1.00]\n0.500 [0, 1.00]\n0 [0, 1.00]\n\n\nage\n\n\n\n\n\n\nMean (SD)\n50.0 (15.9)\n55.1 (17.9)\n65.3 (10.9)\n52.5 (16.7)\n\n\nMedian [Min, Max]\n52.0 [4.00, 84.0]\n56.0 [14.0, 95.0]\n65.0 [49.0, 86.0]\n54.0 [4.00, 95.0]\n\n\nulcer\n\n\n\n\n\n\nMean (SD)\n0.313 (0.466)\n0.719 (0.453)\n0.500 (0.519)\n0.439 (0.497)\n\n\nMedian [Min, Max]\n0 [0, 1.00]\n1.00 [0, 1.00]\n0.500 [0, 1.00]\n0 [0, 1.00]\n\n\nthickness\n\n\n\n\n\n\nMean (SD)\n2.24 (2.33)\n4.31 (3.57)\n3.72 (3.63)\n2.92 (2.96)\n\n\nMedian [Min, Max]\n1.36 [0.100, 12.9]\n3.54 [0.320, 17.4]\n2.26 [0.160, 12.6]\n1.94 [0.100, 17.4]\n\n\n\n\n\n\n\nAs you can see above, we have the incorrect values provided of the explanatory variables. For example, in the variable of sex, we expect to see the number of individuals who identify as male or female, but instead we observe the mean, which is not a proper descriptive statistic as sex is a categorical variable.\nTo avoid this issue as well as problems in other procedures (like logistic regressions), it is crucial that we remember to factor the variables before we run any function. But because we don’t have nice labels for the variables and categories, it doesn’t look great. To improve things, we can create factors with descriptive labels for the categorical variables (sex and ulcer), label each variable the way we want, and specify units for the continuous variables (age and thickness). According to the code book, the patient’s sex: 1 = male, 0 = female, and ulcer is an indicator of ulceration : 1 = present, 0 = absent. We also specify that the overall column to be labeled “Total” and be positioned on the left, and add a caption and footnote:\n\nmelanoma_data$sex &lt;- as.factor(melanoma_data$sex)\n\n# print the first six observations\nhead(melanoma_data$sex)\n\n[1] 1 1 1 0 1 1\nLevels: 0 1\n\n# Recode\nmelanoma_data$sex &lt;- recode_factor(\n  melanoma_data$sex, \n  \"0\" = \"Female\",\n  \"1\" = \"Male\"\n)\n\n# Print the first six observations\nhead(melanoma_data$sex)\n\n[1] Male   Male   Male   Female Male   Male  \nLevels: Female Male\n\n\n\ntypeof(melanoma_data$ulcer)\n\n[1] \"double\"\n\nmelanoma_data$ulcer &lt;- as.factor(melanoma_data$ulcer)\n\n# print the first six observations\nhead(melanoma_data$ulcer)\n\n[1] 1 0 0 0 1 1\nLevels: 0 1\n\n# Recode\nmelanoma_data$ulcer &lt;- recode_factor(\n  melanoma_data$ulcer, \n  \"0\" = \"Absent\",\n  \"1\" = \"Present\"\n)\n\n# Print the first six observations\nhead(melanoma_data$ulcer)\n\n[1] Present Absent  Absent  Absent  Present Present\nLevels: Absent Present\n\n\nIn addition, we need to add units to the two continuous variables age and thickness. According to the code book, age is the patient’s age measured in years and thickness corresponds to the tumor’s thickness in millimeters (mm). The package table1 provides an easy way to demonstrate measurement information:\n\nunits(melanoma_data$age) &lt;- \"years\"\nunits(melanoma_data$thickness) &lt;- \"mm\"\n\nAdditionally, for visual and descriptive purposes, the function table1 is able to easily provide labels for the variables that will be shown in the final table using the label() function. Also, (caption \\&lt;-) provides a title for the table and (footnote \\&lt;-) provides any footnote information.\n\nlabel(melanoma_data$sex) &lt;- \"Sex\"\nlabel(melanoma_data$age) &lt;- \"Age\"\nlabel(melanoma_data$ulcer) &lt;- \"Ulceration\"\nlabel(melanoma_data$thickness) &lt;-\"Thickness*\"\n\ncaption_char &lt;- \"Table 1. Melanoma Dataset Descriptive Statistics\"\nfootnote_char &lt;- \"*Also known as Breslow thickness\"\n\nBelow, we can demonstrate the final table1 layout. As you can see, you no longer use factor() in front of the variable as we already factorized it in the previous steps.\n\ntable1(\n  ~ sex + age + ulcer + thickness | status, \n  data = melanoma_data,\n  overall = c(left = \"Total\"), \n  caption = caption_char, \n  footnote = footnote_char\n)\n\n\nTable 1. Melanoma Dataset Descriptive Statistics\n\n\n\nTotal(N=205)\nAlive(N=134)\nDied from melanoma(N=57)\nNon-Melanoma death(N=14)\n\n\n*Also known as Breslow thickness\n\n\n\nSex\n\n\n\n\n\n\nFemale\n126 (61.5%)\n91 (67.9%)\n28 (49.1%)\n7 (50.0%)\n\n\nMale\n79 (38.5%)\n43 (32.1%)\n29 (50.9%)\n7 (50.0%)\n\n\nAge (years)\n\n\n\n\n\n\nMean (SD)\n52.5 (16.7)\n50.0 (15.9)\n55.1 (17.9)\n65.3 (10.9)\n\n\nMedian [Min, Max]\n54.0 [4.00, 95.0]\n52.0 [4.00, 84.0]\n56.0 [14.0, 95.0]\n65.0 [49.0, 86.0]\n\n\nUlceration\n\n\n\n\n\n\nAbsent\n115 (56.1%)\n92 (68.7%)\n16 (28.1%)\n7 (50.0%)\n\n\nPresent\n90 (43.9%)\n42 (31.3%)\n41 (71.9%)\n7 (50.0%)\n\n\nThickness* (mm)\n\n\n\n\n\n\nMean (SD)\n2.92 (2.96)\n2.24 (2.33)\n4.31 (3.57)\n3.72 (3.63)\n\n\nMedian [Min, Max]\n1.94 [0.100, 17.4]\n1.36 [0.100, 12.9]\n3.54 [0.320, 17.4]\n2.26 [0.160, 12.6]",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Demographics Table With Table1</span>"
    ]
  },
  {
    "objectID": "lessons/01_table1.html#changing-the-tables-appearance",
    "href": "lessons/01_table1.html#changing-the-tables-appearance",
    "title": "\n6  Demographics Table With Table1\n",
    "section": "\n6.6 Changing the table’s appearance",
    "text": "6.6 Changing the table’s appearance\nThe default style of table1 uses an Arial font, and resembles the booktabs style commonly used in LaTeX. While this default style is not ugly, inevitably there will be a desire to customize the visual appearance of the table (fonts, colors, gridlines, etc). The package provides a limited number of built-in options for changing the style, while further customization can be achieved in R Markdown documents using CSS.3\n\n6.6.1 Using built-in styles\nThe package includes a limited number of built-in styles including:\n\nzebra: alternating shaded and unshaded rows (zebra stripes)\ngrid: show all grid lines\nshade: shade the header row(s) in gray\ntimes: use a serif font\n\nThese styles can be selected using the topclass argument of table1. Some examples follow:\n\ntable1(~ sex + age + ulcer + thickness | status, \n       data = melanoma_data,\n       overall = c(left = \"Total\"), \n       caption = caption_char, \n       footnote = footnote_char, \n       topclass=\"Rtable1-zebra\"\n)\n\n\nTable 1. Melanoma Dataset Descriptive Statistics\n\n\n\nTotal(N=205)\nAlive(N=134)\nDied from melanoma(N=57)\nNon-Melanoma death(N=14)\n\n\n*Also known as Breslow thickness\n\n\n\nSex\n\n\n\n\n\n\nFemale\n126 (61.5%)\n91 (67.9%)\n28 (49.1%)\n7 (50.0%)\n\n\nMale\n79 (38.5%)\n43 (32.1%)\n29 (50.9%)\n7 (50.0%)\n\n\nAge (years)\n\n\n\n\n\n\nMean (SD)\n52.5 (16.7)\n50.0 (15.9)\n55.1 (17.9)\n65.3 (10.9)\n\n\nMedian [Min, Max]\n54.0 [4.00, 95.0]\n52.0 [4.00, 84.0]\n56.0 [14.0, 95.0]\n65.0 [49.0, 86.0]\n\n\nUlceration\n\n\n\n\n\n\nAbsent\n115 (56.1%)\n92 (68.7%)\n16 (28.1%)\n7 (50.0%)\n\n\nPresent\n90 (43.9%)\n42 (31.3%)\n41 (71.9%)\n7 (50.0%)\n\n\nThickness* (mm)\n\n\n\n\n\n\nMean (SD)\n2.92 (2.96)\n2.24 (2.33)\n4.31 (3.57)\n3.72 (3.63)\n\n\nMedian [Min, Max]\n1.94 [0.100, 17.4]\n1.36 [0.100, 12.9]\n3.54 [0.320, 17.4]\n2.26 [0.160, 12.6]\n\n\n\n\n\n\ntable1(~ sex + age + ulcer + thickness | status, \n       data = melanoma_data,\n       overall = c(left = \"Total\"), \n       caption = caption_char, \n       footnote = footnote_char, \n       topclass=\"Rtable1-grid\"\n)\n\n\nTable 1. Melanoma Dataset Descriptive Statistics\n\n\n\nTotal(N=205)\nAlive(N=134)\nDied from melanoma(N=57)\nNon-Melanoma death(N=14)\n\n\n*Also known as Breslow thickness\n\n\n\nSex\n\n\n\n\n\n\nFemale\n126 (61.5%)\n91 (67.9%)\n28 (49.1%)\n7 (50.0%)\n\n\nMale\n79 (38.5%)\n43 (32.1%)\n29 (50.9%)\n7 (50.0%)\n\n\nAge (years)\n\n\n\n\n\n\nMean (SD)\n52.5 (16.7)\n50.0 (15.9)\n55.1 (17.9)\n65.3 (10.9)\n\n\nMedian [Min, Max]\n54.0 [4.00, 95.0]\n52.0 [4.00, 84.0]\n56.0 [14.0, 95.0]\n65.0 [49.0, 86.0]\n\n\nUlceration\n\n\n\n\n\n\nAbsent\n115 (56.1%)\n92 (68.7%)\n16 (28.1%)\n7 (50.0%)\n\n\nPresent\n90 (43.9%)\n42 (31.3%)\n41 (71.9%)\n7 (50.0%)\n\n\nThickness* (mm)\n\n\n\n\n\n\nMean (SD)\n2.92 (2.96)\n2.24 (2.33)\n4.31 (3.57)\n3.72 (3.63)\n\n\nMedian [Min, Max]\n1.94 [0.100, 17.4]\n1.36 [0.100, 12.9]\n3.54 [0.320, 17.4]\n2.26 [0.160, 12.6]\n\n\n\n\n\n\ntable1(~ sex + age + ulcer + thickness | status, \n       data = melanoma_data,\n       overall = c(left = \"Total\"), \n       caption = caption_char, \n       footnote = footnote_char, \n       topclass=\"Rtable1-grid Rtable1-shade Rtable1-times\"\n)\n\n\nTable 1. Melanoma Dataset Descriptive Statistics\n\n\n\nTotal(N=205)\nAlive(N=134)\nDied from melanoma(N=57)\nNon-Melanoma death(N=14)\n\n\n*Also known as Breslow thickness\n\n\n\nSex\n\n\n\n\n\n\nFemale\n126 (61.5%)\n91 (67.9%)\n28 (49.1%)\n7 (50.0%)\n\n\nMale\n79 (38.5%)\n43 (32.1%)\n29 (50.9%)\n7 (50.0%)\n\n\nAge (years)\n\n\n\n\n\n\nMean (SD)\n52.5 (16.7)\n50.0 (15.9)\n55.1 (17.9)\n65.3 (10.9)\n\n\nMedian [Min, Max]\n54.0 [4.00, 95.0]\n52.0 [4.00, 84.0]\n56.0 [14.0, 95.0]\n65.0 [49.0, 86.0]\n\n\nUlceration\n\n\n\n\n\n\nAbsent\n115 (56.1%)\n92 (68.7%)\n16 (28.1%)\n7 (50.0%)\n\n\nPresent\n90 (43.9%)\n42 (31.3%)\n41 (71.9%)\n7 (50.0%)\n\n\nThickness* (mm)\n\n\n\n\n\n\nMean (SD)\n2.92 (2.96)\n2.24 (2.33)\n4.31 (3.57)\n3.72 (3.63)\n\n\nMedian [Min, Max]\n1.94 [0.100, 17.4]\n1.36 [0.100, 12.9]\n3.54 [0.320, 17.4]\n2.26 [0.160, 12.6]\n\n\n\n\n\nNote that the style name needs to be preceded by the prefix Rtable1-. Multiple styles can be applied in combination by separating them with a space.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Demographics Table With Table1</span>"
    ]
  },
  {
    "objectID": "lessons/01_table1.html#conclusion",
    "href": "lessons/01_table1.html#conclusion",
    "title": "\n6  Demographics Table With Table1\n",
    "section": "\n6.7 Conclusion",
    "text": "6.7 Conclusion\nIn conclusion, table1 is one of the most utilized tools in the scientific research field. Understanding how to use the table1 package in R can be of benefit to many. It is important to note that this presentation is just a brief summary with what is possible with this package. For example, you can add extra columns to the table, other than descriptive statistics. This can be accomplished using the extra.col option. In addition, you can also stratify the response variable to highlight two of the responses, like dead or alive in our example.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Demographics Table With Table1</span>"
    ]
  },
  {
    "objectID": "lessons/01_table1.html#references",
    "href": "lessons/01_table1.html#references",
    "title": "\n6  Demographics Table With Table1\n",
    "section": "References",
    "text": "References\n\n\n\n\n1. \nHayes-Larson E, Kezios KL, Mooney SJ, Lovasi G. Who is in this study, anyway? Guidelines for a useful Table 1. Journal of Clinical Epidemiology [Internet] 2019;114:125–32. Available from: http://dx.doi.org/10.1016/j.jclinepi.2019.06.011\n\n\n\n2. \nA. C. Davison, D. V. Hinkley. Bootstrap methods and their applications [Internet]. Cambridge: Cambridge University Press; 1997. Available from: doi:10.1017/CBO9780511802843\n\n\n\n3. \nCohen J. Statistical power analysis for the behavioral sciences. 2nd ed. Hillsdale, N.J: L. Erlbaum Associates; 1988.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Demographics Table With Table1</span>"
    ]
  },
  {
    "objectID": "lessons/01_gtsummary.html",
    "href": "lessons/01_gtsummary.html",
    "title": "\n7  Table by gtsummary\n",
    "section": "",
    "text": "7.1 Packages for this Lesson\n# Installing Required Packages\n# install.packages(\"public.ctn0094data\")\n# install.packages(\"tidyverse\")\n# install.packages(\"gtsummary\")\n\n# Loading Required Packages\nlibrary(public.ctn0094data)\nlibrary(tidyverse)\nlibrary(gtsummary)\nlibrary(dplyr) # for re-coding",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Table by gtsummary</span>"
    ]
  },
  {
    "objectID": "lessons/01_gtsummary.html#introduction-to-gtsummary",
    "href": "lessons/01_gtsummary.html#introduction-to-gtsummary",
    "title": "\n7  Table by gtsummary\n",
    "section": "\n7.2 Introduction to ‘gtsummary’",
    "text": "7.2 Introduction to ‘gtsummary’\nThe gtsummary package is useful mainly for creating publication-ready tables (i.e.demographic table, simple summary table, contingency-table, regression table, etc.). The best feature of this package is it can automatically detect if the data is continuous, dichotomous or categorical, and which descriptive statistics needs to apply.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Table by gtsummary</span>"
    ]
  },
  {
    "objectID": "lessons/01_gtsummary.html#data-source-and-description",
    "href": "lessons/01_gtsummary.html#data-source-and-description",
    "title": "\n7  Table by gtsummary\n",
    "section": "\n7.3 Data Source and Description",
    "text": "7.3 Data Source and Description\nThe public.ctn0094data package provides harmonized and normalized data sets from the CTN-0094 clinical trial. These data sets describe the experiences of care-seeking individuals suffering from opioid use disorder (OUD). The trial is part of the Clinical Trials Network (CTN) protocol number 0094, funded by the US National Institute of Drug Abuse (NIDA). It is used by the NIDA to develop, validate, refine, and deliver new treatment options to patients.\nIn this lesson, I used the demographics, and fagerstrom data sets from the public.ctn0094data package to demonstrate the gtsummary function. The demographics part contains the demographic variables such as age, sex, race, marital status etc. The fagerstrom part contains data on smoking habit (smoker/non-smoker, Fagerstrom Test for Nicotine Dependence Score (ranging from 0 to 10) ~ FTND, Number of cigarettes smoked per day.). The FTND is a questionnaire that assesses the physical dependence of adults on nicotine. The test uses yes/no questions scored from 0 to 1 and multiple-choice questions scored from 0 to 3, and the total score ranges from 0 to 10. The higher the score, the more intense the patient’s nicotine dependence is. The score categories are: 8+: High dependence, 7–5: Moderate dependence, 4–3: Low to moderate dependence and 0–2: Low dependence.\n\n# Searching suitable data sets: You can skip \ndata(package = \"public.ctn0094data\")\n#data(demographics, package = \"public.ctn0094data\")\n#names(demographics)\n#data(fagerstrom, package = \"public.ctn0094data\")\n#names(fagerstrom)\n#table(fagerstrom$ftnd)",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Table by gtsummary</span>"
    ]
  },
  {
    "objectID": "lessons/01_gtsummary.html#creating-model-data-frames",
    "href": "lessons/01_gtsummary.html#creating-model-data-frames",
    "title": "\n7  Table by gtsummary\n",
    "section": "\n7.4 Creating Model Data Frames",
    "text": "7.4 Creating Model Data Frames\nThe demographics and fagerstrom data sets within the public.ctn0094data package were joined by ID (who variable) and a new dta frame smoking_df is created.\n\n# Joining data sets: \nsmoking_df &lt;- demographics %&gt;% \n  left_join(fagerstrom, by = \"who\")",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Table by gtsummary</span>"
    ]
  },
  {
    "objectID": "lessons/01_gtsummary.html#demographic-table-with-tbl_summary-function",
    "href": "lessons/01_gtsummary.html#demographic-table-with-tbl_summary-function",
    "title": "\n7  Table by gtsummary\n",
    "section": "\n7.5 Demographic Table with tbl_summary Function",
    "text": "7.5 Demographic Table with tbl_summary Function\n\n7.5.1 Creating Table 1: Demographic Characteristic\nIn order to create a basic demographic table, I will now select which variables I want to show in the table and then use the tbl_summary function to create the table. I am also adding the description of the variables I included in my table.\n\n\nage: an integer variable that indicates the Age of the patient.\n\nrace: a factor variable with levels ‘Black’, ‘Other Refused/missing’, and ‘White’, which represents the Self-reported race of the patient.\n\neducation: a factor variable denotes the Education level at intake, with levels ‘HS/GED’ for high school graduate or equivalent, ‘Less than HS’ for less than high school education, ‘More than HS’ for some education beyond high school, and ‘Missing’ if the information is not provided.\n\nis_male: a factor variable with levels ‘No’ and ‘Yes’, describing the Sex (not gender) of the patient, where ‘Yes’ indicates male.\n\nmarital: a factor variable indicating the Marital status at intake, with levels ‘Married or Partnered’, ‘Never married’, ‘Separated/Divorced/Widowed’, and ‘Not answered’ if the question was not asked during intake.\n\nis_smoker: a factor indicating whether the patient is a smoker or not. Levels include “No” (not a smoker) and “Yes” (a smoker).\n\n\n# Selecting variables in a new data frame `table_1df` for table 1\ntable_1df &lt;- smoking_df %&gt;% \n  select(age, race, education, is_male, marital, is_smoker)\n\n# Table 1\ntable_1 &lt;- table_1df  %&gt;% tbl_summary()\n\ntable_1\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nN = 3,5601\n\n\n\n\nage\n34 (27, 45)\n\n\n    Unknown\n208\n\n\nrace\n\n\n\n    Black\n365 (10%)\n\n\n    Other\n506 (14%)\n\n\n    Refused/missing\n58 (1.6%)\n\n\n    White\n2,631 (74%)\n\n\neducation\n\n\n\n    HS/GED\n691 (39%)\n\n\n    Less than HS\n352 (20%)\n\n\n    More than HS\n724 (41%)\n\n\n    Unknown\n1,793\n\n\nis_male\n2,351 (66%)\n\n\n    Unknown\n4\n\n\nmarital\n\n\n\n    Married or Partnered\n329 (19%)\n\n\n    Never married\n1,028 (59%)\n\n\n    Separated/Divorced/Widowed\n394 (23%)\n\n\n    Unknown\n1,809\n\n\nis_smoker\n2,631 (85%)\n\n\n    Unknown\n460\n\n\n\n\n1 Median (IQR); n (%)\n\n\n\n\n\n\n\n7.5.2 Customizing Table 1: Changing the Label\nI am using label function to change the label of all variables. Other customization will be shown in the next contingency table.\n\n# Changing the Label\n\ntable_1 &lt;-\n  table_1df %&gt;% \n  tbl_summary(\n    label = list(\n      age = \"Age\",\n      race = \"Race\",\n      education = \"Education level\",\n      is_male = \"Male\",\n      marital = \"Marital status\",\n      is_smoker = \"Smoker\"\n    )\n  )\n\ntable_1\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nN = 3,5601\n\n\n\n\nAge\n34 (27, 45)\n\n\n    Unknown\n208\n\n\nRace\n\n\n\n    Black\n365 (10%)\n\n\n    Other\n506 (14%)\n\n\n    Refused/missing\n58 (1.6%)\n\n\n    White\n2,631 (74%)\n\n\nEducation level\n\n\n\n    HS/GED\n691 (39%)\n\n\n    Less than HS\n352 (20%)\n\n\n    More than HS\n724 (41%)\n\n\n    Unknown\n1,793\n\n\nMale\n2,351 (66%)\n\n\n    Unknown\n4\n\n\nMarital status\n\n\n\n    Married or Partnered\n329 (19%)\n\n\n    Never married\n1,028 (59%)\n\n\n    Separated/Divorced/Widowed\n394 (23%)\n\n\n    Unknown\n1,809\n\n\nSmoker\n2,631 (85%)\n\n\n    Unknown\n460\n\n\n\n\n1 Median (IQR); n (%)",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Table by gtsummary</span>"
    ]
  },
  {
    "objectID": "lessons/01_gtsummary.html#contingency-table-with-tbl_summary-function",
    "href": "lessons/01_gtsummary.html#contingency-table-with-tbl_summary-function",
    "title": "\n7  Table by gtsummary\n",
    "section": "\n7.6 Contingency Table with tbl_summary Function",
    "text": "7.6 Contingency Table with tbl_summary Function\n\n7.6.1 Creating Table 2: Demographic Variables by Smoking Status\nI will now show the table 1 demographic variables by smoking habit status (is_smoker, Yes = smoker and No = non-smokers)\n\n# Contingency table \ntable_2 &lt;- table_1df %&gt;% tbl_summary(by = is_smoker) \n\ntable_2\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nNo, N = 4691\n\n\nYes, N = 2,6311\n\n\n\n\nage\n36 (28, 47)\n33 (26, 44)\n\n\n    Unknown\n7\n79\n\n\nrace\n\n\n\n\n    Black\n46 (9.8%)\n259 (9.8%)\n\n\n    Other\n68 (14%)\n376 (14%)\n\n\n    Refused/missing\n1 (0.2%)\n18 (0.7%)\n\n\n    White\n354 (75%)\n1,978 (75%)\n\n\neducation\n\n\n\n\n    HS/GED\n104 (34%)\n531 (41%)\n\n\n    Less than HS\n23 (7.5%)\n290 (22%)\n\n\n    More than HS\n179 (58%)\n488 (37%)\n\n\n    Unknown\n163\n1,322\n\n\nis_male\n336 (72%)\n1,724 (66%)\n\n\nmarital\n\n\n\n\n    Married or Partnered\n75 (25%)\n236 (18%)\n\n\n    Never married\n152 (50%)\n775 (59%)\n\n\n    Separated/Divorced/Widowed\n77 (25%)\n293 (22%)\n\n\n    Unknown\n165\n1,327\n\n\n\n\n1 Median (IQR); n (%)\n\n\n\n\n\n\n\n7.6.2 Removing Missing Data\nIf I do not want to show the missing data in my table, I will use missing = \"no\".\n\n# Removing Missing Data\ntable_2nm &lt;- table_1df %&gt;% tbl_summary(by = is_smoker,\n                                   missing = \"no\") \ntable_2nm\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nNo, N = 4691\n\n\nYes, N = 2,6311\n\n\n\n\nage\n36 (28, 47)\n33 (26, 44)\n\n\nrace\n\n\n\n\n    Black\n46 (9.8%)\n259 (9.8%)\n\n\n    Other\n68 (14%)\n376 (14%)\n\n\n    Refused/missing\n1 (0.2%)\n18 (0.7%)\n\n\n    White\n354 (75%)\n1,978 (75%)\n\n\neducation\n\n\n\n\n    HS/GED\n104 (34%)\n531 (41%)\n\n\n    Less than HS\n23 (7.5%)\n290 (22%)\n\n\n    More than HS\n179 (58%)\n488 (37%)\n\n\nis_male\n336 (72%)\n1,724 (66%)\n\n\nmarital\n\n\n\n\n    Married or Partnered\n75 (25%)\n236 (18%)\n\n\n    Never married\n152 (50%)\n775 (59%)\n\n\n    Separated/Divorced/Widowed\n77 (25%)\n293 (22%)\n\n\n\n\n1 Median (IQR); n (%)\n\n\n\n\n\n\n\n7.6.3 Applying Statistical Tests\nI will use add_p function to show the statistical analysis. This will automatically detect if data in each variable is continuous, dichotomous or categorical, and apply the appropriate descriptive statistics accordingly.\n\n# Adding p-value\ntable_2 &lt;- table_1df %&gt;% tbl_summary(by = is_smoker,\n                                   missing = \"no\") %&gt;% \n  add_p()\n\ntable_2\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nNo, N = 4691\n\n\nYes, N = 2,6311\n\n\np-value2\n\n\n\n\nage\n36 (28, 47)\n33 (26, 44)\n&lt;0.001\n\n\nrace\n\n\n0.8\n\n\n    Black\n46 (9.8%)\n259 (9.8%)\n\n\n\n    Other\n68 (14%)\n376 (14%)\n\n\n\n    Refused/missing\n1 (0.2%)\n18 (0.7%)\n\n\n\n    White\n354 (75%)\n1,978 (75%)\n\n\n\neducation\n\n\n&lt;0.001\n\n\n    HS/GED\n104 (34%)\n531 (41%)\n\n\n\n    Less than HS\n23 (7.5%)\n290 (22%)\n\n\n\n    More than HS\n179 (58%)\n488 (37%)\n\n\n\nis_male\n336 (72%)\n1,724 (66%)\n0.010\n\n\nmarital\n\n\n0.006\n\n\n    Married or Partnered\n75 (25%)\n236 (18%)\n\n\n\n    Never married\n152 (50%)\n775 (59%)\n\n\n\n    Separated/Divorced/Widowed\n77 (25%)\n293 (22%)\n\n\n\n\n\n\n1 Median (IQR); n (%)\n\n\n\n2 Wilcoxon rank sum test; Fisher’s exact test; Pearson’s Chi-squared test\n\n\n\n\n\n\n\nNote: The footnote 2 shows all the statistical tests applied to this table. It can be understandable from the table that for categorical variable it applied Pearson’s Chi-squared test, for continuous non-normal distributed variable it applied Wilcoxon rank sum test; and for small sample data, it applied Fisher’s exact test. It would be great to see different footnotes for each of the test next to each p-value, however, I did not find a way to do that.\n\n7.6.4 Customizing Table 2(a)\nI will now customize the table 2 to show total number and overall number and show missing values by using the following functions:\n\n# Adding total and overall number \ntable_2 &lt;- table_1df %&gt;% tbl_summary(by = is_smoker,\n                                   label = list(\n                                     age = \"Age\",\n                                     race = \"Race\",\n                                     education = \"Education level\",\n                                     is_male = \"Male\",\n                                     marital = \"Marital status\"\n                                   ),\n                                   missing_text = \"(Missing)\"\n                                  ) %&gt;% \n  add_p() %&gt;%\n  add_n() %&gt;%\n  add_overall() \n\ntable_2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nN\n\nOverall, N = 3,1001\n\n\nNo, N = 4691\n\n\nYes, N = 2,6311\n\n\np-value2\n\n\n\n\nAge\n3,014\n34 (27, 45)\n36 (28, 47)\n33 (26, 44)\n&lt;0.001\n\n\n    (Missing)\n\n86\n7\n79\n\n\n\nRace\n3,100\n\n\n\n0.8\n\n\n    Black\n\n305 (9.8%)\n46 (9.8%)\n259 (9.8%)\n\n\n\n    Other\n\n444 (14%)\n68 (14%)\n376 (14%)\n\n\n\n    Refused/missing\n\n19 (0.6%)\n1 (0.2%)\n18 (0.7%)\n\n\n\n    White\n\n2,332 (75%)\n354 (75%)\n1,978 (75%)\n\n\n\nEducation level\n1,615\n\n\n\n&lt;0.001\n\n\n    HS/GED\n\n635 (39%)\n104 (34%)\n531 (41%)\n\n\n\n    Less than HS\n\n313 (19%)\n23 (7.5%)\n290 (22%)\n\n\n\n    More than HS\n\n667 (41%)\n179 (58%)\n488 (37%)\n\n\n\n    (Missing)\n\n1,485\n163\n1,322\n\n\n\nMale\n3,100\n2,060 (66%)\n336 (72%)\n1,724 (66%)\n0.010\n\n\nMarital status\n1,608\n\n\n\n0.006\n\n\n    Married or Partnered\n\n311 (19%)\n75 (25%)\n236 (18%)\n\n\n\n    Never married\n\n927 (58%)\n152 (50%)\n775 (59%)\n\n\n\n    Separated/Divorced/Widowed\n\n370 (23%)\n77 (25%)\n293 (22%)\n\n\n\n    (Missing)\n\n1,492\n165\n1,327\n\n\n\n\n\n\n1 Median (IQR); n (%)\n\n\n\n2 Wilcoxon rank sum test; Fisher’s exact test; Pearson’s Chi-squared test\n\n\n\n\n\n\n\n\n7.6.5 Customizing Table 2(b)\nI will now customize the title, caption and header and made the variable names bold of table 2 by using the following functions:\n\n# Adding title, caption and header \ntable_2 &lt;- table_1df %&gt;% tbl_summary(by = is_smoker,\n                                   label = list(\n                                     age = \"Age\",\n                                     race = \"Race\",\n                                     education = \"Education level\",\n                                     is_male = \"Male\",\n                                     marital = \"Marital status\"\n                                   ),\n                                   missing_text = \"(Missing)\"\n                                  ) %&gt;% \n  add_p() %&gt;%\n  add_n() %&gt;%\n  add_overall() %&gt;%\n  bold_labels() %&gt;%\n  modify_caption(\"Table 2. Demographic characteristics according to smoking status\") %&gt;%\n  modify_header(label ~ \"**Demographic characteristics**\") %&gt;%\n  modify_spanning_header(c(\"stat_1\", \"stat_2\") ~ \"**Smoking status**\") \n  \ntable_2\n\n\n\n\n\nTable 2. Demographic characteristics according to smoking status\n\n\n\n\n\n\n\n\n\n\nDemographic characteristics\nN\n\nOverall, N = 3,1001\n\nSmoking status\n\np-value2\n\n\n\n\nNo, N = 4691\n\n\nYes, N = 2,6311\n\n\n\n\n\nAge\n3,014\n34 (27, 45)\n36 (28, 47)\n33 (26, 44)\n&lt;0.001\n\n\n    (Missing)\n\n86\n7\n79\n\n\n\nRace\n3,100\n\n\n\n0.8\n\n\n    Black\n\n305 (9.8%)\n46 (9.8%)\n259 (9.8%)\n\n\n\n    Other\n\n444 (14%)\n68 (14%)\n376 (14%)\n\n\n\n    Refused/missing\n\n19 (0.6%)\n1 (0.2%)\n18 (0.7%)\n\n\n\n    White\n\n2,332 (75%)\n354 (75%)\n1,978 (75%)\n\n\n\nEducation level\n1,615\n\n\n\n&lt;0.001\n\n\n    HS/GED\n\n635 (39%)\n104 (34%)\n531 (41%)\n\n\n\n    Less than HS\n\n313 (19%)\n23 (7.5%)\n290 (22%)\n\n\n\n    More than HS\n\n667 (41%)\n179 (58%)\n488 (37%)\n\n\n\n    (Missing)\n\n1,485\n163\n1,322\n\n\n\nMale\n3,100\n2,060 (66%)\n336 (72%)\n1,724 (66%)\n0.010\n\n\nMarital status\n1,608\n\n\n\n0.006\n\n\n    Married or Partnered\n\n311 (19%)\n75 (25%)\n236 (18%)\n\n\n\n    Never married\n\n927 (58%)\n152 (50%)\n775 (59%)\n\n\n\n    Separated/Divorced/Widowed\n\n370 (23%)\n77 (25%)\n293 (22%)\n\n\n\n    (Missing)\n\n1,492\n165\n1,327\n\n\n\n\n\n\n1 Median (IQR); n (%)\n\n\n\n2 Wilcoxon rank sum test; Fisher’s exact test; Pearson’s Chi-squared test\n\n\n\n\n\n\n\n\n7.6.6 Customizing Table 2(c)\nHere, I am keeping only those customization that I prefer to have in my final table 2.\n\n# Final table\ntable_2 &lt;- table_1df %&gt;% tbl_summary(by = is_smoker,\n                                   label = list(\n                                     age = \"Age\",\n                                     race = \"Race\",\n                                     education = \"Education level\",\n                                     is_male = \"Male\",\n                                     marital = \"Marital status\"\n                                   ),\n                                   missing = \"no\"\n                                  ) %&gt;% \n  add_p() %&gt;%\n  #add_n() %&gt;%\n  #add_overall() %&gt;%\n  bold_labels() %&gt;%\n  modify_caption(\"Table 2. Demographic characteristics according to smoking status\") %&gt;%\n  modify_header(label ~ \"**Characteristics**\") %&gt;%\n  modify_spanning_header(c(\"stat_1\", \"stat_2\") ~ \"**Smoking Status**\") %&gt;%\n  modify_footnote(all_stat_cols() ~ \"Median (IQR) for Age; n (%) for all other variables\") \n  \ntable_2\n\n\n\n\n\nTable 2. Demographic characteristics according to smoking status\n\n\n\n\n\n\n\n\nCharacteristics\nSmoking Status\n\np-value2\n\n\n\n\nNo, N = 4691\n\n\nYes, N = 2,6311\n\n\n\n\n\nAge\n36 (28, 47)\n33 (26, 44)\n&lt;0.001\n\n\nRace\n\n\n0.8\n\n\n    Black\n46 (9.8%)\n259 (9.8%)\n\n\n\n    Other\n68 (14%)\n376 (14%)\n\n\n\n    Refused/missing\n1 (0.2%)\n18 (0.7%)\n\n\n\n    White\n354 (75%)\n1,978 (75%)\n\n\n\nEducation level\n\n\n&lt;0.001\n\n\n    HS/GED\n104 (34%)\n531 (41%)\n\n\n\n    Less than HS\n23 (7.5%)\n290 (22%)\n\n\n\n    More than HS\n179 (58%)\n488 (37%)\n\n\n\nMale\n336 (72%)\n1,724 (66%)\n0.010\n\n\nMarital status\n\n\n0.006\n\n\n    Married or Partnered\n75 (25%)\n236 (18%)\n\n\n\n    Never married\n152 (50%)\n775 (59%)\n\n\n\n    Separated/Divorced/Widowed\n77 (25%)\n293 (22%)\n\n\n\n\n\n\n1 Median (IQR) for Age; n (%) for all other variables\n\n\n\n2 Wilcoxon rank sum test; Fisher’s exact test; Pearson’s Chi-squared test\n\n\n\n\n\n\n\n\n7.6.7 Interpretation of Table 2\nInterpreting the variable Education level:\nNull Hypothesis (H₀): There is no association between education level and smoking status.\nAlternative Hypothesis (H₁): There is an association between education level and smoking status.\nSince the p-value is less than 0.001, we reject the null hypothesis. This indicates that there is a statistically significant association between education level and smoking status. However, to understand the nature of this association (whether education level affects smoking status or vice versa), further analysis would be needed.\n\n7.6.8 Missing value distribution in Table 2\nWe often want to see the missing value distribution among the the demographic variables. For example, we want to see the missing value distribution for the smoking status variable. First, we need to re-code the NA into a new category for is_smoker variable and recreate the table.\n\n7.6.8.1 Missing value data creation\n\n# Recoding `is_smoker` variable into `is_smoker_new`\ntable_1df &lt;- table_1df %&gt;% \n  mutate(is_smoker_new = ifelse(is.na(is_smoker), 99, is_smoker))  # converting all NA to 99\n\n# Convert into factor\ntable_1df$is_smoker_new &lt;- factor(table_1df$is_smoker_new,\n                                  levels = c(1, 2, 99),\n                                  labels = c(\"No\", \"Yes\", \"Missing\"))\n\n# New data frame \ntable_1df_new &lt;- table_1df %&gt;% \n  select(age, race, education, is_male, marital, is_smoker_new)\n\n\n7.6.8.2 Missing value table creation\n\n# Final table\ntable_2miss &lt;- table_1df_new %&gt;% tbl_summary(by = is_smoker_new,\n                                   label = list(\n                                     age = \"Age\",\n                                     race = \"Race\",\n                                     education = \"Education level\",\n                                     is_male = \"Male\",\n                                     marital = \"Marital status\"\n                                   ),\n                                   missing = \"no\"\n                                  ) %&gt;% \n  add_p() %&gt;%\n  #add_n() %&gt;%\n  #add_overall() %&gt;%\n  bold_labels() %&gt;%\n  modify_caption(\"Table 2. Demographic characteristics according to smoking status\") %&gt;%\n  modify_header(label ~ \"**Characteristics**\") %&gt;%\n  modify_spanning_header(c(\"stat_1\", \"stat_2\", \"stat_3\") ~ \"**Smoking Status**\") %&gt;%\n  modify_footnote(all_stat_cols() ~ \"Median (IQR) for Age; n (%) for all other variables\") \n  \ntable_2miss\n\n\n\n\n\nTable 2. Demographic characteristics according to smoking status\n\n\n\n\n\n\n\n\n\nCharacteristics\nSmoking Status\n\np-value2\n\n\n\n\nNo, N = 4691\n\n\nYes, N = 2,6311\n\n\nMissing, N = 4601\n\n\n\n\n\nAge\n36 (28, 47)\n33 (26, 44)\n39 (29, 47)\n&lt;0.001\n\n\nRace\n\n\n\n&lt;0.001\n\n\n    Black\n46 (9.8%)\n259 (9.8%)\n60 (13%)\n\n\n\n    Other\n68 (14%)\n376 (14%)\n62 (13%)\n\n\n\n    Refused/missing\n1 (0.2%)\n18 (0.7%)\n39 (8.5%)\n\n\n\n    White\n354 (75%)\n1,978 (75%)\n299 (65%)\n\n\n\nEducation level\n\n\n\n&lt;0.001\n\n\n    HS/GED\n104 (34%)\n531 (41%)\n56 (37%)\n\n\n\n    Less than HS\n23 (7.5%)\n290 (22%)\n39 (26%)\n\n\n\n    More than HS\n179 (58%)\n488 (37%)\n57 (38%)\n\n\n\nMale\n336 (72%)\n1,724 (66%)\n291 (64%)\n0.019\n\n\nMarital status\n\n\n\n&lt;0.001\n\n\n    Married or Partnered\n75 (25%)\n236 (18%)\n18 (13%)\n\n\n\n    Never married\n152 (50%)\n775 (59%)\n101 (71%)\n\n\n\n    Separated/Divorced/Widowed\n77 (25%)\n293 (22%)\n24 (17%)\n\n\n\n\n\n\n1 Median (IQR) for Age; n (%) for all other variables\n\n\n\n2 Kruskal-Wallis rank sum test; Pearson’s Chi-squared test",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Table by gtsummary</span>"
    ]
  },
  {
    "objectID": "lessons/01_gtsummary.html#regression-table-with-tbl_regression-function",
    "href": "lessons/01_gtsummary.html#regression-table-with-tbl_regression-function",
    "title": "\n7  Table by gtsummary\n",
    "section": "\n7.7 Regression Table with tbl_regression() Function",
    "text": "7.7 Regression Table with tbl_regression() Function\n\n7.7.1 Creating Regression Model\nHere, we are creating a logistic regression model where smoking status is the response variable, education is exploratory variable and age, race and sex are considered as confounders.\n\n# Building the Multivariable logistic model\nm1 &lt;- glm(is_smoker ~  education + age + race + is_male, \n          table_1df, \n          family = binomial)\n\n# View raw model results\nsummary(m1)$coefficients\n\n                         Estimate  Std. Error     z value     Pr(&gt;|z|)\n(Intercept)            3.50434562 0.389242905  9.00297879 2.196748e-19\neducationLess than HS  1.01143171 0.252724965  4.00210447 6.278157e-05\neducationMore than HS -0.60886151 0.144410039 -4.21619932 2.484542e-05\nage                   -0.04564764 0.006417912 -7.11253757 1.139285e-12\nraceOther             -0.24842217 0.315210858 -0.78811425 4.306299e-01\nraceRefused/missing    0.39602359 1.124629178  0.35213704 7.247355e-01\nraceWhite             -0.01922531 0.251971208 -0.07629961 9.391807e-01\nis_maleYes            -0.39712021 0.147363550 -2.69483338 7.042384e-03\n\n\n\n7.7.2 Creating Table 3: Regression Table\nHere, I am using tbl_regression function to see the regression results in the table. The exponentiate = TRUE shows the data as Odds Ratio after exponentiation of the beta values.\n\n# Creating Regression Table \ntable_3 &lt;- tbl_regression(m1, exponentiate = TRUE)\n\ntable_3\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nOR1\n\n\n95% CI1\n\np-value\n\n\n\neducation\n\n\n\n\n\n    HS/GED\n—\n—\n\n\n\n    Less than HS\n2.75\n1.71, 4.61\n&lt;0.001\n\n\n    More than HS\n0.54\n0.41, 0.72\n&lt;0.001\n\n\nage\n0.96\n0.94, 0.97\n&lt;0.001\n\n\nrace\n\n\n\n\n\n    Black\n—\n—\n\n\n\n    Other\n0.78\n0.42, 1.44\n0.4\n\n\n    Refused/missing\n1.49\n0.23, 29.4\n0.7\n\n\n    White\n0.98\n0.59, 1.59\n&gt;0.9\n\n\nis_male\n\n\n\n\n\n    No\n—\n—\n\n\n\n    Yes\n0.67\n0.50, 0.89\n0.007\n\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n7.7.3 Customizing Table 3\nHere, I have customized the table 3 by using functions I applied in table 1.\n\n# Customizing Regression Table \ntable_3 &lt;- tbl_regression(m1, exponentiate = TRUE,\n                           label = list(\n                             age = \"Age\",\n                             race = \"Race\",\n                             education = \"Education level\",\n                             is_male = \"Male\"\n                             ),\n                          missing = \"no\"\n                          ) %&gt;% \n  bold_labels() %&gt;%\n  bold_p(t = 0.10) %&gt;%  \n  italicize_levels() %&gt;%\n  modify_caption(\"Table 3. Logistic Regression for smoking status as response varialbe (n=3014)\")\n\ntable_3\n\n\n\n\n\nTable 3. Logistic Regression for smoking status as response varialbe (n=3014)\n\n\n\n\n\n\n\nCharacteristic\n\nOR1\n\n\n95% CI1\n\np-value\n\n\n\nEducation level\n\n\n\n\n\n    HS/GED\n—\n—\n\n\n\n    Less than HS\n2.75\n1.71, 4.61\n&lt;0.001\n\n\n    More than HS\n0.54\n0.41, 0.72\n&lt;0.001\n\n\nAge\n0.96\n0.94, 0.97\n&lt;0.001\n\n\nRace\n\n\n\n\n\n    Black\n—\n—\n\n\n\n    Other\n0.78\n0.42, 1.44\n0.4\n\n\n    Refused/missing\n1.49\n0.23, 29.4\n0.7\n\n\n    White\n0.98\n0.59, 1.59\n&gt;0.9\n\n\nMale\n\n\n\n\n\n    No\n—\n—\n\n\n\n    Yes\n0.67\n0.50, 0.89\n0.007\n\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n7.7.4 Interpreting Table 3\nInterpreting the variable Education level:\nFor individuals with less than high school education, the odds of being a smoker are 2.75 times higher compared to those with HS/GED, after adjusting for age, race, and sex.\nConversely, for individuals with more than high school education, the odds of being a smoker are 0.54 times lower compared to those with HS/GED, after adjusting for age, race, and sex.\nInterpreting the variable Age:\nFor each unit increase in age, the odds of being a smoker decrease by a factor of 0.96 (or 4%), after adjusting for education, race, and sex.\nIn R, for interpreting categorical variables, reference level is selected by alphabetic order, therefore, the HS/GED is selected as reference level (H), next one is Less than HS (L) and then More than HS (M).\n\n7.7.5 Changing the Reference Level in Table 3\nOften, we need to change the reference level as per our analysis need or aim of the study. We can select the specific reference level and run the table 3. First step is to check if the variable is in factor format. If it is not in factor format, we need to convert it into factor. Next, we can use the following codes to refer and use in table 3.\n\n7.7.5.1 New Model with New Reference Level\nHere I am creating model 2 (m2) wit the new reference as Less than HS for the education variable.\n\n# Check factor format\nstr(table_1df$education) # It shows that it is in factor format.\n\n Factor w/ 3 levels \"HS/GED\",\"Less than HS\",..: 3 3 3 3 NA 1 3 NA 1 3 ...\n\n# Building the glm model with specific reference level for education  = \"Less than HS\".\nm2 &lt;- glm(is_smoker ~  relevel(factor(education), ref = \"Less than HS\")  + age + race + is_male, \n          table_1df, \n          family = binomial)\n\n# View raw model results\nsummary(m2)$coefficients\n\n                                                                Estimate\n(Intercept)                                                   4.51577733\nrelevel(factor(education), ref = \"Less than HS\")HS/GED       -1.01143171\nrelevel(factor(education), ref = \"Less than HS\")More than HS -1.62029322\nage                                                          -0.04564764\nraceOther                                                    -0.24842217\nraceRefused/missing                                           0.39602359\nraceWhite                                                    -0.01922531\nis_maleYes                                                   -0.39712021\n                                                              Std. Error\n(Intercept)                                                  0.436459823\nrelevel(factor(education), ref = \"Less than HS\")HS/GED       0.252724965\nrelevel(factor(education), ref = \"Less than HS\")More than HS 0.244106320\nage                                                          0.006417912\nraceOther                                                    0.315210858\nraceRefused/missing                                          1.124629178\nraceWhite                                                    0.251971208\nis_maleYes                                                   0.147363550\n                                                                 z value\n(Intercept)                                                  10.34637575\nrelevel(factor(education), ref = \"Less than HS\")HS/GED       -4.00210447\nrelevel(factor(education), ref = \"Less than HS\")More than HS -6.63765370\nage                                                          -7.11253757\nraceOther                                                    -0.78811425\nraceRefused/missing                                           0.35213704\nraceWhite                                                    -0.07629961\nis_maleYes                                                   -2.69483338\n                                                                 Pr(&gt;|z|)\n(Intercept)                                                  4.346284e-25\nrelevel(factor(education), ref = \"Less than HS\")HS/GED       6.278157e-05\nrelevel(factor(education), ref = \"Less than HS\")More than HS 3.187156e-11\nage                                                          1.139285e-12\nraceOther                                                    4.306299e-01\nraceRefused/missing                                          7.247355e-01\nraceWhite                                                    9.391807e-01\nis_maleYes                                                   7.042384e-03\n\n\n\n7.7.5.2 Creating and Customizing New Table 3 with New Reference Level\nHere, I have created the new table 3 for m2 model and customized it accordingly.\n\n# Customizing Regression Table \ntable_3n &lt;- tbl_regression(m2, exponentiate = TRUE,  # Creating the table\n                           label = list(\n                             age = \"Age\",\n                             race = \"Race\",\n                             education = \"Education level\",\n                             is_male = \"Male\"\n                             ),\n                          missing = \"no\"\n                          ) %&gt;% \n  bold_labels() %&gt;%\n  bold_p(t = 0.10) %&gt;%  \n  italicize_levels() %&gt;%\n  modify_caption(\"Table 3. Logistic Regression for smoking status as response varialbe (n=3014)\")\n\ntable_3n\n\n\n\n\n\nTable 3. Logistic Regression for smoking status as response varialbe (n=3014)\n\n\n\n\n\n\n\nCharacteristic\n\nOR1\n\n\n95% CI1\n\np-value\n\n\n\nrelevel(factor(education), ref = \"Less than HS\")\n\n\n\n\n\n    Less than HS\n—\n—\n\n\n\n    HS/GED\n0.36\n0.22, 0.59\n&lt;0.001\n\n\n    More than HS\n0.20\n0.12, 0.31\n&lt;0.001\n\n\nAge\n0.96\n0.94, 0.97\n&lt;0.001\n\n\nRace\n\n\n\n\n\n    Black\n—\n—\n\n\n\n    Other\n0.78\n0.42, 1.44\n0.4\n\n\n    Refused/missing\n1.49\n0.23, 29.4\n0.7\n\n\n    White\n0.98\n0.59, 1.59\n&gt;0.9\n\n\nMale\n\n\n\n\n\n    No\n—\n—\n\n\n\n    Yes\n0.67\n0.50, 0.89\n0.007\n\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Table by gtsummary</span>"
    ]
  },
  {
    "objectID": "lessons/01_gtsummary.html#conclusion-take-home-message",
    "href": "lessons/01_gtsummary.html#conclusion-take-home-message",
    "title": "\n7  Table by gtsummary\n",
    "section": "\n7.8 Conclusion (Take Home Message)",
    "text": "7.8 Conclusion (Take Home Message)\n\nWe can use gtsummary package for creating publication-ready tables.\nThe tbl_summary() and the tbl_regression() are the frequently used functions in this package.\nMultiple other functions can be used to customize the table and can address the journal requirements.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Table by gtsummary</span>"
    ]
  },
  {
    "objectID": "02_header_one-sample.html",
    "href": "02_header_one-sample.html",
    "title": "One-Sample Tests",
    "section": "",
    "text": "Text Outline\nThis text covers the basics of:",
    "crumbs": [
      "One-Sample Tests"
    ]
  },
  {
    "objectID": "02_header_one-sample.html#text-outline",
    "href": "02_header_one-sample.html#text-outline",
    "title": "One-Sample Tests",
    "section": "",
    "text": "Exploring data\nOne-sample tests\nTwo-sample tests\nLinear regression and ANOVA\nGeneralized linear models\nSome special topics\nMaybe a bit about power calculations…",
    "crumbs": [
      "One-Sample Tests"
    ]
  },
  {
    "objectID": "02_header_one-sample.html#part-outline",
    "href": "02_header_one-sample.html#part-outline",
    "title": "One-Sample Tests",
    "section": "Part Outline",
    "text": "Part Outline\nThis part of the text contains the chapters on various one-sample statistical tests:\n\n\\(Z\\)-test\nPaired \\(t\\)-test\nPaired Wilcoxon test\nTransformations to Normality\nMcNemar’s Test\nFisher’s Exact Test\nChi-Square Goodness of Fit\nBootstrapped Confidence Intervals",
    "crumbs": [
      "One-Sample Tests"
    ]
  },
  {
    "objectID": "lessons/02_z-test_one_prop.html",
    "href": "lessons/02_z-test_one_prop.html",
    "title": "\n8  Z-Test for One Proportion\n",
    "section": "",
    "text": "8.1 Introduction to One-Sample \\(Z\\)-Tests\nThe one-sample \\(Z\\)-test is used to compare a sample proportion to a population proportion.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Z-Test for One Proportion</span>"
    ]
  },
  {
    "objectID": "lessons/02_z-test_one_prop.html#mathematical-definition-of-the-one-sample-z-test",
    "href": "lessons/02_z-test_one_prop.html#mathematical-definition-of-the-one-sample-z-test",
    "title": "\n8  Z-Test for One Proportion\n",
    "section": "\n8.2 Mathematical definition of the One-Sample \\(Z\\)-Test",
    "text": "8.2 Mathematical definition of the One-Sample \\(Z\\)-Test\nConsider a sample of size \\(n\\) with binary values (such as “true” or “false”). Let \\(p_{s}\\) and \\(p_{E}\\) be the observed sample and expected (population) proportions, respectively. The formula to calculate the \\(z\\) statistic is\n\\[\nz \\equiv \\frac{\n  p_s - p_E\n}{\n  \\sqrt{\n    \\frac{1}{n}p_s(1 - p_s)\n  }\n}.\n\\]",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Z-Test for One Proportion</span>"
    ]
  },
  {
    "objectID": "lessons/02_z-test_one_prop.html#data-source-and-description",
    "href": "lessons/02_z-test_one_prop.html#data-source-and-description",
    "title": "\n8  Z-Test for One Proportion\n",
    "section": "\n8.3 Data source and description",
    "text": "8.3 Data source and description\nWe will use the CTN-0094 data set, a data set of harmonized clinical trials for opioid use disorder. The full database is in public.ctn0094data::, engineered features are in public.ctn0094extra::, and clinical trial outcomes (wrangled dependent variables) are in CTNote::. We will install all three packages, but only use CTNote:: for now.\n\n# install.packages(\"public.ctn0094data\")\n# install.packages(\"public.ctn0094extra\")\n# install.packages(\"CTNote\")\n\nlibrary(CTNote)\nlibrary(tidyverse)",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Z-Test for One Proportion</span>"
    ]
  },
  {
    "objectID": "lessons/02_z-test_one_prop.html#cleaning-the-data-to-create-a-model-data-frame",
    "href": "lessons/02_z-test_one_prop.html#cleaning-the-data-to-create-a-model-data-frame",
    "title": "\n8  Z-Test for One Proportion\n",
    "section": "\n8.4 Cleaning the data to create a model data frame",
    "text": "8.4 Cleaning the data to create a model data frame\nBecause our method requires only one sample, we have very little work to do. We will use the Kosten et al. (1993) definition of opioid abstinence, provided in the data set outcomesCTN0094 as the column kosten1993_isAbs.\n\n# What do the values look like?\nsummary(outcomesCTN0094$kosten1993_isAbs)\n\n   Mode   FALSE    TRUE \nlogical    2158    1402 \n\n# How many samples are there?\nnrow(outcomesCTN0094)\n\n[1] 3560\n\n\nThere are 3560 logical values, and TRUE indicates that the trial participant achieved abstinence according to the definition used in Kosten et al. (1993).",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Z-Test for One Proportion</span>"
    ]
  },
  {
    "objectID": "lessons/02_z-test_one_prop.html#assumptions-of-the-one-sample-z-test",
    "href": "lessons/02_z-test_one_prop.html#assumptions-of-the-one-sample-z-test",
    "title": "\n8  Z-Test for One Proportion\n",
    "section": "\n8.5 Assumptions of the One-Sample \\(Z\\)-Test",
    "text": "8.5 Assumptions of the One-Sample \\(Z\\)-Test\nTo use a one-sample \\(Z\\)-test, we make the following assumptions:\n\nThe data are from a random sample\nEach observation in the data are independent\nNeither the sample proportion nor population proportions are “extreme”; usually we apply this method if these proportions are between 5% and 95%.\nThe data can be described as “successes” and “failures”, and there are at least 10 samples in each category.\n\nIf these assumptions hold, then \\[\nz \\sim N(0, 1).\n\\]",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Z-Test for One Proportion</span>"
    ]
  },
  {
    "objectID": "lessons/02_z-test_one_prop.html#checking-the-assumptions-with-plots",
    "href": "lessons/02_z-test_one_prop.html#checking-the-assumptions-with-plots",
    "title": "\n8  Z-Test for One Proportion\n",
    "section": "\n8.6 Checking the assumptions with plots",
    "text": "8.6 Checking the assumptions with plots\n\n8.6.1 Independence and Randomness\nBecause the samples were collected at random via an FDA approved clinical trial protocol, we assume that all the participants were randomly selected and are independent of each other.\n\n8.6.2 “Extreme” Proportions\nAccording to Ling et al. (2020), the 12-month abstinence proportion of all 533 participants in their study was 40.5 percent. As we can see here, our abstinence rates are 39.4. Neither these proportions are smaller than 5% or greater than 95%.\n\n(pExpected &lt;- 0.508 * (425/533))\n\n[1] 0.4050657\n\n# Count the number of TRUE values\n(nAbstinent &lt;- sum(outcomesCTN0094$kosten1993_isAbs))\n\n[1] 1402\n\n\n\n8.6.3 Type and Counts of Data\nWe observe binary data, and we see at least 10 successes and at least 10 failures.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Z-Test for One Proportion</span>"
    ]
  },
  {
    "objectID": "lessons/02_z-test_one_prop.html#code-to-run-a-one-sample-z-test",
    "href": "lessons/02_z-test_one_prop.html#code-to-run-a-one-sample-z-test",
    "title": "\n8  Z-Test for One Proportion\n",
    "section": "\n8.7 Code to run a One-Sample \\(Z\\)-Test",
    "text": "8.7 Code to run a One-Sample \\(Z\\)-Test\nNow that we have checked our assumptions, we can perform the one-sample \\(Z\\)-test for proportions.\n\nprop.test(\n  x = nAbstinent,\n  n = nrow(outcomesCTN0094),\n  p = pExpected\n)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  nAbstinent out of nrow(outcomesCTN0094), null probability pExpected\nX-squared = 1.8218, df = 1, p-value = 0.1771\nalternative hypothesis: true p is not equal to 0.4050657\n95 percent confidence interval:\n 0.3777537 0.4101176\nsample estimates:\n        p \n0.3938202",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Z-Test for One Proportion</span>"
    ]
  },
  {
    "objectID": "lessons/02_z-test_one_prop.html#brief-interpretation-of-the-output",
    "href": "lessons/02_z-test_one_prop.html#brief-interpretation-of-the-output",
    "title": "\n8  Z-Test for One Proportion\n",
    "section": "\n8.8 Brief interpretation of the output",
    "text": "8.8 Brief interpretation of the output\nThe 95% confidence interval contains the population proportion, so we fail to reject the hypothesis that the patients from these clinical trials achieve different abstinence rates than the general population.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Z-Test for One Proportion</span>"
    ]
  },
  {
    "objectID": "lessons/02_wilcoxon_one_samp.html",
    "href": "lessons/02_wilcoxon_one_samp.html",
    "title": "\n9  Wilcoxon Signed Rank Test for One Sample\n",
    "section": "",
    "text": "9.1 Introduction to Wilcoxson Signed Rank Test\nThe one-sample Wilcoxson Signed Rank Test is used to compare a sample proportion to a population proportion.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Wilcoxon Signed Rank Test for One Sample</span>"
    ]
  },
  {
    "objectID": "lessons/02_wilcoxon_one_samp.html#mathematical-definition-of-the-wilcoxson-signed-rank-test",
    "href": "lessons/02_wilcoxon_one_samp.html#mathematical-definition-of-the-wilcoxson-signed-rank-test",
    "title": "\n9  Wilcoxon Signed Rank Test for One Sample\n",
    "section": "\n9.2 Mathematical definition of the Wilcoxson Signed Rank Test",
    "text": "9.2 Mathematical definition of the Wilcoxson Signed Rank Test\nLet’s assume that we have one sample of size \\(n\\), \\(x_1, x_2, \\ldots, x_n\\), which cannot be approximated by a normal distribution. Because of this, we are no longer comparing \\(\\bar{x}\\) to \\(\\mu\\), but we are instead asking if the sample median is equal to a population median, \\(M\\). For more detail, see the maths here: https://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test.\nHere are the steps to calculate this test statistic manually:\n\nSubtract the population median from each sample: \\(x^*_i := x_1 - M\\)\n\nTake the absolute value of the shifted samples, \\(|x^*_i|\\).\nRank these absolute values.\nMultiply the signs of the shifted samples by the ranks of the absolute values.\nSum these products and compare them to a normal distribution with mean 0 and \\(\\sigma^2 = \\frac{1}{6}(2n+1)(n+1)n\\). (We will not explain the maths here to show why this can be approximately normal, or why this is the estimated variance.)\n\nConsider a simple example: we want to ask if the number of people visiting a local clinic per hour is different from the county median of 2.9 visits per hour. Here is a small sample of simulated (non-normal) data:\n\nset.seed(123)\n\nN &lt;- 15\nnClinicVisits &lt;- rpois(n = N, lambda = 4)\n\n# Plot the data and visually compare to the county median.\nhist(nClinicVisits)\nabline(v = 2.9, lwd = 2, col = \"red\")\n\n\n\n\n\n\n\nNow let’s go through our steps:\n\nsteps_df &lt;- tibble::tibble(x = nClinicVisits)\n\n# 1. shift the sample by the population median\nsteps_df$xStar &lt;- steps_df$x - 2.9\n\n# 2. absolute value\nsteps_df$absXStar &lt;- abs(steps_df$xStar)\n\n# 3. ranks\nsteps_df$xRank &lt;- rank(steps_df$absXStar)\n\n# 4. signs x ranks\nsteps_df$signRank &lt;- sign(steps_df$xStar) * steps_df$xRank\n\n# Inspect our steps\nsteps_df\n\n# A tibble: 15 × 5\n       x  xStar absXStar xRank signRank\n   &lt;int&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1     3  0.100    0.100   1.5      1.5\n 2     6  3.1      3.1    11.5     11.5\n 3     3  0.100    0.100   1.5      1.5\n 4     6  3.1      3.1    11.5     11.5\n 5     7  4.1      4.1    13.5     13.5\n 6     1 -1.9      1.9     9       -9  \n 7     4  1.1      1.1     6        6  \n 8     7  4.1      4.1    13.5     13.5\n 9     4  1.1      1.1     6        6  \n10     4  1.1      1.1     6        6  \n11     8  5.1      5.1    15       15  \n12     4  1.1      1.1     6        6  \n13     5  2.1      2.1    10       10  \n14     4  1.1      1.1     6        6  \n15     2 -0.9      0.9     3       -3  \n\n\nNow we can calculate the Wilcoxon Signed Rank test statisic and compare it to its asymptotic \\(p\\)-value.\n\n# 5. Compare sum to normal distribution and calculate the p-value\noneTailP &lt;- pnorm(\n  q = sum(steps_df$signRank),\n  mean = 0,\n  sd = sqrt((2 * N + 1) * (N + 1) * N / 6)\n)\n(1 - oneTailP) / 2\n\n[1] 0.001601623\n\n\nHow does this compare to the exact distribution \\(p\\)-value?\n\nwilcox.test(x = nClinicVisits, mu = 2.9)\n\nWarning in wilcox.test.default(x = nClinicVisits, mu = 2.9): cannot compute\nexact p-value with ties\n\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  nClinicVisits\nV = 108, p-value = 0.00672\nalternative hypothesis: true location is not equal to 2.9",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Wilcoxon Signed Rank Test for One Sample</span>"
    ]
  },
  {
    "objectID": "lessons/02_wilcoxon_one_samp.html#data-source-and-description",
    "href": "lessons/02_wilcoxon_one_samp.html#data-source-and-description",
    "title": "\n9  Wilcoxon Signed Rank Test for One Sample\n",
    "section": "\n9.3 Data source and description",
    "text": "9.3 Data source and description\nNow that we have seen how the test works, we will apply it to a real data scenario. We will use gene-level \\(p\\)-values from the Golub and Van Loan (1999) data set from the R package multtest:: (https://rdrr.io/bioc/multtest/man/golub.html); the original is a data set of data set of gene expression values for leukemia, but we have gene-specific \\(p\\)-values from a gene-level hypothesis test. We created these \\(p\\)-values in the script R/create_golub_data_20240523.R, but they do not represent any real analysis results.\n\nlibrary(tidyverse)",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Wilcoxon Signed Rank Test for One Sample</span>"
    ]
  },
  {
    "objectID": "lessons/02_wilcoxon_one_samp.html#cleaning-the-data-to-create-a-model-data-frame",
    "href": "lessons/02_wilcoxon_one_samp.html#cleaning-the-data-to-create-a-model-data-frame",
    "title": "\n9  Wilcoxon Signed Rank Test for One Sample\n",
    "section": "\n9.4 Cleaning the data to create a model data frame",
    "text": "9.4 Cleaning the data to create a model data frame\nBecause our method requires only one sample, we have very little work to do. We import the data set of \\(p\\)-values.\n\ngolub_pVals_num &lt;- readRDS(file = \"../data/02_golub_pVals_20240523.rds\")\n\nThere are 3051 \\(p\\)-values. The null hypothesis would be that there is no statistically significant effects in the data, so the distribution of these \\(p\\)-values should be a Uniform distribution. Our hypothesis is that the population mean is then 0.5 (the average value of a Uniform distribution).",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Wilcoxon Signed Rank Test for One Sample</span>"
    ]
  },
  {
    "objectID": "lessons/02_wilcoxon_one_samp.html#assumptions-of-the-wilcoxson-signed-rank-test",
    "href": "lessons/02_wilcoxon_one_samp.html#assumptions-of-the-wilcoxson-signed-rank-test",
    "title": "\n9  Wilcoxon Signed Rank Test for One Sample\n",
    "section": "\n9.5 Assumptions of the Wilcoxson Signed Rank Test",
    "text": "9.5 Assumptions of the Wilcoxson Signed Rank Test\nTo use a one-sample Wilcoxson Signed Rank Test, we make the following assumptions:\n\nThe data are from a random sample\nEach observation in the data are independent\nThe values can be “ranked” (this assumption gets fuzzy when you have discrete data, because it’s possible to get ties or values that are exactly 0 in those cases)\n\nIf these assumptions hold, then the test statistic is asymptotically normal. If your data has lots of zeros or equal values (which would result in tied ranks), then use this method with caution.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Wilcoxon Signed Rank Test for One Sample</span>"
    ]
  },
  {
    "objectID": "lessons/02_wilcoxon_one_samp.html#checking-the-assumptions",
    "href": "lessons/02_wilcoxon_one_samp.html#checking-the-assumptions",
    "title": "\n9  Wilcoxon Signed Rank Test for One Sample\n",
    "section": "\n9.6 Checking the assumptions",
    "text": "9.6 Checking the assumptions\n\n9.6.1 Independence and Randomness\nThese are gene-level \\(p\\)-values, so we do not have “independence”. However, because this is a pedagogical example, we will take a random sample of these genes to test (and this random sample should be independent enough, but we have no guarantee of this).\n\n# Create random sample of genes to test\nset.seed(20150516)\ngene_sample &lt;- sample(\n  x = golub_pVals_num,\n  size = 200,\n  replace = FALSE\n)\n\nWhat does the data distribution look like?\n\nhist(gene_sample)\n\n\n\n\n\n\n\nRemember, this is a “fake” analysis (all 38 samples in this data are leukemia cases, and I tested one half against the other—there should absolutely NOT be any real biological signal in this data).\n\n9.6.2 Type of Data\nThese values are \\(p\\)-values, so they can be ranked.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Wilcoxon Signed Rank Test for One Sample</span>"
    ]
  },
  {
    "objectID": "lessons/02_wilcoxon_one_samp.html#code-to-run-a-wilcoxson-signed-rank-test",
    "href": "lessons/02_wilcoxon_one_samp.html#code-to-run-a-wilcoxson-signed-rank-test",
    "title": "\n9  Wilcoxon Signed Rank Test for One Sample\n",
    "section": "\n9.7 Code to run a Wilcoxson Signed Rank Test",
    "text": "9.7 Code to run a Wilcoxson Signed Rank Test\nNow that we have checked our assumptions, we can perform the Wilcoxson Signed Rank Test on random samples of the genes to test if they have an average value of 0.5.\n\nwilcox.test(\n  x = gene_sample,\n  mu = 0.5, # average from all theoretical p-values under H0\n  alternative = \"less\" # H1: random p-values &lt; 0.5\n)\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  gene_sample\nV = 5859, p-value = 1.584e-07\nalternative hypothesis: true location is less than 0.5",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Wilcoxon Signed Rank Test for One Sample</span>"
    ]
  },
  {
    "objectID": "lessons/02_wilcoxon_one_samp.html#brief-interpretation-of-the-output",
    "href": "lessons/02_wilcoxon_one_samp.html#brief-interpretation-of-the-output",
    "title": "\n9  Wilcoxon Signed Rank Test for One Sample\n",
    "section": "\n9.8 Brief interpretation of the output",
    "text": "9.8 Brief interpretation of the output\nThe \\(p\\)-value for this test is less than 0.05, so we reject the hypothesis that the average gene-specific \\(p\\)-value for this set of results is greater than or equal to 0.5 (the theoretical average of \\(p\\)-values under the null hypothesis).",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Wilcoxon Signed Rank Test for One Sample</span>"
    ]
  },
  {
    "objectID": "lessons/02_mcnemar_paired_samp.html",
    "href": "lessons/02_mcnemar_paired_samp.html",
    "title": "\n10  McNemar’s Test\n",
    "section": "",
    "text": "10.1 Introduction to McNemar’s Test\nMcNemar’s Test is a non-parametric test used to analyze dichotomous data (in a 2 x 2) for paired samples. It is similar to a paired \\(t\\)-test, but for dichotomous rather than continuous variables. It is also akin to a Fisher’s exact test, but for paired data rather than un-paired data. The test requires one nominal dependent variable with 2 categories, and one independent variable with 2 dependent, mutually exclusive, groups. It is important to note that a “pair” can also represent a single individual’s pre- and post-test/intervention results.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>McNemar's Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_mcnemar_paired_samp.html#mathematical-definition-of-mcnemars-test",
    "href": "lessons/02_mcnemar_paired_samp.html#mathematical-definition-of-mcnemars-test",
    "title": "\n10  McNemar’s Test\n",
    "section": "\n10.2 Mathematical definition of McNemar’s Test",
    "text": "10.2 Mathematical definition of McNemar’s Test\nConsider binary repeated measured data for \\(n\\) observations; the observations come from one group under two conditions (e.g. “time period 1” vs “time period 2”, 0 vs. 1), the samples are treated with some treatment within equally across each condition, and the outcome of interest is dichotomous (such as “success” or “failure”, 1 vs. 0). If these restrictive assumptions are met, then the data can be compactly represented as a contingency table that looks like this:\n\n\n\nCondition 2 + (1)\nCondition 2 - (0)\n\n\n\nCondition 1 + (1)\n(a)\n(b)\n\n\nCondition 1 - (0)\n(c)\n(d)\n\n\n\nWhere:\n\n\na: is the number of pairs where both conditions are positive. E.g., the count of participants for whom the treatment was effective at time points 1 and 2.\n\nb: is the number of pairs where the first condition is positive and the second condition is negative. E.g., the count of participants for whom the treatment was effective at time 1 but not effective at time 2.\n\nc: is the number of pairs where the first condition is negative and the second condition is positive. E.g., the count of participants for whom the treatment was not effective at time 1 but was effective at time 2.\n\nd: is the number of pairs where both conditions are negative. E.g., the count of participants for whom the treatment was neither effective at time 1 nor at time 2.\n\nThe test focuses on the discordant pairs, b and c, which are pairs that change from one condition to the other. With a sufficiently large number of discordant pairs, McNemar’s Test follows a Chi-squared distribution with 1 degree of freedom. The formula for the test statistic, \\(\\chi^{2}_{\\text{Obs}}\\), is as follows:\n\\(\\chi^{2}_{\\text{Obs}} := \\frac{(b - c)^2}{b + c};\\ \\chi^{2}_{\\text{Obs}} \\sim \\chi^{2}_{\\nu = 1}.\\)",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>McNemar's Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_mcnemar_paired_samp.html#data-description",
    "href": "lessons/02_mcnemar_paired_samp.html#data-description",
    "title": "\n10  McNemar’s Test\n",
    "section": "\n10.3 Data Description",
    "text": "10.3 Data Description\nFor this lesson, we will look at patients in treatment for opioid use disorder, We will measure abstinence a the start and end of treatment via weekly Urine Drug Screen (UDS). The abstinence measurements are in the public.ctn0094extra::derived_weeklyOpioidPattern data set. Our definition of abstinence will be “3 consecutive negative urine screens” from weeks 2-4 (start of treatment) and weeks 10-12 (end of treatment).\n\n10.3.1 Cleaning UDS Data\n\ntxSuccess_df &lt;- \n  public.ctn0094extra::derived_weeklyOpioidPattern %&gt;% \n  # Combine UDS across treatment phases\n  mutate(udsPattern = paste0(Phase_1, Phase_2)) %&gt;% \n  # Extract the UDS patterns for the start and end of treatment\n  mutate(\n    startTxPattern = str_sub(udsPattern, start = 2, end = 4),\n    endTxPattern = str_sub(udsPattern, start = 10, end = 12)\n  ) %&gt;% \n  # Check for abstinence during the start and end of treatment\n  mutate(\n    startAbs = startTxPattern == \"---\",\n    endAbs = endTxPattern == \"---\"\n  ) %&gt;% \n  select(who, udsPattern, startAbs, endAbs) \n\ntxSuccess_df\n\n# A tibble: 3,560 × 4\n     who udsPattern                startAbs endAbs\n   &lt;int&gt; &lt;chr&gt;                     &lt;lgl&gt;    &lt;lgl&gt; \n 1     1 ooooooooooooooo           FALSE    FALSE \n 2     2 ----oo-o-o-o+o            TRUE     FALSE \n 3     3 o-ooo-ooooooooooooooooo   FALSE    FALSE \n 4     4 --------------------o-oo  TRUE     TRUE  \n 5     5 ooooooooooooooo           FALSE    FALSE \n 6     6 -ooooooooooooo            FALSE    FALSE \n 7     7 ----oooooooooooooooooooo  TRUE     FALSE \n 8     8 ooooooooooooooooooooooooo FALSE    FALSE \n 9     9 oooooooooooooooooooooo    FALSE    FALSE \n10    10 --o--*++o-++++++++o+-o    FALSE    FALSE \n# ℹ 3,550 more rows\n\n\n\n10.3.2 Creating Comparison Table from this Dataset\nNow that we have a binary measure of success at two time points (start and end of treatment), we can create a 2 x 2 contingency table:\n\ntxAbs_tbl &lt;- table(\n  # Rows of the table\n  txSuccess_df$startAbs,\n  # Columns of the table\n  txSuccess_df$endAbs\n)\n\ntxAbs_tbl\n\n       \n        FALSE TRUE\n  FALSE  2676  268\n  TRUE    395  221\n\n\nHere are the categories for each of the states of the patients:\n\n\na (FALSE & FALSE) means that the subject was abstinent from opioids neither at the start nor end of the trial,\n\nb (FALSE & TRUE) means that the subject was not abstinent from opioids at the start of the trial but abstinent at the end,\n\nc (TRUE & FALSE) means that the subject was abstinent from opioids at the start of the trial but not abstinent at the end, and\n\nd (TRUE & TRUE) means that the subject was abstinent from opioids both at the start and end of the trial.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>McNemar's Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_mcnemar_paired_samp.html#assumptions-of-mcnemars-test",
    "href": "lessons/02_mcnemar_paired_samp.html#assumptions-of-mcnemars-test",
    "title": "\n10  McNemar’s Test\n",
    "section": "\n10.4 Assumptions of McNemar’s Test",
    "text": "10.4 Assumptions of McNemar’s Test\nThe assumptions of McNemar’s Test are as follows:\n\n\nAssumption 1: You have one categorical dependent variable with two categories (i.e., a dichotomous variable) and one categorical independent variable with two related groups.\n\nAssumption 2: The two groups of the dependent variable are mutually exclusive, which means that the groups do not overlap—a participant can only be in one of the two groups.\n\nAssumption 3: The cases are a random sample from the population of interest.\n\nAssumption 4: At least 25 discordant pairs (\\(c + b \\geq 25\\))",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>McNemar's Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_mcnemar_paired_samp.html#checking-the-assumptions-of-mcnemars-test",
    "href": "lessons/02_mcnemar_paired_samp.html#checking-the-assumptions-of-mcnemars-test",
    "title": "\n10  McNemar’s Test\n",
    "section": "\n10.5 Checking the Assumptions of McNemar’s Test",
    "text": "10.5 Checking the Assumptions of McNemar’s Test\nTo check our assumptions, we will review and inspect the contingency table with the two variables of interest.\n\n# Printing the contigency table with margin totals and overal totals\naddmargins(txAbs_tbl)\n\n       \n        FALSE TRUE  Sum\n  FALSE  2676  268 2944\n  TRUE    395  221  616\n  Sum    3071  489 3560\n\n\nThe categorical dependent variable is abstinence of opioids in urine samples (e.g positive or negative for the substance); we see that the dependent variable at the start of treatment is related to the dependent variable at the end of treatment because we are detecting opioids within the same person. The categorical independent variable is the two time periods (start and end of treatment). The two groups are mutually exclusive, as urine cannot be simultaneously positive and negative and the participant cannot simultaneously be at the start and end of treatment. Finally, we note that the sum of the off-diagonal cells is at least 25.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>McNemar's Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_mcnemar_paired_samp.html#code-to-run-mcnemars-test",
    "href": "lessons/02_mcnemar_paired_samp.html#code-to-run-mcnemars-test",
    "title": "\n10  McNemar’s Test\n",
    "section": "\n10.6 Code to run McNemar’s Test",
    "text": "10.6 Code to run McNemar’s Test\nRecall that we created a 2x2 contingency table with the table() function above. This table object is one of the the data structures which can be supplied to the function mcnemar.test(). The other is the two columns of binary values (which can be coercible to binary factors).\n\n# Table Input Syntax\nmcnemar.test(txAbs_tbl)\n\n\n    McNemar's Chi-squared test with continuity correction\n\ndata:  txAbs_tbl\nMcNemar's chi-squared = 23.946, df = 1, p-value = 9.909e-07\n\n# Factor Vector Input Syntax\nmcnemar.test(\n  x = txSuccess_df$startAbs,\n  y = txSuccess_df$endAbs\n)\n\n\n    McNemar's Chi-squared test with continuity correction\n\ndata:  txSuccess_df$startAbs and txSuccess_df$endAbs\nMcNemar's chi-squared = 23.946, df = 1, p-value = 9.909e-07\n\n\nThe output from the McNemar’s test will show the Chi-Squared value, degrees of freedom (expected to be 1 as both categories only have 2 possible values), and the \\(p\\)-value.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>McNemar's Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_mcnemar_paired_samp.html#brief-interpretation-of-the-output",
    "href": "lessons/02_mcnemar_paired_samp.html#brief-interpretation-of-the-output",
    "title": "\n10  McNemar’s Test\n",
    "section": "\n10.7 Brief Interpretation of the Output",
    "text": "10.7 Brief Interpretation of the Output\nThe resulting \\(p\\)-value in this case is below 0.05, which indicates that the marginal probabilities between startAbs and endAbs are different. What is curious is that the count for start of treatment abstinence is higher than the count for end of treatment abstinence.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>McNemar's Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_mcnemar_paired_samp.html#conclusion",
    "href": "lessons/02_mcnemar_paired_samp.html#conclusion",
    "title": "\n10  McNemar’s Test\n",
    "section": "\n10.8 Conclusion",
    "text": "10.8 Conclusion\nIf you have paired samples, and the variable of interest is binary, then use McNemar’s test. The pairing could be within subject but across time or space, or it could represent different measures of success or failure on the same subject (of use in psychometrics). Regardless, it’s often that you’d like to include some covariate or additional factor, which this technique does not allow. In those cases, you may want Survival analysis (an event may occur or not within a time interval) or some other technique.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>McNemar's Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_chiSq_GoF.html",
    "href": "lessons/02_chiSq_GoF.html",
    "title": "\n11  The Chi Squared Goodness-of-Fit Test\n",
    "section": "",
    "text": "11.1 Introduction to the \\(\\chi^2\\) Goodness of Fit (GoF) Test\nNon-parametric tests and robust tests often have lower statistical power than their traditional counterparts. These more traditional hypothesis tests make distributional assumptions. For instance, a test may assume that the original observations are approximately normal or that counts follow a Poisson distribution. While we most commonly assess distributional assumptions visually, for instance with a Q-Q plot or histogram, there are some instances where we need to test adherence of data to a specified distribution with a proper statistical hypothesis test. One such test is the \\(\\chi^2\\) Goodness of Fit (GoF) test, published in the textbook Statistical Methods (Snedecor and Cochran, 1989). We draw some of our formulation and examples from this US NIST handbook: https://www.itl.nist.gov/div898/handbook/eda/section3/eda35f.htm.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>The Chi Squared Goodness-of-Fit Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_chiSq_GoF.html#mathematical-definition-of-the-chi2-goodness-of-fit-gof-test",
    "href": "lessons/02_chiSq_GoF.html#mathematical-definition-of-the-chi2-goodness-of-fit-gof-test",
    "title": "\n11  The Chi Squared Goodness-of-Fit Test\n",
    "section": "\n11.2 Mathematical definition of the \\(\\chi^2\\) Goodness of Fit (GoF) Test",
    "text": "11.2 Mathematical definition of the \\(\\chi^2\\) Goodness of Fit (GoF) Test\nThe idea of this test is to “bin” all the observations (think histogram) and then test the counts of the observations in each bin (observed) against the number of counts in the same bins from the target distribution (expected).\nConsider a sample of \\(n\\) observations, \\(x_1, x_2, \\ldots, x_n\\), from an unknown distribution with cumulative distribution function (CDF) \\(\\mathcal{F}_0\\) with support \\((-\\infty, \\infty)\\). Let \\(\\mathcal{F}_A\\) denote the CDF distribution to compare against (such as a Normal distribution, Gamma distribution, Negative Binomial distribution, or whatever distribution you think best describes the population data). Partition the \\(n\\) observations into \\(k+1\\) bins using \\(k\\) cut points \\(T_1, T_2, \\ldots, T_k\\), creating the non-intersecting intervals \\(\\{ {(-\\infty, T_1]},\\ (T_1, T_2],\\ \\ldots,\\ (T_k, \\infty)\\}\\) which span the support of \\(\\mathcal{F}_0\\). For the sake of notation, let \\(T_0 = -\\infty\\) and \\(T_{k+1} = \\infty\\).\nOnce we have the data partitions, we then tally the observed count in each bin, \\(O_1, O_2, \\ldots, O_{k+1}\\). We also find the probabilities of being in each bin \\((T_{i-1}, T_i],\\ i \\in 1, 2, \\ldots, k+1\\) from the target distribution \\(\\mathcal{F}_A\\). That is, calculate \\(p_i \\equiv \\mathcal{F}_A(T_i) - \\mathcal{F}_A(T_{i-1})\\), noting that \\(\\sum_{i = 1}^{k+1} p_i = 1\\). Multiply the probabilities for each bin, \\(p_i\\), by the total sample size, \\(n\\), to generate the expected count for each bin, \\(E_1, E_2, \\ldots, E_{k+1}\\).\nNow that we have the observed counts, \\(O_i\\), and the expected counts, \\(E_i\\), we calculate the \\(\\chi^2\\) Goodness of Fit Test statistic as \\[\n\\chi^2_{\\text{Obs}} \\equiv \\sum\\limits_{i = 1}^{k+1} \\frac{(O_i - E_i)^2}{E_i}.\n\\]\nThe distribution of this test statistic is approximately \\(\\chi^2\\) with \\(k^* + (p + 1)\\) degrees of freedom, where \\(k^*\\) is the number of non-empty bins and \\(p\\) is the number of parameters of \\(\\mathcal{F}_A\\). For example, let’s assume that we are comparing the data against a Beta distribution with two parameters \\(\\{\\alpha,\\beta\\}\\). Further, assume we choose to bin the observed data into deciles so that we have at least one observation in each bin. Thus, we would have \\(k^* = 10\\) bins and \\(p = 2\\) parameters. So, we would compare the test statistic \\(\\chi^2_{\\text{Obs}}\\) against a \\(\\chi^2\\) distribution with \\(k^* + (p + 1) = (10) + (2 + 1) = 13\\) degrees of freedom.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>The Chi Squared Goodness-of-Fit Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_chiSq_GoF.html#data-source-and-description",
    "href": "lessons/02_chiSq_GoF.html#data-source-and-description",
    "title": "\n11  The Chi Squared Goodness-of-Fit Test\n",
    "section": "\n11.3 Data source and description",
    "text": "11.3 Data source and description\nWhen we treat people who are addicted to licit and/or illicit substances, one potential predictor of recovery is baseline “risky health behaviours”; the Addictions, Drug, and Alcohol Institute hosts a copy of the Risk Behavior Survey questionnaire: https://adai.uw.edu/instruments/pdf/Risk_Behavior_Survey_209.pdf. While exploring the results of this questionnaire among patients in treatment, we might want to know if the total number of sexual partners (column txx_frq in the data set sex) follow a Poisson distribution.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>The Chi Squared Goodness-of-Fit Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_chiSq_GoF.html#cleaning-the-data-to-create-a-model-data-frame",
    "href": "lessons/02_chiSq_GoF.html#cleaning-the-data-to-create-a-model-data-frame",
    "title": "\n11  The Chi Squared Goodness-of-Fit Test\n",
    "section": "\n11.4 Cleaning the data to create a model data frame",
    "text": "11.4 Cleaning the data to create a model data frame\nThis data is already relatively clean, other than the missing values. Let’s remove them.\n\nsummary(public.ctn0094data::sex$txx_frq)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n    1.0     3.0     6.0    10.4    16.0    44.0    1327 \n\nnPartners_int &lt;- \n  public.ctn0094data::sex %&gt;% \n  drop_na(txx_frq) %&gt;% \n  pull(txx_frq) %&gt;% \n  as.integer()\n\nHere is a histogram of the total number of sexual partners for participants in the CTN-0094 data warehouse:\n\nhist(\n  nPartners_int,\n  main = \"Number of Sexual Partners per Patient\",\n  xlab = \"Count, using 'Sturges' Bins\"\n)\n\n\n\n\n\n\nFigure 11.1\n\n\n\n\nIt does not appear that this data follows a Poisson distribution. That said, a journal reviewer may still ask for a \\(p\\)-value as evidence.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>The Chi Squared Goodness-of-Fit Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_chiSq_GoF.html#assumptions-of-the-chi2-goodness-of-fit-gof-test",
    "href": "lessons/02_chiSq_GoF.html#assumptions-of-the-chi2-goodness-of-fit-gof-test",
    "title": "\n11  The Chi Squared Goodness-of-Fit Test\n",
    "section": "\n11.5 Assumptions of the \\(\\chi^2\\) Goodness of Fit (GoF) Test",
    "text": "11.5 Assumptions of the \\(\\chi^2\\) Goodness of Fit (GoF) Test\nRecall that the \\(k^*\\) parameter counts only the number of non-empty bins, so we should choose a small enough number of bins to ensure that all bins have at least a couple data points. I don’t actually know for sure what the minimum should be, but I’d say at least 5 per bin, but the traditional \\(\\chi^2\\) Test of Independence assumes at least 10 values per bin. The more bins you use (up to a point), the better the empirical CDF can approximate the shape of the distributional CDF. However, two things work against you: 1) more bins require more samples, and 2) more bins mean that you’re more likely to reject the null hypothesis even when you shouldn’t.\nSummary of Assumptions:\n\nYour samples are independent\nYour sample was taken at random\nYour samples are (or can be represented as) count data\nYou have enough samples to have a few per bin (at least 5-10)",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>The Chi Squared Goodness-of-Fit Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_chiSq_GoF.html#choosing-the-correct-number-of-bins",
    "href": "lessons/02_chiSq_GoF.html#choosing-the-correct-number-of-bins",
    "title": "\n11  The Chi Squared Goodness-of-Fit Test\n",
    "section": "\n11.6 Choosing the Correct Number of Bins",
    "text": "11.6 Choosing the Correct Number of Bins\nFor a test that depends on the number of bins, we would expect there to be some “correct” number. This is a reasonably important question. Unfortunately, as best as these authors are aware, there isn’t one best rule for selecting the correct number of bins. Thankfully, there has been some research done on the optimal number of bins for histograms (which is very similar). We can apply these various “rules” to the \\(\\chi^2\\) GoF test as well.\nThe standard method to accomplish this is to set breaks = k+1 in the hist() call. However, this number of bins doesn’t always change when you want it to: the help file says that if you give a number of bins for the breaks argument, that “the number is a suggestion only”. In order to “force” the number of bins to be what you want, you have to create a sequence of cut points yourself, using the seq() function. But note that the cut points include the minimum and the maximum of the data (\\(T_0\\) and \\(T_{k+1}\\)); so if you want 10 bins, you’ll need a vector of “cuts” with length 11. An example of this code is:\n\nnBins &lt;- 10\nseq(\n  from = min(nPartners_int),\n  to = max(nPartners_int),\n  length.out = nBins + 1\n)\n\n [1]  1.0  5.3  9.6 13.9 18.2 22.5 26.8 31.1 35.4 39.7 44.0\n\n\n\n11.6.1 Bins by Decile\nWe could divide the observations by into bins at every 10th percentile of the data; this would yield 197.2 participants per bin, on average. It would work pretty well for this data set, but that’s purely a coincidence (but see the odd trivia below the Sturge’s Rule section). This would yield the same default histogram as above.\n\nhist(\n  nPartners_int,\n  breaks = seq(\n    from = min(nPartners_int),\n    to = max(nPartners_int),\n    length.out = 10 + 1\n  ),\n  main = \"Number of Sexual Partners per Patient\",\n  xlab = \"Count, using Decile Bins\"\n)\n\n\n\n\n\n\nFigure 11.2\n\n\n\n\n\n11.6.2 A Bin for Each Unique Count\nA simple rule for discrete data would be a bin for each unique value. In our example, the most number of bins which make sense are all the observed counts (in our case, 44 bins). Let’s rebuild the above histogram with a bin for each unique count value.\n\nhist(\n  nPartners_int,\n  breaks = seq(\n    from = min(nPartners_int),\n    to = max(nPartners_int),\n    length.out = length(unique(nPartners_int)) + 1\n  ),\n  main = \"Number of Sexual Partners per Patient\",\n  xlab = \"Count, using Unique Value Bins\"\n)\n\n\n\n\n\n\nFigure 11.3\n\n\n\n\nThis retains some of the shape of the data we saw with 10 bins, but it’s very “noisy”. This is probably not a good option to use.\n\n11.6.3 Sturges Breaks\nWe should recall that in the first histogram, we changed the number of breaks from the default. The default number of histogram breaks in R is an option called \"Sturges\". Sturge’s Rule states that an optimal number of bins for a histogram (and subsequently, a \\(\\chi^2\\) GoF test) is \\(\\log_2(n) + 1\\), rounded up. For our example, this would be 12 breaks:\n\n# Sample size\nn_int &lt;- length(nPartners_int)\n\n# Sturge's rule: number of breaks (round up)\nlog2(n_int) + 1\n\n[1] 11.94544\n\n# Histogram\nhist(\n  nPartners_int,\n  breaks = seq(\n    from = min(nPartners_int),\n    to = max(nPartners_int),\n    length.out = 12 + 1\n  ),\n  main = \"Number of Sexual Partners per Patient\",\n  xlab = \"Count, using Sturge's Bins\"\n)\n\n\n\n\n\n\nFigure 11.4\n\n\n\n\n\nAside: why does using decile bins work here? According to Sturge’s rule, for any data set with number of samples between 70 and 2000, 10 bins is reasonably close to the optimal number calculated by Sturge’s rule (for 70 samples, the optimal Sturge’s bins is 8; for 2000, it’s 12). So while it’s purely coincidence for our data set, using decile-based bins will work for many of the data sets that students will encounter in their statistics and methods classes.\n\n\n11.6.4 Other Rules to Calculate the Breaks\nThere are a few other rules to calculate the number of bins, all rounded up:\n\nThe Square Root rule: \\(\\sqrt{n}\\)\n\nThe Rice rule: \\(\\sqrt[3]{2n}\\)\n\nScott’s rule: \\(R / \\frac{3.49s}{\\sqrt[3]{n}}\\), where \\(R\\) is the range of the data and \\(s\\) is the sample standard deviation (this rule assumes the data can be approximated by a Normal distribution)\nFreedman-Diaconis’s rule: \\(R / \\frac{3.49I}{\\sqrt[3]{n}}\\), where \\(R\\) is the range of the data and \\(I\\) is the sample interquartile range (this rule is an extension of Scott’s rule, and therefore also assumes the data can be approximated by a Normal distribution)\n\n\nNote: from what I’ve found online, there are a few different formulations to these rules, but I’m using those in the link above, as corroborated by page 26 of these notes from Prof. Fawcett at the University of Newcastle: http://www.mas.ncl.ac.uk/~nlf8/teaching/mas1343/notes/chap4-5.pdf\n\n\n# Range (for Scott's and F-D rules)\nrange_int &lt;- max(nPartners_int) - min(nPartners_int)\n\n# Square Root rule\nsqrt(n_int)\n\n[1] 44.40721\n\n# The Rice rule\n(2 * n_int) ^ (1/3)\n\n[1] 15.79958\n\n# Scott's rule\nrange_int / ( 3.49 * sd(nPartners_int) * n_int ^ (-1/3) )\n\n[1] 18.55387\n\n# Freedman-Diaconis's rule (two versions)\nrange_int / ( 2 * IQR(nPartners_int) / (n_int ^ (1/3)) )\n\n[1] 20.73946\n\nrange_int / ( 3.49 * IQR(nPartners_int) / (n_int ^ (1/3)) )\n\n[1] 11.88508\n\n\nSo here are the histograms using these rules:\n\npar(mfrow = c(2, 2))\n\nhist(\n  nPartners_int,\n  breaks = seq(\n    from = min(nPartners_int),\n    to = max(nPartners_int),\n    length.out = 45 + 1\n  ),\n  main = \"Number of Sexual Partners per Patient\",\n  xlab = \"Count, using Square Root rule for Bins\"\n)\n\nhist(\n  nPartners_int,\n  breaks = seq(\n    from = min(nPartners_int),\n    to = max(nPartners_int),\n    length.out = 16 + 1\n  ),\n  main = \"Number of Sexual Partners per Patient\",\n  xlab = \"Count, using The Rice rule for Bins\"\n)\n\nhist(\n  nPartners_int,\n  breaks = seq(\n    from = min(nPartners_int),\n    to = max(nPartners_int),\n    length.out = 21 + 1\n  ),\n  main = \"Number of Sexual Partners per Patient\",\n  xlab = \"Count, using Scott's rule for Bins\"\n)\n\nhist(\n  nPartners_int,\n  breaks = seq(\n    from = min(nPartners_int),\n    to = max(nPartners_int),\n    length.out = 12 + 1\n  ),\n  main = \"Number of Sexual Partners per Patient\",\n  xlab = \"Count, using Freedman-Diaconis's rule for Bins\"\n)\n\n\npar(mfrow = c(1, 1))\n\n\n\n\n\n\nFigure 11.5",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>The Chi Squared Goodness-of-Fit Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_chiSq_GoF.html#code-to-run-the-chi2-goodness-of-fit-gof-test",
    "href": "lessons/02_chiSq_GoF.html#code-to-run-the-chi2-goodness-of-fit-gof-test",
    "title": "\n11  The Chi Squared Goodness-of-Fit Test\n",
    "section": "\n11.7 Code to run the \\(\\chi^2\\) Goodness of Fit (GoF) Test",
    "text": "11.7 Code to run the \\(\\chi^2\\) Goodness of Fit (GoF) Test\nThe Sturge and Freedman-Diaconis rules both yielded 12 bins, so we will use this number. Here is a list of the information we need:\n\nthe original observed tally of sexual partners per patient\na choice for the number of bins (12, in this case), and\na target distribution (Poisson).\n\nRecall that from our summary statement, the minimum and maximum number of sexual partners were 1 and 44, respectively. The cut() function will return the number of intervals requested which divide these observed values. These would correspond to \\(x\\)-axis values of the each left and right column of our histogram. Here are our steps to assign each observed tally to its appropriate bin:\n\nbins_df &lt;- \n  tibble(original = nPartners_int) %&gt;% \n  # Create 12 bins\n  mutate(partition = cut(original, breaks = 12)) %&gt;% \n  # Extract lower and upper limit of the bins; retain the original partition\n  #   column to compare our work\n  separate(partition, into = c(\"lower\", \"upper\"), sep = \",\", remove = FALSE) %&gt;%\n  # Remove the leading \"(\" and trailing \"]\", then transform to numeric\n  mutate(\n    lower = as.numeric(str_sub(lower, start = 2L)),\n    upper = as.numeric(str_sub(upper, end = -2L))\n  ) %&gt;% \n  # Replace the smallest and largest limits with the support of the Poisson\n  #   distribution. Note that this will depend on the target distribution you \n  #   choose.\n  mutate(\n    lower = case_when(\n      lower == min(lower) ~ 0,\n      lower &gt; min(lower) ~ lower\n    ),\n    upper = case_when(\n      upper == max(upper) ~ Inf,\n      upper &lt; max(upper) ~ upper\n    )\n  )\n\nbins_df\n\n# A tibble: 1,972 × 4\n   original partition    lower upper\n      &lt;int&gt; &lt;fct&gt;        &lt;dbl&gt; &lt;dbl&gt;\n 1       16 (15.3,18.9]  15.3  18.9 \n 2        6 (4.58,8.17]   4.58  8.17\n 3       16 (15.3,18.9]  15.3  18.9 \n 4        9 (8.17,11.8]   8.17 11.8 \n 5       16 (15.3,18.9]  15.3  18.9 \n 6       20 (18.9,22.5]  18.9  22.5 \n 7        5 (4.58,8.17]   4.58  8.17\n 8        3 (0.957,4.58]  0     4.58\n 9        2 (0.957,4.58]  0     4.58\n10        5 (4.58,8.17]   4.58  8.17\n# ℹ 1,962 more rows\n\n\nNow, we can calculate the observed counts, the bin expected probabilities according to the specified Poisson distribution, and the expected counts\n\n# Poisson parameter\nestLambda_num &lt;- mean(nPartners_int)\n\nsteps_df &lt;- \n  bins_df %&gt;% \n  group_by(partition) %&gt;% \n  summarise(\n    lower = unique(lower),\n    upper = unique(upper),\n    observed = n()\n  ) %&gt;% \n  # Now calculate the probabilities of a random value from the target Poisson\n  #   distribution falling into these bins\n  mutate(\n    p = ppois(q = upper, lambda = estLambda_num) - \n      ppois(q = lower, lambda = estLambda_num)\n  ) %&gt;% \n  # And now the expected counts (I'm rounding here just for readability, so I \n  #   also include the non-rounded version for computation)\n  mutate(expected = p * n_int) %&gt;% \n  mutate(expected_rd = round(p * n_int, 1)) %&gt;% \n  # (O - E)^2 / E\n  mutate(summand = (observed - expected)^2 / expected)\n\n# Do our probabilities sum to 1?\nsum(steps_df$p)\n\n[1] 0.9999697\n\nsteps_df\n\n# A tibble: 12 × 8\n   partition    lower  upper observed        p expected expected_rd      summand\n   &lt;fct&gt;        &lt;dbl&gt;  &lt;dbl&gt;    &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;\n 1 (0.957,4.58]  0      4.58      610 2.24e- 2  4.42e+1        44.2      7.24e 3\n 2 (4.58,8.17]   4.58   8.17      440 2.67e- 1  5.26e+2       526.       1.40e 1\n 3 (8.17,11.8]   8.17  11.8        84 3.61e- 1  7.12e+2       712.       5.53e 2\n 4 (11.8,15.3]  11.8   15.3        31 2.86e- 1  5.64e+2       564.       5.03e 2\n 5 (15.3,18.9]  15.3   18.9       556 5.37e- 2  1.06e+2       106.       1.91e 3\n 6 (18.9,22.5]  18.9   22.5        47 1.01e- 2  1.98e+1        19.8      3.73e 1\n 7 (22.5,26.1]  22.5   26.1       130 4.92e- 4  9.70e-1         1        1.72e 4\n 8 (26.1,29.7]  26.1   29.7        32 1.22e- 5  2.41e-2         0        4.24e 4\n 9 (29.7,33.2]  29.7   33.2         5 5.55e- 7  1.10e-3         0        2.28e 4\n10 (33.2,36.8]  33.2   36.8        27 5.47e- 9  1.08e-5         0        6.76e 7\n11 (36.8,40.4]  36.8   40.4         4 1.30e-10  2.57e-7         0        6.22e 7\n12 (40.4,44]    40.4  Inf           6 6.11e-13  1.20e-9         0        2.99e10\n\n\nWe can now calculate the test statistic and \\(p\\) value.\n\n# Test statistic\n(chiSq_ts &lt;- sum(steps_df$summand))\n\n[1] 30026566644\n\n# critical value (k* = 12, p = 1)\n(chiSq_cv &lt;- qchisq(p = 1 - 0.025, df = 12 + 1 + 1))\n\n[1] 26.11895\n\n# The test statistic &gt; the critical value, so reject F_A as the distribution\n\n# p-value\n1 - pchisq(q = chiSq_ts, df = 12 + 1 + 1)\n\n[1] 0",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>The Chi Squared Goodness-of-Fit Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_chiSq_GoF.html#brief-interpretation-of-the-output",
    "href": "lessons/02_chiSq_GoF.html#brief-interpretation-of-the-output",
    "title": "\n11  The Chi Squared Goodness-of-Fit Test\n",
    "section": "\n11.8 Brief interpretation of the output",
    "text": "11.8 Brief interpretation of the output\nShockingly (this is sarcasm), the data that had two distinct peaks and looked nothing like a Poisson distribution can not actually be approximated by a Poisson distribution. Go figure. Regardless, this process can be replicated for your own data as well. We are showing off the \\(\\chi^2\\) GoF version because it works for ANY distribution you can think of.\n\n11.8.1 Brief Aside: Simple Normality Tests\nHowever, this all gets much simpler if you only care about testing for normality (which is usually a dumb thing to do anyway, but I digress). If your data are continuous and “normal-ish”, just use the Kolmogorov-Smirnov test, Shapiro-Wilk test, or the Anderson-Darling test instead. Our data are NOT “normal-ish”, so these results are invalid (but I’m showing you the code anyway).\n\n# Kolmogorov-Smirnov test\nks.test(nPartners_int, \"pnorm\")\n\nWarning in ks.test.default(nPartners_int, \"pnorm\"): ties should not be present\nfor the one-sample Kolmogorov-Smirnov test\n\n\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  nPartners_int\nD = 0.9311, p-value &lt; 2.2e-16\nalternative hypothesis: two-sided\n\n# Shapiro-Wilk test\nshapiro.test(nPartners_int)\n\n\n    Shapiro-Wilk normality test\n\ndata:  nPartners_int\nW = 0.86427, p-value &lt; 2.2e-16\n\n# Anderson-Darling test (requires the nortest:: package)\nnortest::ad.test(nPartners_int)\n\n\n    Anderson-Darling normality test\n\ndata:  nPartners_int\nA = 102.73, p-value &lt; 2.2e-16\n\n\nLook at those magical tiny \\(p\\)-values! So we’ve learned that non-normal data isn’t normal. How surprising (again, sarcasm).",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>The Chi Squared Goodness-of-Fit Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_chiSq_GoF.html#wrapping-up",
    "href": "lessons/02_chiSq_GoF.html#wrapping-up",
    "title": "\n11  The Chi Squared Goodness-of-Fit Test\n",
    "section": "\n11.9 Wrapping Up",
    "text": "11.9 Wrapping Up\nIf you need to check if your data are approximately normal, or approximately any other distribution, just plot the data. Use a density, Q-Q plot, or even a histogram. The material in this lesson is to help you when you run into a pesky reviewer / boss / collaborator (who just happens to be addicted to \\(p\\)-values) and they want to perform a test for distributional assumptions.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>The Chi Squared Goodness-of-Fit Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_transformations.html",
    "href": "lessons/02_transformations.html",
    "title": "\n12  Transformations to Normality\n",
    "section": "",
    "text": "12.1 Introduction\nThe pattern of values obtained when a variable is measured in a large number of individuals is called a distribution. Distributions can be broadly classified as normal and non-normal. The normal distribution is also called ‘Gaussian distribution’ as it was first described by K.F. Gauss. This chapter outlines the process of transforming data to achieve a normal distribution in R. Parametric methods, such as t-tests and ANOVA, require that the dependent (outcome) variable is approximately normally distributed within each group being compared. When the normality assumption is not satisfied, transforming the data can correct the non-normal distributions. For t-tests and ANOVA, it is sufficient to transform the dependent variable. However, for linear regression, transformations may be applied to the independent variable, the dependent variable, or both to achieve a linear relationship between variables and ensure homoscedasticity.\nHere are the libraries we will use for this material:\nlibrary(ggplot2)\nlibrary(gridExtra)\nlibrary(GGally)\nlibrary(moments)\nlibrary(knitr)\nlibrary(MASS)",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Transformations to Normality</span>"
    ]
  },
  {
    "objectID": "lessons/02_transformations.html#when-to-apply-transformations-to-normality",
    "href": "lessons/02_transformations.html#when-to-apply-transformations-to-normality",
    "title": "\n12  Transformations to Normality\n",
    "section": "\n12.2 When to Apply Transformations to Normality",
    "text": "12.2 When to Apply Transformations to Normality\nOne of the critical assumptions of statistical hypothesis testing is that the data are samples from a normal distribution. Therefore, it is essential to identify whether distributions are skewed or normal. There are several straightforward methods to detect skewness. Firstly, if the mean is less than twice the standard deviation, the distribution is likely skewed. Additionally, in a normally distributed population, the mean and standard deviation of the samples are independent. This characteristic can be used to detect skewness; if the standard deviation increases as the mean increases across groups from a population, the distribution is skewed. Beyond these simple methods, normality can be verified using statistical tests such as the Shapiro-Wilk test, the Kolmogorov-Smirnov test, and the Anderson-Darling test. Additionally, the moments package in R can be used to calculate skewness quantitatively. The skewness is determined using the third standardized moment, providing a measure of the asymmetry of the data distribution. If skewness is identified, efforts should be made to transform the data to achieve a normal distribution. This transformation is crucial for applying robust parametric tests in the analysis.\nTransformations can also be employed to facilitate comparison and interpretation. A classical example of a variable commonly reported after logarithmic transformation is the hydrogen ion concentration (pH). Another instance where transformation aids in data comparison is the logarithmic transformation of a dose-response curve. When plotted, the dose-response relationship is curvilinear; however, plotting the response against the logarithm of the dose (log dose-response plot) results in an elongated S-shaped curve. The middle portion of this curve forms a straight line, making it easier to compare two straight lines by measuring their slopes than to compare two curves. Thus, transformation can significantly enhance data comparison.\nIn summary, transformations can be applied to normalize data distribution or to simplify interpretation and comparison.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Transformations to Normality</span>"
    ]
  },
  {
    "objectID": "lessons/02_transformations.html#types-of-transformations-to-normality",
    "href": "lessons/02_transformations.html#types-of-transformations-to-normality",
    "title": "\n12  Transformations to Normality\n",
    "section": "\n12.3 Types of Transformations to Normality",
    "text": "12.3 Types of Transformations to Normality\nOften, the transformation that normalizes the distribution also equalizes the variance. While there are several types of transformations available, such as logarithmic, square root, reciprocal, cube root, and Box-Cox, the first three are the most commonly used. Among the transformations discussed in this section, the logarithmic transformation is the most often used. The following guidelines can help in selecting the appropriate method of transformation:\n\n12.3.1 Logarithmic Transformation\nIf the standard deviation is proportional to the mean, the distribution is positively skewed, making logarithmic transformation ideal. Note that when using a log transformation, a constant should be added to all values to ensure they are positive before transformation. The log tranformation is \\[\ny' = \\log(y + c),\n\\] where \\(y\\) is the original value and \\(c\\) is a constant added to ensure all values are positive.\n\n\n\n\n\n\n\n\n\n\n\n12.3.2 Square Root Transformation\nWhen the variance is proportional to the mean, square root transformation is preferred. This is particularly applicable to variables measured as counts, such as the number of malignant cells in a microscopic field or the number of deaths from swine flu. The square root transformation is: \\[\ny' = \\sqrt{y}.\n\\]\n\n\n\n\n\n\n\n\n\n\n\n12.3.3 Arithmetic Reciprocal Transformation\nIf the observations are truncated on the right (such as often the case for academic grade distributions), then one preliminary transformation is to “reverse” the values by subtracting each value from the maximum of all observed values (or from the maximum possible value for observations on a defined scale). This operation “flips” the data distribution from having a heavy left tail to having a heavy right tail, which allows us to perform a secondary transformation (such as a log or square root). This transformation is: \\[\ny' = \\max(y) - y.\n\\]\n\n\n\n\n\n\n\n\nNOTE: now that the data are right-skewed, other transformations can be applied as usual.\n\n12.3.4 Geometric Reciprocal Transformation\nIf the standard deviation is proportional to the mean squared, a reciprocal transformation is appropriate. This is typically used for highly variable quantities, such as serum creatinine levels. Note that this transformation requires all values to be positive or all values to be negative before applying it. \\[\ny' = \\frac{1}{y \\pm c},\n\\] where \\(y\\) is the original value and \\(c\\) is a constant added (subtracted) to ensure all values are positive (negative).\n\n\n\n\n\n\n\n\n\n\n\n12.3.5 Box-Cox Transformation\nThe Box-Cox transformation is a family of power transformations that can be used to stabilize variance and make the data more closely conform to a normal distribution, especially when the best power transformation (e.g., square root, logarithmic) is uncertain. By estimating an optimal parameter \\(\\lambda\\) from the data, the Box-Cox transformation tailors the transformation to the specific dataset’s needs. The transformation is defined as:\n\\[\ny(\\lambda) = \\begin{cases}\n  \\frac{y^\\lambda - 1}{\\lambda} & \\text{if } \\lambda \\neq 0 \\\\\n  \\log(y) & \\text{if } \\lambda = 0\n\\end{cases}\n\\] Here, \\(\\lambda\\) is a parameter that is estimated from the data. The Box-Cox transformation is particularly useful because it includes many of the other transformations (such as the logarithmic and square root transformations) as special cases.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Transformations to Normality</span>"
    ]
  },
  {
    "objectID": "lessons/02_transformations.html#examples-of-transformations-to-normality",
    "href": "lessons/02_transformations.html#examples-of-transformations-to-normality",
    "title": "\n12  Transformations to Normality\n",
    "section": "\n12.4 Examples of Transformations to Normality",
    "text": "12.4 Examples of Transformations to Normality\n\n12.4.1 Data Source and Description\nThe USJudgeRatings dataset is a built-in dataset in R that contains ratings of 43 judges in the US Superior Court. The ratings are based on the evaluations from lawyers who have had cases before these judges. The dataset includes multiple variables that represent different aspects of the judges’ performance.\n\n12.4.1.1 Variables in the Dataset\n\n\nCONT: Judicial “controlling” or authoritative nature.\n\nINTG: Judicial integrity.\n\nDMNR: Judicial demeanor.\n\nDILG: Judicial diligence.\n\nCFMG: Case flow management.\n\nDECI: Judicial decision-making.\n\nPREP: Judicial preparation.\n\nFAMI: Familiarity with the law.\n\nORAL: Oral skills.\n\nWRIT: Written skills.\n\nPHYS: Physical ability.\n\nRTEN: Willingness to follow trends.\n\nThis dataset is useful for analyzing various performance metrics of judges and can be used to explore relationships between different aspects of judicial performance. In the following examples, we’ll consider two variables:\n\n\nCONT: Number of contacts of lawyer with judge. Positively skewed.\n\nPHYS: Physical ability. Negatively skewed\n\n12.4.2 Loading the Data\n\n# Load the USJudgeRatings dataset\ndata(\"USJudgeRatings\")\ndf &lt;- USJudgeRatings\n\n# Display the first few rows of the dataset\nhead(df)\n\n               CONT INTG DMNR DILG CFMG DECI PREP FAMI ORAL WRIT PHYS RTEN\nAARONSON,L.H.   5.7  7.9  7.7  7.3  7.1  7.4  7.1  7.1  7.1  7.0  8.3  7.8\nALEXANDER,J.M.  6.8  8.9  8.8  8.5  7.8  8.1  8.0  8.0  7.8  7.9  8.5  8.7\nARMENTANO,A.J.  7.2  8.1  7.8  7.8  7.5  7.6  7.5  7.5  7.3  7.4  7.9  7.8\nBERDON,R.I.     6.8  8.8  8.5  8.8  8.3  8.5  8.7  8.7  8.4  8.5  8.8  8.7\nBRACKEN,J.J.    7.3  6.4  4.3  6.5  6.0  6.2  5.7  5.7  5.1  5.3  5.5  4.8\nBURNS,E.B.      6.2  8.8  8.7  8.5  7.9  8.0  8.1  8.0  8.0  8.0  8.6  8.6\n\n\n\n12.4.3 Visualizations of CONT and PHYS Variables\n\nggplot(df) + \n  aes(x = CONT) + \n  scale_x_continuous(limits = c(3, 12))+\n  labs(title = \"Density Plot of CONT\", x = \"CONT\", y = \"Density\") +\n  geom_density(fill = \"lightgray\") +\n  stat_function(\n    fun = dnorm,\n    args = list(mean = mean(df$CONT), sd = sd(df$CONT)),\n    color = \"red\",\n    linetype = \"dashed\"\n  ) \n\n\n\n\n\n\nFigure 12.1: Distribution of CONT Variable\n\n\n\n\n\nggplot(df) +\n  aes(x = PHYS) + \n  scale_x_continuous(limits = c(3, 12)) +\n  labs(title = \"Density Plot of PHYS\", x = \"PHYS\", y = \"Density\") +\n  geom_density(fill = \"lightgray\") +\n  stat_function(\n    fun = dnorm,\n    args = list(mean = mean(df$PHYS), sd = sd(df$PHYS)),\n    color = \"red\",\n    linetype = \"dashed\"\n  )\n\n\n\n\n\n\nFigure 12.2: Distribution of PHYS Variable\n\n\n\n\n\n12.4.4 Summary Statistics for CONT and PHYS Variables\n\n# Get the summary statistics for CONT and PHYS variables; note that the\n#   summary() function returns a named numeric vector, so to preserve the names\n#   we transform this vector to a matrix first (before creating the data frame).\nsummary_df &lt;- data.frame(\n  CONT = as.matrix(summary(df$CONT)),\n  PHYS = as.matrix(summary(df$PHYS))\n)\n\n# Display summary statistics as a table\nkable(summary_df)\n\n\nTable 12.1: Summary Statistics for CONT and PHYS Variables\n\n\n\n\n\nCONT\nPHYS\n\n\n\nMin.\n5.700000\n4.700000\n\n\n1st Qu.\n6.850000\n7.700000\n\n\nMedian\n7.300000\n8.100000\n\n\nMean\n7.437209\n7.934884\n\n\n3rd Qu.\n7.900000\n8.500000\n\n\nMax.\n10.600000\n9.100000\n\n\n\n\n\n\n\n\n\n12.4.5 Skewness and Kurtosis for CONT and PHYS Variables\n\n# Calculate skewness and kurtosis for CONT and PHYS variables with moments:: \nskewness_vals &lt;- sapply(df[, c(\"CONT\", \"PHYS\")], moments::skewness)\nkurtosis_vals &lt;- sapply(df[, c(\"CONT\", \"PHYS\")], moments::kurtosis)\n\n# Create a data frame to display skewness and kurtosis\nskew_kurt_df &lt;- data.frame(\n  Skewness = skewness_vals,\n  Kurtosis = kurtosis_vals\n)\n\n# Display the data frame as a table\nkable(skew_kurt_df)\n\n\nTable 12.2: Skewness and Kurtosis for CONT and PHYS Variables\n\n\n\n\n\nSkewness\nKurtosis\n\n\n\nCONT\n1.085973\n4.729637\n\n\nPHYS\n-1.558215\n5.408086\n\n\n\n\n\n\n\n\n\n12.4.6 Visualizations of Transformed CONT and PHYS Variables\nWe will first apply a natural log transformation to the “controlling/authoritarian” variable.\n\n# Apply log transformation to CONT variable\ndf$LOG_CONT &lt;- log(df$CONT)\n\n# Plot density of log-transformed CONT variable\nggplot(df) + \n  aes(x = LOG_CONT) +\n  geom_density(fill = \"lightgray\") +\n  labs(\n    title = \"Density Plot of Log-Transformed CONT\",\n    x = \"Log-Transformed CONT\",\n    y = \"Density\"\n  ) +\n  stat_function(\n    fun = dnorm,\n    args = list(mean = mean(df$LOG_CONT), sd = sd(df$LOG_CONT)),\n    color = \"red\",\n    linetype = \"dashed\"\n  )\n\n\n\n\n\n\nFigure 12.3: Distribution of log transformed CONT Variable\n\n\n\n\nNow we will apply a Box-Cox transformation to the “physical ability” variable.\n\n# Apply Box-Cox transformation to PHYS using MASS:: package\nbc &lt;- MASS::boxcox(df$PHYS ~ 1, lambda = seq(-5, 5, 0.1), plotit = TRUE)\nlambda &lt;- bc$x[which.max(bc$y)]\ndf$BOX_COX_PHYS &lt;- (df$PHYS^lambda - 1) / lambda\n\n\n\n\n\n\nFigure 12.4: Distribution of optimal lambda determined by the boxcox function\n\n\n\n\n\n# Plot density of Box-Cox transformed PHYS variable\nggplot(df) +\n  aes(x = BOX_COX_PHYS) +\n  labs(\n    title = \"Density Plot of Box-Cox Transformed PHYS\",\n    x = \"Box-Cox Transformed PHYS\",\n    y = \"Density\"\n  ) + \n  geom_density(fill = \"lightgray\") +\n  stat_function(\n    fun = dnorm,\n    args = list(mean = mean(df$BOX_COX_PHYS), sd = sd(df$BOX_COX_PHYS)),\n    color = \"red\",\n    linetype = \"dashed\"\n  )\n\n\n\n\n\n\nFigure 12.5: Distribution of Box-Cox transformed PHYS Variable\n\n\n\n\n\n12.4.7 Skewness and Kurtosis for Transformed CONT and PHYS Variables\n\n# Calculate skewness and kurtosis for transformed variables\nskewness_vals_trans &lt;- sapply(df[, c(\"LOG_CONT\", \"BOX_COX_PHYS\")], skewness)\nkurtosis_vals_trans &lt;- sapply(df[, c(\"LOG_CONT\", \"BOX_COX_PHYS\")], kurtosis)\n\n# Create a data frame to display skewness and kurtosis for transformed variables\nskew_kurt_df_trans &lt;- data.frame(\n  Skewness = skewness_vals_trans,\n  Kurtosis = kurtosis_vals_trans\n)\n\n# Display the data frame as a table\nkable(skew_kurt_df_trans)\n\n\nTable 12.3: Skewness and Kurtosis for Transformed Variables (LOG_CONT and BOX_COX_PHYS)\n\n\n\n\n\nSkewness\nKurtosis\n\n\n\nLOG_CONT\n0.6555572\n3.758254\n\n\nBOX_COX_PHYS\n-0.3813574\n2.501916",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Transformations to Normality</span>"
    ]
  },
  {
    "objectID": "lessons/02_transformations.html#results",
    "href": "lessons/02_transformations.html#results",
    "title": "\n12  Transformations to Normality\n",
    "section": "\n12.5 Results",
    "text": "12.5 Results\nThe application of log and Box-Cox transformations has effectively improved the normality of the CONT and PHYS variables, respectively. For the CONT variable, the original distribution exhibited a positive skewness of 1.086 and a kurtosis of 4.730, indicating a right-skewed distribution with heavy tails and a sharp peak. The log transformation reduced the skewness to 0.656 and the kurtosis to 3.758, demonstrating a significant move towards normality, though the distribution still retains some right-skewness and heavier tails compared to a normal distribution. The PHYS variable originally had a negative skewness of -1.558 and a kurtosis of 5.408, reflecting a left-skewed distribution with heavy tails and a pronounced peak. Following the Box-Cox transformation, the skewness was reduced to -0.381 and the kurtosis to 2.502. These results indicate that the transformed PHYS distribution is much closer to normality, with reduced skewness and lighter tails, achieving a more symmetric distribution. In summary, the transformations have substantially mitigated the skewness and kurtosis of both variables, enhancing their suitability for statistical analyses that assume normality. This adjustment ensures more reliable and valid results in subsequent analyses.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Transformations to Normality</span>"
    ]
  },
  {
    "objectID": "lessons/02_transformations.html#conclusion-and-discussion",
    "href": "lessons/02_transformations.html#conclusion-and-discussion",
    "title": "\n12  Transformations to Normality\n",
    "section": "\n12.6 Conclusion and Discussion",
    "text": "12.6 Conclusion and Discussion\nThe transformations applied to the CONT and PHYS variables demonstrate the effectiveness of data transformation techniques in improving the normality of distributions. By addressing skewness and kurtosis, transformations like the log and Box-Cox methods help in stabilizing variance and making data more symmetric. This enhancement is crucial for statistical analyses that rely on the assumption of normality, ensuring more accurate and reliable results. Overall, the use of appropriate transformations is a vital step in data preprocessing, significantly enhancing the suitability of data for various analytical procedures and improving the robustness of statistical inferences. However, caution is warranted in the interpretation of results after transformation. Transformed data can sometimes complicate the understanding of results and their real-world implications, as the transformed scale may not directly relate to the original measurements. It is essential to back-transform results when interpreting findings to ensure they are meaningful and relevant to the original context.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Transformations to Normality</span>"
    ]
  },
  {
    "objectID": "lessons/02_transformations.html#references",
    "href": "lessons/02_transformations.html#references",
    "title": "\n12  Transformations to Normality\n",
    "section": "\n12.7 References",
    "text": "12.7 References\n\nManikandan S. (2010). Data transformation. Journal of pharmacology & pharmacotherapeutics, 1(2), 126–127. https://doi.org/10.4103/0976-500X.72373\nWest R. M. (2022). Best practice in statistics: The use of log transformation. Annals of clinical biochemistry, 59(3), 162–165. https://doi.org/10.1177/00045632211050531\nLee D. K. (2020). Data transformation: a focus on the interpretation. Korean journal of anesthesiology, 73(6), 503–508. https://doi.org/10.4097/kja.20137",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Transformations to Normality</span>"
    ]
  },
  {
    "objectID": "lessons/02_fisher_exact_test.html",
    "href": "lessons/02_fisher_exact_test.html",
    "title": "\n13  Fisher’s Exact Test\n",
    "section": "",
    "text": "13.1 Introduction to Fisher’s Exact test",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Fisher's Exact Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_fisher_exact_test.html#introduction-to-fishers-exact-test",
    "href": "lessons/02_fisher_exact_test.html#introduction-to-fishers-exact-test",
    "title": "\n13  Fisher’s Exact Test\n",
    "section": "",
    "text": "Fisher’s exact test is a non-parametric statistical test used to test an association between categorical variables.\nIt is analogous to Chi-square test, but Fisher’s exact test is conducted when rule of Chi-square test cannot be applied, such as when the sample size in small and more than 20% of cells have expected frequency count of &lt;5 in a contingency table (Bower 2003).\nUsed to assess whether the proportions of categories in two group variables significantly differ from each other.\nUses (hypergeometric) marginal distribution to compute exact p-values which are not approximated, which are also somewhat conservative.\nThis particular test is used to obtain the probability of observing the combination of frequencies that we can actually see.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Fisher's Exact Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_fisher_exact_test.html#assumptions",
    "href": "lessons/02_fisher_exact_test.html#assumptions",
    "title": "\n13  Fisher’s Exact Test\n",
    "section": "\n13.2 Assumptions",
    "text": "13.2 Assumptions\n\nAssumes that the individual observations are independent - variable are not paired or related.\nAssumes that the row and column totals are fixed or conditioned.\nThe variables are categorical and randomly sampled.\nObservations are count data.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Fisher's Exact Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_fisher_exact_test.html#hypotheses",
    "href": "lessons/02_fisher_exact_test.html#hypotheses",
    "title": "\n13  Fisher’s Exact Test\n",
    "section": "\n13.3 Hypotheses",
    "text": "13.3 Hypotheses\nThe hypotheses of Fisher’s exact test are similar to Chi-square test:\n\nNull hypothesis:\\((H_0)\\) There is no significant relationship between the categorical variables (variables are independent).\nAlternative hypothesis: \\((H_1)\\) There is a significant relationship between the categorical variables (variables are dependent).",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Fisher's Exact Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_fisher_exact_test.html#mathematical-definition-of-fishers-exact-test",
    "href": "lessons/02_fisher_exact_test.html#mathematical-definition-of-fishers-exact-test",
    "title": "\n13  Fisher’s Exact Test\n",
    "section": "\n13.4 Mathematical definition of Fisher’s Exact test",
    "text": "13.4 Mathematical definition of Fisher’s Exact test\nThis test is usually used as a one-tailed test. It can also be used as a two tailed test. Fisher’s exact test for a one-tailed \\(p\\)-value is calculated using the following formula:\n\\[\np = {(a+b)!(c+d)!(a+c)!(b+d)! \\over a! b! c! d! n!},\n\\] where \\(a\\),\\(b\\),\\(c\\), and \\(d\\) are the individual frequencies on the 2x2 contingency table and \\(n\\) is the population size (total frequency).",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Fisher's Exact Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_fisher_exact_test.html#intalling-required-packages",
    "href": "lessons/02_fisher_exact_test.html#intalling-required-packages",
    "title": "\n13  Fisher’s Exact Test\n",
    "section": "\n13.5 Intalling required packages",
    "text": "13.5 Intalling required packages\nFirst, we installed and loaded the packages needed for this presentation.\n\n#Installing Required Packages\n#install.packages(\"public.ctn0094data\")\n#install.packages(\"gtsummary\")\n#Install ggstatsplot package\n#install.packages(\"ggstatsplot\")\n#install.packages(\"ggmosaic\")\n#install.packages(\"tidyverse\")\n\n# Loading Required Packages\nlibrary(public.ctn0094data)\nlibrary(gtsummary)\nlibrary(ggstatsplot)\nlibrary(ggmosaic) \nlibrary(tidyverse)",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Fisher's Exact Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_fisher_exact_test.html#data-source-and-description",
    "href": "lessons/02_fisher_exact_test.html#data-source-and-description",
    "title": "\n13  Fisher’s Exact Test\n",
    "section": "\n13.6 Data source and description",
    "text": "13.6 Data source and description\nFor this demonstration of the Fisher’s Exact test , we utilized the demographics, and psychiatric data sets from the public.ctn0094data package the public.ctn0094data package. The public.ctn0094data package contains de-identified and harmonized datasets from the Clinical Trials Network (CTN) protocol number 0094. This project, funded by the US National Institute on Drug Abuse (NIDA), focuses on opioid use disorder (OUD) and includes data from three clinical trials: CTN-0027, CTN-0030, and CTN-0051.The data describe the experiences of patients seeking care for opoid use disorder (OUD).\nThe demographics dataset contains the demographic variables such as age, sex, race, living condition, marital status etc. The psychiatric dataset contains data on different mental health issues and susbstance use, including bipolar, depression, schizophrenia, cocaine use etc.\n\n# # Search for suitable data sets; this lists of all datasets in package\n# data(package = \"public.ctn0094data\") \n\ndata(demographics, package = \"public.ctn0094data\")\ncolnames(demographics)\n\n[1] \"who\"              \"age\"              \"is_hispanic\"      \"race\"            \n[5] \"job\"              \"is_living_stable\" \"education\"        \"marital\"         \n[9] \"is_male\"         \n\ndata(psychiatric, package = \"public.ctn0094data\")\ncolnames(psychiatric) \n\n [1] \"who\"                 \"has_schizophrenia\"   \"has_major_dep\"      \n [4] \"has_bipolar\"         \"has_anx_pan\"         \"has_brain_damage\"   \n [7] \"has_epilepsy\"        \"depression\"          \"anxiety\"            \n[10] \"schizophrenia\"       \"has_opiates_dx\"      \"has_alcol_dx\"       \n[13] \"has_amphetamines_dx\" \"has_cannabis_dx\"     \"has_cocaine_dx\"     \n[16] \"has_sedatives_dx\"   \n\n\n\n13.6.1 Create a model data frame\nWe joined the demographics and psychiatric data sets within the public.ctn0094data package by participants ID (who variable) to create new data frame.\n\n# Joining data sets: \nmodel_df &lt;- \n  demographics %&gt;% \n  left_join(psychiatric, by = \"who\") %&gt;% \n  # Selecting variables of interest for our analysis\n  select(\n    age, race, education, is_male, marital, is_living_stable, has_schizophrenia\n  )\n\n\n13.6.2 Participants characteristics Summary table\nHere, we want to view the frequency of the variables in our dataset using the table summary function tbl_summary ().\n\n# Create Table 1, change the Label using the label function and also view the missing values\nmodel_df %&gt;% \n  tbl_summary(\n    label = list(\n      age = \"Age\",\n      race = \"Race\",\n      education = \"Education_Level\",\n      is_male = \"Male\",\n      marital = \"Marital_Status\",\n      is_living_stable = \"Living_Condition\",\n      has_schizophrenia = \"Schizophrenia\"\n    ),\n    missing_text = \"(Missing)\"\n  )\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nN = 3,5601\n\n\n\n\nAge\n34 (27, 45)\n\n\n    (Missing)\n208\n\n\nRace\n\n\n\n    Black\n365 (10%)\n\n\n    Other\n506 (14%)\n\n\n    Refused/missing\n58 (1.6%)\n\n\n    White\n2,631 (74%)\n\n\nEducation_Level\n\n\n\n    HS/GED\n691 (39%)\n\n\n    Less than HS\n352 (20%)\n\n\n    More than HS\n724 (41%)\n\n\n    (Missing)\n1,793\n\n\nMale\n2,351 (66%)\n\n\n    (Missing)\n4\n\n\nMarital_Status\n\n\n\n    Married or Partnered\n329 (19%)\n\n\n    Never married\n1,028 (59%)\n\n\n    Separated/Divorced/Widowed\n394 (23%)\n\n\n    (Missing)\n1,809\n\n\nLiving_Condition\n1,535 (96%)\n\n\n    (Missing)\n1,962\n\n\nSchizophrenia\n73 (2.4%)\n\n\n    (Missing)\n469\n\n\n\n\n1 Median (IQR); n (%)\n\n\n\n\n\n\n\n13.6.3 Recode to indicate variable factor levels\n\n# Recoding `is_living_stable` and has_schizophrenia`converting all NA to 99\nmodel_df &lt;- \n  model_df %&gt;% \n  mutate(\n    Living_stable = ifelse(is.na(is_living_stable), 99, is_living_stable),\n    Living_stable = factor(\n      Living_stable, levels = c(1, 2, 99), labels = c(\"No\", \"Yes\", \"Missing\")\n    )\n  ) %&gt;% \n  mutate(\n    schizophrenia = ifelse(is.na(has_schizophrenia), 99, has_schizophrenia),\n    schizophrenia = factor(\n      schizophrenia, levels = c(1, 2, 99), labels = c(\"No\", \"Yes\", \"Missing\")\n    )\n  ) \n\nmodel_df \n\n# A tibble: 3,560 × 9\n     age race  education    is_male marital   is_living_stable has_schizophrenia\n   &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;        &lt;fct&gt;   &lt;fct&gt;     &lt;fct&gt;            &lt;fct&gt;            \n 1    43 White More than HS Yes     Married … Yes              No               \n 2    30 White More than HS No      Never ma… Yes              No               \n 3    23 Black More than HS No      Never ma… Yes              No               \n 4    19 White More than HS Yes     Never ma… Yes              No               \n 5    31 White &lt;NA&gt;         No      &lt;NA&gt;      &lt;NA&gt;             &lt;NA&gt;             \n 6    43 White HS/GED       Yes     Married … Yes              No               \n 7    33 White More than HS No      Never ma… Yes              No               \n 8    44 White &lt;NA&gt;         Yes     &lt;NA&gt;      &lt;NA&gt;             &lt;NA&gt;             \n 9    25 Black HS/GED       No      Never ma… Yes              No               \n10    29 Other More than HS No      Never ma… Yes              Yes              \n# ℹ 3,550 more rows\n# ℹ 2 more variables: Living_stable &lt;fct&gt;, schizophrenia &lt;fct&gt;",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Fisher's Exact Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_fisher_exact_test.html#contingency-table-with-tbl_summary-function",
    "href": "lessons/02_fisher_exact_test.html#contingency-table-with-tbl_summary-function",
    "title": "\n13  Fisher’s Exact Test\n",
    "section": "\n13.7 Contingency Table with tbl_summary Function",
    "text": "13.7 Contingency Table with tbl_summary Function\n\nThis is a table that shows the distribution of a variable in the rows and columns. Sometimes referred to as a 2x2 table. They are useful in summarizing categorical variables.\nWe want to create a contingency table of the demographic variable by living_stable to Check the distribution of the frequency count of variables (is_living_stable, Yes = stable and No = unstable has_schizophrenia, Yes = schizophrenia diagnosed and No = no schizophrenia).\n\n\n# creating new data frame keeping only the categorical variable of interest\n#   for our contingency table in the next section\n\nfinalModel_df &lt;- select(model_df, schizophrenia, Living_stable)\n\n# Adding label and overall number \n\nfinalModel_df %&gt;% \n  tbl_summary(by = Living_stable) %&gt;%\n  #add_n() %&gt;%\n  add_overall() %&gt;%\n  modify_spanning_header(c(\"stat_1\", \"stat_2\") ~ \"**Living_stable**\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nOverall, N = 3,5601\n\nLiving_stable\n\nMissing, N = 1,9621\n\n\n\n\nNo, N = 631\n\n\nYes, N = 1,5351\n\n\n\n\n\nschizophrenia\n\n\n\n\n\n\n    No\n3,018 (85%)\n59 (94%)\n1,485 (97%)\n1,474 (75%)\n\n\n    Yes\n73 (2.1%)\n2 (3.2%)\n21 (1.4%)\n50 (2.5%)\n\n\n    Missing\n469 (13%)\n2 (3.2%)\n29 (1.9%)\n438 (22%)\n\n\n\n\n1 n (%)\n\n\n\n\n\n\nFrom the table, it seems like the patients who were homeless (answered no to living_stable) were less likely to be diagnosed with schizophrenia. However, this is not conclusive as we cannot tell if this relationship was a true correlation or it was due to random sampling error. So we will perform the Fisher’s Exact test to confirm the relationship.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Fisher's Exact Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_fisher_exact_test.html#performing-the-fishers-exact-test-using-the-function-fisher.test",
    "href": "lessons/02_fisher_exact_test.html#performing-the-fishers-exact-test-using-the-function-fisher.test",
    "title": "\n13  Fisher’s Exact Test\n",
    "section": "\n13.8 Performing the Fisher’s exact test using the function fisher.test()",
    "text": "13.8 Performing the Fisher’s exact test using the function fisher.test()\n\nA priori, we hypothesized that people who are diagnosed as being schizophrenic are more likely to homeless (unstable living). So we conducted a one-tailed Fisher’s Exact test and specify the direction of the test as “greater”.\nFor a two-tailed test, the alternative argument has a default value of \"two.sided\".\n\n\n# running one-tailed fisher's exact test\nfModelGreater_ls &lt;- fisher.test(\n  x = model_df$is_living_stable, \n  y = model_df$has_schizophrenia, \n  alternative = \"greater\"\n)\nfModelGreater_ls\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  model_df$is_living_stable and model_df$has_schizophrenia\np-value = 0.9429\nalternative hypothesis: true odds ratio is greater than 1\n95 percent confidence interval:\n 0.1159862       Inf\nsample estimates:\nodds ratio \n 0.4175209 \n\n# running two-tailed Fisher's exact test\nfisher.test(\n  x = model_df$is_living_stable, \n  y = model_df$has_schizophrenia\n)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  model_df$is_living_stable and model_df$has_schizophrenia\np-value = 0.2246\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 0.09821534 3.75679596\nsample estimates:\nodds ratio \n 0.4175209",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Fisher's Exact Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_fisher_exact_test.html#interpretation-of-results",
    "href": "lessons/02_fisher_exact_test.html#interpretation-of-results",
    "title": "\n13  Fisher’s Exact Test\n",
    "section": "\n13.9 Interpretation of results",
    "text": "13.9 Interpretation of results\n\n13.9.1 One-Tailed test\n\nNull Hypothesis, \\(H_0\\): people who reported unstable living (homeless) are not more often diagnosed with schizophrenia.\nAlternative Hypothesis, \\(H_A\\): people who reported unstable living(homeless) are more often diagnosed with schizophrenia.\n\nThe \\(p\\) value is greater than 0.05 (\\(p\\) = 0.9429), so we fail to reject the null hypothesis. We conclude that people who reported unstable living are not significantly more likely to be diagnosed with schizophrenia than those who reported stable living.\n\n13.9.2 Two-tailed analysis\n\nNull Hypothesis, \\(H_0\\): There is no association between living-stable and schizophrenia diagnosis variable\nAlternative Hypothesis, \\(H_A\\): There is an association between living condition and schizophrenia diagnosis.\n\nOur \\(p\\)-value = 0.2246, so we fail to reject the null hypothesis, indicating that there is no statistically significant association between living condition and schizophrenia diagnosis.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Fisher's Exact Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_fisher_exact_test.html#visualization-of-the-statistical-test-output",
    "href": "lessons/02_fisher_exact_test.html#visualization-of-the-statistical-test-output",
    "title": "\n13  Fisher’s Exact Test\n",
    "section": "\n13.10 Visualization of the statistical test output",
    "text": "13.10 Visualization of the statistical test output\nWe would like to have a visual representation of the distribution of the categories of our analysis. As shown in the Fisher’s Exact test, there was no statistical significant association between living-stability and schizophrenia diagnosis. Hence, the interpretation of the plots below is based on description of the charts.\n\n13.10.1 Barplots\nWe will first construct a barplot using ggstatsplot:: (for statistical details).\n\ndisplayP_char &lt;- ifelse(\n  test = fModelGreater_ls$p.value &lt; 0.001,\n  yes = \"&lt; 0.001\",\n  no = as.character(round(fModelGreater_ls$p.value, 3))\n)\n\n# combine plot and statistical test with ggbarstats\nggbarstats( \n  data = finalModel_df,\n  x = Living_stable,\n  y = schizophrenia,\n  results.subtitle = FALSE,\n  subtitle = paste0(\"Fisher's exact test, p-value = \", displayP_char)\n)\n\n\n\n\n\n\n\nPatients who indicated living_stable (not homeless) had the highest proportion of not having being diagnosed with schizophrenia. Patients who indicated not having stable living (homeless) had the highest proportion of being diagnosed with schizophrenia.\n\n13.10.2 Mosaic plots\n\n# Basic Mosaic Plot\nmosaic_basic &lt;- finalModel_df %&gt;% \n  ggplot() +\n  geom_mosaic(\n    aes(\n      x = product(schizophrenia),\n      fill = Living_stable\n    )\n  ) +\n  labs(\n    #y = \"Living_stable\",\n    #x = \"schizophrenia\",\n    title = \"Mosaic Plot of schizophrenia by living_stability\") +\n  # Specifies default `geom_mosaic` aesthetics, e.g white panel background, \n  # removes grid lines, adjusts widths and heights of rows and columns to \n  # reflect frequencies\n  theme_mosaic() +\n  theme(legend.position = \"None\") +\n  theme(axis.text.x = element_text(angle = 60, hjust = 1, vjust = 1))\n  \nmosaic_basic\n\n\n\n\n\n\n\nCompared to those who are not homeless, those who are homeless had the highest proportion of being diagnosed with schizophrenia.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Fisher's Exact Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_fisher_exact_test.html#references",
    "href": "lessons/02_fisher_exact_test.html#references",
    "title": "\n13  Fisher’s Exact Test\n",
    "section": "\n13.11 References",
    "text": "13.11 References\n\nBower, Keith M. 2003. “When to Use Fisher’s Exact Test.” In American Society for Quality, Six Sigma Forum Magazine, 2:35–37. 4.\nMcCrum-Gardner, Evie. 2008. “Which Is the Correct Statistical Test to Use?” British Journal of Oral and Maxillofacial Surgery 46 (1): 38–41.\nWong KC. Chi squared test versus Fisher’s exact test. Hong Kong Med J. 2011 Oct;17(5):427\nPatil, I. (2021). Visualizations with statistical details: The ‘ggstatsplot’ approach. Journal of Open Source Software, 6(61), 3167, doi:10.21105/joss.03167\nZach Bobbit. (2021). Fisher’s Exact Test: Definition, Formula, and Example",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Fisher's Exact Test</span>"
    ]
  },
  {
    "objectID": "03_header_two-sample.html",
    "href": "03_header_two-sample.html",
    "title": "Two-Sample Tests",
    "section": "",
    "text": "Text Outline\nThis text covers the basics of:",
    "crumbs": [
      "Two-Sample Tests"
    ]
  },
  {
    "objectID": "03_header_two-sample.html#text-outline",
    "href": "03_header_two-sample.html#text-outline",
    "title": "Two-Sample Tests",
    "section": "",
    "text": "Exploring data\nOne-sample tests\nTwo-sample tests\nLinear regression and ANOVA\nGeneralized linear models\nSome special topics\nMaybe a bit about power calculations…",
    "crumbs": [
      "Two-Sample Tests"
    ]
  },
  {
    "objectID": "03_header_two-sample.html#part-outline",
    "href": "03_header_two-sample.html#part-outline",
    "title": "Two-Sample Tests",
    "section": "Part Outline",
    "text": "Part Outline\nThis part of the text contains the chapters on various two-sample statistical tests:\n\n\\(t\\)-test\nWelch’s \\(t\\)-test\nMann-Whitney \\(U\\) test\nCochran’s \\(Q\\) test\n\\(\\chi^2\\) Test for Independence",
    "crumbs": [
      "Two-Sample Tests"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_ttest.html",
    "href": "lessons/03_two_sample_ttest.html",
    "title": "\n14  Two sample \\(t\\)-test\n",
    "section": "",
    "text": "14.1 Two sample \\(t\\)-test\nThis is also called the independent sample t test. It is used to see whether the population means of two groups are equal or different. This test requires one variable which can be the exposure x and another variable which can be the outcome y. If there are more than two groups, Analysis of Variance (ANOVA) would be more suitable. If data is nonparametric then an alternative test to use would be the Mann Whitney U test. Cressie, N.A., 1986\nThere are two types of independent t tests: the first is the Student’s t test, which assumes the variance of the two groups is equal, and the second being the Welch’s t test (default in R), which assumes the variance in the two groups is different.\nIn this article we will be discussing the Student’s t test.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Two sample $t$-test</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_ttest.html#two-sample-t-test",
    "href": "lessons/03_two_sample_ttest.html#two-sample-t-test",
    "title": "\n14  Two sample \\(t\\)-test\n",
    "section": "",
    "text": "14.1.1 Assumptions\n\nMeasurements for one observation do not affect measurements for any other observation (assumes independence).\nData values in dependent variable are continuous.\nData in each group are normally distributed.\nThe variances for the two independent groups are equal in the Student’s t test.\nThere should be no significant outliers.\n\n14.1.2 Hypotheses\n\n\\((H_0)\\): the mean of group A \\((m_A)\\) is equal to the mean of group B \\((m_B)\\)- two tailed test.\n\\((H_0)\\): \\((m_A)\\ge (m_B)\\)- one tailed test.\n\\((H_0)\\): \\((m_A)\\le (m_B)\\)- one tailed test.\n\nThe corresponding alternative hypotheses would be as follows:\n\n\n\\((H_1)\\): \\((m_A)\\neq(m_B)\\)- two tailed test.\n\n\\((H_1)\\): \\((m_A)&lt;(m_B)\\)- one tailed test.\n\n\\((H_1)\\): \\((m_A)&gt; (m_B)\\)- one tailed test.\n\n14.1.3 Statistical hypotheses formula\nFor the Student’s t test which assumes equal variance, here is an example of how the |t| statistic may be calculated using groups A and B:\n\\(t ={ {m_{A} - m_{B}} \\over \\sqrt{ {S^2 \\over n_{A} } + {S^2 \\over n_{B}}   }}\\)\nThis can be described as the sample mean difference divided by the sample standard deviation of the sample mean difference where:\n\\(m_A\\) and \\(m_B\\) are the mean values of A and B,\n\\(n_A\\) and \\(n_B\\) are the size of group A and B,\n\\(S^2\\) is the estimator for the pooled variance, with the degrees of freedom (df) = \\(n_A + n_B - 2\\),\nand \\(S^2\\) is calculated as follows:\n\\(S^2 = { {\\sum{ (x_A-m_{A})^2} + \\sum{ (x_B-m_{B})^2}} \\over {n_{A} + n_{B} - 2 }}\\)\nWhat if the data is not independent?\nIf the data is not independent such as paired data in the form of matched pairs which are correlated, we use the paired t test. This test checks whether the means of two paired groups are different from each other. It is usually applied in clinical trial studies with a “before and after” or case control studies with matched pairs. For this test we only assume the difference of each pair to be normally distributed (the paired groups are the ones important for analysis) unlike the independent t test which assumes that data from both samples are independent and variances are equal. Xu, M., 2017\n\n14.1.4 Example\n\n14.1.4.1 Prerequisites\n\n\ntidyverse: data manipulation and visualization.\n\nrstatix: providing pipe friendly R functions for easy statistical analysis.\n\ncar: providing variance tests.\n\n\n#install.packages(\"ggstatplot\") \n#install.packages(\"car\")\n#install.packages(\"rstatix\")\n#install.packages(tidyVerse)\n\n\n14.1.4.2 Dataset\nThis example dataset sourced from kaggle was obtained from surveys of students in Math and Portuguese classes in secondary school. It contains demographic information on gender, social and study information.\n\n# load relevant libraries\nlibrary(rcompanion)\nlibrary(car)\nlibrary (gt)\nlibrary(gtsummary)\nlibrary(ggpubr)\nlibrary(rstatix)\nlibrary(tidyverse)\n\n\n# load the dataset\nstu_math &lt;- read_csv(\"../data/03_student-mat.csv\")\n\nRows: 395 Columns: 33\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (17): school, sex, address, famsize, Pstatus, Mjob, Fjob, reason, guardi...\ndbl (16): age, Medu, Fedu, traveltime, studytime, failures, famrel, freetime...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nChecking the data\n\n# check the data\nglimpse(stu_math)\n\nRows: 395\nColumns: 33\n$ school     &lt;chr&gt; \"GP\", \"GP\", \"GP\", \"GP\", \"GP\", \"GP\", \"GP\", \"GP\", \"GP\", \"GP\",…\n$ sex        &lt;chr&gt; \"F\", \"F\", \"F\", \"F\", \"F\", \"M\", \"M\", \"F\", \"M\", \"M\", \"F\", \"F\",…\n$ age        &lt;dbl&gt; 18, 17, 15, 15, 16, 16, 16, 17, 15, 15, 15, 15, 15, 15, 15,…\n$ address    &lt;chr&gt; \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\",…\n$ famsize    &lt;chr&gt; \"GT3\", \"GT3\", \"LE3\", \"GT3\", \"GT3\", \"LE3\", \"LE3\", \"GT3\", \"LE…\n$ Pstatus    &lt;chr&gt; \"A\", \"T\", \"T\", \"T\", \"T\", \"T\", \"T\", \"A\", \"A\", \"T\", \"T\", \"T\",…\n$ Medu       &lt;dbl&gt; 4, 1, 1, 4, 3, 4, 2, 4, 3, 3, 4, 2, 4, 4, 2, 4, 4, 3, 3, 4,…\n$ Fedu       &lt;dbl&gt; 4, 1, 1, 2, 3, 3, 2, 4, 2, 4, 4, 1, 4, 3, 2, 4, 4, 3, 2, 3,…\n$ Mjob       &lt;chr&gt; \"at_home\", \"at_home\", \"at_home\", \"health\", \"other\", \"servic…\n$ Fjob       &lt;chr&gt; \"teacher\", \"other\", \"other\", \"services\", \"other\", \"other\", …\n$ reason     &lt;chr&gt; \"course\", \"course\", \"other\", \"home\", \"home\", \"reputation\", …\n$ guardian   &lt;chr&gt; \"mother\", \"father\", \"mother\", \"mother\", \"father\", \"mother\",…\n$ traveltime &lt;dbl&gt; 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 3, 1, 2, 1, 1, 1, 3, 1, 1,…\n$ studytime  &lt;dbl&gt; 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 3, 1, 2, 3, 1, 3, 2, 1, 1,…\n$ failures   &lt;dbl&gt; 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0,…\n$ schoolsup  &lt;chr&gt; \"yes\", \"no\", \"yes\", \"no\", \"no\", \"no\", \"no\", \"yes\", \"no\", \"n…\n$ famsup     &lt;chr&gt; \"no\", \"yes\", \"no\", \"yes\", \"yes\", \"yes\", \"no\", \"yes\", \"yes\",…\n$ paid       &lt;chr&gt; \"no\", \"no\", \"yes\", \"yes\", \"yes\", \"yes\", \"no\", \"no\", \"yes\", …\n$ activities &lt;chr&gt; \"no\", \"no\", \"no\", \"yes\", \"no\", \"yes\", \"no\", \"no\", \"no\", \"ye…\n$ nursery    &lt;chr&gt; \"yes\", \"no\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes…\n$ higher     &lt;chr&gt; \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"ye…\n$ internet   &lt;chr&gt; \"no\", \"yes\", \"yes\", \"yes\", \"no\", \"yes\", \"yes\", \"no\", \"yes\",…\n$ romantic   &lt;chr&gt; \"no\", \"no\", \"no\", \"yes\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\"…\n$ famrel     &lt;dbl&gt; 4, 5, 4, 3, 4, 5, 4, 4, 4, 5, 3, 5, 4, 5, 4, 4, 3, 5, 5, 3,…\n$ freetime   &lt;dbl&gt; 3, 3, 3, 2, 3, 4, 4, 1, 2, 5, 3, 2, 3, 4, 5, 4, 2, 3, 5, 1,…\n$ goout      &lt;dbl&gt; 4, 3, 2, 2, 2, 2, 4, 4, 2, 1, 3, 2, 3, 3, 2, 4, 3, 2, 5, 3,…\n$ Dalc       &lt;dbl&gt; 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1,…\n$ Walc       &lt;dbl&gt; 1, 1, 3, 1, 2, 2, 1, 1, 1, 1, 2, 1, 3, 2, 1, 2, 2, 1, 4, 3,…\n$ health     &lt;dbl&gt; 3, 3, 3, 5, 5, 5, 3, 1, 1, 5, 2, 4, 5, 3, 3, 2, 2, 4, 5, 5,…\n$ absences   &lt;dbl&gt; 6, 4, 10, 2, 4, 10, 0, 6, 0, 0, 0, 4, 2, 2, 0, 4, 6, 4, 16,…\n$ G1         &lt;dbl&gt; 5, 5, 7, 15, 6, 15, 12, 6, 16, 14, 10, 10, 14, 10, 14, 14, …\n$ G2         &lt;dbl&gt; 6, 5, 8, 14, 10, 15, 12, 5, 18, 15, 8, 12, 14, 10, 16, 14, …\n$ G3         &lt;dbl&gt; 6, 6, 10, 15, 10, 15, 11, 6, 19, 15, 9, 12, 14, 11, 16, 14,…\n\n\nIn total there are 395 observations and 33 variables. We will drop the variables we do not need and keep the variables that will help us answer the following: Is there a difference between the finals grades of boys and girls in maths?\n\\(H_0\\): There is no statistical difference between the final grades between boys and girls.\n\\(H_1\\): There is a statistically significant differencehe in the final grades between the two groups.\n\n# creating a subset of the data \nmath &lt;- stu_math %&gt;% subset %&gt;% select (sex, G3)\nglimpse(math)\n\nRows: 395\nColumns: 2\n$ sex &lt;chr&gt; \"F\", \"F\", \"F\", \"F\", \"F\", \"M\", \"M\", \"F\", \"M\", \"M\", \"F\", \"F\", \"M\", \"…\n$ G3  &lt;dbl&gt; 6, 6, 10, 15, 10, 15, 11, 6, 19, 15, 9, 12, 14, 11, 16, 14, 14, 10…\n\n\nSummary statistics The dependent variable is continuous (grades=G3) and the independent variable is character but binary (sex).\n\n# summarizing our data\n summary(math)\n\n     sex                  G3       \n Length:395         Min.   : 0.00  \n Class :character   1st Qu.: 8.00  \n Mode  :character   Median :11.00  \n                    Mean   :10.42  \n                    3rd Qu.:14.00  \n                    Max.   :20.00  \n\n\nWe see that data ranges from 0-20 with 0 being people who were absent and could not take the test therefore missing data.\nIdentifying outliers - Outliers can also be identified through boxplot.\n\n# creating a boxplot to visualize the outliers (without removing score of zero)\nggplot(data = math) + \n  aes(\n    x = sex, \n    y = G3\n  ) +\n  labs(\n    title = \"Boxplot to visualize the outlier\",\n    x  = \"sex\", \n    y = \"Maths Grades\"\n  ) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n# creating a boxplot to visualize the data with no outliers\nmath2 &lt;- math %&gt;% filter(G3&gt;0)\nggplot(data = math2) + \n  aes(\n    x = sex, \n    y = G3\n  ) +\n  labs(\n    title = \"Boxplot without the outlier\",\n    x  = \"sex\", \n    y = \"Maths Grades\"\n  ) +\n  geom_boxplot()\n\n\n\n\n\n\n\nThe second box plot shows us that there are no outliers as students with a score of zero have been removed. This score is not truly reflective of the performance between boys and girls as a grade of 0 may represent absence or other reasons for the test not been taken.\nWe remove the outliers before running the t test. However, other models can be considered such as the zero inflated model to differentiate those who truly got a 0 and those who were not present to take test.\n\n# finding the mean for the groups with outliers (score of zero included)\nmean(math$G3[math$sex==\"F\"])\n\n[1] 9.966346\n\nmean(math$G3[math$sex==\"M\"])\n\n[1] 10.91444\n\n# finding the mean for the groups without outliers (score of zero not included)\nmean(math2$G3[math2$sex==\"F\"])\n\n[1] 11.20541\n\nmean(math2$G3[math2$sex==\"M\"])\n\n[1] 11.86628\n\n\nThe mean has increased slightly in both groups and the difference in mean has been decreased in both groups after removing the outliers.\nVisualizing the data - We can use barplots to look at the difference in sample sizes between the groups and histograms check distribution of the data for normality.\n\nsample_size &lt;- table(math2$sex)\nsample_size_df &lt;- as.data.frame(sample_size)\ncolnames(sample_size_df) &lt;- c(\"sex\", \"count\")\n\n# plotting bar plot to see the distribution in sample size\nggplot(data = sample_size_df) + \n  aes(\n    x = sex, \n    y = count,\n  ) +\n  labs(\n    title = \"Distribution of sample size by sex\",\n    x = \"Sex\",\n    y = \"Number of students\"\n  ) +\n  geom_bar(stat = \"identity\")\n\n\n\n\n\n\n\nThe bar graph shows that there are slightly more females in the sample than males.\n\n# Histograms for data by groups \n\nmale = math2$G3[math2$sex == \"M\"]\nfemale = math2$G3[math2$sex == \"F\"]\n\n# plotting distribution for males\nplotNormalHistogram(\n  male, \n  breaks= 20,\n  xlim=c(0,20),\n  main=\"Distribution of the grades for males \", \n  xlab= \"Math Grades\"\n  )\n\n\n\n\n\n\n\nThe final grades for males seem to be normally distributed. If it is difficult to be certain whether the data is normally distributed, the histogram of the square root transformed data can be plotted. It can help reveal underlying normality in skewed data by compressing the spread.\n\n# plotting square root transformed distribution for males\nplotNormalHistogram(\n  sqrt(male), \n  breaks= 20,\n  main=\"Distribution of the grades for males \", \n  xlab= \"Math Grades\"\n  )\n\n\n\n\n\n\n\nA highly skewed data might still not appear normal after square root transformation, so this is not a solution to a skewed data, but rather an approach to check normality more precisely.\n\n# plotting distribution for females\nplotNormalHistogram(\n  female, \n  breaks= 20,\n  xlim=c(0,20),\n  main=\"Distribution of the grades for females \", \n  xlab= \"Math Grades\"\n  )\n\n\n\n\n\n\n\nSimilar to males, looking at the square root transformed distribution for females\n\n# plotting square root transformed distribution for males\nplotNormalHistogram(\n  sqrt(female), \n  breaks= 20,\n  main=\"Distribution of the grades for females \", \n  xlab= \"Math Grades\"\n  )\n\n\n\n\n\n\n\nFinal grades for females also appear to be normally distributed. The final score across both is almost evenly distributed.\nCheck the equality of variances (homogeneity) - By looking at the two box plots above for two groups, it does not appear that the variances are different between the two groups.\nWe can use the Levene’s test or the Bartlett’s test to check for homogeneity of variances. The former is in the car library and the later in the rstatix library. If the variances are homogeneous, the p value will be greater than 0.05.\nOther tests include F test 2 sided, Brown-Forsythe and O’Brien but we shall not cover these.\n\n# running the Levene's test to check equal variance\nmath2 %&gt;% levene_test(G3~sex)\n\n# A tibble: 1 × 4\n    df1   df2 statistic     p\n  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1     1   355     0.614 0.434\n\n#don't do this unless worried about the data\n\n\n# running the Bartlett's test to check equal variance\nbartlett.test(G3~sex, data=math2)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  G3 by sex\nBartlett's K-squared = 0.12148, df = 1, p-value = 0.7274\n\n#don't do this unless worried about the data\n\nThe p value is greater than 0.05 from both the test suggesting there is no difference between the variances of the two groups.\n\n14.1.4.3 Assessment\n\nData is continuous(G3)\nData is independent (males and females are distinct and not the same individual)\nData is normally distributed. We might still want to do a square root transformation.\nNo significant outliers.\nThere are equal variances.\n\nAs the assumptions are met we go ahead to perform the Student’s \\(t\\)-test.\n\n14.1.4.4 Performing the two-sample \\(t\\)-test\nSince the default is the Welch t test we use the \\(\\color{blue}{\\text{var.eqaul = TRUE }}\\) code to signify a Student’s t test.\n\n# perfoming the two sample t test\nstat.test &lt;- math2 %&gt;% \n  #mutate(sqrt_G3 = sqrt(G3)) %&gt;%\n  t_test(G3~ sex, var.equal=TRUE) %&gt;%\n  add_significance()\nstat.test\n\n# A tibble: 1 × 9\n  .y.   group1 group2    n1    n2 statistic    df      p p.signif\n  &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   \n1 G3    F      M        185   172     -1.94   355 0.0531 ns      \n\n\n\nstat.test$statistic\n\n        t \n-1.940477 \n\n\nIf you decided to work with square root transformed data, you could use the mutate function to create the ‘sqrt_G3’ variable in the ‘stat.test’ dataset, which produces quite similar p-value and t test statistic.\nThe results are represented as follows;\n\ny - dependent variable\ngroup1, group 2 - compared groups (independent variables)\ndf - degrees of freedom\np - p value\n\ngtsummary table of results\n\n math2 |&gt; \n  tbl_summary(\n    by = sex,\n    statistic =\n      list(\n        all_continuous() ~ \"{mean} ({sd})\")\n    ) |&gt; \n   add_n() |&gt; \n  add_overall() |&gt; \n  add_difference()\n\n\n\n\n\n\nCharacteristic\nN\n\nOverall, N = 3571\n\n\nF, N = 1851\n\n\nM, N = 1721\n\n\nDifference2\n\n\n95% CI2,3\n\n\np-value2\n\n\n\nG3\n357\n11.5 (3.2)\n11.2 (3.2)\n11.9 (3.3)\n-0.66\n-1.3, 0.01\n0.053\n\n\n\n\n1 Mean (SD)\n\n\n\n2 Welch Two Sample t-test\n\n\n\n3 CI = Confidence Interval\n\n\n\n\n\n\n\nInterpretation of results\nFor the Student’s t test, the obtained t statistic of -1.940477 is greater than the critical value at 355 degree of freedom (n1+n2-2) i.e. -1.984, due to which it is not statistically significant. Also, the p-value is greater than alpha of 0.05, due to which we we fail to reject the null hypothesis and conclude that there is no statistical difference between the mean grades of boys and girls. (A significant |t| would be equal to -1.984 or smaller; or equal to 1.984 or greater).\nEffect size Note: Effect size should only be calculated if the null hypothesis is rejected. We are showing how to do this for pedagogical reasons. In practice, we would not calculate an effect size because we fail to reject the null hypothesis.\nCohen’s d can be an used as an effect size statistic for the two sample t test. It is the difference between the means of each group divided by the pooled standard deviation.\n\\(d= {m_A-m_B \\over SDpooled}\\)\nIt ranges from 0 to infinity, with 0 indicating no effect where the means are equal. 0.5 means that the means differ by half the standard deviation of the data and 1 means they differ by 1 standard deviation. It is divided into small, medium or large using the following cut off points.\n\nsmall 0.2-&lt;0.5\nmedium 0.5-&lt;0.8\nlarge &gt;=0.8\n\nFor the above test the following is how we can find the effect size;\n\n#perfoming cohen's d\nmath2 %&gt;% \n  cohens_d(G3~sex,var.equal = TRUE)\n\n# A tibble: 1 × 7\n  .y.   group1 group2 effsize    n1    n2 magnitude\n* &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;ord&gt;    \n1 G3    F      M       -0.206   185   172 small    \n\n\nThe effect size is small d= -0.20.\nIn conclusion, a two-samples t-test showed that the difference was not statistically significant, t(355) = -1.940477, p &lt; 0.0531, d = -0.20; where, t(355) is shorthand notation for a t-statistic that has 355 degrees of freedom and d is Cohen’s d. We can conclude that the females mean final grade is greater than males final grade (d= -0.20) but this result is not significant.\nWhat if it is one tailed t test?\nUse the \\(\\color{blue}{\\text{alternative =}}\\) option to determine if one group is \\(\\color{blue}{\\text{\"less\"}}\\) or \\(\\color{blue}{\\text{\"greater\"}}\\). For example if we want to check the null hypothesis whether the final grades for females are greater than or equal to males we can use the following code:\n\n# perfoming the one tailed two sample t test\nstat.test &lt;- math2 %&gt;% \n  t_test(G3~ sex, var.equal=TRUE, alternative = \"less\") %&gt;%\n  add_significance()\nstat.test\n\n# A tibble: 1 × 9\n  .y.   group1 group2    n1    n2 statistic    df      p p.signif\n  &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   \n1 G3    F      M        185   172     -1.94   355 0.0266 *       \n\n\nSince, the p value is smaller than 0.05 (p=0.027), we reject the null hypothesis. We conclude that the final grades for females are significantly lesser than that for males.\nWhat about running the paired sample t test?\nWe can simply add the syntax \\(\\color{blue}{\\text{paired= TRUE}}\\) to the t_test() function to run the analysis for matched pairs data.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Two sample $t$-test</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_ttest.html#conclusion",
    "href": "lessons/03_two_sample_ttest.html#conclusion",
    "title": "\n14  Two sample \\(t\\)-test\n",
    "section": "\n14.2 Conclusion",
    "text": "14.2 Conclusion\nThis article covers the Student’s t test and how we run it in R. It also shows how we find the effect size and how we can conclude the results.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Two sample $t$-test</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_ttest.html#references",
    "href": "lessons/03_two_sample_ttest.html#references",
    "title": "\n14  Two sample \\(t\\)-test\n",
    "section": "\n14.3 References",
    "text": "14.3 References\n\nCressie, N.A.C. and Whitford, H.J. (1986). How to Use the Two Sample t-Test. Biom. J., 28: 131-148. https://doi.org/10.1002/bimj.4710280202\nXu, M., Fralick, D., Zheng, J. Z., Wang, B., Tu, X. M., & Feng, C. (2017). The Differences and Similarities Between Two-Sample T-Test and Paired T-Test. Shanghai archives of psychiatry, 29(3), 184–188. https://doi.org/10.11919/j.issn.1002-0829.217070",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Two sample $t$-test</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_mann_whitney.html",
    "href": "lessons/03_two_sample_mann_whitney.html",
    "title": "\n15  Mann-Whitney-U Test Example\n",
    "section": "",
    "text": "15.1 Introduction\nOverall goal: Identify whether the distribution of two groups significantly differs.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Mann-Whitney-U Test Example</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_mann_whitney.html#introduction",
    "href": "lessons/03_two_sample_mann_whitney.html#introduction",
    "title": "\n15  Mann-Whitney-U Test Example\n",
    "section": "",
    "text": "Mann Whitney U test, also known as the Wilcoxon Rank-Sum test, is a nonparametric statistical test of the null hypothesis, which is commonly used to compare the means or medians of two independent groups with the assumption that the at least one group is not normally distributed and when the sample size is small.\n\nThe Welch U test should be used when signs of skewness and variance of heterogeneity.\n\n\n\n\nIt is useful for numerical/continuous variables.\n\nFor example, if researchers want to compare the age or height of two different groups (as continuous variables) in a study with non-normally distributed data.\n\n\n\nWhen conducting this test, aside from reporting the p-value, the spread and the shape of the data should be described.\n\n\n\n15.1.0.1 Assumptions\n\nSamples are independent: Each dependent variable must be related to only one independent variable.\nThe response variable is ordinal or continuous.\nAt least one variable is not normally distributed.\n\n15.1.1 Hypotheses\nNull Hypothesis (H0): Distribution1 = Distribution2\n\nMean/Median Ranks of two levels are equal.\n\n\nAlternate Hypothesis (H1): Distribution1 ≠ Distribution2\n\nMean/Median Ranks of two levels are significantly different.\n\n\n\n15.1.1.1 Mathematical Equation\n\\(U_1 = n_1n_2 + \\frac{n_1 \\cdot (n_1 + 1)}{2} - R_1\\)\n\\(U_2 = n_1n_2 + \\frac{n_2 \\cdot (n_2 + 1)}{2} - R_2\\)\nWhere:\n\n\n\\(U_1\\) and \\(U_2\\) represent the test statistics for two groups (Male & Female).\n\n\\(R_1\\) and \\(R_2\\) represent the sum of the ranks of the observations for two groups.\n\n\\(n_1\\) and \\(n_2\\) are the sample sizes for two groups.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Mann-Whitney-U Test Example</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_mann_whitney.html#performing-mann-whitney-u-test-in-r",
    "href": "lessons/03_two_sample_mann_whitney.html#performing-mann-whitney-u-test-in-r",
    "title": "\n15  Mann-Whitney-U Test Example\n",
    "section": "\n15.2 Performing Mann-Whitney U Test in R",
    "text": "15.2 Performing Mann-Whitney U Test in R\n\n15.2.1 Data Source\nIn this example, we will perform the Mann-Whitney U Test using wave 8 (2012-2013) data of a longitudinal epidemiological study titled Hispanic Established Populations For the Epidemiological Study of Elderly (HEPESE).\nThe HEPESE provides data on risk factors for mortality and morbidity in Mexican Americans in order to contrast how these factors operate differently in non-Hispanic White Americans, African Americans, and other major ethnic groups. The data is publicly available and can be obtained from the University of Michigan website. For the purposes of this report/chapter, the example in the analysis uses synthetic data. Using this data, we want to explore whether there are significant gender differences in age when Type 2 diabetes mellitus (T2DM) is diagnosed. Type 2 diabetes is a chronic disease condition that has affected 37 million people living in the United States. Type 2 diabetes is the eighth leading cause of death and disability in US. Type 2 diabetes generally occurs among adults aged 45 or older, but may also occur amongst young adults and children. Diabetes and its complications are often preventable by following lifestyle guidelines and taking medication in a timely manner. 1 in 5 of US people don’t know they have diabetes.\nResearch has shown that men are more likely to develop type 2 diabetes, while women are more likely to experience complications from type 2 diabetes, including heart and kidney disease.\nIn this report, we want to test whether there are significant differences in age at which diabetes is diagnosed among males and females.\nDependent Response Variable: ageAtDx = Age_Diagnosed = Age at which diabetes is diagnosed.\nIndependent Variable: isMale = Gender\nResearch Question: Does the age at which diabetes is diagnosed significantly differ among men and women?\nNull Hypothesis (H0): Mean rank of age at which diabetes is diagnosed is equal among men and women.\nAlternate Hypothesis (H1): Mean rank of age at which diabetes is diagnosed is not equal among men and women.\n\n15.2.2 Packages\n\ngmodels: Helps to compute and display confidence intervals (CI) for model estimates.\nDescTools: Provides tools for basic statistics e.g. to compute Median CI for an efficient data description.\nggplot2: Helps to create boxplots.\nqqplotr: Helps to create QQ plot.\ndplyr: Used to manipulate data and provide summary statistics.\nhaven: Helps to import SPSS data into r.\n\nDependencies = TRUE : Indicates that while installing packages, it must also install all dependencies of the specified package.\n\n# install.packages(\"gmodels\", dependencies = TRUE)\n# install.packages(\"car\", dependencies = TRUE)\n# install.packages(\"DescTools\", dependencies = TRUE)\n# install.packages(\"ggplot2\", dependencies = TRUE)\n# install.packages(\"qqplotr\", dependencies = TRUE)\n# install.packages(\"gtsummary\", dependencies = TRUE)\n\nLoading Library\n\nsuppressPackageStartupMessages(library(haven))\nsuppressPackageStartupMessages(library(ggpubr))\nsuppressPackageStartupMessages(library(gmodels))\nsuppressPackageStartupMessages(library(DescTools))\nsuppressPackageStartupMessages(library(ggplot2))\nsuppressPackageStartupMessages(library(qqplotr))\nsuppressPackageStartupMessages(library(gtsummary))\nsuppressPackageStartupMessages(library(tidyverse))\n\nData Importing\n\nHEPESE &lt;- read_csv(\"../data/03_HEPESE_synthetic_20240510.csv\")\n\nRows: 744 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (1): ageAtDx\nlgl (1): isMale\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n15.2.3 Data Exploration\n\n# str(HEPESE)\nstr(HEPESE$isMale)\n\n logi [1:744] FALSE FALSE FALSE TRUE FALSE TRUE ...\n\nstr(HEPESE$ageAtDx)\n\n num [1:744] 87 70 68 60 55 33 38 65 50 68 ...\n\n\nAfter inspecting the data, we found that values of our dependent and independent variable values are in character format. We want them to be numerical and categorical, respectively. First, we will convert the dependent variable into numerical form, and our independent variable into categorical. Then, we will recode the factors as male and female. For simplicity’s sake, we will also rename our dependent and independent variable.\n\n# convert to number and factor\nHEPESE$ageAtDx &lt;- as.numeric(HEPESE$ageAtDx)\nclass(HEPESE$ageAtDx)\n\n[1] \"numeric\"\n\nHEPESE$isMale &lt;- as_factor(HEPESE$isMale)\nclass(HEPESE$isMale)\n\n[1] \"factor\"\n\n\nThe next step is to calculate some of the descriptive data to give us a better idea of the data that we are dealing with. This can be done using the summarise function.\nDescriptive Data\n\nDes &lt;- \n HEPESE %&gt;% \n select(isMale, ageAtDx) %&gt;% \n group_by(isMale) %&gt;%\n summarise(\n   n = n(),\n   mean = mean(ageAtDx, na.rm = TRUE),\n   sd = sd(ageAtDx, na.rm = TRUE),\n   stderr = sd/sqrt(n),\n   LCL = mean - qt(1 - (0.05 / 2), n - 1) * stderr,\n   UCL = mean + qt(1 - (0.05 / 2), n - 1) * stderr,\n   median = median(ageAtDx, na.rm = TRUE),\n   min = min(ageAtDx, na.rm = TRUE), \n   max = max(ageAtDx, na.rm = TRUE),\n   IQR = IQR(ageAtDx, na.rm = TRUE),\n   LCLmed = MedianCI(ageAtDx, na.rm = TRUE)[2],\n   UCLmed = MedianCI(ageAtDx, na.rm = TRUE)[3]\n )\n\nDes\n\n# A tibble: 2 × 13\n  isMale     n  mean    sd stderr   LCL   UCL median   min   max   IQR LCLmed\n  &lt;fct&gt;  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 FALSE    455  67.6  14.1  0.661  66.3  68.9     70    18    93    19     68\n2 TRUE     289  67.1  15.1  0.886  65.3  68.8     70    20    94    18     68\n# ℹ 1 more variable: UCLmed &lt;dbl&gt;\n\n\n\nn: The number of observations for each gender.\nmean: The mean age when diabetes is diagnosed for each gender.\nsd: The standard deviation of each gender.\nstderr: The standard error of each gender level. That is the standard deviation / sqrt (n).\nLCL, UCL: The upper and lower confidence intervals of the mean. This values indicates the range at which we can be 95% certain that the true mean falls between the lower and upper values specified for each gender group assuming a normal distribution.\nmedian: The median value for each gender.\nmin, max: The minimum and maximum value for each gender.\nIQR: The interquartile range of each gender. That is the 75th percentile – 25th percentile.\nLCLmed, UCLmed: The 95% confidence interval for the median.\n\nChecking Assumptions and Visualizing the Data\nThe next step is to visualize the data. This can be done using different functions under the ggplot package.\n1) Box plot\n\nggplot(\n HEPESE, \n aes(\n   x = isMale, \n   y = ageAtDx, \n   fill = isMale\n )\n) +\n stat_boxplot(\n   geom = \"errorbar\", \n   width = 0.5\n ) +\n geom_boxplot(\n   fill = \"light blue\"\n ) + \n stat_summary(\n   fun = mean, \n   geom = \"point\", \n   shape = 10, \n   size = 3.5, \n   color = \"black\"\n ) + \n ggtitle(\n   \"Boxplot of Gender\"\n ) + \n theme_bw() + \n theme(\n   legend.position = \"none\"\n )\n\n\n\n\n\n\n\n2) QQ plot\n\nlibrary(conflicted)\nconflict_prefer(\"stat_qq_line\", \"qqplotr\", quiet = TRUE)\n\n\n# Perform QQ plots by group\nQQ_Plot &lt;- \nggplot(\n data = HEPESE, \n aes(\n   sample = ageAtDx, \n   color = isMale, \n   fill = isMale\n )\n) +\n stat_qq_band(\n   alpha = 0.5, \n   conf = 0.95, \n   qtype = 1, \n   bandType = \"boot\"\n ) +\n stat_qq_line(\n   identity = TRUE\n ) +\n stat_qq_point(\n   col = \"black\"\n ) +\n facet_wrap(\n   ~ isMale, scales = \"free\"\n ) +\n labs(\n   x = \"Theoretical Quantiles\", \n   y = \"Sample Quantiles\"\n ) + theme_bw()\n\nQQ_Plot\n\n\n\n\n\n\n\n\nstat_qq_line: Draws a reference line based on the data quantiles.\n\nStat_qq_band: Draws confidence bands based on three methods; “pointwise”/“boot”,“Ks” and “ts”.\n\n\"pointwise\" constructs simultaneous confidence bands based on the normal distribution;\n\"boot\" creates pointwise confidence bands based on a parametric boostrap;\n\"ks\" constructs simultaneous confidence bands based on an inversion of the Kolmogorov-Smirnov test;\n\"ts\" constructs tail-sensitive confidence bands\n\n\n\nStat_qq_Point: Is a modified version of ggplot: : stat_qq with some parameters adjustments and a new option to detrend the points.\n3) Histogram\nA histogram is the most commonly used graph to show frequency distributions.\n\n\n\nggplot(HEPESE) +\n  aes(x = ageAtDx, fill = isMale) +\n  geom_histogram() +\n  facet_wrap(~ isMale) \n\n\n\n\n\n\n\n**3b) Density curve in Histogram**\nA density curve gives us a good idea of the “shape” of a distribution, including whether or not a distribution has one or more “peaks” of frequently occurring values and whether or not the distribution is skewed to the left or the right.\n\nggplot(HEPESE) +\n  aes(\n    x = ageAtDx,\n    fill = isMale\n  ) +\n  labs(\n    x = \"Age When diabetes is diagnosed\",\n    y = \"Density\",\n    fill = \"Gender\",\n    title = \"A Density Plot of Age when diabetes is diagnosed\",\n    caption = \"Data Source: HEPESE Wave 8 (ICPSR 36578)\"\n  ) + \n  geom_density() +\n  facet_wrap(~isMale)\n\n\n\n\n\n\n\nThis density curve shows that our data does not have a bell shaped distribution and it is slightly skewed towards the left.\n4) Statistical test for normality\n\nHEPESE %&gt;%\n  group_by(isMale) %&gt;%\n  summarise(\n    `W Stat` = shapiro.test(ageAtDx)$statistic,\n    p.value = shapiro.test(ageAtDx)$p.value\n  )\n\n# A tibble: 2 × 3\n  isMale `W Stat`  p.value\n  &lt;fct&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 FALSE     0.959 6.50e-10\n2 TRUE      0.937 9.99e-10\n\n\nInterpretation\nFrom the above table, we see that the value of the Shapiro-Wilk Test is 0.0006 and 0.000002, which are both less than 0.05. Therefore we have enough evidence to reject the null hypothesis and confirm that the data significantly deviates from a normal distribution.\n\n15.2.4 Mann Whitney U Test\n\nresult &lt;- wilcox.test(\n  ageAtDx ~ isMale, \n  data = HEPESE, \n  na.rm = TRUE, \n  exact = FALSE, \n  conf.int = TRUE\n)\n\ntibble(\n  Test_Statistic = result$statistic,\n  P_Value = result$p.value,\n  Method = result$method\n)\n\n# A tibble: 1 × 3\n  Test_Statistic P_Value Method                                           \n           &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                                            \n1          66178   0.880 Wilcoxon rank sum test with continuity correction",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Mann-Whitney-U Test Example</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_mann_whitney.html#results",
    "href": "lessons/03_two_sample_mann_whitney.html#results",
    "title": "\n15  Mann-Whitney-U Test Example\n",
    "section": "\n15.3 Results",
    "text": "15.3 Results\nWhile the analysis above is for synthetic data, we see that the mean age at which diabetes is diagnosed is not significantly different in males (69 years old) and females (66 years old). Of note, the Mann-Whitney U-Test applied in the real data (not shown in this report) showed that this difference is not statistically significant at 0.05 level of significance because the statistical p value (p=.155) is greater than the critical value (p=0.05). For the real data, the test statistic is W = 5040.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Mann-Whitney-U Test Example</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_mann_whitney.html#conclusion",
    "href": "lessons/03_two_sample_mann_whitney.html#conclusion",
    "title": "\n15  Mann-Whitney-U Test Example\n",
    "section": "\n15.4 Conclusion",
    "text": "15.4 Conclusion\nFrom the above result, we can conclude that gender does not play a significant role in the age at which one is diagnosed with diabetes. Diabetes is the 8th leading cause of death and disability in the US, and 1 in 5 US adults are currently unaware of their diabetes condition. This urges the need for increased policy efforts towards timely diabetes testing and diagnosing. Although previous research has suggested that there are gender based differences in diabetes related severity of inquiries, our findings suggest that this difference is not due to age, and may be due to other gender based differences, such as willingness to seek medical care, underlying health issues, etc. There may not necessarily be a need for gender-based approaches to interventions aimed at increasing diabetes surveillance, and efforts should focus on targeting the population as a whole.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Mann-Whitney-U Test Example</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_bootstrap_conf_int.html",
    "href": "lessons/03_two_sample_bootstrap_conf_int.html",
    "title": "\n16  Bootstrap Tests\n",
    "section": "",
    "text": "16.1 Introduction to Bootstrapping\nBootstrapping, introduced by Brad Efron in 1979, is founded on a straightforward concept: when our data is a sample from a larger population, why not generate additional samples by resampling from our existing data? However, since we lack access to new data, we resort to repeatedly sampling from our dataset with replacement.\nThe primary objective of bootstrapping is to augment the sample size for analysis, particularly in scenarios where the provided sample size is limited.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Bootstrap Tests</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_bootstrap_conf_int.html#mathematical-definition",
    "href": "lessons/03_two_sample_bootstrap_conf_int.html#mathematical-definition",
    "title": "\n16  Bootstrap Tests\n",
    "section": "\n16.2 Mathematical definition",
    "text": "16.2 Mathematical definition\nBootstrapping involves randomly selecting n observations from a sample with replacement to create a bootstrap sample. The process of sampling with replacement allows each observation in the original dataset to be selected multiple times or not at all in the bootstrap sample.\nOnce a bootstrap sample is obtained, the statistic of interest (e.g., mean, median, standard deviation) is calculated from this sample. This process is repeated multiple times to generate a distribution of the statistic under the assumption that the original dataset is representative of the population.\nBootstrapping can be used to construct confidence intervals for a population parameter (e.g., mean, median) by calculating the desired quantiles (e.g., percentiles) of the bootstrap distribution of the statistic.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Bootstrap Tests</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_bootstrap_conf_int.html#penguins-data",
    "href": "lessons/03_two_sample_bootstrap_conf_int.html#penguins-data",
    "title": "\n16  Bootstrap Tests\n",
    "section": "\n16.3 penguins Data",
    "text": "16.3 penguins Data\nThe penguins dataset contains measurements collected in the Palmer Archipelago of Antarctica, made available by Dr. Kristen Gorman and the Palmer Station Long Term Ecological Research (LTER) Program. This dataset, included in the palmerpenguins package, comprises observations on various attributes of penguins, including species, island of origin, physical measurements (such as flipper length, body mass, and bill dimensions), sex, and year of observation. In total, the dataset consists of 344 rows and 8 variables.\nIn this tests we are going to obtain the 95% confidence interval for flipper length of the Adelie penguin from two different Islands.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Bootstrap Tests</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_bootstrap_conf_int.html#cleaning-the-data",
    "href": "lessons/03_two_sample_bootstrap_conf_int.html#cleaning-the-data",
    "title": "\n16  Bootstrap Tests\n",
    "section": "\n16.4 Cleaning the data",
    "text": "16.4 Cleaning the data\n\nCodenew_penguins_df &lt;- \n  filter(penguins, species == \"Adelie\", island != \"Dream\") %&gt;% \n  select(species, island, flipper_length_mm) %&gt;% \n  arrange(island, .by_group = TRUE) %&gt;% \n  drop_na()\n\nview(new_penguins_df)\n\n\nThe island of Dream penguin population was excluded because their population size was much larger compared to Torgersen and Biscoe populations.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Bootstrap Tests</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_bootstrap_conf_int.html#assumptions-of-bootstrap",
    "href": "lessons/03_two_sample_bootstrap_conf_int.html#assumptions-of-bootstrap",
    "title": "\n16  Bootstrap Tests\n",
    "section": "\n16.5 Assumptions of Bootstrap",
    "text": "16.5 Assumptions of Bootstrap\n\nThe dataset is a random sample drawn representative of the population of interest.\nResampling with replacement accurately simulates the sampling process.\nObservations in the dataset are independent and identically distributed",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Bootstrap Tests</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_bootstrap_conf_int.html#checking-distribution",
    "href": "lessons/03_two_sample_bootstrap_conf_int.html#checking-distribution",
    "title": "\n16  Bootstrap Tests\n",
    "section": "\n16.6 Checking Distribution",
    "text": "16.6 Checking Distribution\n\nCode# check the boxplot of the data\nboxplot(\n  new_penguins_df$flipper_length_mm ~ new_penguins_df$island, las = 1, \n  ylab = \"Flipper Length (mm)\",\n  xlab = \"Island\",\n  main = \"Flipper Length by Island\"\n)\n\n\n\n\n\n\nCode# check the histogram of the data\nhist(\n  x = new_penguins_df$flipper_length_mm,\n  main = \"Distribution of Flipper Length (mm)\",\n  xlab = \"Flipper Length\"\n)",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Bootstrap Tests</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_bootstrap_conf_int.html#code-to-run-bootstrap",
    "href": "lessons/03_two_sample_bootstrap_conf_int.html#code-to-run-bootstrap",
    "title": "\n16  Bootstrap Tests\n",
    "section": "\n16.7 Code to run Bootstrap",
    "text": "16.7 Code to run Bootstrap\n\nCode# set a seed so that our random results can be replicated by other people:\nset.seed(20150516)\n\n# take a random re-sample of the data that is the *same size*\nN &lt;- length(new_penguins_df$flipper_length_mm)\n\n# a random sample:\nsample(new_penguins_df$flipper_length_mm, size = N, replace = TRUE)\n\n [1] 184 192 198 195 195 176 188 183 184 193 199 198 184 190 198 195 195 199 193\n[20] 197 198 189 197 188 189 199 200 190 183 198 194 190 191 196 189 195 198 197\n[39] 191 184 198 180 195 186 193 193 191 195 190 198 189 181 197 196 182 200 188\n[58] 184 202 189 197 186 181 195 181 191 185 193 196 185 192 199 186 196 180 190\n[77] 190 195 197 193 191 181 195 190 186 189 192 187 190 195 195 182 172 194 181\n\nCode# number of bootstrap samples\nB_int &lt;- 10000\n\n# create a list of these thousands of samples \nbootstrapSamples_ls &lt;- map(\n  .x = 1:B_int,\n  .f = ~{\n    sample(new_penguins_df$flipper_length_mm, size = N, replace = TRUE)\n  }\n)\n\n# subset of the random samples\nbootstrapSamples_ls[1:3]\n\n[[1]]\n [1] 183 190 189 188 181 198 181 172 187 189 189 193 180 197 191 190 196 191 195\n[20] 181 193 190 190 186 188 195 190 197 198 190 180 198 194 188 195 191 203 199\n[39] 190 189 195 186 189 199 202 197 189 190 194 190 181 190 190 181 186 196 174\n[58] 185 174 202 191 184 181 184 193 190 190 190 191 196 189 195 195 198 193 190\n[77] 197 184 186 188 193 190 191 195 198 180 191 185 189 192 183 192 199 186 195\n\n[[2]]\n [1] 187 194 187 189 184 188 187 187 184 197 193 191 187 189 190 172 187 186 180\n[20] 193 191 195 195 180 184 189 197 191 187 186 186 187 184 188 190 193 198 190\n[39] 195 198 184 197 195 195 195 198 194 191 198 197 198 186 194 195 189 186 181\n[58] 180 191 180 191 193 196 191 202 191 187 181 199 172 181 191 195 195 194 198\n[77] 191 191 190 192 190 199 195 193 195 197 188 181 190 185 186 191 174 193 195\n\n[[3]]\n [1] 191 196 203 195 185 195 193 186 186 202 186 203 187 180 185 186 192 202 186\n[20] 192 200 195 184 185 195 193 199 190 189 185 181 181 188 197 181 190 188 185\n[39] 187 184 184 195 199 186 200 186 192 195 190 182 189 191 203 193 195 191 191\n[58] 199 195 198 187 191 195 190 190 187 189 192 186 199 193 190 187 181 190 191\n[77] 190 190 183 193 190 197 181 190 187 198 187 190 200 184 190 184 186 191 193\n\n\n\nCode# The Sample Mean\nbootMeans_num &lt;-\n  bootstrapSamples_ls %&gt;%\n  # the map_dbl() function takes in a list and returns an atomic vector of type\n  #   double (numeric)\n  map_dbl(mean)\n\n# a normally distributed histogram using the samples from bootstrapping\nhist(bootMeans_num)\n\n\n\n\n\n\nCode# 95% confidence interval?\nquantile(bootMeans_num, probs = c(0.025, 0.975))\n\n    2.5%    97.5% \n188.7682 191.3684 \n\n\n\n16.7.1 Code for spearman correlation\n\nCode# Custom function to find correlation between the bill length and depth \ncorr.fun &lt;- function(data, idx) {\n  \n# vector of indices that the boot function uses\n  df &lt;- data[idx, ]\n\n# Find the spearman correlation between\n# the 3rd (length) and 4th (depth) columns of dataset\n  cor(df[, 3], df[, 4], method = 'spearman')\n}\n\n# Setting the seed for reproducability of results\nset.seed(42)\n\n# Calling the boot function with the dataset\nbootstrap &lt;- boot(iris, corr.fun, R = 1000)\n\n# Display the result of boot function\nbootstrap\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = iris, statistic = corr.fun, R = 1000)\n\n\nBootstrap Statistics :\n     original       bias    std. error\nt1* 0.9376668 -0.002717295 0.009436212\n\nCode# Plot the bootstrap sampling distribution using ggplot\nplot(bootstrap)\n\n\n\n\n\n\nCode# Function to find the bootstrap CI\nboot.ci(\n  boot.out = bootstrap,\n    type = \"perc\"\n)\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 1000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = bootstrap, type = \"perc\")\n\nIntervals : \nLevel     Percentile     \n95%   ( 0.9142,  0.9519 )  \nCalculations and Intervals on Original Scale\n\n\nThis code utilizes bootstrapping to estimate the sampling distribution and confidence interval of the Spearman correlation coefficient between bill length and depth in the iris dataset. The boot() function generates bootstrap samples, while the boot.ci() function calculates the bootstrap confidence interval. Visualizations are provided to aid in understanding the sampling distribution.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Bootstrap Tests</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_bootstrap_conf_int.html#conclusion",
    "href": "lessons/03_two_sample_bootstrap_conf_int.html#conclusion",
    "title": "\n16  Bootstrap Tests\n",
    "section": "\n16.8 Conclusion",
    "text": "16.8 Conclusion\nIn conclusion, bootstrap testing emerges as a valuable tool, particularly when confronted with small sample sizes. By leveraging resampling techniques, it offers a robust method to estimate parameters, assess uncertainty, and make reliable inferences about population statistics.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Bootstrap Tests</span>"
    ]
  },
  {
    "objectID": "lessons_original/03_two_sample_chi_sq_independence.html",
    "href": "lessons_original/03_two_sample_chi_sq_independence.html",
    "title": "17  Chi-Squared Independence",
    "section": "",
    "text": "17.1 Introduction\nThe Chi-square test of independence (also known as the Pearson Chi-square test, or simply the Chi-square) is one of the most useful statistics for testing hypotheses when the variables are nominal. It is a non-parametric tool that does not require equality of variances among the study groups or homoscedasticity in the data.(mchugh2013?)\nBeing a non-parametric test tool, Chi-square test can be used when any one of the following conditions pertains to the data:(mchugh2013?)",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Chi-Squared Independence</span>"
    ]
  },
  {
    "objectID": "lessons_original/03_two_sample_chi_sq_independence.html#introduction",
    "href": "lessons_original/03_two_sample_chi_sq_independence.html#introduction",
    "title": "17  Chi-Squared Independence",
    "section": "17.2 Assumptions(mchugh2013?)",
    "text": "Variables are nominal or ordinal.\nThe frequency count is &gt;5 for more than 80% of the cells in a contingency table.\nThe sample sizes of the study groups may be of equal size or unequal size but samples do not have equal variance.\nThe original data were measured at an interval or ratio level, but violate one of the following assumptions of a parametric test:\n\nThe distribution of the data was seriously skewed or kurtotic.\nThe data violate the assumptions of equal variance or homoscedasticity.\nFor any reasons , the continuous data were collapsed into a small number of categories, and thus the data are no longer interval or ratio.(miller1982?)\n\n17.2 Assumptions(mchugh2013?)\n\nThe data in the cells should be frequencies, or counts of cases rather than % or some other transformation of the data.\nThe levels (or categories) of the variables are mutually exclusive.\nEach subject may contribute data to one and only one cell in the χ2. If, for example, the same subjects are tested over time such that the comparisons are of the same subjects at Time 1, Time 2, Time 3, etc., then χ2 may not be used.\nThe study groups must be independent. This means that a different test must be used if the two groups are related. For example, a different test must be used if the researcher’s data consists of paired samples, such as in studies in which a parent is paired with his or her child.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Chi-Squared Independence</span>"
    ]
  },
  {
    "objectID": "lessons_original/03_two_sample_chi_sq_independence.html#hypotheses",
    "href": "lessons_original/03_two_sample_chi_sq_independence.html#hypotheses",
    "title": "17  Chi-Squared Independence",
    "section": "17.3 Hypotheses",
    "text": "17.3 Hypotheses\nNull Hypothesis (H0): Outcome variable is independent of type of exposure variables. There is no significant difference in the association of group A/B with outcome variable.\nAlternate Hypothesis: (H1) Outcome variable varies significantly depending upon the type of exposure variable. There is a significant difference in the association of group A/B with outcome variable.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Chi-Squared Independence</span>"
    ]
  },
  {
    "objectID": "lessons_original/03_two_sample_chi_sq_independence.html#chi-squared-independence-test-equation",
    "href": "lessons_original/03_two_sample_chi_sq_independence.html#chi-squared-independence-test-equation",
    "title": "17  Chi-Squared Independence",
    "section": "17.4 Chi-Squared Independence Test Equation",
    "text": "17.4 Chi-Squared Independence Test Equation\n\\(\\chi^2 = \\sum \\frac{{(O_{ij} - E_{ij})^2}}{{E_{ij}}}\\)\nWhere:\n\n\\(\\chi^2\\) The Chi-Squared test statistic.\n\\(\\sum\\chi^2\\) Formula instructions to sum all the cell Chi-square values.\n\\(O_{ij}\\) Observed (the actual frequency in each cell (i, j) of the contingency table.\n\\(E_{ij}\\) Expected frequency in cell (i, j) calculated below.\n\\(\\chi^2{i-j}\\) i-j is the correct notation to represent all the cells, from the first cell (i) to the last cell(j).\n\nCalculating Expected Value\n\\(E = \\frac{M{r} * M{c}}n\\)\nWhere:\n\n\\(E\\) represents the cell expected value,\n\\(M{r}\\) represents the row marginal for that cell,\n\\(M{c}\\) represents the column marginal for that cell, and\n\\(n\\) represents the total sample size.\n\n\n17.4.1 Formula Description\n\nThe first step in calculating a χ2 is to calculate the sum of each row, and the sum of each column. These sums are called the “marginals” and there are row marginal values and column marginal values. \nThe second step is to calculate the expected values for each cell. In the Chi-square statistic, the “expected” values represent an estimate of how the cases would be distributed if there were no effect of exposure variables.\nThen third step is to compute the \\(\\chi^2\\) with above formula.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Chi-Squared Independence</span>"
    ]
  },
  {
    "objectID": "lessons_original/03_two_sample_chi_sq_independence.html#performing-chi-squared-independence-test-in-r",
    "href": "lessons_original/03_two_sample_chi_sq_independence.html#performing-chi-squared-independence-test-in-r",
    "title": "17  Chi-Squared Independence",
    "section": "17.5 Performing Chi-Squared Independence Test in R",
    "text": "17.5 Performing Chi-Squared Independence Test in R\nThe first step is to load the required packages that will allow us to conduct the test statistic.\n\n# install.package(\"openintro\")\n# install.package(\"gtsummary\")\n# install.packages(\"rstatix\")\n# install.packages(\"vcd\")\n# install.package(\"tidyverse\")\n\n\nlibrary(openintro)  # for data\n\nLoading required package: airports\n\n\nLoading required package: cherryblossom\n\n\nLoading required package: usdata\n\nlibrary(gtsummary)  # for tables\nlibrary(vcd)        # for mosaic plot\n\nLoading required package: grid\n\nlibrary(rstatix)    # for post hoc tests\n\n\nAttaching package: 'rstatix'\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\nlibrary(tidyverse)  # for data wrangling and visualization\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks rstatix::filter(), stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n17.5.1 Data Source\nThe openintro package contains data sets used in open-source textbooks such as Introduction to Modern Statistics (1st Ed). (mineçetinkaya-rundel2023?) It is often used for teaching purposes and create examples for how to run various test statistics and functions using R. This package can be installed using the install.packages(\"openintro\") feature. You can also find more information about this package here.\nFor the purposes of this presentations we will be using the diabetes2 dataset found within this package.\nIn the data there are 699 diabetes patients. Each of the 699 patients in the experiment were randomized to one of the following treatments: (1) continued treatment with metformin (coded as met), (2) formin combined with rosiglitazone (coded as rosi), or or (3) a lifestyle-intervention program (coded as lifestyle).Three treatments were compared to test their relative efficacy (effectiveness) in treating Type 2 Diabetes in patients aged 10-17 who were being treated with metformin. The primary outcome was lack of glycemic control (or not); lacking glycemic control means the patient still needed insulin, which is not the preferred outcome for a patient.(todaystudygroup2012?)\n\nstr(diabetes2)\n\ntibble [699 × 2] (S3: tbl_df/tbl/data.frame)\n $ treatment: Factor w/ 3 levels \"lifestyle\",\"met\",..: 2 3 3 1 2 1 1 3 3 2 ...\n $ outcome  : Factor w/ 2 levels \"failure\",\"success\": 2 1 2 2 2 2 2 2 2 1 ...\n\nprint(diabetes2)\n\n# A tibble: 699 × 2\n   treatment outcome\n * &lt;fct&gt;     &lt;fct&gt;  \n 1 met       success\n 2 rosi      failure\n 3 rosi      success\n 4 lifestyle success\n 5 met       success\n 6 lifestyle success\n 7 lifestyle success\n 8 rosi      success\n 9 rosi      success\n10 met       failure\n# ℹ 689 more rows\n\n\n\n\n17.5.2 Contingency Tables\nThe first step in a Chi-Squared Independence Test involves creating a contingency table that is used to calculate the expected frequencies for each variable. This will help us summarize the data and show the distribution of the variables. This is done using the table() function as seen in the code below.\n\n# Create the table\ndiabetes_table &lt;- table(\n  diabetes2$outcome,\n  diabetes2$treatment\n)\n\nprint(diabetes_table)\n\n         \n          lifestyle met rosi\n  failure       109 120   90\n  success       125 112  143\n\n\n\n\n17.5.3 Mosaic Plots\nYou can also use a mosaic plot to visualize the data better. Our data is in interger format so we first need to reformat it into factor form. This can we done with the code below.\n\n#reformat treatment\ndiabetes2$treatment &lt;- \n  as.factor(diabetes2$treatment)\n\n#print\nhead(diabetes2$treatment)\n\n[1] met       rosi      rosi      lifestyle met       lifestyle\nLevels: lifestyle met rosi\n\n#recode treatment\ndiabetes2$treatment &lt;- \n  recode_factor(\n    diabetes2$treatment,\n            \"lifestyle\" = \"Lifestyle\",\n            \"met\" = \"Metform\",\n            \"rosi\" = \"Rosiglitazone Plus Metformin\"\n)\n\n#print\nhead(diabetes2$treatment)\n\n[1] Metform                      Rosiglitazone Plus Metformin\n[3] Rosiglitazone Plus Metformin Lifestyle                   \n[5] Metform                      Lifestyle                   \nLevels: Lifestyle Metform Rosiglitazone Plus Metformin\n\n#reformat outcome\ndiabetes2$outcome &lt;- \n  as.factor(diabetes2$outcome)\n\n#print\nhead(diabetes2$outcome)\n\n[1] success failure success success success success\nLevels: failure success\n\n#recode\ndiabetes2$outcome &lt;- \n  recode_factor(\n    diabetes2$outcome,\n            \"failure\" = \"Failure\",\n            \"success\" = \"Success\"\n)\n\n#print\nhead(diabetes2$outcome)\n\n[1] Success Failure Success Success Success Success\nLevels: Failure Success\n\n\nNext, we can create the mosaic plot using the mosaic() function.\n\n# Creating the mosaic plot\nmosaic(\n  ~ treatment + outcome, \n       data = diabetes2,\n          highlighting = \"outcome\", \n          highlighting_fill = c(\"lightgrey\", \"black\"),\n          main = \"Outcome of Interventions for Type 2 Diabetes\",\n          gp_varnames = gpar(fontsize = 14, fontface = 2),\n          gp_labels = gpar(fontsize = 10)\n)\n\n\n\n\n\n\n\n\n\n\n17.5.4 Running the Chi-Squared Test\nThe next step is to run our actual test statistic. This is done using the chisq.test() function as seen in the code below.The correct argument is used to indicate whether to apply continuity correction when computing the test. We are setting this to TRUE since we are dealing with a 2x2 contingency table where we have two categorical variables, each with two levels.\n\nchi_sq_result &lt;- chisq.test(diabetes_table, \n                           correct = TRUE)\n\nprint(chi_sq_result)\n\n\n    Pearson's Chi-squared test\n\ndata:  diabetes_table\nX-squared = 8.1645, df = 2, p-value = 0.01687\n\n\n\n17.5.4.1 Tabulating the chisq output in a publishable format using gt_summary\n\ntable1 &lt;-   \n  tbl_summary(\n    diabetes2,\n    by = treatment\n) %&gt;% \n  add_p() %&gt;%\n  modify_caption(\"Results of Chi Square Test\") %&gt;%\n  bold_labels()\n\n\ntable1\n\n\n\n\n\n\nResults of Chi Square Test\n\n\n\n\n\n\n\n\n\nCharacteristic\nLifestyle, N = 2341\nMetform, N = 2321\nRosiglitazone Plus Metformin, N = 2331\np-value2\n\n\n\n\noutcome\n\n\n\n\n\n\n0.017\n\n\n    Failure\n109 (47%)\n120 (52%)\n90 (39%)\n\n\n\n\n    Success\n125 (53%)\n112 (48%)\n143 (61%)\n\n\n\n\n\n1 n (%)\n\n\n2 Pearson’s Chi-squared test\n\n\n\n\n\n\n\n\n\n\n\n\n17.5.5 Test Options\nThe Chi-Squared Independence test statistic has various options in R. A brief description of those options is summarized in the table below.\n\n\n\n\n\n\n\n\nOption\nDescription\nExample\n\n\n\n\nx\nA numeric vector or matrix. x and y can also both be factors.\nx &lt;- matrix(c(10, 20, 30, 40), nrow = 2)\n\n\ncorrect\nA logical indicating whether to apply continuity correction – This is done when the expected frequencies in the contingency table are small (&lt;5).\ncorrect &lt;- TRUE\n\n\np\nA vector of probabilities of the same length as x. An error is given if any entry of p is negative.\np &lt;- c(0.4, 0.6)\n\n\nrescale.p\nA logical scalar; if TRUE then p is rescaled (if necessary) to sum to 1. If rescale.p is FALSE, and p does not sum to 1, an error is given.\nrescale.p &lt;- FALSE\n\n\nsimulate.p.value\nA logical indicating whether to compute p-values by Monte Carlo simulation.\nsimulate.p.value &lt;- FALSE\n\n\nB\nAn integer specifying the number of replicates used in the Monte Carlo test.*\nB &lt;- 1000\n\n\n\n*The Monte Carlo test is a technique that involves simulation to estimate the p-value or test statistic for hypothesis testing. This is used when using complex models and exact p-values cannot be calculated or when the distribution assumptions are violated. (christianp.robert2010?)",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Chi-Squared Independence</span>"
    ]
  },
  {
    "objectID": "lessons_original/03_two_sample_chi_sq_independence.html#interpretation",
    "href": "lessons_original/03_two_sample_chi_sq_independence.html#interpretation",
    "title": "17  Chi-Squared Independence",
    "section": "17.6 Interpretation",
    "text": "17.6 Interpretation\nOur results give us a chi-squared test statistic of 8.1645 with a p-value of 0.017. Since p value is smaller than critical p value (0.05), we have enough evidence to reject the null hypothesis and conclude that there is a strong association between the type of treatment on Type 2 diabetes. However, we dont’t know which treatment option is significantly different so we are going to do a post hoc test in below code chunk.\n\n17.6.1 Post hoc test\n\npost_hoc_test &lt;- pairwise_prop_test(diabetes_table)\n\npost_hoc_test\n\n# A tibble: 3 × 5\n  group1    group2       p  p.adj p.adj.signif\n* &lt;chr&gt;     &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;       \n1 lifestyle met    0.309   0.309  ns          \n2 lifestyle rosi   0.1     0.2    ns          \n3 met       rosi   0.00606 0.0182 *",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Chi-Squared Independence</span>"
    ]
  },
  {
    "objectID": "lessons_original/03_two_sample_chi_sq_independence.html#conclusion",
    "href": "lessons_original/03_two_sample_chi_sq_independence.html#conclusion",
    "title": "17  Chi-Squared Independence",
    "section": "17.7 Conclusion",
    "text": "17.7 Conclusion\nFrom our post hoc test result we see that there is a statistically significant difference between Met and Rosi. Given our data, we can conclude that Rosi is the better treatment option for the population tested.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Chi-Squared Independence</span>"
    ]
  },
  {
    "objectID": "04_header_anova-and-regression.html",
    "href": "04_header_anova-and-regression.html",
    "title": "ANOVA and Regression",
    "section": "",
    "text": "Text Outline\nThis text covers the basics of:",
    "crumbs": [
      "ANOVA and Regression"
    ]
  },
  {
    "objectID": "04_header_anova-and-regression.html#text-outline",
    "href": "04_header_anova-and-regression.html#text-outline",
    "title": "ANOVA and Regression",
    "section": "",
    "text": "Exploring data\nOne-sample tests\nTwo-sample tests\nANOVA and linear regression\nGeneralized linear models\nSome special topics\nMaybe a bit about power calculations…",
    "crumbs": [
      "ANOVA and Regression"
    ]
  },
  {
    "objectID": "04_header_anova-and-regression.html#part-outline",
    "href": "04_header_anova-and-regression.html#part-outline",
    "title": "ANOVA and Regression",
    "section": "Part Outline",
    "text": "Part Outline\nThis part of the text contains the chapters on various ways to perform ANOVA and linear regression:\n\nOne-Way ANOVA\nTwo-way ANOVA\nWelch’s ANOVA\nKruskal-Wallace Test\nTukey HSD Post-Hoc Test\nRepeated Measures ANOVA\nRandom Intercept Models\nCorrelation Matrices and Covariances\nMultiple Regression (linear)\nPolynomial regression",
    "crumbs": [
      "ANOVA and Regression"
    ]
  },
  {
    "objectID": "lessons/04_anova_one_way.html",
    "href": "lessons/04_anova_one_way.html",
    "title": "\n18  One-Way ANOVA\n",
    "section": "",
    "text": "18.1 Introduction to one-way ANOVA\nOne-way analysis of variance (ANOVA) is an extension of a two-samples t-test for comparing means between three or more independent groups. A single factor variable is employed to categorize the data into several groups.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>One-Way ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_one_way.html#introduction-to-one-way-anova",
    "href": "lessons/04_anova_one_way.html#introduction-to-one-way-anova",
    "title": "\n18  One-Way ANOVA\n",
    "section": "",
    "text": "Hypothesis Testing\n\n\n\nNull Hypothesis \\((H_0)\\) the means between groups are not statistically different (they ARE the same).  Alternative Hypothesis \\((H_a)\\) the means between groups are statistically different (they ARE NOT the same).",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>One-Way ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_one_way.html#mathematical-definition-of-one-way-anova",
    "href": "lessons/04_anova_one_way.html#mathematical-definition-of-one-way-anova",
    "title": "\n18  One-Way ANOVA\n",
    "section": "\n18.2 Mathematical definition of one-way ANOVA",
    "text": "18.2 Mathematical definition of one-way ANOVA\n\\[\nY_{ij} = \\mu + \\tau_{i} + \\epsilon_{ij}\n\\]\nWhere,\n\n\n\\(Y_{ij}\\) represents the j-th observation (j = 1, 2, …, \\(n_{i}\\)) on the i-th treatment (i = 1, 2, …, k levels).\n\nSo, \\(Y_{23}\\) represents the third observation for the second factor level.\n\n\n\n\\(\\mu\\) is the common effect.\n\n\\(\\tau_{i}\\) represents the i-th treatment effect.\n\n\\(\\epsilon_{ij}\\) represents the random error present in the j-th observation on the i-th treatment.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>One-Way ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_one_way.html#data-description",
    "href": "lessons/04_anova_one_way.html#data-description",
    "title": "\n18  One-Way ANOVA\n",
    "section": "\n18.3 Data Description",
    "text": "18.3 Data Description\nThe PlantGrowth data set is already included in {base R} within the {datasets} package. The PlantGrowth data set includes results from an experiment to compare yield obtained under a control condition and two different treatment conditions. The measurements were obtained through the dried weight of each plant.\n\n# Call the data set into the Global environment\ndata(\"PlantGrowth\")\n\n# Use the `glimpse()` or `head()` function to skim observations\n# glimpse(PlantGrowth)\nhead(PlantGrowth)\n\n  weight group\n1   4.17  ctrl\n2   5.58  ctrl\n3   5.18  ctrl\n4   6.11  ctrl\n5   4.50  ctrl\n6   4.61  ctrl\n\n# Optional: You can use the `summary()` function to confirm that all variables\n# are being read correctly. E.g. the `group` variable is displayed as counts\n# for each level\nsummary(PlantGrowth)\n\n     weight       group   \n Min.   :3.590   ctrl:10  \n 1st Qu.:4.550   trt1:10  \n Median :5.155   trt2:10  \n Mean   :5.073            \n 3rd Qu.:5.530            \n Max.   :6.310            \n\n# Optional: You can use the `str()` function to view the factor levels and\n# confirm the appropriate reference level for analysis\nstr(PlantGrowth)\n\n'data.frame':   30 obs. of  2 variables:\n $ weight: num  4.17 5.58 5.18 6.11 4.5 4.61 5.17 4.53 5.33 5.14 ...\n $ group : Factor w/ 3 levels \"ctrl\",\"trt1\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n# R assigns reference level in alphabetical order. E.g. From our factor levels \n# (ctrl, trt1, trt2) - ctrl will be assigned as the reference level \n# automatically by R.\n# Here, we see that `group` is a factor with 3 levels,\n# with 1 corresponding to 1 (the first level).\n\n# If you need to re-assign the reference level, you can `relevel()` your factor\n# The first variable after `Levels:` is your new reference group.\nrelevel(PlantGrowth$group, ref = \"ctrl\")\n\n [1] ctrl ctrl ctrl ctrl ctrl ctrl ctrl ctrl ctrl ctrl trt1 trt1 trt1 trt1 trt1\n[16] trt1 trt1 trt1 trt1 trt1 trt2 trt2 trt2 trt2 trt2 trt2 trt2 trt2 trt2 trt2\nLevels: ctrl trt1 trt2",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>One-Way ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_one_way.html#assumptions-of-one-way-anova",
    "href": "lessons/04_anova_one_way.html#assumptions-of-one-way-anova",
    "title": "\n18  One-Way ANOVA\n",
    "section": "\n18.4 Assumptions of One-Way ANOVA",
    "text": "18.4 Assumptions of One-Way ANOVA\nThe assumptions of one-way ANOVA are as follows:\n\n\nAssumption 1: All observations are independent and randomly selected from the population as defined by the factor variable.\n\nAssumption 2: The data within each factor level are approximately normally distributed.\n\nAssumption 3: The variance of the data of interest is similar across each factor level (aka: Homogeneity of variance).",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>One-Way ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_one_way.html#checking-the-assumptions-of-the-one-way-anova",
    "href": "lessons/04_anova_one_way.html#checking-the-assumptions-of-the-one-way-anova",
    "title": "\n18  One-Way ANOVA\n",
    "section": "\n18.5 Checking the Assumptions of the One-Way ANOVA",
    "text": "18.5 Checking the Assumptions of the One-Way ANOVA\nAssumption 2: The data within each factor level are approximately normally distributed.\nTo check this assumption, we can examine if the data are approximately normally distributed across groups with 2 plots, the Q-Q plot or histograms, or with the Shapiro-Wilk test.\nFirst, we will look at the Q-Q plot:\n\n# Check for missing values to ensure balanced data\ncolSums(is.na(PlantGrowth))\n\nweight  group \n     0      0 \n\n# No missing datapoints\n\n# Normality assumption: Q-Q plot\nqq_plot_plantgrowth &lt;- ggplot(PlantGrowth) +\n  aes(sample = weight) +\n  facet_wrap(~ group) +\n  stat_qq() +\n  stat_qq_line()\n\nqq_plot_plantgrowth\n\n\n\n\n\n\n\nThe weight variable seems to be approximately normally distributed across each of the three groups based on the Q-Q plots.\nNext, we will look at histograms:\n\n# Normality assumption: Histograms\nhistogram_plantgrowth &lt;- ggplot(PlantGrowth) +\n  aes(x = weight) +\n  geom_histogram(binwidth = 0.5) +\n  facet_wrap(~ group)\n\nhistogram_plantgrowth\n\n\n\n\n\n\n\nAgain, the weight variable appears to be approximately normally distributed across the groups.\nLastly, we can use the Shapiro-Wilk test to test if the data is approximately normally distributed. P-values greater than 0.05 indicate that the data is likely approximately normally distributed.\n\n# Normality: Shapiro Wilks\nshapiro_plantgrowth &lt;- PlantGrowth %&gt;%\n  group_by(group) %&gt;%\n  summarise(\n    statistic = shapiro.test(weight)$statistic,\n    p.value = shapiro.test(weight)$p.value\n  )\n\nshapiro_plantgrowth\n\n# A tibble: 3 × 3\n  group statistic p.value\n  &lt;fct&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 ctrl      0.957   0.747\n2 trt1      0.930   0.452\n3 trt2      0.941   0.564\n\n\nThe p-value for each of our groups is above 0.05, so we can assume that weight is approximately normally distributed within each group.\nAssumption 3: Homogeneity of variance.\nWe can check this assumption with Levene’s test. A p-value greater than 0.05 indicates that the variance is similar across groups, and we therefore meet the requirements of this assumption.\n\n# Homogeneity of Variance: Levene's test\nlevenes_plantgrowth &lt;- levene_test(weight ~ group, data = PlantGrowth)\nlevenes_plantgrowth\n\n# A tibble: 1 × 4\n    df1   df2 statistic     p\n  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1     2    27      1.12 0.341\n\n\nThe p-value is 0.341, so we can therefore assume that our data meets the assumption of homogeneity of variance.\nWe can also check assumption 3 by visually examining a box plot of the outcome variable across groups:\n\n# Variance assumption: Box plot for weight by group\nbox_plantgrowth &lt;- ggplot(PlantGrowth) +\n  aes(y = weight) +\n  geom_boxplot() +\n  facet_wrap(~ group)\n\nbox_plantgrowth\n\n\n\n\n\n\n\nTo compare the box plot visually, we take a look at the interquartile range between each three plots. All three have approximately similar size and length, which satisfies our assumption homogeneity of variance.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>One-Way ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_one_way.html#code-to-run-one-way-anova",
    "href": "lessons/04_anova_one_way.html#code-to-run-one-way-anova",
    "title": "\n18  One-Way ANOVA\n",
    "section": "\n18.6 Code to Run One-Way ANOVA",
    "text": "18.6 Code to Run One-Way ANOVA\n\n# Include an outcome variable, `weight`, and predictor(s), `group`. as well as \n# which data set to pull variables from with `data = ` argument\nanova_fit &lt;- aov(weight ~ group, data = PlantGrowth)\n\n# Call summary statistics from `anova_fit` object\nsummary(anova_fit)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)  \ngroup        2  3.766  1.8832   4.846 0.0159 *\nResiduals   27 10.492  0.3886                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe summary includes the independent variable group being tested within the model. All variation that is not explained by group is considered residual variance.\n\nThe Df column tabulates the degrees of freedom for the independent variable, which is the number of levels in the variable minus 1. For example, group has 3 levels (3 - 1 = 2 degrees of freedom).\nThe Sum Sq column tabulates the sum of squares (total variation between group means and overall mean).\nThe Mean Sq column tabulates the mean of the sum of squares, which is calculated by dividing the sum of squares by the degrees of freedom for each parameter.\nThe F value column tabulates the test statistic from the F test, which is the mean square of each independent variable (only one in this case) by the mean square of the residuals.\nThe Pr(&gt;F) column is the resulting p-value of the F statistic. Remember: the p-value shows how likely it is that the calculated F value would have occurred if the null hypothesis were true.\n\nThe p value of the fertilizer variable is low (p &lt; 0.01), so it appears that the type of fertilizer used (group) has a real impact on the final crop yield (weight).\n\n18.6.1 Post-Hoc Test for Pairwise Comparisons\nTo compare the group means directly, we can conduct a post-hoc test to see which group means are different from one another. One post-hoc test is Tukey’s Test.\n\n# perform Tukey's Test for multiple comparisons\nanova_post_hoc &lt;- TukeyHSD(anova_fit, conf.level=.95)\n\nanova_post_hoc\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = weight ~ group, data = PlantGrowth)\n\n$group\n            diff        lwr       upr     p adj\ntrt1-ctrl -0.371 -1.0622161 0.3202161 0.3908711\ntrt2-ctrl  0.494 -0.1972161 1.1852161 0.1979960\ntrt2-trt1  0.865  0.1737839 1.5562161 0.0120064\n\n\nIn the above output, we can see that:\n\n\ntrt1-ctrl: P-value is approximately 0.391 when comparing treatment 1 (trt1) and control (ctrl), which is greater than 0.05. This indicates there is not a statistically significant difference in mean between these groups. Their means are the SAME.\n\ntrt2-ctrl: P-value is approximately 0.198 when comparing treatment 2 (trt2) and control (ctrl), which is greater than 0.05. This indicates there is not a statistically significant difference in mean between these groups. Their means are the SAME.\n\ntrt2-trt1: P-value is 0.012 when comparing treatment 1 (trt1) and treatment 2 (trt2), which is less than 0.05. This indicates there is a statistically significant difference in mean between these groups. Their means are NOT the SAME.\n\n18.6.2 Regression for Comparison\nTo compare the ANOVA results with those of a simple linear regression, we can run a regression model to examine if there are differences between the control group and the two treatment groups.\n\n# Simple linear regression\nlm_plantgrowth &lt;- lm(weight ~ group, data = PlantGrowth)\n\nsummary(lm_plantgrowth)\n\n\nCall:\nlm(formula = weight ~ group, data = PlantGrowth)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.0710 -0.4180 -0.0060  0.2627  1.3690 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   5.0320     0.1971  25.527   &lt;2e-16 ***\ngrouptrt1    -0.3710     0.2788  -1.331   0.1944    \ngrouptrt2     0.4940     0.2788   1.772   0.0877 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6234 on 27 degrees of freedom\nMultiple R-squared:  0.2641,    Adjusted R-squared:  0.2096 \nF-statistic: 4.846 on 2 and 27 DF,  p-value: 0.01591\n\n\nFrom the above output, we can see that:\n\nThe F-statistic is 4.846, which is the same F-statistic produced by the ANOVA.\nThe p-value of the model (0.01591) is the same as the p-value produced by the ANOVA.\nThe degrees of freedom for the ANOVA and linear model are also the same (2 and 27).\nThe Multiple R-squared value from the regression (0.2641) is related to the Sum of Squares for our factor and the Sum of Squares for the residual in the following way: \\(R^{2} = \\frac{SS_{factor}}{(SS_{factor} + SS_{residuals})}\\).\nThe linear model confirms that there is no difference in the treatment groups compared to the control group since the p-values for the treatment groups are larger than 0.05 (0.194 and 0.088 for treatment 1 and treatment 2 versus control, respectively).",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>One-Way ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_one_way.html#brief-interpretation-of-the-output",
    "href": "lessons/04_anova_one_way.html#brief-interpretation-of-the-output",
    "title": "\n18  One-Way ANOVA\n",
    "section": "\n18.7 Brief Interpretation of the Output",
    "text": "18.7 Brief Interpretation of the Output\nThe resulting p-value of the one-way ANOVA is 0.0159, which is less than our significance level of 0.05. Therefore we must reject our null hypothesis as the data supports that the mean weight between groups is statistically different. In other words, the mean weight(s) are NOT the same between group (ctrl, trt1, trt2).\nThe Tukey test results illustrate a true difference that only exists between treatment groups 1 and 2 (trt1 and trt2), and not between the control group and either of the treatment groups. This is confirmed by the output from the simple linear regression model.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>One-Way ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_two_way.html",
    "href": "lessons/04_anova_two_way.html",
    "title": "\n19  Two-Way ANOVA\n",
    "section": "",
    "text": "19.1 Introduction\nThe following two-way ANOVA lesson is based on “Two-Way ANOVA | Examples & When to Use It”, a tutorial by Rebecca Bevans.\nA two-way ANOVA examines the influence of two or more independent categorical variables, also known as factors, on a continuous dependent variable. Each factor must have at least two levels, which are the groups within each factor being analyzed. In this lesson, we will see an example of how to test for these assumptions and how to conduct a type I two-way ANOVA once we know they have been met.\nAn interaction occurs when the effect of one factor depends on the level of another factor. The two-way ANOVA method allows us to measure:",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Two-Way ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_two_way.html#introduction",
    "href": "lessons/04_anova_two_way.html#introduction",
    "title": "\n19  Two-Way ANOVA\n",
    "section": "",
    "text": "Main Effects - the individual effect of one factor on the dependent variable, and\nInteraction Effects - the extent to which the effects of one factor change across different levels of another factor.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Two-Way ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_two_way.html#how-does-a-two-way-anova-work",
    "href": "lessons/04_anova_two_way.html#how-does-a-two-way-anova-work",
    "title": "\n19  Two-Way ANOVA\n",
    "section": "\n19.2 How Does a Two-Way ANOVA Work?",
    "text": "19.2 How Does a Two-Way ANOVA Work?\nThe F test, a group-wise comparison test, is used in an ANOVA to determine statistical significance. The outcome of an F test, the F statistic, measures how different a groups’ variances are to the overall variance of the dependent variable. When the variance is higher between groups than within the group, the F statistic will be greater. A large F statistic suggests there’s a low likelihood that the observed difference is due to chance, and thus, the factor likely has an effect on the outcome.\nFor comparison, a critical value is calculated based on the desired alpha (α) and degrees of freedom of all groups. If the F statistic is greater than the F critical value, the results are considered statistically significant. For example, if the F statistic is greater than the F critical value at α = 0.05, then the p-value will be less than 0.05. For a model with two factors, we can generate three F statistics and three associated p values to test three possible hypotheses:\n\n\n\n\n\n\nNull Hypotheses\n\n\n\n\nH01: The population means of the first factor are equal.\n\nH02: The population means of the second factor are equal.\n\nH03: There is no interaction effect, that is, the effect of one factor does not depend on the value of the other factor.\n\n\n\n\n\n\n\n\n\n\nAlternative Hypotheses\n\n\n\n\nHA1: The population means of the first factor are not equal.\n\nHA2: The population means of the second factor are not equal.\n\nHA3: There is an interaction effect, that is, the effect of one factor depends on the value of the other factor.\n\n\n\n\n\n19.2.1 Assumptions\nThree assumptions must be met before conducting a two-way ANOVA:\n\nHomogeneity of variance: The variances for each group should be roughly equal. If the groups do not have equal variances, a non-parametric test, such as the Kruskal-Wallis test, may be used.\nIndependence of observations: The observations in each group are independent of each other and the observations within groups were obtained by a random sample. This can be assumed to be true so long as the experiment is properly designed. If observations are grouped in categories, this effect should be accounted for with the use of a blocking variable or a repeated-measures ANOVA.\nNormally distributed dependent variable: The values of the dependent variable should follow a normal distribution.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Two-Way ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_two_way.html#two-way-anova-table",
    "href": "lessons/04_anova_two_way.html#two-way-anova-table",
    "title": "\n19  Two-Way ANOVA\n",
    "section": "\n19.3 Two-Way ANOVA Table",
    "text": "19.3 Two-Way ANOVA Table\nThe following table includes the calculations used in a two-way ANOVA. For more detailed information on these calculations, please read Models and Calculations for the Two-Way ANOVA, a lesson by the National Institute of Standards and Technology.\nIn the ANOVA table, the main effect A has k levels and the main effect B has l levels. N represents the total sample size.\n\n\n\n\n\n\n\n\n\nSource of Variation\nSum of Squares\nDegrees of freedom\nMean Squares\nF value\n\n\n\nFactor A\n\\(SS_A\\)\n\\(k-1\\)\n\\(MS_A\\)\n\\(F_A\\)\n\n\nFactor B\n\\(SS_B\\)\n\\(l-1\\)\n\\(MS_B\\)\n\\(F_B\\)\n\n\nInteraction AB\n\\(SS_{AB}\\)\n\\((k-1)(l-1)\\)\n\\(MS_{AB}\\)\n\\(F_{AB}\\)\n\n\nError\n\\(SS_E\\)\n\\(N - kl\\)\n\\(MS_E\\)\n\n\n\nTotal\n\\(SS_T\\)\n\\(N-1\\)\n\n\n\n\n\nWhere\n\n\\[ MS_E := \\frac{SS_E}{N-kl} \\]\n\\[ MS_A := \\frac{SS_A}{k-1} \\text{ and } F_A := \\frac{MS_A}{MS_E} \\]\n\\[ MS_B := \\frac{SS_B}{l-1} \\text{ and } F_B := \\frac{MS_B}{MS_E} \\]\n\\[ MS_{AB} := \\frac{SS_{AB}}{(k-1)(l-1)} \\text{ and } F_{AB} := \\frac{MS_{AB}}{MS_E} \\]\n\nWe explain these components as:\n\n\n\\(SS_A\\): Factor \\(A\\) main effect sums of squares, df = \\(k-1\\)\n\n\n\\(SS_B\\): Factor \\(B\\) main effect sums of squares, df = \\(l-1\\)\n\n\n\\(SS_{AB}\\): interaction sum of squares, df = \\((k-1)(l-1)\\)\n\n\n\\(SS_E\\): error sum of squares, df = \\(N-kl\\)\n\n\n\\(SS_T\\): Total sums of squares, df = \\(N-1\\)",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Two-Way ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_two_way.html#crop-yield-two-way-anova-example",
    "href": "lessons/04_anova_two_way.html#crop-yield-two-way-anova-example",
    "title": "\n19  Two-Way ANOVA\n",
    "section": "\n19.4 Crop Yield Two-Way ANOVA Example",
    "text": "19.4 Crop Yield Two-Way ANOVA Example\nWe will use the agricultural crop yield dataset from the Two-Way ANOVA lesson lesson in our example, which can be directly downloaded here.\nIn this example, corn was planted in one of four blocks within a field, in either high or low density, and fertilized with one of three types of fertilizer. Their yield in bushels per acre was then measured. The three factors of this experiment with their associated levels are:\n\nType of fertilizer (type 1, 2, or 3)\nPlanting density (1 = low, 2 = high)\nBlock number in the field (block 1, 2, 3, or 4)\n\n\n19.4.1 Hypotheses\nSuppose we want to use a two-way ANOVA to examine whether the type of fertilizer and planting density (factors) have an effect on the average crop yield (dependent variable). To answer this question, the following three hypotheses will be tested:\n\n\n\n\n\n\nNull Hypotheses\n\n\n\n\nH01: Fertilizer type has no effect on average crop yield\n\nH02: Planting density has no effect on average crop yield\n\nH03: The effects of fertilizer type and planting density on average yield are independent of each other (no interaction exists)\n\n\n\n\n\n\n\n\n\n\nAlternative Hypotheses\n\n\n\n\nH11: Fertilizer type has an effect on average crop yield\n\nH12: Planting density has an effect on average crop yield\n\nH13: The effects of fertilizer type and planting density on average yield are not independent of each other (no interaction occurs)\n\n\n\n\n\n19.4.2 Loading Libraries and Data\nFirst, we will load our required packages and read in the crop yield dataset. The tidyverse package will be used to transform the data. The tidymodels and gt packages are necessary for creating presentation-ready tables of results. Finally, the AICcmodavg package will be used to construct an AIC table with which to compare our models (more on this later).\n\n# Uncomment the following code to install required packages\n\n# gt and tidymodels used for presentation-ready table:\n# install.packages(\"gt\")\n# install.packages(\"tidymodels\")\n\n# AICcmodavg used for AIC table:\n# install.packages(\"AICcmodavg\")\n\n# tidyverse for data transformation:\n# install.packages(\"tidyverse\")\n\n# Load the required packages\nlibrary(gt)\nlibrary(tidymodels)\nlibrary(AICcmodavg)\nlibrary(tidyverse)\n\n# read in the crop data dataset downloaded from\n# https://www.scribbr.com/wp-content/uploads//2020/03/crop.data_.anova_.zip\ncrop_data_df &lt;- read_csv(\"../data/04_crop_data.csv\")\n\n\n19.4.3 Data Exploration\nThe first few datapoints of the crop yield dataset were inspected and summary statistics were performed on the entire dataset to examine the structure, center, and spread of the data.\n\n# show first six rows of the dataset\nhead(crop_data_df)\n\n# overview of summary statistics\nsummary(crop_data_df)\n\n# A tibble: 6 × 4\n  density block fertilizer yield\n    &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;\n1       1     1          1  177.\n2       2     2          1  178.\n3       1     3          1  176.\n4       2     4          1  178.\n5       1     1          1  177.\n6       2     2          1  177.\n    density        block        fertilizer     yield      \n Min.   :1.0   Min.   :1.00   Min.   :1    Min.   :175.4  \n 1st Qu.:1.0   1st Qu.:1.75   1st Qu.:1    1st Qu.:176.5  \n Median :1.5   Median :2.50   Median :2    Median :177.1  \n Mean   :1.5   Mean   :2.50   Mean   :2    Mean   :177.0  \n 3rd Qu.:2.0   3rd Qu.:3.25   3rd Qu.:3    3rd Qu.:177.4  \n Max.   :2.0   Max.   :4.00   Max.   :3    Max.   :179.1  \n\n\n\n19.4.4 Transform Treatment Factors Into R Factors\nNote that the fertilizer, density, and block vectors are read in as numeric class objects, so we must convert them to the factor class. We will also specify the reference groups for each factor: fertilizer 1, low density, and block 1. If not specified, reference groups are automatically assigned in alphabetical order.\n\n# transform numeric vectors to factors and assign reference groups\ncrop_data_df$fertilizer &lt;- as.factor(crop_data_df$fertilizer) %&gt;% \n  relevel(ref = 1)\ncrop_data_df$density &lt;- as.factor(crop_data_df$density) %&gt;% \n  relevel(ref = 1)\ncrop_data_df$block &lt;- as.factor(crop_data_df$block) %&gt;% \n  relevel(ref = 1)\n\n\n19.4.4.1 Boxplots of Factors\nBox plots for each factor were created to further examine the spread and center of the data using ggplot2.\n\n# create box plot of yield by fertilizer\nplot_fertilizer &lt;- \n  ggplot(data = crop_data_df) +\n    aes(\n      x = as.factor(fertilizer),\n      y = yield,\n      color = fertilizer \n    ) +\n  labs(\n    title = \"Crop Yield by Fertilizer Type\",\n    x = \"Fertilizer Type\",\n    y = \"Crop Yield\"\n  ) +\n  geom_boxplot()\n\nplot_fertilizer\n\n\n\n\n\n\n# yield by density\nplot_density &lt;- \n  ggplot(data = crop_data_df) +\n    aes(\n      x = as.factor(density),\n      y = yield,\n      color = density\n    ) +\n  labs(\n    title = \"Crop Yield by Planting Density\",\n    x = \"Planting Density\",\n    y = \"Crop Yield\"\n  ) +\n  geom_boxplot()\n\nplot_density\n\n\n\n\n\n\n# yield by block\nplot_block &lt;- \n  ggplot(data = crop_data_df) +\n    aes(\n      x = as.factor(block),\n      y = yield,\n      color = block \n    ) +\n  labs(\n    title = \"Crop Yield by Block\",\n    x = \"Block\",\n    y = \"Crop Yield\"\n  ) +\n  geom_boxplot()\n\nplot_block\n\n\n\n\n\n\n\nThe data for each level appear to be approximately normally distributed with roughly equal variance and few outliers.\n\n19.4.4.2 Combined Boxplot of Crop Yield by Fertilizer and Density\nTo help visualize the data, a combined box plot visualizing the effects of fertilizer and density on average crop yield was created. The y-axis indicates the average crop yield. The x-axis is divided by fertilizer type (labeled at the top of the graph) and indicates the planting density for each level of the fertilizer factor.\n\nanova_plot &lt;- \n  ggplot(crop_data_df) +\n  aes(\n    x = density, \n    y = yield, \n    group = fertilizer,\n    color = fertilizer\n  ) +\n  geom_point(\n    cex = 1.5, \n    pch = 1.0, \n# set point positions to randomly jitter so points don't overlap\n    position = position_jitter(w = 0.1, h = 0.1)\n  )\n\nanova_plot &lt;- anova_plot + \n# compute summary stats to create error bars\n  stat_summary(\n# set calculation for error bar parameters to mean_se\n    fun.data = 'mean_se',\n    geom = 'errorbar',\n    width = 0.2,\n    color = \"grey50\"\n  ) +\n# create points for level means\n  stat_summary(\n# set calculation for mean parameter to mean_se\n    fun.data = 'mean_se',\n    geom = 'pointrange'\n  )\n\nanova_plot &lt;- anova_plot +\n# create three boxplots side by side for each fertilizer level\n  facet_wrap(~ fertilizer)\n\nanova_plot &lt;- anova_plot +\n  theme_classic() +\n  labs(\n    title = \"Crop Yield Averages by Fertilizer Types and Planting Density\",\n    x = \"Planting Density (1 = low density, 2 = high density)\",\n    y = \"Average Yield\"\n  )\n\nanova_plot\n\n\n\n\n\n\n\nThe differing means for each level of fertilizer and density suggest that the factors have an effect on average yield. Before we can test whether these potential effects are statistically significant, we must first check that the data meet the ANOVA assumptions. To do this, we will generate a two-way ANOVA and run diagnostics on the model.\n\n19.4.5 Performing the Two-Way ANOVA with Interaction and Blocking Variables\nRecall that the two-way ANOVA can actually account for more than two factors. The crops were planted across various blocks whose conditions may differ in terms sunlight, moisture, etc. This could possibly lead to confounding, so it is important to control for the possible effect of these differences by adding this third factor to our model.\nA two-way ANOVA was created to model the effects of fertilizer, density, the interaction between fertilizer and density, and the blocking factor. In the following code, the argument fertilizer * density is equivalent to fertilizer + density + fertilizer : density, where fertilizer : density represents the interaction term.\n\n\n\n\n\n\nAre your data balanced?\n\n\n\nBecause the crop data are balanced (the sample sizes across the levels within each factor are equal), we will use the base-R function aov(), which uses Type I sums of squares. If your data are unbalanced, use a different function to conduct a Type II ANOVA (for data with no significant interaction) or Type III ANOVA (for data with significant interaction).\n\n\n\n# performing two-way anova with fertilizer, density, fertilizer:density \n#   interaction and blocking factor\nfull_model &lt;- \n# 'fertilizer * density' = fertilizer + density + fertilizer:density interaction\n  aov(yield ~ fertilizer * density + block, data = crop_data_df)\n\n\n19.4.6 Checking ANOVA Assumptions\nWe will now create diagnostic plots, which include a residuals vs fitted plot, scale-location plot, Q-Q plot, and constant leverage plot, to evaluate the homoscedasticity of our data. Please read Understanding Diagnostic Plots for Linear Regression Analysis for more information on these plots.\n\n# set plot parameter to display 2 x 2 plots in output\npar(mfrow=c(2,2))\n# plot default diagnostic plots of full_model\nplot(full_model)\n\n\n\n\n\n\n# set plot parameter back to 1 x 1 format\npar(mfrow=c(1,1))\n\nThe residual vs fitted plot shows the data are randomly spread about the “0” line with no large outliers, so we can assume the factors have equal variances. Similarly, the scale-location plot shows an approximately horizontal line with randomly spread points, indicating equal variances. Thus, the homoscedasticity assumption is met.\nThe points of the Q-Q residuals plot roughly follow the reference line, indicating the data are normally distributed. We can conclude the normality assumption is met.\nThe constant leverage plot displays a horizontal “0” value line with points randomly spread around it, indicating the spread of the points are the same at different levels. No points lie outside of the critical value lines (not visible in the graph due to scale), indicating no outliers exist that could skew the data.\n\n19.4.7 Two-Way ANOVA - Full Model Interpretation\nNow that we have verified that the data satisfy the ANOVA assumptions, we may conduct the two-way ANOVA to test the hypotheses. Let’s interpret the results of the ANOVA of the full model:\n\ntable_full &lt;- full_model %&gt;% \n# turn model into data.frame of parameters\n  tidy() %&gt;% \n# create gt table\n  gt()\n\n# customizing gt table header\ntable_full |&gt;\n   tab_header(\n      title = \"Two-Way ANOVA of Crop Yield - Full Model\",\n      subtitle = \"for main full model (y ~ fertilizer * density + block)\"\n    )\n\n\n\n\n\n\n\nTwo-Way ANOVA of Crop Yield - Full Model\n\n\nfor main full model (y ~ fertilizer * density + block)\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nfertilizer\n2\n6.0680466\n3.0340233\n8.9443603\n0.0002909717\n\n\ndensity\n1\n5.1216812\n5.1216812\n15.0988167\n0.0001972614\n\n\nblock\n2\n0.4861389\n0.2430695\n0.7165735\n0.4912504731\n\n\nfertilizer:density\n2\n0.4278183\n0.2139091\n0.6306083\n0.5346558069\n\n\nResiduals\n88\n29.8505477\n0.3392108\nNA\nNA\n\n\n\n\n\n\n\nThe block variable’s p value of p = 0.49 and the interaction term’s p value of p = 0.53 are not significant at the α = 0.05 level. Hence, neither have a statistically significant effect on the crop yield. Because they do not add information to the model, They may be removed from the final model.\n\n19.4.8 Two-Way ANOVA - Density + Fertilizer + Interaction\nWe will now run a two-way ANOVA on a new model using the density, fertilizer, and interaction factors to investigate whether these terms are significant.\n\n# performing the two-way ANOVA with interaction\ninteraction &lt;- \n# 'fertilizer * density' = fertilizer + density + fertilizer:density interaction\n  aov(yield ~ fertilizer * density, data = crop_data_df)\n\ntable_int &lt;- interaction %&gt;%\n# turn model into data.frame of parameters\n  tidy() %&gt;% \n# create gt table\n  gt()\n\n# customizing table header\ntable_int |&gt;\n   tab_header(\n      title = \"Two-way ANOVA of Crop Yield - Interaction Model\",\n      subtitle = \"for interaction model (y ~ fertilizer * density)\"\n    )\n\n\n\n\n\n\n\nTwo-way ANOVA of Crop Yield - Interaction Model\n\n\nfor interaction model (y ~ fertilizer * density)\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nfertilizer\n2\n6.0680466\n3.0340233\n9.0010522\n0.0002731890\n\n\ndensity\n1\n5.1216812\n5.1216812\n15.1945174\n0.0001864075\n\n\nfertilizer:density\n2\n0.4278183\n0.2139091\n0.6346053\n0.5325000914\n\n\nResiduals\n90\n30.3366866\n0.3370743\nNA\nNA\n\n\n\n\n\n\n\nThe p value for the interaction term is greater than 0.05, hence we fail to reject the null hypothesis (H03) and conclude that there is no statistically significant interaction effect between fertilizer type and crop density on average yield. The interaction term should also be removed from the model.\n\n19.4.9 Two-Way ANOVA - Density + Fertilizer\nBecause the interaction term was not significant, we will remove it from our model and perform a two-way ANOVA on a model with only the density and fertilizer factors.\n\n# performing the two-way ANOVA without the interaction term\nmain_effects &lt;- \n  aov(yield ~ fertilizer + density, data = crop_data_df)\n\ntable_main &lt;- main_effects %&gt;% \n# turn model into data.frame of parameters\n  tidy() %&gt;%\n# create gt table\n  gt()\n\n# customizing table header\ntable_main |&gt;\n   tab_header(\n      title = \"Two-Way ANOVA of Crop Yield - Main Effects Model\",\n      subtitle = \"for main effects model (y ~ fertilizer + density)\"\n   )\n\n\n\n\n\n\n\nTwo-Way ANOVA of Crop Yield - Main Effects Model\n\n\nfor main effects model (y ~ fertilizer + density)\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nfertilizer\n2\n6.068047\n3.0340233\n9.073123\n0.0002532992\n\n\ndensity\n1\n5.121681\n5.1216812\n15.316179\n0.0001741418\n\n\nResiduals\n92\n30.764505\n0.3343968\nNA\nNA\n\n\n\n\n\n\n\nWithout the interaction term, we see that the p values for both fertilizer type and planting density are significant at the α = 0.05 level. Thus, we reject null hypotheses H01 and H02 and conclude that both fertilizer and density have a statistically significant main effect on crop yield.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Two-Way ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_two_way.html#determining-the-best-fitting-model-using-aic",
    "href": "lessons/04_anova_two_way.html#determining-the-best-fitting-model-using-aic",
    "title": "\n19  Two-Way ANOVA\n",
    "section": "\n19.5 Determining the Best-Fitting Model Using AIC",
    "text": "19.5 Determining the Best-Fitting Model Using AIC\nThe Akaike information criterion (AIC) is another method that can be used to determine the model that best fits the data. The model that explains the greatest amount of the variation in the data using the fewest possible independent variables is considered the best-fitting model. The lower the AIC value, the more variation is explained by the model.\n\n# creating a list of models to compare and their respective names\nmodel_set &lt;- list(main_effects, interaction, full_model)\nmodel_names &lt;- c(\"main_effects\", \"interaction\", \"full_model\")\n\n# using AICtab to compare models \ngt_fmt &lt;-\n  aictab(model_set, modnames = model_names) \n\n# create gt table using AIC\ngt_print &lt;- \n  gt(gt_fmt)\n\ngt_print\n\n\n\n\n\n\nModnames\nK\nAICc\nDelta_AICc\nModelLik\nAICcWt\nLL\nCum.Wt\n\n\n\nmain_effects\n5\n173.8562\n0.000000\n1.00000000\n0.81041300\n-81.59474\n0.8104130\n\n\ninteraction\n7\n177.1178\n3.261693\n0.19576377\n0.15864950\n-80.92256\n0.9690625\n\n\nfull_model\n9\n180.3873\n6.531150\n0.03817497\n0.03093749\n-80.14714\n1.0000000\n\n\n\n\n\n\n\nAs shown in this table, the main_effects model, which only includes the fertilizer and density variables without their interaction term, has the lowest AIC and is therefore the best fit for our crop data analysis.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Two-Way ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_two_way.html#post-hoc-testing-tukey-hsd",
    "href": "lessons/04_anova_two_way.html#post-hoc-testing-tukey-hsd",
    "title": "\n19  Two-Way ANOVA\n",
    "section": "\n19.6 Post-Hoc Testing (Tukey HSD)",
    "text": "19.6 Post-Hoc Testing (Tukey HSD)\nWe now know which parameters are significant, however, we are also interested in learning how the levels of the factors differ from each other. To quantify these differences, the Tukey’s Honestly-Significant-Difference test can be used.\n\n# Performing Tukey HSD on the final model (yield ~ fertilizer + density)\ntukey_crop &lt;- TukeyHSD(main_effects)  \n  \ntukey_crop %&gt;% \n# turn model into data.frame of parameters\n  tidy %&gt;% \n# create gt table\n  gt() %&gt;% \n  tab_header(\n      title = \"Tukey Multiple Comparisons of Means\",\n      subtitle = \"for main effects model (y ~ fertilizer + density)\"\n  )\n\n\n\n\n\n\n\nTukey Multiple Comparisons of Means\n\n\nfor main effects model (y ~ fertilizer + density)\n\n\nterm\ncontrast\nnull.value\nestimate\nconf.low\nconf.high\nadj.p.value\n\n\n\n\nfertilizer\n2-1\n0\n0.1761687\n-0.16822506\n0.5205625\n0.4452958212\n\n\nfertilizer\n3-1\n0\n0.5991256\n0.25473179\n0.9435194\n0.0002218678\n\n\nfertilizer\n3-2\n0\n0.4229569\n0.07856306\n0.7673506\n0.0119381379\n\n\ndensity\n2-1\n0\n0.4619560\n0.22752045\n0.6963916\n0.0001741423\n\n\n\n\n\n\n\nThis table shows the pairwise differences between each level of the factors. Comparisons with p values less than 0.05 are considered significant:\n\nfertilizer type 1 vs 3\nfertilizer type 2 vs 3\nlow vs high density\n\nTo visualize the differences between the levels of each factor, the 95% family-wise confidence intervals of the pairs for fertilizer and density were plotted below. The x-axes of the plots display the difference in means between the paired levels. The y-axis denotes the pair being compared.\n\n# plot tukey confidence intervals and set tick marks to horizontal (las = 1)\n#   position\nplot(tukey_crop, las = 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the significant confidence intervals do not include zero. From this plot, we see that only the fertilizer comparison of type 1 and 2 confidence interval includes 0. Thus, there is no statistically significant difference in the average crop yield produced by fertilizer 1 vs fertilizer 2. 95% family-wise confidence intervals for all other comparisons are statistically significant at the α = 0.05 level.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Two-Way ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_two_way.html#anova-vs.-linear-regression",
    "href": "lessons/04_anova_two_way.html#anova-vs.-linear-regression",
    "title": "\n19  Two-Way ANOVA\n",
    "section": "\n19.7 ANOVA vs. Linear Regression",
    "text": "19.7 ANOVA vs. Linear Regression\nThe model underlying the ANOVA we just performed is actually a linear regression. Let’s see what results we would get if we used linear regression to model the same relationships:\n\n# create linear regression model of yield ~ fertilizer + density\ncrop_lm &lt;- lm(yield ~ fertilizer + density, data = crop_data_df)\n\nsummary(crop_lm)\n\n\nCall:\nlm(formula = yield ~ fertilizer + density, data = crop_data_df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.16523 -0.30208 -0.05802  0.42576  1.47375 \n\nCoefficients:\n            Estimate Std. Error  t value Pr(&gt;|t|)    \n(Intercept) 176.5261     0.1180 1495.490  &lt; 2e-16 ***\nfertilizer2   0.1762     0.1446    1.219 0.226115    \nfertilizer3   0.5991     0.1446    4.144 7.57e-05 ***\ndensity2      0.4620     0.1180    3.914 0.000174 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5783 on 92 degrees of freedom\nMultiple R-squared:  0.2667,    Adjusted R-squared:  0.2428 \nF-statistic: 11.15 on 3 and 92 DF,  p-value: 2.601e-06\n\n\nThe output from the linear regression model indicate a significant difference in average yield exists between crops fertilized with fertilizer 3 and 1 (p = 7.57 e-05) but not fertilizers 1 and 2 (p = 0.22). Furthermore, the average yield for high density acres is significantly different than low density acres (p &lt; 0.001). These findings are consistent with the results of the Tukey’s HSD tests.\nRecall that the two-way ANOVA compares the variance within all levels of a factor to the total variation of the model. Similarly, the linear regression quantifies the variation of each group (or “level”) from the mean using dummy variables. Both use the same computations and thus the sum of squares for the independent variables are the same in the outputs of both the two-way ANOVA and linear regression. Because the density variable consists of two groups, its p value (p = 0.00017) is also the same in both models. The p value(s) for fertilizer differs between the two, however, because the three fertilizer groups are split into three dummy variables in the linear regression and compared against the group mean individually. The two-way ANOVA, on the other hand, is only concerned with the total variance within the fertilizer factor. Read “Common statistical tests are linear models” to learn more about how ANOVAs and many other statistical tests are simply different flavors of linear regression.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Two-Way ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_two_way.html#conclusions",
    "href": "lessons/04_anova_two_way.html#conclusions",
    "title": "\n19  Two-Way ANOVA\n",
    "section": "\n19.8 Conclusions",
    "text": "19.8 Conclusions\nThere is a statistically significant difference in average crop yield by both the fertilizer type and planting density variables with F values of 9.018 (p &lt; 0.001) and 15.316 (p &lt; 0.001) respectively. The interaction between these two terms was not significant.\nThe Tukey post-hoc test showed significant pairwise differences in average yield between fertilizer types 1 and 3 and between type 2 and 3. It also depicted significant differences in average yield between low and high planting density. Therefore, corn fertilized with fertilizer type 3 produced significantly more bushels per acre than those with fertilizers type 1 and 2, and acres of corn planted at high density produced more bushels than those planted at low density.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Two-Way ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_anova_kruskal_wallis.html",
    "href": "lessons_original/04_anova_kruskal_wallis.html",
    "title": "20  Non-Parametric ANOVA: The Kruskal-Wallis Test",
    "section": "",
    "text": "20.1 Introduction\nThe Kruskal-Wallis test (H-test) is a hypothesis test for multiple independent samples, which is used when the assumptions for a one factor analysis of variance are violated. In other word, it is the non-parametric alternative to the One Way ANOVA. Non-parametric means that the data does not follow normal distribution. It is sometimes called the one-way ANOVA on ranks, as the ranks of the data values are used in the test rather than the actual data points.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Non-Parametric ANOVA: The Kruskal-Wallis Test</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_anova_kruskal_wallis.html#introduction",
    "href": "lessons_original/04_anova_kruskal_wallis.html#introduction",
    "title": "20  Non-Parametric ANOVA: The Kruskal-Wallis Test",
    "section": "",
    "text": "20.1.1 Mathematical Equation\n\\[H = \\frac{{n-1}}{{n}}\\sum^k_{i=1}\\frac{n_i({\\bar{R_i} - E})^2}{{σ^2_R}}\\]\nWhere:\n\n\\(H\\) is the Kruskal-Wallis test statistic,\n\\(n\\) is the total number of observations,\n\\(R_i\\) is the sum of ranks for each group,\n\\(E\\) is the expected value of the sum of ranks under the null hypothesis.\n\\(σ^2_R\\) is the square of standard deviation of Rank sum.\n\nEquation for Expected Rank\n\\[E = \\frac{{n+1}}{{2}}\\]\nWhere:\n-n represents total number of observations.\nEquation for Rank Mean for group i\n\\[R_i = \\frac{{\\sum{R}}}{{n_g}}\\]\nWhere:\n\nR_i represents mean rank for \\(i^{th}\\) group,\n\\(\\sum{R}\\) represents sum of ranks in \\(i^{th}\\) group,\n\\(n_g\\) represents number of observation in \\(i^{th}\\) group.\n\nExample\n\n\n\nAssigning ranks/ E and mean rank calculated/ ready for H calculation\n\n\n\n\n20.1.2 Assumptions\n1. Ordinal or Continuous Response Variable – the response variable should be an ordinal or continuous variable.\n2. Independence – the observations in each group need to be independent of each other.\n3. Sample Size and distribution – each group must have a sample size of 5 or more and the distributions in each group need to have a similar shape but groups does not follow normal distribution.\n\n\n20.1.3 Hypothesis\nThe test determines whether two or more independent groups have same central tendency.\n\nH0: population rank sum average are equal for independent group and therefore come from same population.\nH1: population rank sum average are significantly different for at-least two or more independent group and therefore come from different population.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Non-Parametric ANOVA: The Kruskal-Wallis Test</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_anova_kruskal_wallis.html#running-kruskal-wallis-in-r",
    "href": "lessons_original/04_anova_kruskal_wallis.html#running-kruskal-wallis-in-r",
    "title": "20  Non-Parametric ANOVA: The Kruskal-Wallis Test",
    "section": "20.2 Running Kruskal-Wallis in R",
    "text": "20.2 Running Kruskal-Wallis in R\n\n20.2.1 Packages\n\n# install.packages(\"FSA\") # Houses dunnTest for pair wise comparison\n# install.packages(\"ggpubr\")  # For density plot and for creating and customizing 'ggplot2'- based publication ready plots\n# install.packages(\"ggstatplot\") # Houses gbetweenstats() function that allows building a combination of box and violin plots along with                                        statistical details.\n# install.packages(\"tidyverse\") # For wrangling and tidying the data\n# install.packages(\"MultNonParam\")\n\nlibrary(MultNonParam)\n\nLoading required package: ICSNP\n\n\nLoading required package: mvtnorm\n\n\nLoading required package: ICS\n\nsuppressPackageStartupMessages(library(ggpubr))   \nsuppressPackageStartupMessages(library(ggstatsplot))\nsuppressPackageStartupMessages(library(FSA))       \nsuppressPackageStartupMessages(library(gtsummary))\nsuppressPackageStartupMessages(library(tidyr))     \nsuppressPackageStartupMessages(library(tidyverse)) \n\n\n\n20.2.2 Data\nAs an example we will manually create a data, details of which can be found Here.\nThe data represents antibody production after receiving a vaccine. A hospital administered one of three different vaccines - A, B, or C to 6 individuals per group and measured the antibody presence (\\(\\mu\\)g/mL) in their blood after a chosen time period. The data is as follows: The goal of this exercise will be to determine how the three vaccines performed compared to each other. Essentially, we are looking to determine if the antibody data for each vaccine originates from the same distribution. The sample size is small and normal distribution cannot be assumed. Therefore, we will be conducting the Kruskal-Wallis test.\nNull Hypothesis (H0): The vaccines induce equal amounts of antibody production. (all three groups originate from the same distribution and have the same median)\nAlternative Hypothesis (H1): At least one vaccine induces different amount of antibodies to be produced.(at least one group originates from a different distribution and has a different median)\n\n# Creating dataframe for antibodies produced (in $\\mu$g/mL$) by three different vaccines;\n\nA &lt;- c(1232, 751, 339, 848, 447, 542)\nB &lt;- c(302, 57, 521, 278, 176, 201)\nC &lt;- c(839, 342, 473, 1128, 242, 475)\n\ndf &lt;- data.frame(A, B, C)\n\ndf_tidy &lt;- pivot_longer(\n  data = df,\n  cols = c(\"A\", \"B\", \"C\"),\n  names_to = \"Vaccines\",\n  values_to = \"Antibody\"\n)\n\ndf_tidy_sorted &lt;- \n  df_tidy %&gt;% \n  arrange(Vaccines)\n\ndf_tidy_sorted\n\n# A tibble: 18 × 2\n   Vaccines Antibody\n   &lt;chr&gt;       &lt;dbl&gt;\n 1 A            1232\n 2 A             751\n 3 A             339\n 4 A             848\n 5 A             447\n 6 A             542\n 7 B             302\n 8 B              57\n 9 B             521\n10 B             278\n11 B             176\n12 B             201\n13 C             839\n14 C             342\n15 C             473\n16 C            1128\n17 C             242\n18 C             475\n\nvaccine_efficacy = df_tidy_sorted\n\n\nstr(vaccine_efficacy)\n\ntibble [18 × 2] (S3: tbl_df/tbl/data.frame)\n $ Vaccines: chr [1:18] \"A\" \"A\" \"A\" \"A\" ...\n $ Antibody: num [1:18] 1232 751 339 848 447 ...\n\n\n\nvaccine_efficacy$Vaccines &lt;- as_factor(vaccine_efficacy$Vaccines)\n\nstr(vaccine_efficacy)\n\ntibble [18 × 2] (S3: tbl_df/tbl/data.frame)\n $ Vaccines: Factor w/ 3 levels \"A\",\"B\",\"C\": 1 1 1 1 1 1 2 2 2 2 ...\n $ Antibody: num [1:18] 1232 751 339 848 447 ...\n\n\n\n\n20.2.3 Computing summary statistics by group\nThe first step is to inspect the data and calculate a summary of statistics. This can be done by using the summarise function.\n\ngroup_by(vaccine_efficacy, Vaccines) %&gt;%\n  summarise(\n    count = n(),\n    mean = mean(Antibody, na.rm = TRUE),\n    sd = sd(Antibody, na.rm = TRUE),\n    median = median(Antibody, na.rm = TRUE),\n    IQR = IQR(Antibody, na.rm = TRUE)\n  )\n\n# A tibble: 3 × 6\n  Vaccines count  mean    sd median   IQR\n  &lt;fct&gt;    &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 A            6  693.  325.   646.  353 \n2 B            6  256.  156.   240.  114.\n3 C            6  583.  335.   474   373.\n\n\n\n\n20.2.4 Box Plot\nThe next step will be to visualize the dataset using a box plot. This will allow us to estimate differences in distribution.\n\nvaccine_efficacy %&gt;% \n  ggplot(aes(Vaccines, Antibody)) + \n  geom_boxplot() +\n  ggtitle(\"Vaccine Efficacy\") +\n  xlab(\"Vaccines\") + ylab(\"Antibodies\")\n\n\n\n\n\n\n\n\nBased on the box plot, we see that there is similarity in distribution of A and C while B looks to be different. We can also add the individual data points and connect the boxes to visually see the density distribution and compare with normal distribution for each vaccines.\n\n20.2.4.1 Adding error bars: mean_se\n\nggline(vaccine_efficacy, x = \"Vaccines\", y = \"Antibody\",\n       add = c(\"mean_se\", \"jitter\"),\n       order = c(\"A\", \"B\", \"C\"),\n       ylab = \"Antibody\", xlab = \"Vaccines\")\n\n\n\n\n\n\n\n\n\n\n20.2.4.2 Density plot with overlaid normal plot\nNext, we want to create a density plot to further visualize the data and compare it to what a normal distribution of these data should look like. This can be done by using the ggdensity function as seen below.\n\n# Plot the distribution of the antibodies for vaccine \nggdensity(df$A, fill = \"lightgray\", title = \"Distribution plot of Antibody Production for Vaccine A\") +\n  scale_x_continuous() +\n  xlab(\"A\") +\n  stat_overlay_normal_density(color = \"red\", linetype = \"dashed\")\n\n\n\n\n\n\n\n\n\n# Plot the distribution of the antibodies for vaccine \nggdensity(df$B, fill = \"lightgray\", title = \"Distribution plot of Antibody Production for Vaccine B\") +\n  scale_x_continuous() +\n  xlab(\"B\") +\n  stat_overlay_normal_density(color = \"red\", linetype = \"dashed\")\n\n\n\n\n\n\n\n\n\n# Plot the distribution of the antibodies for vaccine \nggdensity(df$C, fill = \"lightgray\", title = \"Distribution plot of Antibody Production for Vaccine C\") +\n  scale_x_continuous() +\n  xlab(\"C\") +\n  stat_overlay_normal_density(color = \"red\", linetype = \"dashed\")\n\n\n\n\n\n\n\n\nFrom these density plots, we see that our data is not normally distributed and distribution shape for two vaccines looks similar while one vaccine deviates. As our data is not normally distributed and has small sample size, we will now perform Kruskal-Wallis test to find out whether there are any significant differences between the three vaccines in terms of their efficacy (antibodies production in the body).\n\n\n\n20.2.5 Kruskal-Wallis Test\nThe Kruskal-Wallis test can be done in R using the kruskal.test function as seen below.\n\nresult &lt;- kruskal.test(Antibody ~ Vaccines, data = vaccine_efficacy)\n\nprint(result)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  Antibody by Vaccines\nKruskal-Wallis chi-squared = 7.2982, df = 2, p-value = 0.02601\n\n\n\n\n20.2.6 Tabulating the result\n\ntable1 &lt;-   \n  tbl_summary(\n    vaccine_efficacy,\n    by = Vaccines,\n) %&gt;% \n  add_p() %&gt;%\n  modify_caption(\"Antibody Production of Different Vaccines\") %&gt;%\n  bold_labels()\n\n\ntable1\n\n\n\n\n\n\nAntibody Production of Different Vaccines\n\n\nCharacteristic\nA, N = 61\nB, N = 61\nC, N = 61\np-value2\n\n\n\n\nAntibody\n647 (471, 824)\n240 (182, 296)\n474 (375, 748)\n0.026\n\n\n\n1 Median (IQR)\n\n\n2 Kruskal-Wallis rank sum test\n\n\n\n\n\n\n\n\n\n\n\n20.2.7 Interpretation\nFrom the Kruskal-Wallis test, we get that our test statistic is 26.63 with p-value 0.026, which is smaller than our level of significance 0.05. This gives us enough evidence to reject the null hypothesis. Therefore, we conclude that there is a significant difference in the efficacy of at least two of the three vaccines.\n\n20.2.7.1 Post-hoc-Test\nThe Kruskal-Wallis test helps to determine whether at least two groups differ from each other but it does not specify where in which groups the significance lies. We need to conduct a post-hoc test for this. For this purpose, the Dunn test is the appropriate nonparametric test for the pairwise multiple comparison. We will use Holm adjustment method for multiple comparison. You can read about various adjustment methods for multiple comparison herechen2017?\n\npair_wise_compare &lt;- dunnTest(Antibody~Vaccines, \n                              data = vaccine_efficacy,\n                              method = \"holm\"\n  \n)\n\npair_wise_compare\n\nDunn (1964) Kruskal-Wallis multiple comparison\n\n\n  p-values adjusted with the Holm method.\n\n\n  Comparison          Z     P.unadj     P.adj\n1      A - B  2.5955427 0.009444166 0.0283325\n2      A - C  0.6488857 0.516412268 0.5164123\n3      B - C -1.9466571 0.051575864 0.1031517\n\n\nWhen looking at the adjusted p-values in the last column for each pairwise comparison, we can see that only the A-B vaccine comparison has a p-value that is less than our level of significance of 0.05. Therefore, we conclude that there is significant difference in vaccine A-B while there is no significant difference between vaccines A-C, and B-C.\n\n\n\n20.2.8 Alternative method\nA very good alternative for performing a Kruskal-Wallis and the post-hoc tests in R is with the ggbetweenstats() function from the {ggstatsplot} package: It provides a combination of box and violin plots along with jittered data points for between-subjects designs with statistical details included in the plot as a subtitle.\n\nggbetweenstats(\n  data = vaccine_efficacy,\n  x = Vaccines,\n  y = Antibody,\n  type = \"nonparametric\", # ANOVA or Kruskal-Wallis\n  plot.type = \"box\",\n  pairwise.comparisons = TRUE,\n  pairwise.display = \"significant\",\n  centrality.type = \"nonparametric\", # It displays median for non parametric data by default.\n  bf.message = FALSE # Logical that decides whether to display Bayes Factor in favor of the null hypothesis. This argument is relevant only for parametric test\n)\n\n\n\n\n\n\n\n\nThis method has the advantage that all necessary statistical results are displayed directly on the plot. It also provides a more efficient and concise code.\nThe results of the Kruskal-Wallis test are shown in the subtitle above the plot (the p-value is after p =). Moreover, the results of the post-hoc test are displayed between each group via accolades, and the boxplots allow to visualize the distribution for each species.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Non-Parametric ANOVA: The Kruskal-Wallis Test</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_anova_kruskal_wallis.html#conclusion",
    "href": "lessons_original/04_anova_kruskal_wallis.html#conclusion",
    "title": "20  Non-Parametric ANOVA: The Kruskal-Wallis Test",
    "section": "20.3 Conclusion",
    "text": "20.3 Conclusion\nIn conclusion, the Kruskal-Wallace test is a non-parametric hypothesis test that can be used to determine if there are significant differences between two or more groups using the ranks of the data values. The first step involves visualizing the data to confirm it violates the rules of normality. Next, you conduct the Kruskal-Wallis test to determine if there are significant differences. Finally, you run a post-hoc test to calculate pairwise comparisons and determine which specific groups are significantly different.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Non-Parametric ANOVA: The Kruskal-Wallis Test</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_anova_kruskal_wallis.html#references",
    "href": "lessons_original/04_anova_kruskal_wallis.html#references",
    "title": "20  Non-Parametric ANOVA: The Kruskal-Wallis Test",
    "section": "References",
    "text": "References",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Non-Parametric ANOVA: The Kruskal-Wallis Test</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_random_intercept.html",
    "href": "lessons/04_anova_random_intercept.html",
    "title": "\n21  Repeated Measures ANOVA\n",
    "section": "",
    "text": "21.1 Introduction\nRepeated measures ANOVA is used when you have the same measure that participants were rated on at more than two time points. With only two time points a paired \\(t\\)-test will be sufficient, but for more times a repeated measures ANOVA is required. (anovafo2013?-) There are many complex designs that can make use of repeated measures, but throughout this guide, we will be referring to the most simple case, that of a one-way repeated measures ANOVA. This particular test requires one independent variable and one dependent variable. The dependent variable needs to be continuous (interval or ratio) and the independent variable categorical (either nominal or ordinal). (anovare2018?-)",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Repeated Measures ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_random_intercept.html#neccessary-packages",
    "href": "lessons/04_anova_random_intercept.html#neccessary-packages",
    "title": "\n21  Repeated Measures ANOVA\n",
    "section": "\n21.2 Neccessary packages",
    "text": "21.2 Neccessary packages\nMake sure that you have installed the following R packages:\n\n\ntidyverse for data manipulation and visualization.\n\nggpubr for creating easily publication ready plots.\n\nrstatix provides pipe-friendly R functions for easy statistical analyses.(anovare2018?-)\n\n\ndatarium contains required data sets for this chapter.\n\nStart by loading the following R packages\n\n# Install packages first and then load the libraries. \n# install.packages(\"datarium\")\n\nlibrary(tidyverse)\nlibrary(ggpubr)\nlibrary(rstatix)",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Repeated Measures ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_random_intercept.html#data-source-and-description",
    "href": "lessons/04_anova_random_intercept.html#data-source-and-description",
    "title": "\n21  Repeated Measures ANOVA\n",
    "section": "\n21.3 Data source and description",
    "text": "21.3 Data source and description\nFor this example we will be using this dataset from the datarium package that contains 10 individuals’ self-esteem score on three time points during a specific diet to determine whether their self-esteem improved.\nOne-way repeated measures ANOVA can be performed in order to determine the effect of time on the self-esteem score.\n\n# Data preparation; wide format\ndata(\"selfesteem\", package = \"datarium\")\nselfesteem\n\n# A tibble: 10 × 4\n      id    t1    t2    t3\n   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     1  4.01  5.18  7.11\n 2     2  2.56  6.91  6.31\n 3     3  3.24  4.44  9.78\n 4     4  3.42  4.71  8.35\n 5     5  2.87  3.91  6.46\n 6     6  2.05  5.34  6.65\n 7     7  3.53  5.58  6.84\n 8     8  3.18  4.37  7.82\n 9     9  3.51  4.40  8.47\n10    10  3.04  4.49  8.58\n\n\nNow we “gather” columns t1, t2, and t3 into “long” format, then convert id and time into factor variables.\n\nselfesteem_df &lt;- \n  selfesteem %&gt;%\n  gather(key = \"time\", value = \"score\", t1, t2, t3) %&gt;%\n  convert_as_factor(id, time)\n\nselfesteem_df\n\n# A tibble: 30 × 3\n   id    time  score\n   &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt;\n 1 1     t1     4.01\n 2 2     t1     2.56\n 3 3     t1     3.24\n 4 4     t1     3.42\n 5 5     t1     2.87\n 6 6     t1     2.05\n 7 7     t1     3.53\n 8 8     t1     3.18\n 9 9     t1     3.51\n10 10    t1     3.04\n# ℹ 20 more rows\n\n\nThe one-way repeated measures ANOVA can be used to determine whether the means self-esteem scores are significantly different between the three time points.\nNote: Whilst the repeated measures ANOVA is used when you have just “one” independent variable, if you have “two” independent variables (e.g., you measured time and condition), you will need to use a two-way repeated measures ANOVA. Two and Three-way Repeated Measures ANOVA examples with this data can be found here.\n\n21.3.1 Summary statistics\nCompute some summary statistics of the self-esteem score by groups (time): mean and sd (standard deviation)\n\n# Statistics-summary\nselfesteem_df %&gt;%\n  group_by(time) %&gt;%\n  get_summary_stats(score, type = \"mean_sd\")\n\n# A tibble: 3 × 5\n  time  variable     n  mean    sd\n  &lt;fct&gt; &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 t1    score       10  3.14 0.552\n2 t2    score       10  4.93 0.863\n3 t3    score       10  7.64 1.14 \n\n\n\n21.3.2 Visualization\nCreate a box plot and add points corresponding to individual values:\n\nbxp &lt;- ggboxplot(selfesteem_df, x = \"time\", y = \"score\", add = \"point\")\nbxp\n\n\n\nVisualization of DATA",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Repeated Measures ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_random_intercept.html#test-assumptions",
    "href": "lessons/04_anova_random_intercept.html#test-assumptions",
    "title": "\n21  Repeated Measures ANOVA\n",
    "section": "\n21.4 Test Assumptions",
    "text": "21.4 Test Assumptions\nBefore computing repeated measures ANOVA test, you need to perform some preliminary tests to check if the assumptions are met.\n\n21.4.1 Outiliers\nOutliers can be easily identified using box plot methods, implemented in the R function identify_outliers() inside the rstatix package.\n\nselfesteem_df %&gt;%\n  group_by(time) %&gt;%\n  identify_outliers(score)\n\n# A tibble: 2 × 5\n  time  id    score is.outlier is.extreme\n  &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;lgl&gt;      &lt;lgl&gt;     \n1 t1    6      2.05 TRUE       FALSE     \n2 t2    2      6.91 TRUE       FALSE     \n\n\nThere were no extreme outliers. In the situation where we have extreme outliers, we can include the outlier in the analysis anyway if we do not believe the result will be substantially affected. This can be evaluated by comparing the result of the ANOVA with and without the outlier. It’s also possible to keep the outliers in the data and perform robust ANOVA test using the WRS2 package. WRS2 Package\n\n21.4.2 Normality Assumption\nThe outcome (or dependent) variable should be approximately normally distributed in each cell of the design. This can be checked using the Shapiro-Wilk normality test (shapiro_test() in rstatix package) or by visual inspection using QQ plot (ggqqplot() in the ggpubr package). If the data is normally distributed, the \\(p\\)-value should be greater than 0.05.\n\nselfesteem_df %&gt;%\n  group_by(time) %&gt;%\n  shapiro_test(score)\n\n# A tibble: 3 × 4\n  time  variable statistic     p\n  &lt;fct&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;\n1 t1    score        0.967 0.859\n2 t2    score        0.876 0.117\n3 t3    score        0.923 0.380\n\n\nThe self-esteem score was normally distributed at each time point, as assessed by Shapiro-Wilk’s test (\\(p &gt; 0.05\\)).\nNote that, if your sample size is greater than 50, the normal QQ plot is preferred because at larger sample sizes the Shapiro-Wilk test becomes very sensitive even to a minor deviation from normality. QQ plot draws the correlation between a given data and the normal distribution. Create QQ plots for each time point:\n\nggqqplot(selfesteem_df, \"score\", facet.by = \"time\")\n\n\n\nQQ Plot\n\n\n\nFrom the plot above, as all the points fall approximately along the reference line, we can assume normality.\n\n21.4.3 Assumption of Sphericity\nThe variance of the differences between groups should be equal. This can be checked using the Mauchly’s test of sphericity. This assumption will be automatically checked during the computation of the ANOVA test using the R function anova_test() in rstatix package. The Mauchly’s test is internally used to assess the sphericity assumption. Click HERE to know more about the Assumption of Sphericity and the Mauchly’s Test and to understand why is important.\nBy using the function get_anova_table() to extract the ANOVA table, the Greenhouse-Geisser sphericity correction is automatically applied to factors violating the sphericity assumption.\n\nres.aov &lt;- anova_test(\n  data = selfesteem_df, \n  # Selfesteem variable\n  dv = score,\n  # Sample individuals\n  wid = id, \n  # Independent variable time \n  within = time\n)\n\n# Get table\nget_anova_table(res.aov)\n\nANOVA Table (type III tests)\n\n  Effect DFn DFd      F        p p&lt;.05   ges\n1   time   2  18 55.469 2.01e-08     * 0.829\n\n\nThe self-esteem score was statistically significantly different at the different time points during the diet, \\(F_{(2, 18)} = 55.5\\), \\(p &lt; 0.0001\\), \\(\\eta^2_g = 0.83\\). where,\n\n\nF Indicates that we are comparing to an \\(F\\)-distribution (\\(F\\)-test),\n\n(2, 18) indicates the degrees of freedom in the numerator (DFn) and the denominator (DFd), respectively,\n\n55.5 indicates the obtained \\(F\\)-statistic value;\n\np specifies the \\(p\\)-value, and\n\n\\(\\eta^2_g\\) is the generalized effect size (amount of variability due to the within-subjects factor).\n\n21.4.4 Post-hoc test\nYou can perform multiple pairwise paired \\(t\\)-tests between the levels of the within-subjects factor (here time). We adjust \\(p\\)-values using the Bonferroni multiple testing correction method.\n\n# pairwise comparisons\npwc &lt;- pairwise_t_test(\n  data = selfesteem_df,\n  formula = score ~ time,\n  paired = TRUE,\n  p.adjust.method = \"bonferroni\"\n)\n\npwc\n\n# A tibble: 3 × 10\n  .y.   group1 group2    n1    n2 statistic    df           p p.adj p.adj.signif\n* &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;       \n1 score t1     t2        10    10     -4.97     9 0.000772     2e-3 **          \n2 score t1     t3        10    10    -13.2      9 0.000000334  1e-6 ****        \n3 score t2     t3        10    10     -4.87     9 0.000886     3e-3 **          \n\n\nAll the pairwise differences are statistically significant.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Repeated Measures ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_random_intercept.html#results",
    "href": "lessons/04_anova_random_intercept.html#results",
    "title": "\n21  Repeated Measures ANOVA\n",
    "section": "\n21.5 Results",
    "text": "21.5 Results\nWe could report the results of the post-hoc test as follows: post-hoc analyses with a Bonferroni adjustment revealed that all the pairwise differences, between time points, were statistically significantly different (\\(p &lt; 0.05\\)).\n\npwc &lt;- pwc %&gt;% add_xy_position(x = \"time\")\n\nbxp + \n  stat_pvalue_manual(pwc) +\n  labs(\n    subtitle = get_test_label(res.aov, detailed = TRUE),\n    caption = get_pwc_label(pwc)\n  )\n\n\n\nVisualization With Results",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Repeated Measures ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_random_intercept.html#conclusion",
    "href": "lessons/04_anova_random_intercept.html#conclusion",
    "title": "\n21  Repeated Measures ANOVA\n",
    "section": "\n21.6 Conclusion",
    "text": "21.6 Conclusion\nThis chapter describes how to compute, interpret and report repeated measures ANOVA in R, specifically one-way repeated measures ANOVA. We also explain the assumptions made by one-way repeated measures ANOVA tests and provide practical examples of R codes to check whether the test assumptions are met.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Repeated Measures ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_corr_cov.html",
    "href": "lessons/04_corr_cov.html",
    "title": "\n22  Correlation and Covariance Matrices\n",
    "section": "",
    "text": "22.1 Introduction to Correlation and Covariance Matrices\nA prominent theme in statistical analysis is evaluating whether or not a relationship exists between two or more variables, and the degree to which that relationship exists.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Correlation and Covariance Matrices</span>"
    ]
  },
  {
    "objectID": "lessons/04_corr_cov.html#introduction-to-correlation-and-covariance-matrices",
    "href": "lessons/04_corr_cov.html#introduction-to-correlation-and-covariance-matrices",
    "title": "\n22  Correlation and Covariance Matrices\n",
    "section": "",
    "text": "Covariance measures the degree to which the deviation of one variable (\\(X\\)) from its mean changes in relation to the deviation of another variable (\\(Y\\)) from its mean. In other words, covariance measures the joint variability of two random variables or how these increase or decrease in relation with each other.\n\nFor instance, if greater values of one variable tend to correspond with greater values of another variable, this suggests a positive covariance.\nCovariance can have both positive and negative values.\nA covariance of zero indicates that the variables are independent of each other, meaning that there is no linear relationship between them.\n\n\nA Covariance Matrix shows the covariance between different variables of a data set. Covariance matrices are helpful for:\n\nCharacterizing multivariate normal distributions.\nDimensionality reduction techniques, such as Principal Component Analysis, where the matrix is used to calculate the principal components (i.e., linear combinations of the original variables that capture the maximum variance in the data).\nMachine learning models, like Gaussian mixture models, where they are used to estimate the parameters of the model.\n\n\n\nCorrelation tells us both the strength and the direction of the relationship between two variables by listing the Correlation Coefficient or \\(r\\) (“Pearson”, “Spearman”, or “Kendall”) for the pair as measure of association.\n\nCorrelation differs from covariance in that it is standardized between -1 and 1, making it easier to interpret.\nA correlation of -1 indicates a perfect negative linear relationship between two variables, and a correlation of 1 indicates a perfect positive linear relationship between two variables.\nA correlation of 0 indicates no correlation.\nThe magnitude of the correlation coefficient indicates the strength of the association:\n\n.1 &lt;  \\(r\\) &lt; .3 (small / weak correlation).\n.3 &lt;  \\(r\\) &lt; .5 (medium / moderate correlation).\n.5 &lt;  \\(r\\) (large / strong correlation).\n\n\nCorrelation is helpful to test for multicollinarity in regression models.\n\n\nA Correlation matrix allows for the exploration of the correlation among the multiple variables in a data set simultaneously. It provides this information through a table listing the correlation coefficients (\\(r\\)) for each pair of variables in the data set.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Correlation and Covariance Matrices</span>"
    ]
  },
  {
    "objectID": "lessons/04_corr_cov.html#mathematical-definition",
    "href": "lessons/04_corr_cov.html#mathematical-definition",
    "title": "\n22  Correlation and Covariance Matrices\n",
    "section": "\n22.2 Mathematical Definition",
    "text": "22.2 Mathematical Definition\n\n22.2.1 Variance\nThe equation for the variance of a single variable is as follows: \\[\nvar(x) = \\frac{1}{n-1}{\\sum_{i = 1}^{n}{(x_i - \\bar{x})^2}}\n\\]\n\n22.2.2 Covariance\nFor a sample of \\(n\\) data points, the sample covariance is calculated as: \\[\nCov(x,y) = \\frac{1}{n-1}{\\sum_{i = 1}^{n}{(x_i - \\bar{x})(y_i-\\bar{y})}}\n\\] Where,\n\n\n\\(x_i\\) denotes all the possible values of x.\n\n\\(y_i\\) denotes all the possible values of y.\n\n\\(\\bar{x}\\) denotes the mean of variable x.\n\n\\(\\bar{y}\\) denotes the mean of variable y.\n\n\\(n\\) denotes the sample size.\n\n\n22.2.2.1 Covariance Matrix\nThe covariance matrix, \\(\\sum\\), can be represented as: \\[\n\\sum = \\begin{bmatrix} Var(x_{1}) & ... & Cov(x_{1},x_{n})\\\\ : &: & :\\\\ :& :& :\\\\ Cov(x_{n},x_{1}) & ... & Var(x_{n}) \\end{bmatrix}.\n\\] Where,\n\n\n\\(x_1\\) is the first variable of interest in the data set.\nUp to the last variable of interest in the data set, \\(x_n\\).\n\n22.2.3 Correlation\nHere is a simplified equation for the Pearson Correlation Coefficient, \\(r\\) that allows us to see the relationship between correlation and covariance: \\[\nr = \\frac {Cov(x,y)}{(\\sqrt{var(x)})(\\sqrt{var(y)})}\n\\] This video explains the standardization and mathematical properties of \\(r\\):\n\n\n22.2.3.1 Correlation Matrix\nHere is an example of a correlation matrix:\n\n\n\nVariable 1\nVariable 2\n…\nVariable n\n\n\n\nVariable 1\n1.00\n\\(r_{12}\\)\n…\n\\(r_{1n}\\)\n\n\nVariable 2\n\\(r_{21}\\)\n1.00\n…\n\\(r_{2n}\\)\n\n\n…\n…\n…\n…\n…\n\n\nVariable n\n\\(r_{n1}\\)\n\\(r_{n2}\\)\n…\n1.00\n\n\n\nWhere,\n\nThe diagonal elements are always 1, because it represents the correlation of each variable with itself.\nThe off-diagonal elements represent the correlation coefficients between pairs of variables. For example \\(r_{12}\\) represents the correlation between Variable 1 and Variable 2.\nThe correlation is symmetric, meaning that the correlation between Variable 1 and Variable 2 is the same as the correlation between Variable 2 and Variable 1.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Correlation and Covariance Matrices</span>"
    ]
  },
  {
    "objectID": "lessons/04_corr_cov.html#data-description",
    "href": "lessons/04_corr_cov.html#data-description",
    "title": "\n22  Correlation and Covariance Matrices\n",
    "section": "\n22.3 Data Description",
    "text": "22.3 Data Description\nWe utilized the R data set mtcars to provide examples for developing a Covariance Matrix and checking the assumptions for/development of a Correlation Matrix using the pearson correlation coefficient.\nmtcars is a data set that belongs to base R that contains data about the various models of cars. It contains measurement from 32 different automobiles (1,973,074 models). The variable in the mtcars data set are:\n\n\nmpg: Miles/(US) gallon.\n\ncyl: Number of cylinders.\n\ndisp: Displacement (cu.in.).\n\nhp: Gross horsepower.\n\ndrat: Rear axle ratio.\n\nwt: Weight (1000 lbs).\n\nqsec: 1/4 mile time.\n\nvs: V/S (0 = V-shaped engine, 1 = straight engine).\n\nam: Transmission (0 = automatic, 1 = manual).\n\ngear: Number of forward gears.\n\ncarb: Number of carburetors.\n\n\n# Load data\ndata(\"mtcars\")\n# Print sample\nhead(mtcars) %&gt;% \n  kable(\n    format = \"markdown\",\n    digits = 2,\n    caption = \"The `mtcars` data set\"\n  )\n\n\nThe mtcars data set\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\nMazda RX4\n21.0\n6\n160\n110\n3.90\n2.62\n16.46\n0\n1\n4\n4\n\n\nMazda RX4 Wag\n21.0\n6\n160\n110\n3.90\n2.88\n17.02\n0\n1\n4\n4\n\n\nDatsun 710\n22.8\n4\n108\n93\n3.85\n2.32\n18.61\n1\n1\n4\n1\n\n\nHornet 4 Drive\n21.4\n6\n258\n110\n3.08\n3.21\n19.44\n1\n0\n3\n1\n\n\nHornet Sportabout\n18.7\n8\n360\n175\n3.15\n3.44\n17.02\n0\n0\n3\n2\n\n\nValiant\n18.1\n6\n225\n105\n2.76\n3.46\n20.22\n1\n0\n3\n1",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Correlation and Covariance Matrices</span>"
    ]
  },
  {
    "objectID": "lessons/04_corr_cov.html#assumptions",
    "href": "lessons/04_corr_cov.html#assumptions",
    "title": "\n22  Correlation and Covariance Matrices\n",
    "section": "\n22.4 Assumptions",
    "text": "22.4 Assumptions\n\n22.4.1 Covariance Assumptions\nBecause the covariance measures the degree to which two variables change together, the following assumptions should be met:\n\n\nLinearity: The relationship between the two variables should be linear.\n\nScale: Covariance is sensitive to the units of measurement, so variables should be measured on interval or ratio scale if possible.\n\nAn interval scale means that there are equal intervals between points on the scale (e.g., temperature in Celsius and Fahrenheit).\nA ratio scale is an interval scale with a true/absolute zero point (e.g., time in minutes, height and weight).\n\n\n\nMean centered data: Covariance is based on deviations from the mean, so for accurate calculations the data should be mean centered.\n\nOutliers: There should be no outliers, or outliers should be handled prior to interpreting results.\n\n22.4.2 Correlation Assumptions\nCorrelation examines the strength and direction of a linear relationship between 2 variables. As such, the following assumptions should be met:\n\n\nLinearity: Correlation can underestimate the strength of a relationship if the relationship between variables is non-linear.\n\nScale: Like covariance, correlation assumes interval or ratio scale for valid results.\n\nHomoscedasticity: The range or spread of one variable should be consistent across the range of the second variable.\n\nThis assumption is only relevant for Pearson Correlation. Spearman Rank Correlation and Kendall’s Tau do not assume homoscedasticity.\n\n\n\nNormality: For hypothesis testing, the variables should be approximately normally distributed.\n\nThis assumption is only relevant for Pearson Correlation. Spearman Rank Correlation and Kendall’s Tau do not assume normality.\n\n\n\nIndependence: Each pair of observations/variables should be independent of other pairs (e.g., you should not have repeated measures, clustered data, or time series data).\n\nOutliers: There should be no outliers, or outliers should be handled prior to interpreting results.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Correlation and Covariance Matrices</span>"
    ]
  },
  {
    "objectID": "lessons/04_corr_cov.html#checking-the-assumptions",
    "href": "lessons/04_corr_cov.html#checking-the-assumptions",
    "title": "\n22  Correlation and Covariance Matrices\n",
    "section": "\n22.5 Checking the Assumptions",
    "text": "22.5 Checking the Assumptions\n\n22.5.1 Linearity\nWe can check this assumption with scatterplots of the continuous data.\n\n# The pairs function creates a scatterplot matrix for all continuous variables\npairs(\n  # Notice that all continuous variables come after the tilde (~)\n  ~ mpg + disp + hp + drat + wt + qsec,\n  data = mtcars,\n  main = \"Scatterplot Matrix\"\n)\n\n\n\n\n\n\n\nFrom this visualization, it appears that the continuous variables have a linear relationship (whether negative or positive).\n\n22.5.2 Scale\nUsing the skimr package along with the help (?) function, we can quickly get information about all of the variables in mtcars to understand what kind of scale the variables are on.\n\nskim(mtcars)\n\n\nData summary\n\n\nName\nmtcars\n\n\nNumber of rows\n32\n\n\nNumber of columns\n11\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n11\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nmpg\n0\n1\n20.09\n6.03\n10.40\n15.43\n19.20\n22.80\n33.90\n▃▇▅▁▂\n\n\ncyl\n0\n1\n6.19\n1.79\n4.00\n4.00\n6.00\n8.00\n8.00\n▆▁▃▁▇\n\n\ndisp\n0\n1\n230.72\n123.94\n71.10\n120.83\n196.30\n326.00\n472.00\n▇▃▃▃▂\n\n\nhp\n0\n1\n146.69\n68.56\n52.00\n96.50\n123.00\n180.00\n335.00\n▇▇▆▃▁\n\n\ndrat\n0\n1\n3.60\n0.53\n2.76\n3.08\n3.70\n3.92\n4.93\n▇▃▇▅▁\n\n\nwt\n0\n1\n3.22\n0.98\n1.51\n2.58\n3.33\n3.61\n5.42\n▃▃▇▁▂\n\n\nqsec\n0\n1\n17.85\n1.79\n14.50\n16.89\n17.71\n18.90\n22.90\n▃▇▇▂▁\n\n\nvs\n0\n1\n0.44\n0.50\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▆\n\n\nam\n0\n1\n0.41\n0.50\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▆\n\n\ngear\n0\n1\n3.69\n0.74\n3.00\n3.00\n4.00\n4.00\n5.00\n▇▁▆▁▂\n\n\ncarb\n0\n1\n2.81\n1.62\n1.00\n2.00\n2.00\n4.00\n8.00\n▇▂▅▁▁\n\n\n\n\n# # Checking the help file for mtcars\n# ?mtcars\n\nAs we mentioned previously in the description of the data, only 6 variables (mpg, disp, hp, drat, wt, and qsec) are interval or ratio values.\n\n22.5.3 Outliers\nTo check for outliers in our variables of interest, we can use a box plot to visually examine the data.\n\nbox_mpg &lt;- ggplot(mtcars) +\n  aes(y = mpg) +\n  geom_boxplot()\n\nbox_disp &lt;- ggplot(mtcars) +\n  aes(y = disp) +\n  geom_boxplot()\n\nbox_hp &lt;- ggplot(mtcars) +\n  aes(y = hp) +\n  geom_boxplot()\n\nbox_drat &lt;- ggplot(mtcars) +\n  aes(y = drat) +\n  geom_boxplot()\n\nbox_wt &lt;- ggplot(mtcars) +\n  aes(y = wt) +\n  geom_boxplot()\n\nbox_qsec &lt;- ggplot(mtcars) +\n  aes(y = qsec) +\n  geom_boxplot()\n\n\nbox_mpg + box_disp + box_hp + box_drat + box_wt + box_qsec\n\n\n\n\n\n\nFigure 22.1\n\n\n\n\nmpg, hp, wt, and qsec all appear to have some outliers.\n\n22.5.4 Homoscedasticity\nTo check the homoscedasticity (or homogeneity of variance) assumption for correlations, we can examine resdiual plots from linear models after conducting hypothesis testing. We can also look back at the box plots (Figure 22.1) to check if the interquartile range of the variables looks approximately the same. All of the variables of interest, except for disp appear to have similar variances.\n\n22.5.5 Normality\nTo check this assumption, we can examine if the data are approximately normally distributed using the Q-Q plot or histograms or with the Shapiro-Wilk test.\nFirst, we will look at the Q-Q plot:\n\n# Normality assumption: Q-Q plot\nqq_mpg &lt;- ggplot(mtcars) +\n  aes(sample = mpg) +\n  stat_qq() +\n  stat_qq_line() +\n  ggtitle(\"mpg\")\n\nqq_disp &lt;- ggplot(mtcars) +\n  aes(sample = disp) +\n  stat_qq() +\n  stat_qq_line() +\n  ggtitle(\"disp\")\n\nqq_hp &lt;- ggplot(mtcars) +\n  aes(sample = hp) +\n  stat_qq() +\n  stat_qq_line() +\n  ggtitle(\"hp\")\n\nqq_drat &lt;- ggplot(mtcars) +\n  aes(sample = drat) +\n  stat_qq() +\n  stat_qq_line() +\n  ggtitle(\"drat\")\n\nqq_wt &lt;- ggplot(mtcars) +\n  aes(sample = wt) +\n  stat_qq() +\n  stat_qq_line() +\n  ggtitle(\"wt\")\n\nqq_qsec &lt;- ggplot(mtcars) +\n  aes(sample = qsec) +\n  stat_qq() +\n  stat_qq_line() +\n  ggtitle(\"qsec\")\n\n\nqq_mpg + qq_disp + qq_hp + qq_drat + qq_wt + qq_qsec\n\n\n\n\n\n\nFigure 22.2\n\n\n\n\nAll of the variables seem to be approximately normally distributed.\nNext, we will look at histograms:\n\n# Normality assumption: Histograms\nhistogram_mpg &lt;- ggplot(mtcars) +\n  aes(x = mpg) +\n  geom_histogram(binwidth = 0.5)\n\nhistogram_disp &lt;- ggplot(mtcars) +\n  aes(x = disp) +\n  geom_histogram(binwidth = 1)\n\nhistogram_hp &lt;- ggplot(mtcars) +\n  aes(x = hp) +\n  geom_histogram(binwidth = 1)\n\nhistogram_drat &lt;- ggplot(mtcars) +\n  aes(x = drat) +\n  geom_histogram(binwidth = 0.5)\n\nhistogram_wt &lt;- ggplot(mtcars) +\n  aes(x = wt) +\n  geom_histogram(binwidth = 0.5)\n\nhistogram_qsec &lt;- ggplot(mtcars) +\n  aes(x = qsec) +\n  geom_histogram(binwidth = 0.5)\n\nhistogram_mpg + histogram_disp + histogram_hp + histogram_drat + \n  histogram_wt + histogram_qsec\n\n\n\n\n\n\nFigure 22.3\n\n\n\n\nHere, it appears that wt and possibly qsec, disp, and hp appear to be approximately normally distributed.\nLastly, we can use the Shapiro-Wilk test to test if the data is approximately normally distributed. P-values greater than 0.05 indicate that the data is likely approximately normally distributed.\n\n# Normality: Shapiro Wilks\nshapiro_mpg &lt;- mtcars %&gt;%\n  summarise(\n    statistic = shapiro.test(mpg)$statistic,\n    p.value = shapiro.test(mpg)$p.value\n  )\n\nshapiro_mpg\n\n  statistic   p.value\n1 0.9475647 0.1228814\n\nshapiro_disp &lt;- mtcars %&gt;%\n  summarise(\n    statistic = shapiro.test(disp)$statistic,\n    p.value = shapiro.test(disp)$p.value\n  )\n\nshapiro_disp\n\n  statistic    p.value\n1 0.9200127 0.02080657\n\nshapiro_hp &lt;- mtcars %&gt;%\n  summarise(\n    statistic = shapiro.test(hp)$statistic,\n    p.value = shapiro.test(hp)$p.value\n  )\n\nshapiro_hp \n\n  statistic    p.value\n1 0.9334193 0.04880824\n\nshapiro_drat &lt;- mtcars %&gt;%\n  summarise(\n    statistic = shapiro.test(drat)$statistic,\n    p.value = shapiro.test(drat)$p.value\n  )\n\nshapiro_drat \n\n  statistic   p.value\n1 0.9458839 0.1100608\n\nshapiro_wt &lt;- mtcars %&gt;%\n  summarise(\n    statistic = shapiro.test(wt)$statistic,\n    p.value = shapiro.test(wt)$p.value\n  )\n\nshapiro_wt \n\n  statistic    p.value\n1 0.9432577 0.09265499\n\nshapiro_qsec &lt;- mtcars %&gt;%\n  summarise(\n    statistic = shapiro.test(qsec)$statistic,\n    p.value = shapiro.test(qsec)$p.value\n  )\n\nshapiro_qsec\n\n  statistic   p.value\n1 0.9732509 0.5935176\n\n\nFrom these outputs, we can see that disp and hp are the only variables with a p-value less than 0.05, indicating that they are likely not approximately normally distributed.\n\n\n\n\n\n\nIf the normality assumption is not satisfied, it is recommended to use non-parametric correlation, including Spearman Rank Correlation and Kendall’s Tau tests, which will be discussed later.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Correlation and Covariance Matrices</span>"
    ]
  },
  {
    "objectID": "lessons/04_corr_cov.html#code-to-run",
    "href": "lessons/04_corr_cov.html#code-to-run",
    "title": "\n22  Correlation and Covariance Matrices\n",
    "section": "\n22.6 Code to Run",
    "text": "22.6 Code to Run\n\n22.6.1 Covariance\nAs with correlation, we can use the cov() to calculate covariance, where the three same methods are available dependent on the validity of assumptions.\n\n# cov(dataName$variable1, dataName$variable2)\n\ncov_result &lt;- cov(mtcars$mpg, mtcars$drat)\n\nprint(cov_result)\n\n[1] 2.195064\n\n\nThe resulting covariance from mpg and drat is 2.195064. We can also use cov() to create a matrix. However, the data must be manipulated to become a square matrix.\n\n# We select the six continuous variables within the `mtcars` data set and save \n# into our matrix object.\ndataMatrix &lt;- mtcars[, c(1, 3:7)]\n\n# We save the output to an object for later.\ncovOutput &lt;- cov(dataMatrix)\n\nprint(covOutput)\n\n             mpg        disp         hp         drat          wt         qsec\nmpg    36.324103  -633.09721 -320.73206   2.19506351  -5.1166847   4.50914919\ndisp -633.097208 15360.79983 6721.15867 -47.06401915 107.6842040 -96.05168145\nhp   -320.732056  6721.15867 4700.86694 -16.45110887  44.1926613 -86.77008065\ndrat    2.195064   -47.06402  -16.45111   0.28588135  -0.3727207   0.08714073\nwt     -5.116685   107.68420   44.19266  -0.37272073   0.9573790  -0.30548161\nqsec    4.509149   -96.05168  -86.77008   0.08714073  -0.3054816   3.19316613\n\n\nThe resulting output is the calculated covariances among all the variables specified. The Covariance can take any value from -\\(\\infty\\) to \\(\\infty\\).\nWe can also scale the covariance matrix into a corresponding correlation matrix.\n\ncov2cor(covOutput)\n\n            mpg       disp         hp        drat         wt        qsec\nmpg   1.0000000 -0.8475514 -0.7761684  0.68117191 -0.8676594  0.41868403\ndisp -0.8475514  1.0000000  0.7909486 -0.71021393  0.8879799 -0.43369788\nhp   -0.7761684  0.7909486  1.0000000 -0.44875912  0.6587479 -0.70822339\ndrat  0.6811719 -0.7102139 -0.4487591  1.00000000 -0.7124406  0.09120476\nwt   -0.8676594  0.8879799  0.6587479 -0.71244065  1.0000000 -0.17471588\nqsec  0.4186840 -0.4336979 -0.7082234  0.09120476 -0.1747159  1.00000000\n\n\nThis defaults to Pearson Correlation so should be interpreted with caution in the case of variables that do not meet the proper assumptions.\n\n22.6.2 Correlation\nCorrelation can be calculated using: cor(), which calculates the correlation coefficient or cor.test(), which tests for the association (or correlation) between paired samples.\nThree different methods are available when using cor(), either pearson (which is default if none is specified), kendall, or spearman. Let’s run cor() on the variables that satisfied our assumptions: mpg, drat, wt, and qsec.\n\n# cor(dataName$variable1, dataName$variable2, method = \"methodName\")\n\n# Pearson is the default, and does not need to be specified. However, for \n# completeness is specified below.\ncorrPearson &lt;- cor.test(mtcars$mpg, mtcars$drat, method = \"pearson\")\n\nprint(corrPearson)\n\n\n    Pearson's product-moment correlation\n\ndata:  mtcars$mpg and mtcars$drat\nt = 5.096, df = 30, p-value = 1.776e-05\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.4360484 0.8322010\nsample estimates:\n      cor \n0.6811719 \n\n\nIn the output, the following is included:\n\n\nCorrelation Method: Pearson's Product-Moment Correlation\n\n\nData: mtcars$mpg and mtcars$drat\n\n\nt, which represents the t-test statistic: 5.096.\n\ndf, which represents the degrees of freedom: 30 (n - 2).\n\nalternative hypothesis, where true correlation is not equal to 0. Therefore, the null hypothesis states true correlation is equal to 0.\n\np-value, which is the significance level of the t-test (1.776e-05) and the probability of this correlation if the null hypothesis were true.\n\n95% confidence interval or conf.int, where we are 95% confidence that the true correlation coefficient lies between [0.4360484, 0.8322010].\n\nsample estimates provides the calculated value of the correlation coefficient: 0.6811719.\n\nThe following will illustrate the output from a cor.test with kendall specified as the method, or known as the Kendall Rank Correlation Coefficient. This is typically used if the data does not satisfy the normality assumption, so we will utilize the variables: mpg and disp, where disp did not satisfy our normality assumption as indicated by the Shapiro-Wilks test.\n\ncorrKendall &lt;- cor.test(mtcars$mpg, mtcars$disp, method = \"kendall\")\n\nWarning in cor.test.default(mtcars$mpg, mtcars$disp, method = \"kendall\"):\nCannot compute exact p-value with ties\n\nprint(corrKendall)\n\n\n    Kendall's rank correlation tau\n\ndata:  mtcars$mpg and mtcars$disp\nz = -6.1083, p-value = 1.007e-09\nalternative hypothesis: true tau is not equal to 0\nsample estimates:\n       tau \n-0.7681311 \n\n\nSimilar information is provided as with the Pearson method, where the type of correlation test and alternative hypothesis is specified. However, in this non-parametric test, we are provided the z-statistic accompanied by the resulting p-value (1.007e-09). The sample estimates also provides us with the Kendall correlation coefficient (also known as tau): -0.7681311.\n\ncorrSpearman &lt;- cor.test(mtcars$mpg, mtcars$disp, method = \"spearman\")\n\nWarning in cor.test.default(mtcars$mpg, mtcars$disp, method = \"spearman\"):\nCannot compute exact p-value with ties\n\nprint(corrSpearman)\n\n\n    Spearman's rank correlation rho\n\ndata:  mtcars$mpg and mtcars$disp\nS = 10415, p-value = 6.37e-13\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n       rho \n-0.9088824 \n\n\nSimilar to both outputs above, the output from Spearman’s rank method displays the type of correlation test, the variables being tested, alternative hypothesis, as well as the S test statistic and associated p-value (6.37e-13). The sample estimate also provides us with the Spearman’s correlation coefficient (also known as rho): -0.9088824.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Correlation and Covariance Matrices</span>"
    ]
  },
  {
    "objectID": "lessons/04_corr_cov.html#brief-interpretation-of-the-output",
    "href": "lessons/04_corr_cov.html#brief-interpretation-of-the-output",
    "title": "\n22  Correlation and Covariance Matrices\n",
    "section": "\n22.7 Brief Interpretation of the Output",
    "text": "22.7 Brief Interpretation of the Output\n\n22.7.1 Interpreting the Covariance\nFrom our Covariance Matrix, the covariance between mpg and drat is 2.195064. A positive covariance indicates that when mpg is high, drat also tends to be high. The covariance between mpg and disp is -633.097208. A negative covariance indicates that when mpg is high, disp tends to be low (vice versa).\n\n22.7.2 Interpreting Correlation Coefficients\n\n\n\n\n\n\n\n-1 indicates a strong negative correlation: Each time x increases, y decreases.\n0 means that there is no association between the two variables (x and y).\n+1 indicates a strong positive correlation: Each time x increases, y increases.\n\n\n\n\nFor the Pearson correlation, the resulting p-value was 1.776e-05, which is less than our significance level alpha of 0.05. Therefore, the data provides sufficient evidence to suggest that mpg and drat are significantly correlated (0.6811719). Each time mpg increases, drat increases.\nFor the Kendall’s Tau test, the resulting p-value was 1.007e-09, which is less than our significance level alpha of 0.05. Therefore, the data provides sufficient evidence to suggest that mpg and disp are significantly correlated (-0.7681311). Each time mpg increases, disp decreases.\nFor the Spearman Rank correlation, the resulting p-value was 6.37e-13, which is less than our significance level alpha of 0.05. Therefore, the data provides sufficient evidence to suggest that mpg and disp are significantly correlated (-0.9088824). Each time mpg increases, disp decreases significantly (illustrates a strong relationship as it is close to -1).",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Correlation and Covariance Matrices</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_regression_ols.html",
    "href": "lessons_original/04_regression_ols.html",
    "title": "\n23  Ordinary Least Squares Regression\n",
    "section": "",
    "text": "23.1 Ordinary Least Squares (OLS) Regression\nOLS is a “method that allows to find a line that best describes the relationship between one or more predictor variables and a response variable”howtop?, with our end result being:\n\\[\n\\hat{y} = b_0 + b_1x\n\\]\nThe best fitting line is typically calculated utilizing the least squares, which can be visually described as the deviation in the vertical direction.\nNow, we will examine the relationship between a vehicle’s weight (WT) and its miles per gallon or fuel efficiency (MPG).\nIn a previous lesson, we found a Pearson’s correlation coefficient of r(30) = -0.868, p&lt;0.05. Based on this information, we concluded that there is a strong negative relationship between MPG and WT. Where, heavier vehicles are associated with lower miles per gallon. Essentially this means that heavier vehicles are less fuel efficient. We will use the interpretation of this correlation as the basis of building an OLS regression to predict the value of MPG for a vehicle based on its weight. An OLS regression could be described as a common method used in regression analysis due to its efficiency in fitting the best straight line through a set of points. Thus, an OLS regression model gives best approximate of true population regression line as it minimizes the total distance from all of the points to the line.\nThe OLS model could be expressed as: \\[\\hat{y}_i = \\beta_0 + \\beta_1x_i\\]\nThen the OLS regression model line for our example is:\n\\[\\widehat{MPG_i} = \\beta_0 +\\beta_1*WT_i\\]",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Ordinary Least Squares Regression</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_regression_ols.html#ordinary-least-squares-ols-regression",
    "href": "lessons_original/04_regression_ols.html#ordinary-least-squares-ols-regression",
    "title": "\n23  Ordinary Least Squares Regression\n",
    "section": "",
    "text": "The line has the following properties:\n\nThe intercept (\\(\\beta_0\\)), its measure is defined by the units in Y. In our case, the units used in MPG. It is the predicted value of Y (MPG) when X (WT) is zero\n\n\nThe slope (\\(\\beta_1\\)), is the predicted change in Y for a one-unit increase in X. Like the correlation coefficient, it provides information on the relationship between X and Y. But unlike the correlation coefficient (unitless), it highlights the relationship in real terms of units. In our example this would look at how miles per gallon increase or decrease for a one unit increase in a vehicle’s weight (according to the R docummentation for the mtcars data set, weight is provided as a measure per every 1,000 pounds and miles per gallon are provided as Miles/(US) gallon), therefore the units are defined by the Y (MPG) and the X (WT).\n\n\n\nFor example, the data for a vehicle that weighs 2,000 pounds the unit is given as “2”\nFor example, a vehicle that spends one gallon of fuel per every 19 miles is given as “19”\n\n\n\n\n\n23.1.1 How to develop the best fitting line?\n\n\nSample residual/error terms plot\n\nThe best fitting line is one that minimizes errors in prediction or one with the Minimum sum of squared residuals (SSR). For more details, you can watch this video:khanacademy2018?\n\\[\nSSR = \\sum_{i = 1}^{n}{(y_i - \\hat{y_i})^2}\n\\]\n\\(residual_i = y_i - \\hat{y_i}\\)\nIt is important to note that prior to calculating the residuals, we must visualize and examine the data, which was done in the previous example. Then, we must run the regression line. We can utilize lm() to perform the OLS regression which will provide us with the model summary, including the following:\n\nPr(&gt;|t|) Multiple R-Squared Adjusted R-Squared Residual Standard Error F-statistic P-value\n\nOnce the model summary is given, we can then move on to creating the residual plots. When performing this step, we have to check the assumptions of homoscedasticity and normality.\n* Residuals = error terms\n* $$ Residual = observed value - predicted value $$\n* The larger the error term in absolute value, the worse the prediction\n* Squaring residuals solve issues arising from some residuals being negative and some positive.\n\n\nAssumptions\n\nLinearity: Linear relationship between the dependent variable and the independent variables.\nIndependence: The observations must be independent of each other.\nHomoscedasticity: The variance of the residuals should be constant across all levels of the independent variables.\nNormality: The residuals / errors should be normally distributed.\nNo multicollinearity: In the case of multiple regression (2+ independent variables), the independent variables should not be highly correlated with each other.\n\n\n\n\n\n\n\n\n\nBe careful about outliers\n\n\n\nOutliers can influence the estimates of the relationship.\n\n\n\n23.1.2 Example of an OLS regression in R\nIn R, the lm function command allows us to develop an OLS regression.\n\n# model predicting mpg (fuel efficiency) using wt (weight)\nMPGReg &lt;- lm(mpg ~ wt, data = mtcars)\nsummary(MPGReg)\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n\nThe OLS regression model is then:\n\\(\\widehat{MPG_i} = 37.285 - 5.344*WT_i\\)\n\n\nInterpretation: \\(\\beta_0\\)\n\nThe model predicts that vehicles with no weight will have 37.285 miles per gallon, on average.\n\nThis is not a very meaningful intercept as vehicles with “0” weight do not exist. A meaningful intercept can be created by subtracting a constant from the x variable to move the intercept.In R as part of the lm command, this can be done by surrounding the independent variable with I() which applies the function inside and treats it as a new variable. For our example we used the rounded lowest weight of the data (1.5) to predict miles per gallon.\n\nThis procedure does not change the slope of the line\n\n\n\n\n\n\n# model predicting mpg (fuel efficiency) using wt (weight)\nMPGReg2 &lt;- lm(mpg ~ I(wt-1.5), data = mtcars)\nsummary(MPGReg2)\n\n\nCall:\nlm(formula = mpg ~ I(wt - 1.5), data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  29.2684     1.1008  26.589  &lt; 2e-16 ***\nI(wt - 1.5)  -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n\n\nThen, the meaningful intercept model predicts that vehicles with a weight of 1500 pounds have 29.268 miles per gallon, on average.\n\nInterpretation: \\(\\beta_1\\)\n\nThe model predicts that on average, an increase of 1,000 pounds in the weight of a vehicle is associated with a decrease of 5.344 miles per gallon.\n\n\n\n\n# define residuals \nres &lt;- resid(MPGReg)\n\n# produce residual vs. fitted plot \nplot(fitted(MPGReg), res)\n\n# add a horizontal line at 0\nabline(0,0)\n\n\n\n\n\n\n# create Q-Q- plot for residuals\nqqnorm(res)\n\n# add a straight diagonal line to the plot\nqqline(res)\n\n\n\n\n\n\n\nBased on the graph above, it is visually clear that normality may not be met due to some outliers. This means that we must explore our data even deeper as it is possible that transformation of our data utilizing one of the following methods must take place:\n\nLog transformation Square Root Transformation Cube Root Transformation\n\nOnce the data is transformed, we can run the residual plot over again in order to achieve normality. For the sake of this presentation, we are only using an example with known limitations such as non-normality.\n\n23.1.3 Hypothesis testing in OLS regression\nThe null hypothesis in this case would be that the slope is zero indicating no relationship between x and y. Or in our example, we can state that there is no relationship between a vehicle’s weight and its miles per gallon. The alternative hypothesis is then that the slope is not zero.\n\\[\nH_0: \\beta_1 = 0\n\\]\n\\[\nH_1: \\beta_1 ≠ 0\n\\]\nWe can test this hypothesis by using the lm summary printout which provides the p-value for the wt coefficient. This indicates that there is indeed a significant relationship between the weight of the car and its efficiency (miles per gallon used). R provides a t-value for the ‘wt’ coefficient which has a p-value of p &lt; 0.000 as seen below:\n                  Estimate        Std. Error       t value      Pr(&gt;|t|)   \n    wt           -5.3445             0.5591       -9.559       1.29e-10\n\n23.1.4 R-Squared Value\nR-squared is “a measure of how much of the variation in the dependent variable is explained by the independent variables in the model. It ranges from 0 to 1, with higher values indicating a better fit”ordinary?.\nAdjusted R-squared is “similar to R-squared, but it takes into account the number of independent variables in the model. It is a more conservative estimate of the model’s fit, as it penalizes the addition of variables that do not improve the model’s performance”ordinary?.\n\n23.1.5 F-Statistic\nThe F-statistic “tests the overall significance of the model by comparing the variation in the dependent variable explained by the model to the variation not explained by the model. A large F-statistic indicates that the model as a whole is significant”interpre?.\n\n23.1.6 Visual representation\nThe visual representation of this model using ggplot is the following:\n\nggplot(mtcars, aes(x = wt, y = mpg))+\n  geom_point()+\n  geom_smooth(method = \"lm\", se = FALSE)+ #se is option for coinfidence bar\n  labs(x= \"Weight (per 1,000 pounds)\",\n       y = \"Miles per gallon\")+\n  theme_bw()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nAs shown in the figure above, we see the summary statistics represented in a visual manner with the line of best fit. As indicated previously, we see a steep negative correlation between weight of the car and the miles per gallon (efficiency) utilized.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Ordinary Least Squares Regression</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_regression_ols.html#references",
    "href": "lessons_original/04_regression_ols.html#references",
    "title": "\n23  Ordinary Least Squares Regression\n",
    "section": "\n23.2 References",
    "text": "23.2 References",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Ordinary Least Squares Regression</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_regression_mls.html",
    "href": "lessons_original/04_regression_mls.html",
    "title": "\n24  Multiple Linear Regression\n",
    "section": "",
    "text": "24.1 Multiple Linear Regression\nMultiple regression generally explains the relationship between multiple independent or predictor variables and one dependent or criterion variable. A dependent variable is modeled as a function of several independent variables with corresponding coefficients, along with the constant term. Multiple regression requires two or more predictor variables.\nEquation\n\\[\n\\hat{Y}= b_0 +b_1x_1 +b_2x_2 + ... b_nx_n + \\epsilon\n\\]\nwhere\n\\(\\hat{Y}\\) is the predicted or expected value of the dependent variable, and is always numeric and continuous.\n\\(b_0\\) is the value of \\(\\hat{Y}\\) when all of the independent variables \\(x_1 -x_n\\) are equal to zero,\n\\(x_1 -x_n\\) are distinct independent or predictor variables\n\\(b_1 - b_n\\) are the estimated regression coefficients. Each regression coefficient represents the change in \\(\\hat{Y}\\) relative to a one unit change in the respective independent variable.\nIn the multiple regression situation, \\(b_1\\), for example, is the change in \\(\\hat{Y}\\) relative to a one unit change in \\(x_1\\), holding all other independent variables constant (i.e., when the remaining independent variables are held at the same value or are fixed).\nAssumptions",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_regression_mls.html#multiple-linear-regression",
    "href": "lessons_original/04_regression_mls.html#multiple-linear-regression",
    "title": "\n24  Multiple Linear Regression\n",
    "section": "",
    "text": "Independence: The Y-values are statistically independent of each other as well as the errors. As with simple linear regression, this assumption is violated if several Y observations are made on the same subject.\nLinearity: The mean value of Y for each specific combination of values of the independent variables (\\(X_1, X_2...X_n\\)) is a linear function of the intercept and parameters (\\(\\beta_0, \\beta_1,...\\))\nHomoscedasticity: The variance of Y is the same for any fixed combination of independent variables.\nNormal Distribution: The residual values are normally distributed with mean zero.\nMulticollinearity: Multicollinearity cannot exist among the predictors (the variables are not correlated)\n\n\n24.1.1 Packages\n\nlibrary(ggplot2)\nlibrary(gt)\nlibrary(car)\n\nLoading required package: carData\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n\n\n✔ broom        1.0.5      ✔ rsample      1.2.1 \n✔ dials        1.2.1      ✔ tibble       3.2.1 \n✔ dplyr        1.1.4      ✔ tidyr        1.3.1 \n✔ infer        1.0.7      ✔ tune         1.2.1 \n✔ modeldata    1.3.0      ✔ workflows    1.1.4 \n✔ parsnip      1.2.1      ✔ workflowsets 1.1.0 \n✔ purrr        1.0.2      ✔ yardstick    1.3.1 \n✔ recipes      1.0.10     \n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ dplyr::recode()  masks car::recode()\n✖ purrr::some()    masks car::some()\n✖ recipes::step()  masks stats::step()\n• Search for functions across packages at https://www.tidymodels.org/find/\n\nlibrary(lmtest)\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.5\n✔ lubridate 1.9.3     ✔ stringr   1.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ readr::col_factor() masks scales::col_factor()\n✖ purrr::discard()    masks scales::discard()\n✖ dplyr::filter()     masks stats::filter()\n✖ stringr::fixed()    masks recipes::fixed()\n✖ dplyr::lag()        masks stats::lag()\n✖ dplyr::recode()     masks car::recode()\n✖ purrr::some()       masks car::some()\n✖ readr::spec()       masks yardstick::spec()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n24.1.2 Data\nThe miles/gallon a car takes (dependent) based on the gross horsepower and weight of the car.\n\ndata(\"mtcars\")\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\n\n24.1.3 Testing the Assumptions\n\n24.1.3.1 Linearity\nIs there a linear relationship between the dependent variable and each of the independent variables?\n\n# is there a linear relationship between MPG and HP?\nggplot(data = mtcars) +\n  aes(\n    x = hp,\n    y = mpg\n  ) +\n  geom_point()\n\n\n\n\n\n\n# is there a linear relationship between MPG and WT?\nggplot(data = mtcars) +\n  aes(\n    x = wt,\n    y = mpg\n  ) +\n  geom_point()\n\n\n\n\n\n\n\n\n24.1.3.2 Multicollinearity\nAre the variables correlated? If the VIF score &gt; 10 then there is multicollinearity.\n\nCodemod &lt;- lm(mpg ~ hp + wt, data = mtcars)\n\n# checking the VIF of both predictors\nvif(mod)\n\n      hp       wt \n1.766625 1.766625 \n\nCode# checking the correlation between the predictors\ncor(mtcars$hp, mtcars$wt)\n\n[1] 0.6587479\n\n\n\n24.1.3.3 Homoscedasticity\nChecking the residual plots for homoscedasticity. The line below is relatively horizontal.\n\nmtcars_df &lt;- mtcars %&gt;% \n  mutate(res_sqrt = sqrt(abs(rstandard(mod))))\n\nggplot(mtcars_df, aes(fitted(mod), res_sqrt)) +\n  geom_point() +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n24.1.3.4 Normality\nChecking normality via histogram and QQ plot of the residuals.\n\n# histogram\nhist(mtcars_df$res_sqrt)\n\n\n\n\n\n\n# qq plot\nplot(mod, 2)\n\n\n\n\n\n\n# cook's d -&gt; are the outliers in the qq plot driving the relationship in the model? \ncooks.distance(mod)\n\n          Mazda RX4       Mazda RX4 Wag          Datsun 710      Hornet 4 Drive \n       1.589652e-02        5.464779e-03        2.070651e-02        4.724822e-05 \n  Hornet Sportabout             Valiant          Duster 360           Merc 240D \n       2.736184e-04        2.155064e-02        1.255218e-02        1.677650e-02 \n           Merc 230            Merc 280           Merc 280C          Merc 450SE \n       2.188702e-03        1.554996e-03        1.215737e-02        1.423008e-03 \n         Merc 450SL         Merc 450SLC  Cadillac Fleetwood Lincoln Continental \n       1.458960e-04        6.266049e-03        2.786686e-05        1.780910e-02 \n  Chrysler Imperial            Fiat 128         Honda Civic      Toyota Corolla \n       4.236109e-01        1.574263e-01        9.371446e-03        2.083933e-01 \n      Toyota Corona    Dodge Challenger         AMC Javelin          Camaro Z28 \n       2.791982e-02        2.087419e-02        2.751510e-02        9.943527e-03 \n   Pontiac Firebird           Fiat X1-9       Porsche 914-2        Lotus Europa \n       1.443199e-02        5.920440e-04        5.674986e-06        7.353985e-02 \n     Ford Pantera L        Ferrari Dino       Maserati Bora          Volvo 142E \n       8.919701e-03        5.732672e-03        2.720397e-01        5.600804e-03 \n\n# Plot Cook's Distance with a horizontal line at 4/n to see which observations\n# exceed this thresdhold\nn &lt;- nrow(mtcars)\nplot(cooks.distance(mod), main = \"Cooks Distance for Influential Obs\")\nabline(h = 4/n, lty = 2, col = \"steelblue\")\n\n\n\n\n\n\n\nCook’s distance refers to how far, on average, predicted y-values will move if the observation in question is dropped from the data set.\nWe can clearly see that four observation in the dataset exceed the 4/n threshold. Thus, we would identify these two observations as influential data points that have a negative impact on the regression model.\nSince there are very few extreme values according to cook’s d values, we can assume they are not the driving force between the relationship of the predictor and outcome.\nTentative model:\nmpg = \\(b_0\\) + \\(b_1\\times\\)horsepower + \\(b_2\\times\\)weight + \\(\\epsilon\\)\n\n24.1.4 Hypothesis Testing\nDoes the horsepower and weight of a car contribute significantly to the miles per gallon of the car?\n\nWhat is the relationship between horsepower and weight, with miles per gallon of the car?-\n\n\\[\nH_0: \\beta_1 = \\beta_2 = 0\n\\]\n\\[\nH_1: \\beta_1 \\not= \\beta_2 \\not= 0\n\\]\n\nCodecar_model &lt;- lm(mpg ~ hp + wt, data = mtcars_df)\n\n# get the F statistic and p-value\nsummary(car_model) \n\n\nCall:\nlm(formula = mpg ~ hp + wt, data = mtcars_df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.941 -1.600 -0.182  1.050  5.854 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 37.22727    1.59879  23.285  &lt; 2e-16 ***\nhp          -0.03177    0.00903  -3.519  0.00145 ** \nwt          -3.87783    0.63273  -6.129 1.12e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.593 on 29 degrees of freedom\nMultiple R-squared:  0.8268,    Adjusted R-squared:  0.8148 \nF-statistic: 69.21 on 2 and 29 DF,  p-value: 9.109e-12\n\nCode# creating a table of the model\ncar_model %&gt;%\n  tidy() %&gt;% \n  gt()\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n(Intercept)\n37.22727012\n1.59878754\n23.284689\n2.565459e-20\n\n\nhp\n-0.03177295\n0.00902971\n-3.518712\n1.451229e-03\n\n\nwt\n-3.87783074\n0.63273349\n-6.128695\n1.119647e-06\n\n\n\n\n\n\n\nThe first step in interpreting the multiple regression analysis is to examine the F-statistic and the associated p-value, at the bottom of model summary.\nIn our example, it can be seen that p-value of the F-statistic is &lt; 9.109e-12, which is less than 0.05. This means that we can reject the null and accept that either horsepower and/or weight of a car predict the miles per gallon.\nTo see which predictor variables are significant, we can examine the coefficients table, which shows the estimate and the associated t-statistic p-values:\n\nCodesummary(car_model)$coefficient \n\n               Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept) 37.22727012 1.59878754 23.284689 2.565459e-20\nhp          -0.03177295 0.00902971 -3.518712 1.451229e-03\nwt          -3.87783074 0.63273349 -6.128695 1.119647e-06\n\nCodesummary(car_model)$coefficient %&gt;% \n  as_tibble() %&gt;% \n  add_column(Variable = c(\"Intercept\", \"Horsepower\", \"Weight\"), .before = \"Estimate\") %&gt;% \n  gt()\n\n\n\n\n\n\nVariable\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\nIntercept\n37.22727012\n1.59878754\n23.284689\n2.565459e-20\n\n\nHorsepower\n-0.03177295\n0.00902971\n-3.518712\n1.451229e-03\n\n\nWeight\n-3.87783074\n0.63273349\n-6.128695\n1.119647e-06\n\n\n\n\n\n\n\nFull model: mpg = 37.23 + (-0.03)\\(\\times\\)horsepower + (-3.88)\\(\\times\\)weight\nFor a given predictor, the t-statistic value evaluates whether or not there is significant association between the predictor and the outcome variable, that is whether the beta coefficient of the predictor is significantly different from zero.\nIt can be seen that horsepower and weight are significantly associated to changes in mpg.\n\nCode# The confidence interval of the model\nconfint(car_model)\n\n                  2.5 %      97.5 %\n(Intercept) 33.95738245 40.49715778\nhp          -0.05024078 -0.01330512\nwt          -5.17191604 -2.58374544\n\nCodeconfint(car_model) %&gt;% \n  as_tibble() %&gt;% \n  add_column(Variable = c(\"Intercept\", \"Horsepower\", \"Weight\"), .before = \"2.5 %\") %&gt;% \n  gt()\n\n\n\n\n\n\nVariable\n2.5 %\n97.5 %\n\n\n\nIntercept\n33.95738245\n40.49715778\n\n\nHorsepower\n-0.05024078\n-0.01330512\n\n\nWeight\n-5.17191604\n-2.58374544\n\n\n\n\n\n\n\n\n24.1.5 Likelihood Ratio Test\n\\(H_0\\): The full model and the nested model fit the data equally well. Thus, you should use the nested model.\n\\(H_1\\): The full model fits the data significantly better than the nested model. Thus, you should use the full model.\nA likelihood ratio test compares the goodness of fit of two nested regression models.\nA nested model is simply one that contains a subset of the predictor variables in the overall regression model.\nWe could then carry out another likelihood ratio test to determine if a model with only one predictor variable is significantly different from a model with the two predictors:\n\n#fit full model\nmodel_full &lt;- lm(mpg ~ hp + wt, data = mtcars)\n\n#fit reduced model\nmodel_reduced &lt;- lm(mpg ~ hp, data = mtcars)\n\n#perform likelihood ratio test for differences in models\nlrtest(model_full, model_reduced)\n\nLikelihood ratio test\n\nModel 1: mpg ~ hp + wt\nModel 2: mpg ~ hp\n  #Df  LogLik Df  Chisq Pr(&gt;Chisq)    \n1   4 -74.326                         \n2   3 -87.619 -1 26.586   2.52e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#fit reduced model\nmodel_reduced &lt;- lm(mpg ~ wt, data = mtcars)\n\n#perform likelihood ratio test for differences in models\nlrtest(model_full, model_reduced)\n\nLikelihood ratio test\n\nModel 1: mpg ~ hp + wt\nModel 2: mpg ~ wt\n  #Df  LogLik Df  Chisq Pr(&gt;Chisq)    \n1   4 -74.326                         \n2   3 -80.015 -1 11.377  0.0007436 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nOur p-values are less than 0.05 for both Chi-squared values, suggesting that the full model offers significant improvement in fit over the model with just one of the predictors.\n\n24.1.6 Model Accuracy\nIn multiple linear regression, the \\(R^2\\) represents the correlation coefficient between the observed values of the outcome variable (y) and the fitted (i.e., predicted) values of y. For this reason, the value of R will always be positive and will range from zero to one.\nAn \\(R^2\\) value close to 1 indicates that the model explains a large portion of the variance in the outcome variable.\n\nCodesummary(car_model)$adj.r.squared %&gt;% \n  as_tibble() %&gt;% \n  rename(\"Adjusted R Squared\" = value) %&gt;% \n  gt()\n\n\n\n\n\n\nAdjusted R Squared\n\n\n0.8148396\n\n\n\n\n\n\nOur model explains 81% of the variation in mpg values caused by horsepower and weight values.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_regression_mls.html#conclusion",
    "href": "lessons_original/04_regression_mls.html#conclusion",
    "title": "\n24  Multiple Linear Regression\n",
    "section": "\n24.2 Conclusion",
    "text": "24.2 Conclusion\n\navPlots(car_model)\n\n\n\n\n\n\n\nFor multiple linear regression, there are multiple independent variables affecting the outcome. This outcome must be continuous, however, the independent variables can be numeric or categorical.\nThere is an inverse or negative relationship between our predictors and outcome variable. The mpg decreases with heavier weight. Higher horsepower also reduces miles per gallon performance (higher fuel consumption).",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_regression_polynomial.html",
    "href": "lessons_original/04_regression_polynomial.html",
    "title": "\n25  Introduction to Polynomial Regression\n",
    "section": "",
    "text": "25.1 What is a Polynomial Regression?\nPolynomial regression is a type of regression analysis that models the non-linear relationship between the predictor variable(s) and response variable1. It is an extension of simple linear regression that allows for more complex relationships between predictor and response variables1.\nIn simple terms, it allows us to fit a curve to our data instead of a straight line.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Introduction to Polynomial Regression</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_regression_polynomial.html#what-is-a-polynomial-regression",
    "href": "lessons_original/04_regression_polynomial.html#what-is-a-polynomial-regression",
    "title": "\n25  Introduction to Polynomial Regression\n",
    "section": "",
    "text": "25.1.1 When is a Polynomial Regression Used?\nPolynomial regression is useful when the relationship between the independent and dependent variables is nonlinear.\nIt can capture more complex relationships than linear regression, making it suitable for cases where the data exhibits curvature.\n\n25.1.2 Assumptions of Polynomial Regression\n\n\nLinearity: There is a curvilinear relationship between the independent variable(s) and the dependent variable.\n\nIndependence: The predictor variables are independent of each other.\n\nHomoscedasticity: The variance of the errors should be constant across all levels of the independent variable(s).\n\nNormality: The errors should be normally distributed with mean zero and a constant variance.\n\n25.1.3 Mathematical Equation\nConsider independent samples \\(i = 1, \\ldots, n\\). The general formula for a polynomial regression representing the relationship between the response variable (\\(y\\)) and the predictor variable (\\(x\\)) as a polynomial function of degree \\(d\\) is:\n\\[\ny_i = \\beta_0 + \\beta_1x_i + \\beta_2x_i^2 + \\beta_3x_i^3 + ... \\beta_dx_i^d + \\epsilon_i,\n\\]\nwhere:\n\n\n\\(y_i\\) represents the response variable,\n\n\\(x_i\\) represents the predictor variable,\n\n\\(\\beta_0,\\ \\beta_1,\\ \\ldots,\\ \\beta_d\\) are the coefficients to be estimated, and\n\n\\(\\epsilon_i\\) represents the errors.\n\nFor large degree \\(d\\), polynomial regression allows us to produce an extremely non-linear curve. Therefore, it is not common to use \\(d &gt; 3\\) because the larger value of \\(d\\), the more overly flexible polynomial curve becomes, which can lead to overfitting them model to the data.\nThe coefficients in polynomial function can be estimated using least square linear regression because it can be viewed as a standard linear model with predictors \\(x_i, \\,x_i^2, \\,x_i^3, ..., x_i^d\\). Hence, polynomial regression is also known as polynomial linear regression.\n\n25.1.4 Performing a Polynomial Regression in R\n\nStep 0: Load required packages\nStep 1: Load and inspect the data\nStep 2: Visualize the data\nStep 3: Fit the model\nStep 4: Assess Assumptions\nStep 5: Describe model output",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Introduction to Polynomial Regression</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_regression_polynomial.html#lets-practice",
    "href": "lessons_original/04_regression_polynomial.html#lets-practice",
    "title": "\n25  Introduction to Polynomial Regression\n",
    "section": "\n25.2 Let’s Practice!",
    "text": "25.2 Let’s Practice!\nNow let’s go through the steps to perform a polynomial regression in R. We’ll be using the lm() function to fit the polynomial regression model. This function comes standard in base R.\n\n25.2.1 Hypotheses\nFor this example, we are investigating the following:\n\n\nResearch Question: Is there a significant quadratic relationship between the weight of a car (wt) and its miles per gallon (mpg) in the mtcars dataset?\n\nNull hypothesis (\\(H_0\\)): There is no significant relationship between the weight of a car (wt) and its miles per gallon (mpg).\n\nAlternative hypothesis (\\(H_A\\)): There is a significant relationship between the weight of a car (wt) and its miles per gallon (mpg).\n\nIn this case, the null hypothesis assumes that the coefficients of the quadratic polynomial terms are zero, indicating no relationship between the weight of the car and miles per gallon. The alternative hypothesis, on the other hand, suggests that at least one of the quadratic polynomial terms is non-zero, indicating a significant relationship between the weight of the car and miles per gallon.\nBy performing the polynomial regression analysis and examining the model summary and coefficients, we can evaluate the statistical significance of the relationship and determine whether to reject or fail to reject the null hypothesis.\n\n25.2.2 Step 0: Install and load required package\nIn R, we’ll use the lm() function from the base package to perform polynomial regression. Also, since we want to visualize our data, we will be loading the ggplot2 package for use.\n\n# For data visualization purposes\n# install.packages(\"ggplot2\")\nlibrary(ggplot2)\n\n\n25.2.3 Step 1: Load and inspect the data\nFor this example, we will use the built-in mtcars dataset (from the standard R package datasets) which is publicly available and contains information about various car models.\n\n# Load mtcars dataset\ndata(mtcars)\n\n\n# Print the first few rows\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\n\n25.2.4 Step 2: Visualize the data\nBefore fitting a polynomial regression model, it’s helpful to visualize the data to identify any non-linear patterns. For our example, we will use a scatter plot to visualize the relationship between the independent and dependent variables:\n\n# Scatter plot of mpg (dependent variable) vs. wt (independent variable)\nggplot(mtcars) +\n  theme_minimal() +\n  aes(x = wt, y = mpg) + \n  labs(x = \"Weight (lbs/1000)\", y = \"Miles per Gallon\") +\n  geom_point()\n\n\n\n\n\n\n\n\n25.2.5 Step 3: Fit Models\nLet’s create a function so we can build multiple models. We will build a standard linear model and a quadratic model (degrees 1 and 2, respectively).\n\n# Function to fit and evaluate polynomial regression models\nfit_poly_regression &lt;- function(degree) {\n  formula &lt;- as.formula(paste(\"mpg ~ poly(wt, \", degree, \")\"))\n  lm(formula, data = mtcars)\n}\n\n# Fit polynomial regression models with degrees 1 to 2\nmodel_1 &lt;- fit_poly_regression(1)\nmodel_2 &lt;- fit_poly_regression(2)\n\nTo fit a polynomial regression model, we’ll use the lm() function and create polynomial terms using the poly() function. In this example, we’ll fit a standard linear (degree = 1) and a quadratic polynomial (degree = 2) to the mtcars dataset.\n\n25.2.6 Step 4: Assess Assumptions\nBefore we can interpret the model, we have to check the assumptions. We will check these assumptions via plots:\n\nResiduals vs. Fitted values (used to check the linearity assumption),\na Q-Q plot of the Residuals (used to check the normality of the residuals),\na Scale-Location plot (used to check for heteroskedasticity), and\nResiduals vs. Leverage values (identifies overly influential values, if any exist).\n\n\npar(mfrow = c(2, 2))\nplot(model_1, which = c(1, 2, 3, 5))\n\n\n\n\n\n\nplot(model_2, which = c(1, 2, 3, 5))\n\n\n\n\n\n\npar(mfrow = c(1, 1))\n\nWe see that Model 2 (the quadratic one) satisfies the “linearity” assumption, because the red line in the “Residuals vs Fitted” graph is flat. However, the Q-Q plot shows that the residuals are not normally distributed, so we should take additional steps to transform the response feature (such as via a square root or log transformation, or something similar).\n\n25.2.7 Step 5. Describe Model Output\nAlthough we recognize that this model is not correct (because the residuals are not approximately normal), we will give an example of how to interpret this output.\n\nsummary(model_2)\n\n\nCall:\nlm(formula = formula, data = mtcars)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.483 -1.998 -0.773  1.462  6.238 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   20.0906     0.4686  42.877  &lt; 2e-16 ***\npoly(wt, 2)1 -29.1157     2.6506 -10.985 7.52e-12 ***\npoly(wt, 2)2   8.6358     2.6506   3.258  0.00286 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.651 on 29 degrees of freedom\nMultiple R-squared:  0.8191,    Adjusted R-squared:  0.8066 \nF-statistic: 65.64 on 2 and 29 DF,  p-value: 1.715e-11\n\n\n\n25.2.8 Bonus Step: Visualize the Final Model\nFinally, let’s plot the scatter plot with the polynomial regression line to visualize the fit:\n\n# Create a data frame with data points and predictions \nplot_data &lt;- data.frame(\n  wt = mtcars$wt,\n  mpg = mtcars$mpg, \n  mpg_predicted = predict(model_2, newdata = mtcars)\n)\n\n# Scatter plot with the polynomial regression line\nggplot(plot_data) +\n  theme_minimal() + \n  aes(x = wt, y = mpg) + \n  labs(\n    title = \"Scatter Plot with Polynomial Regression Line\",\n    x = \"Weight (wt)\",\n    y = \"Miles per Gallon (mpg)\"\n  ) +\n  geom_point() +\n  geom_line(aes(y = mpg_predicted), color = \"red\", size = 1)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Introduction to Polynomial Regression</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_regression_polynomial.html#further-discussion",
    "href": "lessons_original/04_regression_polynomial.html#further-discussion",
    "title": "\n25  Introduction to Polynomial Regression\n",
    "section": "\n25.3 Further Discussion",
    "text": "25.3 Further Discussion\n\n\nPiecewise polynomials: Instead of fitting a high-degree polynomial over the entire range of X, piece- wise polynomial regression involves fitting separate low-degree polynomials over different regions of X. The coefficients βi differ in different parts of the range of X. The points where the coefficients change are called knots. Using more knots leads to a more flexible piecewise polynomial2.\n\nConstraints and spline: the technique of reduce the number of degree of freedom on piecewise polynomial to produce a continuous and naturally smooth fit model on data2.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Introduction to Polynomial Regression</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_regression_polynomial.html#references",
    "href": "lessons_original/04_regression_polynomial.html#references",
    "title": "\n25  Introduction to Polynomial Regression\n",
    "section": "\n25.4 References",
    "text": "25.4 References\n\nField, A. (2013). Discovering Statistics Using IBM SPSS Statistics. (4th ed.). Sage Publications.\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An Introduction to Statistical Learning with Applications in R. (2nd ed.). Publisher. (pp. 290-300)",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Introduction to Polynomial Regression</span>"
    ]
  },
  {
    "objectID": "05_header_generalized-linear-models.html",
    "href": "05_header_generalized-linear-models.html",
    "title": "Generalized Linear Models",
    "section": "",
    "text": "Text Outline\nThis text covers the basics of:",
    "crumbs": [
      "Generalized Linear Models"
    ]
  },
  {
    "objectID": "05_header_generalized-linear-models.html#text-outline",
    "href": "05_header_generalized-linear-models.html#text-outline",
    "title": "Generalized Linear Models",
    "section": "",
    "text": "Exploring data\nOne-sample tests\nTwo-sample tests\nANOVA and linear regression\nGeneralized linear models\nSome special topics\nMaybe a bit about power calculations…",
    "crumbs": [
      "Generalized Linear Models"
    ]
  },
  {
    "objectID": "05_header_generalized-linear-models.html#part-outline",
    "href": "05_header_generalized-linear-models.html#part-outline",
    "title": "Generalized Linear Models",
    "section": "Part Outline",
    "text": "Part Outline\nThis part of the text contains the chapters on various different models within the GLM family:\n\nGeneralized Linear Models: Binary\nGeneralized Linear Models: Ordered\nGeneralized Linear Models: Count (Poisson)\nGeneralized Linear Models: Count (Negative Binomial)",
    "crumbs": [
      "Generalized Linear Models"
    ]
  },
  {
    "objectID": "lessons_original/05_glm_ordinal_logistic.html",
    "href": "lessons_original/05_glm_ordinal_logistic.html",
    "title": "\n26  Ordinal Logistic Regression\n",
    "section": "",
    "text": "26.1 Introduction to Logistic Regression\nWe are all familiar with the concept of Logistic regression. It is used to analyze data when the outcome variables is categorical. There are three types of logistic regression, Binary logistic regression where the outcome variable is binary (Yes/No), Multinomial logistic regression when the outcome variable is categorical with three or more categories, Ordinal logistic regression where there is a natural ordering among three or more categories of the outcome variableagresti2002?.",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Ordinal Logistic Regression</span>"
    ]
  },
  {
    "objectID": "lessons_original/05_glm_ordinal_logistic.html#introduction-to-logistic-regression",
    "href": "lessons_original/05_glm_ordinal_logistic.html#introduction-to-logistic-regression",
    "title": "\n26  Ordinal Logistic Regression\n",
    "section": "",
    "text": "Types of Logistic Regression\n\n\nBinary LR\nMultinomial LR\nOrdinal LR\n\n\n\nNumber of categories?\nTwo\nThree or more\nThree or more\n\n\nOrdering matters?\nNo\nNo\nYes",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Ordinal Logistic Regression</span>"
    ]
  },
  {
    "objectID": "lessons_original/05_glm_ordinal_logistic.html#what-is-ordinal-logistic-regression",
    "href": "lessons_original/05_glm_ordinal_logistic.html#what-is-ordinal-logistic-regression",
    "title": "\n26  Ordinal Logistic Regression\n",
    "section": "\n26.2 What is Ordinal Logistic Regression?",
    "text": "26.2 What is Ordinal Logistic Regression?\nOrdinal logistic regression is a statistical modeling technique used to investigate relationships between predictor variables and ordered ordinal outcome variables. It extends traditional logistic regression to account for the response variable’s inherent ordering, making it suitable for situations where the outcome has multiple levels with unequal intervals.\nFor example, cases when ordinal logistic regression can be applicable are,\n\nLikelihood of agreement : In a survey the responses to the outcome variable is categorized in multiple levels such as, Strongly Disagree, Disagree, Agree, Strongly Agree.\nSatisfaction level: Measuring satisfaction level of a service on a scale like, “very dissatisfied,” “dissatisfied,” “neutral,” “satisfied,” and “very satisfied.”\nPain Intensity: Patients participating in medical research may be asked to rate the intensity of their pain on a scale ranging from “no pain” to “mild pain,” “moderate pain,” and “severe pain.”",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Ordinal Logistic Regression</span>"
    ]
  },
  {
    "objectID": "lessons_original/05_glm_ordinal_logistic.html#how-to-do-ordinary-logistic-regression-in-r",
    "href": "lessons_original/05_glm_ordinal_logistic.html#how-to-do-ordinary-logistic-regression-in-r",
    "title": "\n26  Ordinal Logistic Regression\n",
    "section": "\n26.3 How to do Ordinary Logistic Regression in R",
    "text": "26.3 How to do Ordinary Logistic Regression in R\nSome popular R packages that perform Ordinal/Ordered Logistic Regression are,\n\n\nMASS package : function polr()\n\n\nordinal package: function clm()\n\n\nrms package: function orm()\n\n\nIn this demonstration I will be using polr() from MASS package to conduct the analysis.",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Ordinal Logistic Regression</span>"
    ]
  },
  {
    "objectID": "lessons_original/05_glm_ordinal_logistic.html#mathematical-formulation-of-a-ordinal-model",
    "href": "lessons_original/05_glm_ordinal_logistic.html#mathematical-formulation-of-a-ordinal-model",
    "title": "\n26  Ordinal Logistic Regression\n",
    "section": "\n26.4 Mathematical Formulation of a Ordinal model",
    "text": "26.4 Mathematical Formulation of a Ordinal model\nLet us assume Y is an outcome variable with levels, \\(l = 1, 2, ... , L\\). According to the MASS package the parameterization of the outcome variable Y with l levels is,\n\\[\n\\ln\\left(\\frac{P(Y\\le l)}{P(Y&gt;l)}\\right) =\n  \\zeta - \\eta_{1}X_{1}- \\eta_{2}X_{2} - \\ldots - \\eta_{k}X_{k}\n\\]\nHere,\n\n\n\\(\\zeta\\) is the intercept representing the log-odds of \\(Y\\) being less than or equal to \\(l\\) when the other covariates are 0 or in there reference level. Ordinal logistic regression model has one intercept for each level of Y and the total number of intercepts is \\(L-1\\).\nIn case of categorical predictors, each coefficient \\(\\eta_{k}\\) is the log of odds ratio comparing the odds of \\(Y\\le l\\) at a level compared to the reference category. Taking exponent of this term we get \\(e^{\\eta_{k}}\\) which is the odds ratio comparing the odds of \\(Y\\le l\\) at a level compared to the reference category.\nIn case of continuous predictors, each coefficient \\(\\eta_{k}\\) is the log of odds ratio comparing the odds of \\(Y\\le l\\) between subjects who differ by 1 unit. Taking exponent of this term we get \\(e^{\\eta_{k}}\\) which is the odds ratio comparing the odds of \\(Y\\le l\\) between subjects who differ by 1 unit.\n\nSimilar to binary logistic regression the left hand side of this equation is the log-odds of a probability. In case of binary logistic regression it is log-odds of probability of an event whereas here we consider the cumulative probability up to and a specified level including that level.\n\n26.4.1 Model Assumptions\nThe key assumptions of Ordinary logistic Regression which ensures the validity of the model are as follows,\n\nThe outcome variable is ordered.\nThe predictor variables are either continuous, categorical, or ordinal.\nThere is no multicollinearity among the predictors.\nProportional odds.",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Ordinal Logistic Regression</span>"
    ]
  },
  {
    "objectID": "lessons_original/05_glm_ordinal_logistic.html#example",
    "href": "lessons_original/05_glm_ordinal_logistic.html#example",
    "title": "\n26  Ordinal Logistic Regression\n",
    "section": "\n26.5 Example",
    "text": "26.5 Example\nTo demonstrate the methods I will be using the arthritis data from multgee package. The data has Rheumatoid self-assessment scores for 302 patients, measured on a five-level ordinal response scale at three follow-up times. The arthritis dataset is in a data frame with 906 observations with the following 7 variables:\n\n\nid: Patient identifier variable.\n\ny: Self-assessment score of rheumatoid arthritis measured on a five-level ordinal response scale, 1 being the lowest.\n\nsex: Coded as (1) for female and (2) for male.\n\nage: Recorded at the baseline.\n\ntrt: Treatment group variable, coded as (1) for the placebo group and (2) for the drug group.\n\nbaseline: Self-assessment score of rheumatoid arthritis at the baseline.\n\ntime: Follow-up time recorded in months.\n\n\n26.5.1 Libraries\nHere are libraries required to run the analysis.\n\n# install.packages(\"multgee\")\n# install.packages(\"pander\")\n# install.packages(\"table1\")\n# install.packages(\"car\")\n# install.packages(\"mltools\")\n# install.packages(\"pomcheckr\")\nlibrary(conflicted)\nlibrary(table1)\nlibrary(multgee)\nlibrary(skimr)\nlibrary(pander)\nlibrary(gtsummary)\nlibrary(car)\nlibrary(mltools)\nlibrary(MASS)\nlibrary(pomcheckr)\n\nconflict_prefer(\"filter\", \"dplyr\", quiet = TRUE)\nconflict_prefer(\"select\", \"dplyr\", quiet = TRUE)\nlibrary(tidyverse)\n\n\n26.5.1.1 Warning\nInstead of installing package MASS to the global environment use MASS::polr() for running the Ordinal Logistic Regression model. As masking it conflicts wirh the select() function for tidyverse and gtsummary().\n\n26.5.2 Exploring data\nLet’s begin by looking at the data.\n\narthritis_df &lt;- \n  multgee::arthritis %&gt;%\n  mutate(\n    y = factor(y, ordered = TRUE),\n    sex = factor(\n      sex,\n      levels = c(1, 2),\n      labels = c(\"Female\", \"Male\")\n    ),\n    treatment = factor(\n      trt,\n      levels = c(\"1\", \"2\"),\n      labels = c(\"Placebo\", \"Drug\")\n    ),\n    baseline = factor(baseline, ordered = TRUE)\n  ) %&gt;%\n  select(\"y\", \"sex\", \"age\", \"treatment\", \"baseline\") %&gt;%\n  drop_na() %&gt;% \n  as_tibble()\n\n\n26.5.2.1 Summary\n\nskim(arthritis_df)\n\n\nData summary\n\n\nName\narthritis_df\n\n\nNumber of rows\n888\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n4\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\ny\n0\n1\nTRUE\n5\n3: 345, 4: 275, 2: 159, 5: 76\n\n\nsex\n0\n1\nFALSE\n2\nMal: 645, Fem: 243\n\n\ntreatment\n0\n1\nFALSE\n2\nDru: 445, Pla: 443\n\n\nbaseline\n0\n1\nTRUE\n5\n3: 407, 2: 215, 4: 166, 1: 67\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\nage\n0\n1\n50.43\n11.09\n21\n42\n54\n60\n66\n▁▃▃▇▇\n\n\n\n\n\n26.5.2.2 Descriptives\n\narthritis_df %&gt;% \n  tbl_summary(by = treatment) \n\n\nTable 26.1: Predictors by treatment group\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nPlacebo, N = 4431\n\n\nDrug, N = 4451\n\n\n\n\ny\n\n\n\n\n    1\n26 (5.9%)\n7 (1.6%)\n\n\n    2\n96 (22%)\n63 (14%)\n\n\n    3\n165 (37%)\n180 (40%)\n\n\n    4\n129 (29%)\n146 (33%)\n\n\n    5\n27 (6.1%)\n49 (11%)\n\n\nsex\n\n\n\n\n    Female\n127 (29%)\n116 (26%)\n\n\n    Male\n316 (71%)\n329 (74%)\n\n\nage\n55 (42, 60)\n53 (42, 59)\n\n\nbaseline\n\n\n\n\n    1\n33 (7.4%)\n34 (7.6%)\n\n\n    2\n105 (24%)\n110 (25%)\n\n\n    3\n207 (47%)\n200 (45%)\n\n\n    4\n83 (19%)\n83 (19%)\n\n\n    5\n15 (3.4%)\n18 (4.0%)\n\n\n\n\n1 n (%); Median (IQR)\n\n\n\n\n\n\n\n\n\n\n26.5.2.3 Plotting Outcome variable (rheumatoid arthritis score)\n\narthritis_df %&gt;% \n  count(y) %&gt;% \n  mutate(prop = n / sum(n)) %&gt;% \n  rename(score = y) %&gt;% \n  ggplot() + \n    aes(x = score, y = prop) +\n    labs(\n      x = \"Rheumatoid Arthritis Score\", \n      y = \"Relative Frequencies (w Obs. Count)\"\n    ) +\n    scale_y_continuous(labels = scales::percent) +\n    geom_col() +\n    geom_text(aes(label = n), vjust = 1.5, color = \"white\")\n\n\n\n\n\n\n\n\n26.5.2.4 Pairs\n\nGGally::ggpairs(arthritis_df)\n\n\n\n\n\n\n\n\n26.5.3 How to use polr()\n\nThe basic structure of the function looks like this (there are other options we don’t list, but we won’t need them):\n\npolr(\n  # Two required arguments\n  formula,\n  data,\n  # Optional stuff\n  weights,\n  subset,\n  na.action,\n  Hess = FALSE,\n  method = \"logistic\"\n)\n\nHere,\n\n\nformula: a formula expression as for regression models, of the form response ~ predictors. The response should be a factor (preferably an ordered factor), which will be interpreted as an ordinal response, with levels ordered as in the factor.\n\ndata: a data frame which contains the variables occurring in formula.\n\nweights: optional case weights in fitting. Defaults to 1.\n\nsubset: expression saying which subset of the rows of the data should be used in the fit. All observations are included by default.\n\nna.action: a function to filter missing data. We removed all the missing values from our data, so we won’t use this.\n\nHess: logical for whether the Hessian (the observed information matrix) should be returned. We will use this if we intend to call summary or variance covariance on the fit.\n\nmethod: \"logistic\", \"probit\", \"loglog\" (log-log), \"cloglog\" (complementary log-log), or \"cauchit\" (corresponding to a Cauchy latent variable). The default option is to use the Logistic link function.\n\n26.5.4 Fitting the model\nUsing this function, let’s fit the POLR model to the data. By default, the Hess option is turned off, so we turn it on so that we can calculate the odds ratios later.\n\nfit_olr_mod &lt;- MASS::polr(y ~ ., data = arthritis_df, Hess = TRUE)\n\nThe output is a bit ugly, so we clean it up for more professional documents using the pander() function (from the pander:: and knitr:: packages).\n\npander(summary(fit_olr_mod))\n\nCall: MASS::polr(formula = y ~ ., data = arthritis_df, Hess = TRUE)\n\nCoeficients\n\n\n\n\n\n\n\n \nValue\nStd. Error\nt value\n\n\n\nsexMale\n0.1513\n0.1377\n1.099\n\n\nage\n-0.01366\n0.005713\n-2.391\n\n\ntreatmentDrug\n0.5454\n0.1255\n4.347\n\n\nbaseline.L\n3.109\n0.2826\n11\n\n\nbaseline.Q\n0.6897\n0.233\n2.96\n\n\nbaseline.C\n0.09577\n0.1796\n0.5334\n\n\nbaseline^4\n-0.1802\n0.1239\n-1.455\n\n\n\n\nIntercepts\n\n\n\n\n\n\n\n \nValue\nStd. Error\nt value\n\n\n\n1|2\n-4.244\n0.3731\n-11.38\n\n\n2|3\n-2.162\n0.339\n-6.378\n\n\n3|4\n-0.2073\n0.3352\n-0.6186\n\n\n4|5\n2.094\n0.3429\n6.108\n\n\n\nResidual Deviance: 2238.917\nAIC: 2260.917\n\n\nThe summary() function called on a polr model object gives us the coefficients, intercepts, their standard errors, and \\(t\\)-statistics.\n\n26.5.5 Odds Ratio\nIn order to get the Odds Ratio and the predictor’s confidence intervals we take the exponential of the coefficient. There is no straight forward way of doing that in R. Below is one way of solving that issue, which uses the confint() function from the multgee:: package.\n\n# Calculate a matrix of the lower and upper confidence intervals\nCI_mat &lt;- confint(fit_olr_mod)\n\n# Combine results and make them \"pretty\"\norResults_df &lt;- tibble(\n  variable = rownames(CI_mat),\n  oddsRatio = exp(fit_olr_mod$coefficients),\n  lower = exp(CI_mat[, 1]),\n  upper = exp(CI_mat[, 2])\n)\n\npander(orResults_df)\n\n\n\n\n\n\n\n\n\nvariable\noddsRatio\nlower\nupper\n\n\n\nsexMale\n1.163\n0.8883\n1.524\n\n\nage\n0.9864\n0.9754\n0.9975\n\n\ntreatmentDrug\n1.725\n1.35\n2.208\n\n\nbaseline.L\n22.41\n12.94\n39.27\n\n\nbaseline.Q\n1.993\n1.266\n3.161\n\n\nbaseline.C\n1.101\n0.7745\n1.567\n\n\nbaseline^4\n0.8351\n0.6548\n1.064\n\n\n\n\n\n\n26.5.6 Interpreting the Results\n\n\nSex: Compared to female participants, Male participants had orResults_df[1, \"oddsRatio\", drop = TRUE] fold higher odds of reporting high score of rheumatoid arthritis.\n\nAge: For 1 year change in age the odds of reporting high rheumatoid arthritis score changes orResults_df[2, \"oddsRatio\", drop = TRUE] times.\n\nTreatment: Compared to the Placebo group participants, the participant who received the drug had orResults_df[3, \"oddsRatio\", drop = TRUE] times higher odds of reporting high score of rheumatoid arthritis.\n\nBaseline score: It appears that we could perhaps collapse the baseline rheumatoid arthritis score into only three levels, because the confidence intervals for the cubic and quartic polynomial components include 1.\n\n26.5.7 Checking Assumptions\nNext we check the key assumptions to verify whether the model is appropriate to use.\n\n26.5.7.1 Multicollinearity\nTwo of our predictors are binary, one predictor is continuous, and one predictor and the response are ordered. Because of this, there are no “standard” functions to calculate the correlation matrix of these predictors. However, we will use indicator encoding to transform the binary predictors, and we will use polynomial encoding to transform the ordered predictor (polynomial encoding represents ordered predictors as a set of polynomial terms with \\(G-1\\) components, where \\(G\\) is the number of categories). Both actions were already done “behind the scenes” by the polr() function, so we simply need to access the “model matrix” object. We do this via the model.matrix() function, but we remove the first column because it represents the intercept.\n\n# Extract the encoded features used by the POLR model, dropping the intercept\nmodel.matrix(fit_olr_mod)[, -1] %&gt;% \n  # calculate the correlation matrix of the predictors (using the \"spearman\" \n  #   option because some of the predictors are binary)\n  cor(method = \"spearman\")\n\n                   sexMale          age treatmentDrug   baseline.L  baseline.Q\nsexMale        1.000000000 -0.004979624   0.029167450 -0.028388501  0.02835442\nage           -0.004979624  1.000000000  -0.041135132 -0.113609702  0.06570578\ntreatmentDrug  0.029167450 -0.041135132   1.000000000 -0.003582072  0.01949695\nbaseline.L    -0.028388501 -0.113609702  -0.003582072  1.000000000 -0.19856768\nbaseline.Q     0.028354417  0.065705778   0.019496947 -0.198567680  1.00000000\nbaseline.C    -0.045464607  0.090816675   0.009951240 -0.573972635 -0.02112136\nbaseline^4    -0.036878173 -0.043472765  -0.015992296  0.306681433 -0.78207212\n               baseline.C  baseline^4\nsexMale       -0.04546461 -0.03687817\nage            0.09081667 -0.04347276\ntreatmentDrug  0.00995124 -0.01599230\nbaseline.L    -0.57397264  0.30668143\nbaseline.Q    -0.02112136 -0.78207212\nbaseline.C     1.00000000 -0.31490991\nbaseline^4    -0.31490991  1.00000000\n\n\nThe correlation is quite low among most of the predictors. However, we see that the quadratic and quartic (.Q and ^4, respectively) components are just under the “let’s worry about this” threshold of 0.8 in absolute value. This suggests to me that the highest two categories of the rheumatoid arthritis score at baseline (4 and 5) can probably be collapsed without losing a lot of information. However, we’ll keep things simple for now and say there is no multicollinearity.\n\n26.5.7.2 Proportional Odds\nOrdinal logistic regression makes the assumption that the relationship between each pair of outcome groups is the same. In other words, ordinal logistic regression assumes that the coefficients describing the relationship between, say, the lowest and all higher categories of the response variable are the same as those describing the relationship between the next lowest and all higher categories, and so on. This assumption can be vefied several ways. Here, I have used a package calledpomcheckr? that generates graphics to check for proportional odds assumption created by UCLA statistical consulting group see more here .\n\n26.5.7.2.1 Graphics to check for proportional odds\n\npomchk &lt;- pomcheck(\n  y ~ sex + age + treatment + baseline,\n  data = arthritis_df\n)\nplot(pomchk)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere the function is calculating the difference in proportion of the categories in the outcome variable and plotting them against each category of the predictors. In the ideal case scenario, the distance between the dots in each line is somewhat equal; if this is true, then the categories should be considered proportional. It appears that the proportional odds assumption is violated here.\nAdd discussion on how to fix the assumption violation.",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Ordinal Logistic Regression</span>"
    ]
  },
  {
    "objectID": "lessons_original/05_glm_ordinal_logistic.html#references",
    "href": "lessons_original/05_glm_ordinal_logistic.html#references",
    "title": "\n26  Ordinal Logistic Regression\n",
    "section": "References",
    "text": "References",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Ordinal Logistic Regression</span>"
    ]
  },
  {
    "objectID": "06_header_special-topics.html",
    "href": "06_header_special-topics.html",
    "title": "Special Topics in Statistical Modelling",
    "section": "",
    "text": "Text Outline\nThis text covers the basics of:",
    "crumbs": [
      "Special Topics in Statistical Modelling"
    ]
  },
  {
    "objectID": "06_header_special-topics.html#text-outline",
    "href": "06_header_special-topics.html#text-outline",
    "title": "Special Topics in Statistical Modelling",
    "section": "",
    "text": "Exploring data\nOne-sample tests\nTwo-sample tests\nANOVA and linear regression\nGeneralized linear models\nSome special topics\nMaybe a bit about power calculations…",
    "crumbs": [
      "Special Topics in Statistical Modelling"
    ]
  },
  {
    "objectID": "06_header_special-topics.html#part-outline",
    "href": "06_header_special-topics.html#part-outline",
    "title": "Special Topics in Statistical Modelling",
    "section": "Part Outline",
    "text": "Part Outline\nThis part of the text contains the chapters on various different special statistical models:\n\nLinear Mixed Effects Models\nStructural Equation Models\nCox Proportional Hazards Regression\n(TBD) Multivariate Methods for Genetics/Genomics\nRidge, LASSO, and Elastic Net Regression",
    "crumbs": [
      "Special Topics in Statistical Modelling"
    ]
  },
  {
    "objectID": "lessons/06_lesson_template_spec_topc.html",
    "href": "lessons/06_lesson_template_spec_topc.html",
    "title": "\n27  The Method\n",
    "section": "",
    "text": "27.1 Introduction to &lt;the method&gt;",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>The Method</span>"
    ]
  },
  {
    "objectID": "lessons/06_lesson_template_spec_topc.html#mathematical-definition-of-the-method",
    "href": "lessons/06_lesson_template_spec_topc.html#mathematical-definition-of-the-method",
    "title": "\n27  The Method\n",
    "section": "\n27.2 Mathematical definition of <the method>",
    "text": "27.2 Mathematical definition of &lt;the method&gt;",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>The Method</span>"
    ]
  },
  {
    "objectID": "lessons/06_lesson_template_spec_topc.html#data-source-and-description",
    "href": "lessons/06_lesson_template_spec_topc.html#data-source-and-description",
    "title": "\n27  The Method\n",
    "section": "\n27.3 Data source and description",
    "text": "27.3 Data source and description",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>The Method</span>"
    ]
  },
  {
    "objectID": "lessons/06_lesson_template_spec_topc.html#cleaning-the-data-to-create-a-model-data-frame",
    "href": "lessons/06_lesson_template_spec_topc.html#cleaning-the-data-to-create-a-model-data-frame",
    "title": "\n27  The Method\n",
    "section": "\n27.4 Cleaning the data to create a model data frame",
    "text": "27.4 Cleaning the data to create a model data frame",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>The Method</span>"
    ]
  },
  {
    "objectID": "lessons/06_lesson_template_spec_topc.html#assumptions-of-the-method",
    "href": "lessons/06_lesson_template_spec_topc.html#assumptions-of-the-method",
    "title": "\n27  The Method\n",
    "section": "\n27.5 Assumptions of <the method>",
    "text": "27.5 Assumptions of &lt;the method&gt;",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>The Method</span>"
    ]
  },
  {
    "objectID": "lessons/06_lesson_template_spec_topc.html#checking-the-assumptions-with-plots",
    "href": "lessons/06_lesson_template_spec_topc.html#checking-the-assumptions-with-plots",
    "title": "\n27  The Method\n",
    "section": "\n27.6 Checking the assumptions with plots",
    "text": "27.6 Checking the assumptions with plots",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>The Method</span>"
    ]
  },
  {
    "objectID": "lessons/06_lesson_template_spec_topc.html#code-to-run-the-method",
    "href": "lessons/06_lesson_template_spec_topc.html#code-to-run-the-method",
    "title": "\n27  The Method\n",
    "section": "\n27.7 Code to run <the method>",
    "text": "27.7 Code to run &lt;the method&gt;",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>The Method</span>"
    ]
  },
  {
    "objectID": "lessons/06_lesson_template_spec_topc.html#code-output",
    "href": "lessons/06_lesson_template_spec_topc.html#code-output",
    "title": "\n27  The Method\n",
    "section": "\n27.8 Code output",
    "text": "27.8 Code output\nNOTE: this section will be created automatically by the Quarto document. You should not create a section specifically for this. When you run the code in the previous section, you will get the output automatically.",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>The Method</span>"
    ]
  },
  {
    "objectID": "lessons/06_lesson_template_spec_topc.html#brief-interpretation-of-the-output",
    "href": "lessons/06_lesson_template_spec_topc.html#brief-interpretation-of-the-output",
    "title": "\n27  The Method\n",
    "section": "\n27.9 Brief interpretation of the output",
    "text": "27.9 Brief interpretation of the output",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>The Method</span>"
    ]
  },
  {
    "objectID": "07_header_power.html",
    "href": "07_header_power.html",
    "title": "Statistical Power and Sample Size Determination",
    "section": "",
    "text": "Text Outline\nThis text covers the basics of:",
    "crumbs": [
      "Statistical Power and Sample Size Determination"
    ]
  },
  {
    "objectID": "07_header_power.html#text-outline",
    "href": "07_header_power.html#text-outline",
    "title": "Statistical Power and Sample Size Determination",
    "section": "",
    "text": "Exploring data\nOne-sample tests\nTwo-sample tests\nANOVA and linear regression\nGeneralized linear models\nSome special topics\nMaybe a bit about power calculations…",
    "crumbs": [
      "Statistical Power and Sample Size Determination"
    ]
  },
  {
    "objectID": "07_header_power.html#part-outline",
    "href": "07_header_power.html#part-outline",
    "title": "Statistical Power and Sample Size Determination",
    "section": "Part Outline",
    "text": "Part Outline\nThis part of the text will eventually contain some examples on statistical power calculations and sample size determination methods for some of the techniques covered in this text.",
    "crumbs": [
      "Statistical Power and Sample Size Determination"
    ]
  },
  {
    "objectID": "lessons_original/07_power_ols.html",
    "href": "lessons_original/07_power_ols.html",
    "title": "\n28  Power Analysis for OLS Regression\n",
    "section": "",
    "text": "28.1 Introduction\nThe power of a hypothesis test is the probability of correctly rejecting the null hypothesis or the probability that the test will correctly support the alternative hypothesis (detecting an effect when there actually is one)1. Then,\n\\[\nPower = 1-\\beta\n\\]\nWhere, \\(\\beta\\) = probability of committing a Type II Error (the probability that we would accept the null hypothesis even if the alternative hypothesis is actually true). Then, by decreasing \\(\\beta\\) power increases [@(pdf)ef].\nPower is mainly influenced by sample size, effect size, and significance level.",
    "crumbs": [
      "Statistical Power and Sample Size Determination",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Power Analysis for OLS Regression</span>"
    ]
  },
  {
    "objectID": "lessons_original/07_power_ols.html#introduction",
    "href": "lessons_original/07_power_ols.html#introduction",
    "title": "\n28  Power Analysis for OLS Regression\n",
    "section": "",
    "text": "High power: large chance of a test detecting a true effect.\nLow power: test only has a small chance of detecting a true effect or that the results are likely to be distorted by random and systematic error.\n\n\n\n\n\nVisual view of beta",
    "crumbs": [
      "Statistical Power and Sample Size Determination",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Power Analysis for OLS Regression</span>"
    ]
  },
  {
    "objectID": "lessons_original/07_power_ols.html#power-analysis-ols-regression",
    "href": "lessons_original/07_power_ols.html#power-analysis-ols-regression",
    "title": "\n28  Power Analysis for OLS Regression\n",
    "section": "\n28.2 Power Analysis: OLS Regression",
    "text": "28.2 Power Analysis: OLS Regression\nFor this power analysis we will use the univariate (simple) OLS regression example of our last presentation examining the relationship between a vehicle’s weight (WT) and its miles per gallon or fuel efficiency (MPG).\nWhen performed, the paired correlation provided us with a pearson’s correlation coefficient of r(30) = -.868, p&lt;0.05, (n = 32). When we ran this regression we got an (\\(R^2\\) = .75) Therefore for the r2 value (effect size) for a power analysis we will begin with an r2 value of .75 and an n = 32 to account for the observations already collected. However, the power analysis should occur before collecting samples so that we can have an appropriate number of observations required for our hypothesized effect size. In our example, we are also assuming that the variables are normally distributed. Based on our correlation analysis, weight likely needs a cubic transformation, This would mean that our model would have three coefficients of interest.\nFormula for a univariate Ordinary Least Squares (OLS) Regression:\n\\[\n\\hat{y}_i = \\beta_0 + \\beta_1x_i\n\\]\nThe OLS regression model line for our example is:\n\\[\n\\widehat{MPG_i} = \\beta_0 +\\beta_1*WT_i\n\\]\nUsing an alpha value of \\(\\alpha\\) = .05 (The probability of a type I error/rejecting a correct \\(H_0\\), we will identify the number of observations or sample size (n) necessary to obtain statistical power (80% or \\(\\beta\\) = 0.20) given various effect sizes. Statistical power in our example identifies the likelihood that a univariate OLS will detect an effect of a certain size if there is one.\nA power analysis is made up of four main components. We will provide estimates for any three of these, as the following functions in r calculate the fourth component.\nWe found three functions in r to conduct power analyses for an OLS regression:\n\nThe pwrss.f.reg function in the pwrss package\nThe pwr.f2.test function in the pwr package\nThe wp.regression function in the WebPower package\n\n\n28.2.1 The pwrss.f.reg function\nWe will start our power analysis using the The pwrss.f.reg function for one predictor in an OLS regression, with our given observations of n = 32 and \\(R^2\\) = .75. Given these values, we are expecting that one variable (WT) explains 75% of the variance in the outcome or Miles per gallon (R2=0.75 or r2 = 0.75 in the code)2.\n\nRegOne_lm &lt;- pwrss.f.reg(\n  r2 = 0.75,\n  k = 1,\n  n = 32,\n  power = NULL,\n  alpha = 0.05\n)\n\n Linear Regression (F test) \n R-squared Deviation from 0 (zero) \n H0: r2 = 0 \n HA: r2 &gt; 0 \n ------------------------------ \n  Statistical power = 1 \n  n = 32 \n ------------------------------ \n Numerator degrees of freedom = 1 \n Denominator degrees of freedom = 30 \n Non-centrality parameter = 96 \n Type I error rate = 0.05 \n Type II error rate = 0 \n\nRegOne_lm$power\n\n[1] 1\n\n\nGiven the information provided, we get 100% power. Our effect size of r2 = 0.75 is considered a large effect provided the following guidelines by Cohen (1988)3\n\\(f^2\\) = 0.02 indicates a small effect;\n\\(f^2\\) = 0.15 indicates a medium effect;\n\\(f^2\\) = 0.35 indicates a large effect.\nWe will use these guidelines to continue our exploration. We will concentrate on a fixed medium effect size. Where, the paired correlation is approximately r = .40 for a medium correlation and for an \\(f^2\\) or effect size of 0.15. using this fixed effect, we will look at various sample sizes to obtain power of 80% or greater given a medium effect size. In our sequence of possible sample sizes, the minimum n = 1 as n &gt; p(p+1)/2 = 1(2)/2 = 1\n\nOLSReg_df &lt;- tibble(n = seq.int(from = 2, to = 99 + 2))\n\nOLSReg_df$power &lt;- map_dbl(\n  .x = OLSReg_df$n,\n  .f = ~{\n    out_ls &lt;- pwrss.f.reg(\n      # \"Effect size\" of a linear model\n      r2 = 0.15,\n      # number of predictors\n      k = 1,\n      # sample size\n      n = .x,\n      power = NULL,\n      alpha = 0.05,\n      # Stop printing messages\n      verbose = FALSE\n    )\n    out_ls$power\n  }\n)\n\nWarning in qf(alpha, df1 = u, df2 = v, lower.tail = FALSE): NaNs produced\n\n\nThe following is the power curve for a fixed effect of f2 = 0.15\n\nggplot(data = OLSReg_df) +\n  theme_bw() +\n  aes(x = n, y = power) +\n  labs(\n    title = \"Omnibus (F-test) Power for Linear Model\",\n    subtitle = \"Fixed Effect Size R2 = 0.15, 1 Predictor\"\n  ) +\n  scale_y_continuous(limits = c(0, 1)) +\n  geom_point() +\n  geom_abline(slope = 0, intercept = 0.8, colour = \"gold\")\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nGiven the graph, we notice that we need an approximate sample size or n of close to 50 to detect a medium effect size in an OLS Regression.\nThe following is a power analysis for a univariate OLS regression given a fixed sample size. We will create a sequence of effect sizes that capture Cohen’s guidelines as well as the effect size of 0.75 of our sample regression. Our fixed n will be n = 32 as the sample.\n\nOLSRegN_df &lt;- tibble(R2 = seq(0, 0.75, length.out = 100))\n\nOLSRegN_df$power &lt;- map_dbl(\n  .x = OLSRegN_df$R2,\n  .f = ~{\n    out2_ls &lt;- pwrss.f.reg(\n      # \"Effect size\" of a linear model\n      r2 = .x,\n      # number of predictors\n      k = 1,\n      # sample size\n      n = 32,\n      power = NULL,\n      alpha = 0.05,\n      # Stop printing messages\n      verbose = FALSE\n    )\n    out2_ls$power\n  }\n)\n\nThe following is the power curve for a fixed sample size of n = 32\n\nggplot(data = OLSRegN_df) +\n  theme_bw() +\n  aes(x = R2, y = power) +\n  labs(\n    title = \"Omnibus (F-test) Power for Linear Model\",\n    subtitle = \"Fixed Sample Size = 32, 1 Predictor\"\n  ) +\n  scale_y_continuous(limits = c(0, 1)) +\n  geom_point() +\n  geom_abline(slope = 0, intercept = 0.8, colour = \"red\")\n\n\n\n\n\n\n\nGiven the graph, we notice that given n = 32, a power of 80% and higher is achieved when the effect size is at least approximately r2 = 0.20.\n\n28.2.2 The pwr.f2.test function\nPower analysis using the pwr.f2.test: where, u = 1, The F numerator degrees of freedom (u=1) or the number of coefficients(independent variables) in the model\nand we will use Cohen’s criteria for effect sizes and first provide analyses for a medium effect size of 0.15 [3]45\n\n# Using Cohen 1988 criteria, where, \n#f2 = 0.02 small effect;\n#f2 = 0.15 medium effect;\n#f2 = 0.35 indicates a large effect\n\n### Fixed Effect size f2 = 0.15###\n# n = 50\npwr.f2.test(\n  u = 1, \n  v = 50 - 1 - 1,\n  f2 = .15, \n  sig.level = 0.05, \n  power = NULL\n  )\n\n\n     Multiple regression power calculation \n\n              u = 1\n              v = 48\n             f2 = 0.15\n      sig.level = 0.05\n          power = 0.7653128\n\n# n = 25\npwr.f2.test(\n  u = 1, \n  v = 25 - 1 - 1,\n  f2 = 0.15, \n  sig.level = 0.05, \n  power = NULL\n  )\n\n\n     Multiple regression power calculation \n\n              u = 1\n              v = 23\n             f2 = 0.15\n      sig.level = 0.05\n          power = 0.4584646\n\n# n = 12\npwr.f2.test(\n  u = 1, \n  v = 12 - 1 - 1,\n  f2 = 0.15, \n  sig.level = 0.05, \n  power = NULL\n  )\n\n\n     Multiple regression power calculation \n\n              u = 1\n              v = 10\n             f2 = 0.15\n      sig.level = 0.05\n          power = 0.2289402\n\n\nNow, we will explore a fixed n = 32\n\n# ES = .02, r = .14\npwr.f2.test(\n  u = 1, \n  v = 32 - 1 - 1,\n  f2 = .02, \n  sig.level = 0.05, \n  power = NULL\n  )\n\n\n     Multiple regression power calculation \n\n              u = 1\n              v = 30\n             f2 = 0.02\n      sig.level = 0.05\n          power = 0.1210661\n\n# ES = 0.15, r = .39\npwr.f2.test(\n  u = 1, \n  v = 32 - 1 - 1,\n  f2 = 0.15, \n  sig.level = 0.05, \n  power = NULL\n  ) \n\n\n     Multiple regression power calculation \n\n              u = 1\n              v = 30\n             f2 = 0.15\n      sig.level = 0.05\n          power = 0.5637733\n\n# ES = .35, r = .59\npwr.f2.test(\n  u = 1, \n  v = 32 - 1 - 1,\n  f2 = 0.35, \n  sig.level = 0.05, \n  power = NULL\n  )\n\n\n     Multiple regression power calculation \n\n              u = 1\n              v = 30\n             f2 = 0.35\n      sig.level = 0.05\n          power = 0.8993357\n\n\nWe will now look at the 3 types of effect sizes given various sample sizes\n\neffect_sizes &lt;- c(0.02, 0.15, 0.35) \nsample_sizes = seq(20, 100, 20)\n\ninput_df &lt;- crossing(effect_sizes,sample_sizes)\nglimpse(input_df)\n\nRows: 15\nColumns: 2\n$ effect_sizes &lt;dbl&gt; 0.02, 0.02, 0.02, 0.02, 0.02, 0.15, 0.15, 0.15, 0.15, 0.1…\n$ sample_sizes &lt;dbl&gt; 20, 40, 60, 80, 100, 20, 40, 60, 80, 100, 20, 40, 60, 80,…\n\nget_power &lt;- function(df){\n  power_result &lt;- pwr.f2.test(\n    u = 1,\n    v = df$sample_sizes - 1 - 1, \n    f2 = df$effect_sizes,\n    )\n  df$power=power_result$power\n  return(df)\n}\n\n# run get_power for each combination of effect size \n# and sample size\n\npower_curves &lt;- input_df %&gt;%\n  do(get_power(.)) %&gt;%\n  mutate(effect_sizes = as.factor(effect_sizes)) \n\n\nggplot(power_curves, \n       aes(x=sample_sizes,\n           y=power, \n           color=effect_sizes)\n       ) + \n  geom_line() + \n  geom_hline(yintercept = 0.8, \n             linetype='dotdash',\n             color = \"purple\")\n\n\n\n\n\n\n\nBased on the graph, if we have an effect size of 0.15, we need approximately 50 or more observations (recall n = v + 1 + 1)\n\n28.2.3 The wp.regression function\nLastly, we use the wp.regression function to examine the appropriate sample size given an effect size of 0.15 to achieve a power of 80% or higher [6]7\n\n# Using webpower \n#p1 = 1\n### Fixed ES = 0.15 ###\nres &lt;- wp.regression(n = seq(20,100,20), \n                     p1 = 1, \n                     f2 = 0.15, \n                     alpha = 0.05, \n                     power = NULL\n                    )\nres\n\nPower for multiple regression\n\n      n p1 p2   f2 alpha     power\n     20  1  0 0.15  0.05 0.3745851\n     40  1  0 0.15  0.05 0.6654126\n     60  1  0 0.15  0.05 0.8389166\n     80  1  0 0.15  0.05 0.9280168\n    100  1  0 0.15  0.05 0.9695895\n\nURL: http://psychstat.org/regression\n\nplot(res,  main = \"Fixed Effect Size = 0.15\")+\nabline(a = .80, b = 0, col = 'steelblue', lwd = 3, lty = 2)\n\n\n\n\n\n\n\ninteger(0)\n\n\nThe results are similar to the previous functions. However, in this function, given an effect size of 0.15, we need an n of close to 60 to achieve 80% power.\n\n# Using webpower \n#p1 = 1\n### Fixed n = 50 ###\nres &lt;- wp.regression(n = 50, \n                     p1 = 1, \n                     f2 = seq(0.00, 0.35, 0.05), \n                     alpha = 0.05, \n                     power = NULL\n                    )\nres\n\nPower for multiple regression\n\n     n p1 p2   f2 alpha     power\n    50  1  0 0.00  0.05 0.0500000\n    50  1  0 0.05  0.05 0.3409707\n    50  1  0 0.10  0.05 0.5914439\n    50  1  0 0.15  0.05 0.7653128\n    50  1  0 0.20  0.05 0.8725329\n    50  1  0 0.25  0.05 0.9337077\n    50  1  0 0.30  0.05 0.9667049\n    50  1  0 0.35  0.05 0.9837529\n\nURL: http://psychstat.org/regression",
    "crumbs": [
      "Statistical Power and Sample Size Determination",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Power Analysis for OLS Regression</span>"
    ]
  },
  {
    "objectID": "lessons_original/07_power_ols.html#references",
    "href": "lessons_original/07_power_ols.html#references",
    "title": "\n28  Power Analysis for OLS Regression\n",
    "section": "References",
    "text": "References\n\n\n\n\n1. \nCohen J. Statistical power analysis for the behavioral sciences. 2nd ed. Hillsdale, N.J: L. Erlbaum Associates; 1988. \n\n\n2. \nA practical guide to statistical power and sample size calculations in r [Internet]. Available from: https://cran.r-project.org/web/packages/pwrss/vignettes/examples.html#3_Linear_Regression_(F_and_t_Tests)\n\n\n\n3. \nCohen J. Statistical power analysis for the behavioral sciences. 2nd ed. Hillsdale, N.J: L. Erlbaum Associates; 1988. \n\n\n4. \nPower in r | [Internet]. Available from: https://blogs.uoregon.edu/rclub/2015/11/10/power-in-r/\n\n\n\n5. \nStatistical power analysis - jacob cohen, 1992 [Internet]. Available from: https://journals.sagepub.com/doi/10.1111/1467-8721.ep10768783\n\n\n\n6. \nZhang Z, Wang L. Advanced statistics using r [Internet]. ISDSA Press; 2017. Available from: https://advstats.psychstat.org/\n\n\n\n7. \nWp.regression: Statistical power analysis for linear regression in WebPower: Basic and advanced statistical power analysis [Internet]. Available from: https://rdrr.io/cran/WebPower/man/wp.regression.html",
    "crumbs": [
      "Statistical Power and Sample Size Determination",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Power Analysis for OLS Regression</span>"
    ]
  }
]