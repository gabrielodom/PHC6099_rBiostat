[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "An R Cookbook for Public Health",
    "section": "",
    "text": "0.1 Source Code for PHC6099 Course Notes\nThis material is for the course “R Computing for Health Sciences”. The course notes are published here: https://gabrielodom.github.io/PHC6099_rBiostat/",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course Materials for PHC 6099: 'R Computing for Health Sciences'</span>"
    ]
  },
  {
    "objectID": "index.html#source-code-for-phc6099-course-notes",
    "href": "index.html#source-code-for-phc6099-course-notes",
    "title": "An R Cookbook for Public Health",
    "section": "",
    "text": "0.1.1 Topics\nThe chapters are:\n\nExploring Data\n\nggplot2:: mosaic plots, histograms, and violin plots\nggplot2:: scatterplots and facets \nskimr::\ntable1::\ngtsummary::\n\nOne-Sample Tests\n\n\\(Z\\)-test\nPaired \\(t\\)-test\nPaired Wilcoxon test\nTransformations to Normality\nMcNemar’s Test\nFisher’s Exact Test\nChi-Square Goodness of Fit\nBootstrapped Confidence Intervals\n\nTwo-Sample Tests\n\n\\(t\\)-test\nWelch’s \\(t\\)-test\nMann-Whitney \\(U\\) test\nCochran’s \\(Q\\) test\n\\(\\chi^2\\) Test for Independence\n\nANOVA and Linear Regression\n\nOne-Way ANOVA\nTwo-way ANOVA\nWelch’s ANOVA\nKruskal-Wallace Test\nTukey HSD Post-Hoc Test\nRepeated Measures ANOVA\nRandom Intercept Models\nCorrelation Matrices and Covariances\nMultiple Regression (linear)\nPolynomial regression\n\nGeneralized Linear Models\n\nGeneralized Linear Models: Binary\nGeneralized Linear Models: Ordered\nGeneralized Linear Models: Count (Poisson)\nGeneralized Linear Models: Count (Negative Binomial)\n\nSpecial Topics\n\nLinear Mixed Effects Models\nStructural Equation Models\nCox Proportional Hazards Regression\n(TBD) Multivariate Methods for Genetics/Genomics\nRidge, LASSO, and Elastic Net Regression\n\nPower Calculations (in progress)\n\n\n\n0.1.2 Lesson Outline\nThis is a shell of a lesson that can be copied and pasted for new lessons (or to edit and clean up existing lessons). If you copy this shell, then change all the headings from level 4 to 2. Replace &lt;the method&gt; with the name of your method, or its abbreviation. The file lessons/00_lesson_template.qmd has a .qmd template with these sections.\n\n0.1.2.1 Introduction to &lt;the method&gt;\n\n\n0.1.2.2 Mathematical definition of &lt;the method&gt;\n\n\n0.1.2.3 Data source and description\n\n\n0.1.2.4 Cleaning the data to create a model data frame\n\n\n0.1.2.5 Assumptions of &lt;the method&gt;\n\n\n0.1.2.6 Checking the assumptions with plots\n\n\n0.1.2.7 Code to run &lt;the method&gt;\n\n\n0.1.2.8 Code output\n\n\n0.1.2.9 Brief interpretation of the output",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course Materials for PHC 6099: 'R Computing for Health Sciences'</span>"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "2  About this Book",
    "section": "",
    "text": "2.1 About these Chapters\nThese are the written lecture materials for the class PHC 6099 at Florida International University’s Stempel College of Public Health. Each of the lessons were written by students, so we don’t guarantee that they are always mathematically/statistically accurate. You should use this material as a simple place to start to use these methods, but always read more about these methods when you use them to give yourself a better understanding of their theoretical foundations. This material should not be used to replace a traditional textbook in applied biostatistics. Here are some rather standard books on applied biostatistics (there are free/cheap versions on the internet for most of these texts, but I trust you to find them yourself):",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>About this Book</span>"
    ]
  },
  {
    "objectID": "about.html#about-these-chapters",
    "href": "about.html#about-these-chapters",
    "title": "2  About this Book",
    "section": "",
    "text": "Biostatistical Analysis. Jerrold H. Zar. https://www.pearson.com/en-us/subject-catalog/p/biostatistical-analysis/P200000006419/9780134995441.\nRegression Modelling Strategies. Frank E. Harrell, Jr. https://link.springer.com/book/10.1007/978-3-319-19425-7\nCategorical Data Analysis. Alan Agresti. https://onlinelibrary.wiley.com/doi/book/10.1002/0471249688",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>About this Book</span>"
    ]
  },
  {
    "objectID": "about.html#getting-help-in-r",
    "href": "about.html#getting-help-in-r",
    "title": "2  About this Book",
    "section": "2.2 Getting Help in R",
    "text": "2.2 Getting Help in R\nThis is the second semester of the “R” course sequence at FIU, so we spend very little time explaining the basics of the R language (the first semester is PHC6701; the text for that class is available here: https://gabrielodom.github.io/PHC6701_r4ds/). If you are new to R, please go back to the previous semester’s material and work through that first. If you want to see how we made this book, the source code and data sets for this book are available here: https://github.com/gabrielodom/PHC6099_rBiostat.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>About this Book</span>"
    ]
  },
  {
    "objectID": "01_header_EDA.html",
    "href": "01_header_EDA.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Text Outline\nThis text covers the basics of:",
    "crumbs": [
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "01_header_EDA.html#text-outline",
    "href": "01_header_EDA.html#text-outline",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Exploring data\nOne-sample tests\nTwo-sample tests\nLinear regression and ANOVA\nGeneralized linear models\nSome special topics\nMaybe a bit about power calculations…",
    "crumbs": [
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "01_header_EDA.html#part-outline",
    "href": "01_header_EDA.html#part-outline",
    "title": "Exploratory Data Analysis",
    "section": "Part Outline",
    "text": "Part Outline\nThis part of the text contains the chapters on the following packages which are useful for exploratory data analysis:\n\nggplot2:: mosaic plots, histograms, and violin plots\nggplot2:: scatterplots and facets\nskimr::\ntable1::\ngtsummary::",
    "crumbs": [
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "lessons/01_mosaic_violin.html",
    "href": "lessons/01_mosaic_violin.html",
    "title": "\n3  Mosaic & Box/Violin Plots\n",
    "section": "",
    "text": "3.1 Packages for this Lesson\n# Installing Required Packages\n# install.packages(\"public.ctn0094data\")\n# install.packages(\"tidyverse\")\n# install.packages(\"ggmosaic\")\n\n# Loading Required Packages\nlibrary(public.ctn0094data)\nlibrary(tidyverse)\nlibrary(ggmosaic)",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Mosaic & Box/Violin Plots</span>"
    ]
  },
  {
    "objectID": "lessons/01_mosaic_violin.html#introduction-to-mosaic-and-boxviolin-plots",
    "href": "lessons/01_mosaic_violin.html#introduction-to-mosaic-and-boxviolin-plots",
    "title": "\n3  Mosaic & Box/Violin Plots\n",
    "section": "\n3.2 Introduction to Mosaic and Box/Violin Plots",
    "text": "3.2 Introduction to Mosaic and Box/Violin Plots\nMosaic, box, and violin plots are useful for visualizing summary statistics.\nA mosaic plot is a special type of stacked bar chart used for two or more categorical variables. The width of the columns is proportional to the number of observations in each level of the variable plotted on the horizontal, or x-axis. The vertical length of the bars is proportional to the number of observations in the second variable within each level of the first variable.\nBox and violin plots are used for continuous variables by group. Box plots display six summary measures (the minimum, first quartile (Q1), median, third quartile (Q3), the interquartile range, and maximum). A violin plot illustrates the distribution of numerical data for one or more level of a categorical variable by combining summary statistics and density of each variable. Each curve corresponds to the respective frequency of data points within each region. A box plot is typically overlaid to provide additional information.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Mosaic & Box/Violin Plots</span>"
    ]
  },
  {
    "objectID": "lessons/01_mosaic_violin.html#data-source-and-description",
    "href": "lessons/01_mosaic_violin.html#data-source-and-description",
    "title": "\n3  Mosaic & Box/Violin Plots\n",
    "section": "\n3.3 Data Source and Description",
    "text": "3.3 Data Source and Description\nThe National Drug Abuse Treatment Clinical Trials Network (CTN) is a means by which medical and specialty treatment providers, treatment researchers, participating patients, and the National Institute on Drug Abuse cooperatively develop, validate, refine, and deliver new treatment options to patients. The CTN 094 demographics and everybody data sets from the public.ctn0094data package were utilized for the following visualizations. CTN 094 is a comprehensive, harmonized and normalized database of treatment data from CTN_0027, CTN_0030, and CTN_0051, where experiences of individuals with opioid use disorder (OUD) who seek care are described.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Mosaic & Box/Violin Plots</span>"
    ]
  },
  {
    "objectID": "lessons/01_mosaic_violin.html#cleaning-the-data-to-create-a-model-data-frame",
    "href": "lessons/01_mosaic_violin.html#cleaning-the-data-to-create-a-model-data-frame",
    "title": "\n3  Mosaic & Box/Violin Plots\n",
    "section": "\n3.4 Cleaning the Data to Create a Model Data Frame",
    "text": "3.4 Cleaning the Data to Create a Model Data Frame\nThe demographics and everybody data sets within the public.ctn0094data package were joined by ID (who variable). Race, age, is_male (gender), and project were selected features for the following visualizations.\n\n# Creating model data frame to include age, race, project, and is_male\n# from demographics and everybody data sets. Joined by subject ID (who)\ndemographics_df &lt;- demographics %&gt;% \n  left_join(everybody, by = \"who\")  %&gt;%\n  select(age, race, project, is_male)",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Mosaic & Box/Violin Plots</span>"
    ]
  },
  {
    "objectID": "lessons/01_mosaic_violin.html#assumptions-with-mosaic-boxviolin-plots",
    "href": "lessons/01_mosaic_violin.html#assumptions-with-mosaic-boxviolin-plots",
    "title": "\n3  Mosaic & Box/Violin Plots\n",
    "section": "\n3.5 Assumptions with Mosaic & Box/Violin Plots",
    "text": "3.5 Assumptions with Mosaic & Box/Violin Plots\nIn mosaic plots, two categorical variables are plotted along the horizontal (x) and vertical (y) axis. Each combination of categories forms a rectangle or tile within the plot.\nIn box and violin plots, a categorical variable is plotted along the horizontal or x-axis, while a continuous variable is plotted along the vertical or y-axis. Violin plots can be limiting if symmetry, skew, or other shape and variability characteristics are different between groups because precise comparison cannot be easily interpreted between density curves. For this reason, violin plots are typically rendered with another overlaid chart type, like box plot quartiles.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Mosaic & Box/Violin Plots</span>"
    ]
  },
  {
    "objectID": "lessons/01_mosaic_violin.html#code-to-run-mosaic-boxviolin-plots-output",
    "href": "lessons/01_mosaic_violin.html#code-to-run-mosaic-boxviolin-plots-output",
    "title": "\n3  Mosaic & Box/Violin Plots\n",
    "section": "\n3.6 Code to Run Mosaic & Box/Violin Plots & output",
    "text": "3.6 Code to Run Mosaic & Box/Violin Plots & output\n\n3.6.1 Mosaic Plots\nIn order to create a Mosaic plot, you must specify what data object you will be using within the ggplot() function. Then you will set aesthetic mapping options within the following geometric object layer: geom_mosaic().\nIn geom_mosaic(), the following aesthetics can be specified:\n\n\nweight: a weighting variable.\n\nx: categorical variable for the x-axis.\n\nSpecified as x = product(var1, var2, ...)\n\nThe product() function is used to extract the values from the categorical variable specified.\n\n\n\nalpha: a variable specifying transparency.\n\nIf the variable is not called in x:, then alpha: will be added in the first position.\n\n\n\nfill: a variable specifying fill color.\n\nIf the variable is not called in x:, then fill: will be added after the optional alpha: variable.\n\n\n\nconds: a variable specifying conditions.\n\nSpecified as conds = product(var1, var2, ...)\n\n\n\n\nThe ordering of the variables is vital as the product plot is created hierarchically.\n\n3.6.1.1 Basic Mosaic Plot\nIn the following example of a basic mosaic plot, we visualize the distribution of Race among CTN Projects 27, 30, and 51.\n\n# Basic Mosaic Plot\nmosaic_basic &lt;- demographics_df %&gt;% \n  ggplot() +\n  geom_mosaic(\n    aes(\n      # geom_mosaic() does not have one-to-one mapping between a variable and the x- \n      # or y-axis. So you must use the product() function when assigning a variable\n      # to the x-axis to account for the variable number of variables.\n      x = product(project),\n      fill = race\n    )\n  ) +\n  labs(\n    y = \"Race\",\n    x = \"Project\",\n    title = \"Mosaic Plot of Race by CTN Project\") +\n  # Specifies default `geom_mosaic` aesthetics, e.g white panel background, \n  # removes grid lines, adjusts widths and heights of rows and columns to \n  # reflect frequencies\n  theme_mosaic() +\n  # Removes legend illustrating Race and respective fill colors\n  theme(legend.position = \"None\")\n  \nmosaic_basic\n\n\n\n\n\n\nFigure 3.1\n\n\n\n\n\n3.6.1.2 More Advanced Mosaic Plot\nIn a more advanced version of a mosaic plot, we can visualize more than 2 categorical variables. The following example utilizes race, project, and ethnicity among CTN Projects 27, 30, and 51.\n\n# Advanced Mosaic Plot\nmosaic_advanced &lt;- demographics_df %&gt;% \n  ggplot() +\n  geom_mosaic(\n    aes(\n      x = product(race, project),\n      fill = is_male\n    )\n  ) +\n  labs(\n    y = \"Race\",\n    x = \"Project\",\n    title = \"Mosaic Plot of Race by Gender and CTN Project\",\n    fill = \"Gender\"\n  ) +\n  scale_fill_manual(\n    labels = c(\"No\" = \"Female\", \"Yes\" = \"Male\"),\n    values = c(\"darkseagreen2\", \"darkslategray3\", \"grey\")\n  ) +\n  theme_mosaic() +\n  # Adjust axis tick labels to 60 degrees and justification to the right\n  # with hjust (horizontal justification) and vjust (vertical justification)\n  theme(axis.text.x = element_text(angle = 60, hjust = 1, vjust = 1))\n  \nmosaic_advanced\n\n\n\n\n\n\nFigure 3.2\n\n\n\n\n\n3.6.2 Box Plots\nIn order to create a box plot, you must specify what data object you will be using within the ggplot() function. Then you will set aesthetic mapping options within the aes() or aesthetic layer. The geom_boxplot() layer specifies the box plot.\nThe following aesthetics are understood by geom_boxplot():\n\n\nx or y: Specifies the categorical variable along the x- or y-axis.\n\nlower or xlower: Specifies the 25th percentile/first quartile.\n\nupper or xupper: Specifies the 75th percentile/third quartile.\n\nmiddle or xmiddle: Specifies the 50th percentile/second quartile/median.\n\nymin or xmin: Specifies the y or x minimum for the plot.\n\nymax or xmax: Specifies the y or x maximum for the plot.\n\nalpha: Specifies a variable to determine transparency.\n\ncolor: Assigns an outline color to respective levels of a specified categorical variable.\n\nfill: Assigns a fill color to respective levels of a specified categorical variable.\n\ngroup: Partitions data by a discrete variable when no other grouping variable is specified, or grouping is incorrectly defaulted by R.\n\nlinetype: Specifies line type of box plot.\n\nlinewidth: Specifies line width of box plot.\n\nshape: Specifies the shape of the (outlier) points.\n\nsize: Specifies the size of the points and text.\n\nweight: Specifies a weight variable.\n\n\n3.6.2.1 Basic Box Plot\nThe following is a basic box plot showing the relationship between one continuous and one categorical variable.\n\n# Box Plot\nbox_basic &lt;- demographics_df %&gt;% \n  ggplot() +\n  aes(x = race, y = age, color = race) +\n  labs(\n    x = \"Race\",\n    y = \"Age\",\n    title = \"Box Plot of Race and Age\",\n    color = \"Race\"\n  ) +\n  # Using width to adjust the width of the boxes\n  geom_boxplot(width = 0.5) +\n  theme(legend.position = \"None\")\n\nbox_basic\n\n\n\n\n\n\nFigure 3.3\n\n\n\n\n\n3.6.2.2 More Advanced Box Plot\nWith geom_box(), you can also specify a additional categorical variable (different from your x and y variables) to break up your plot by that variable. For example, the following plot takes the previous plot of race and age and adds information side-by-side by gender (is_male).\n\n# Box Plot\nbox_advanced &lt;- demographics_df %&gt;% \n  ggplot() +\n  aes(x = race, y = age, color = is_male) +\n  # changing the labels for is_male, and specifying the colors we want\n  scale_color_manual(\n    labels = c(\"No\" = \"Female\", \"Yes\" = \"Male\"),\n    values = c(\"darkorchid4\", \"darkolivegreen4\")\n  ) +\n  labs(\n    x = \"Race\",\n    y = \"Age\",\n    title = \"Box Plot of Race and Age\",\n    color = \"Gender\"\n  ) +\n  # Using width to adjust the width of the boxes\n  geom_boxplot(width = 0.5)\n\nbox_advanced\n\n\n\n\n\n\nFigure 3.4\n\n\n\n\n\n3.6.3 Violin Plot\nIn order to create a Violin plot, you must specify what data object you will be using within the ggplot() function. Then you will set aesthetic mapping options within the aes() or aesthetic layer. The geom_violin() layer specifies the violin plot. An additional call for geom_boxplot() will overlay box quartiles on the violin plot to display summary statistics.\nThe following aesthetics are understood by geom_violin():\n\n\nx: Specifies the categorical variable along the x-axis.\n\ny: Specifies the continuous variable along the y-axis.\n\nalpha: Specifies a variable to determine transparency.\n\ncolor: Assigns an outline color to respective levels of a specified categorical variable.\n\nfill: Assigns a fill color to respective levels of a specified categorical variable.\n\ngroup: Partitions data by a discrete variable when no other grouping variable is specified, or grouping is incorrectly defaulted by R.\n\nlinetype: Specifies line type of violin plot.\n\nlinewidth: Specifies line width of violin plot.\n\nweight: Specifies a weight variable.\n\n\n# Violin Plot\nviolin_basic &lt;- demographics_df %&gt;% \n  ggplot() +\n  aes(x = race, y = age, color = race) +\n  scale_color_manual(\n    values = c(\"coral1\", \"darkgreen\", \"deepskyblue2\", \"darkorchid2\")\n  ) +\n  labs(\n    x = \"Race\",\n    y = \"Age\",\n    title = \"Violin Plot of Race and Age\",\n    subtitle = \"With Summary Information\",\n    color = \"Race\"\n  ) +\n  geom_violin() +\n  geom_boxplot(width = 0.1) +\n  theme(legend.position = \"None\")\n\nviolin_basic                        \n\n\n\n\n\n\nFigure 3.5",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Mosaic & Box/Violin Plots</span>"
    ]
  },
  {
    "objectID": "lessons/01_mosaic_violin.html#brief-interpretation",
    "href": "lessons/01_mosaic_violin.html#brief-interpretation",
    "title": "\n3  Mosaic & Box/Violin Plots\n",
    "section": "\n3.7 Brief Interpretation",
    "text": "3.7 Brief Interpretation\n\n3.7.1 Mosaic Plot\n\nCompared to Project 27 and Project 51, Project 30 had the highest proportion of participants who indicated that their race is ‘White’.\nCompared to Project 30 and Project 51, Project 27 had the highest proportion of participants who indicated that their race is ‘Other’.\nCompared to Project 27 and Project 51, Project 30 has the lowest proportion of participants who indicated that their race is ‘Other’.\n\n3.7.2 Box Plot\n\nParticipants who indicated that their race is ‘Black’ exhibited the highest median age of around 45 years old\nParticipants who indicated that their race is ‘White’ exhibited the lowest median age at approximately 31 years old.\n\n3.7.3 Violin Plot\nThis plot more clearly shows the bimodality of age by race among Black and ‘Other’ participants in CTN. It also shows the skewness of age in the White participants. Specifically:\n\nParticipants who indicated that their race is ‘White’ exhibited peak density around mid-20s compared to those who indicated that their race is ‘Black’, where peak density is exhibited around late-40s.\nParticipants who indicated that their race is ‘White’ had the lowest median age at approximately 31 years old, where participants who indicated that their race is ‘Black’ had the highest median age at approximately 45 years old.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Mosaic & Box/Violin Plots</span>"
    ]
  },
  {
    "objectID": "lessons/01_mosaic_violin.html#conclusion",
    "href": "lessons/01_mosaic_violin.html#conclusion",
    "title": "\n3  Mosaic & Box/Violin Plots\n",
    "section": "\n3.8 Conclusion",
    "text": "3.8 Conclusion\nThis lesson discusses three different plots for one-dimensional data: the Mosaic, Box, and Violin plots. Figure 3.1 is a basic mosaic plots that shows race by CTN project. In Figure 3.2, we added a third variable to the visualization: gender. The box plots, Figure 3.3 and Figure 3.4 we plotted age (continuous) by race and age by race and gender, respectively. Finally, Figure 3.5 shows a violin plot with an overlaid box plot for age by race in the CTN projects.\nMosaic plots are useful for proportionally visualizing the observations of two or more categorical variables. Box and violin plots can be used to visualize a continuous variable by one, or two in the case of box plots, categorical variables. Violin plots build on box plots in that they are able to provide quick information on the potential multimodal distribution(s) and skewness of a continuous variable across categories, as we saw in Figure 3.5.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Mosaic & Box/Violin Plots</span>"
    ]
  },
  {
    "objectID": "lessons/01_scatterplots.html",
    "href": "lessons/01_scatterplots.html",
    "title": "\n4  How to Create a Scatterplot\n",
    "section": "",
    "text": "4.1 Introduction to Scatterplots\nScatterplots display the relationship between two variables using dots to represent the values for each numeric variable. This presentation will examine the relationship between GDP per capita and Fertility over time using ggplot with facets.\nHypothesis: A negative relationship exists between GDP per capita and fertility i.e. as GDP per capita increases, fertility decreases.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>How to Create a Scatterplot</span>"
    ]
  },
  {
    "objectID": "lessons/01_scatterplots.html#gapminder-data-description",
    "href": "lessons/01_scatterplots.html#gapminder-data-description",
    "title": "\n4  How to Create a Scatterplot\n",
    "section": "\n4.2 Gapminder data description",
    "text": "4.2 Gapminder data description\nData was obtained from the dslabs package and comes from Gapminder a Swedish non-profit organization. The Gapmidner data set has health and income outcomes for 184 countries from 1960 to 2016. Gapminder aims to promote a fact-based worldview by providing accessible and understandable global development data. The dataset covers a wide range of variables, including economic, social, and health-related indicators like GDP, infant mortality, life expectancy, fertility, as well as population, making it a valuable resource for understanding global trends and patterns over time. Countries and territories with missing information were not excluded from the data set as the lack of information can also be looked into and shed light on why data was not collected or provided. To determine whether a country’s health and income outcomes are influenced by population sizes and GDP per capita, the data will be used to create a series of graphs to view different trends.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>How to Create a Scatterplot</span>"
    ]
  },
  {
    "objectID": "lessons/01_scatterplots.html#cleaning-the-data-to-create-a-model-data-frame",
    "href": "lessons/01_scatterplots.html#cleaning-the-data-to-create-a-model-data-frame",
    "title": "\n4  How to Create a Scatterplot\n",
    "section": "\n4.3 Cleaning the data to create a model data frame",
    "text": "4.3 Cleaning the data to create a model data frame\nA tibble was created from the gapminder dataset, and a new column was created to measure GDP per capita. Overall, using tibbles enhances the readability, usability, and compatibility of your code within the tidyverse ecosystem.\n\n# Creating gapminder dataset\\tibble\ngapminder_df &lt;-\n  as_tibble(gapminder) %&gt;%\n  mutate(gdp_per_capita = gdp / population)",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>How to Create a Scatterplot</span>"
    ]
  },
  {
    "objectID": "lessons/01_scatterplots.html#components-of-ggplot2",
    "href": "lessons/01_scatterplots.html#components-of-ggplot2",
    "title": "\n4  How to Create a Scatterplot\n",
    "section": "\n4.4 Components of ggplot2\n",
    "text": "4.4 Components of ggplot2\n\nggplot2 is a package used to create graphs and visualize data. The main three components of ggplot2 are the data, aesthetics and geom layers.\n\nThe data layer - states what data will be used to graph\nThe aesthetics layer - specifies the variables that are being mapped\nThe geom layer - specifies the type of graph to be produced",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>How to Create a Scatterplot</span>"
    ]
  },
  {
    "objectID": "lessons/01_scatterplots.html#code-to-run-interpretable-scatterplots-and-create-facets",
    "href": "lessons/01_scatterplots.html#code-to-run-interpretable-scatterplots-and-create-facets",
    "title": "\n4  How to Create a Scatterplot\n",
    "section": "\n4.5 Code to run interpretable scatterplots and create facets",
    "text": "4.5 Code to run interpretable scatterplots and create facets\nIn order to create a scatter-plot using ggplot, you must specify what data you will be using, state which variables will be mapped and how under aesthetics. What differentiates the scatter-plot from any other type of graph will be specified under the geom layer. For the scatter-plot, geom_point will be used.\nIn this example, we will analyze the relationship between fertility rates and gdp per capita for each country in 2011.\n\nfig_bubble_2011 &lt;-\n  ggplot(data = filter(gapminder_df, year == 2011)) +\n  aes(x = gdp_per_capita, y = fertility) +\n  geom_point()\n\nfig_bubble_2011 \n\n\n\n\n\n\nFigure 4.1: Association between fertility rates and gdp per capita for each country in 2011\n\n\n\n\nIn the example above, we have mapped out fertility as our y-axis and gdp per capita as our x-axis. However, at it’s very basic level, there is not enough information provided to accurately analyze the relationship between the two. For this reason, we can add additional layers that will provide more information to properly analyze the scatter-plot.\n\nfig_bubble_pretty_2011 &lt;-\n  ggplot(data = filter(gapminder_df, year == 2011)) +\n  aes(\n    x = gdp_per_capita,\n    y = fertility,\n    # will change the size of the point based on population size \n    size = population, \n    # will assign colors based on the continent the country is in \n    color = continent\n  ) +\n  # gives a range as to how big or small the points of population should be\n  scale_size(range = c(1, 20)) + \n  # removes N/A from the legend and titles it Continent \n  scale_colour_discrete(na.translate = F, name = \"Continent\") +\n  # removes population size from the legend \n  guides(size = \"none\") +\n  scale_x_continuous(\n    name = \"GDP per Capita\",\n    trans = \"log10\",\n    # transforms numbers from scientific notation to regular number \n    labels = scales::comma\n  ) +\n  labs(\n    title = \"Fertility rate descreases as GDP per capita increases in 2011\",\n    y = \"Fertility rates\",\n    caption = \"Source: Gapminder\"\n  ) +\n  # the ylim was set based on the fertility, lowest was near 1 & highest was above 7\n  ylim(1.2, 8.0) +\n  # alpha increases transparency of the points to ensure they can all be seen\n  geom_point(alpha = 0.5) \n\nfig_bubble_pretty_2011\n\n\n\n\n\n\nFigure 4.2: Association between fertility rates and gdp per capita for each country, grouped by continent, in 2011\n\n\n\n\nFigure 4.2 builds on the previous scatterplot of Fertility Rates (y axis) against GDP per capita (x axis) for 2011. The bubble size depicts respective country populations, and continents are coded by colors according to the key. This figure displays a negative relationship between GDP per capita and Fertility Rates. It supports the Hypothesis which states that as GDP per capita increases, Fertility Rates decreases. This trend can be confirmed for all continents, however, the degree to which fertility rates drop between continents varies. Most European country appear below a fertility rate of 2 babies per woman. The Americas appear to follow closely behind (under 4), followed by Oceania and Asia. A significant number of African countries still maintained higher fertility rates with lower GDP per capita for 2011.\nThis is an example of wanting to create four separate graphs to see the relationship between fertility rates and GDP per capita based on the years 1960, 1975, 1990 and 2005. In this example we omitted the facet argument.\n\nfig_bubble_multiple &lt;-\n  ggplot(data = filter(gapminder_df, year %in% c(1960, 1975, 1990, 2005))) +\n  aes(\n    x = gdp_per_capita,\n    y = fertility,\n    size = population,\n    color = continent\n  ) +\n  scale_x_continuous(\n    name = \"GDP per Capita\",\n    trans = \"log10\",\n    labels = scales::comma\n  ) +\n  scale_size(range = c(0, 20)) +\n  guides(size = \"none\") +\n  scale_colour_discrete(na.translate = FALSE, name = \"Continent\") +\n  labs(\n    title = \"Fertility continues to decrease as GDP per capita increases\",\n    caption = \"Source: Gapminder\",\n    y = \"Fertility rates\"\n  ) +\n  geom_point(alpha = 0.3) \n\nfig_bubble_multiple\n\n\n\n\n\n\nFigure 4.3: Association between fertility rates and GDP per capita based on the years 1960, 1975, 1990 and 2005\n\n\n\n\nWithout having used the facet argument, all points of all four years have been included into one graph. This graph does not provide us with the information we were looking for.\n\nfig_bubble_multiple_facet &lt;-\n  ggplot(data = filter(gapminder_df, year %in% c(1960, 1975, 1990, 2005))) +\n  aes(\n    x = gdp_per_capita,\n    y = fertility,\n    size = population,\n    color = continent\n  ) +\n  scale_x_continuous(\n    name = \"GDP per Capita\",\n    trans = \"log10\",\n    labels = scales::comma\n  ) +\n  scale_size(range = c(0, 20)) +\n  guides(size = \"none\") +\n  scale_colour_discrete(na.translate = FALSE , name = \"Continent\") +\n  labs(\n    title = \"Fertility continues to decrease as GDP per capita increases\",\n    caption = \"Source: Gapminder\",\n    y = \"Fertility rates\"\n  ) +\n  geom_point(alpha = 0.3) +\n  # specifiying we want the graphs split based on year\n  facet_wrap(~ year)\n\nfig_bubble_multiple_facet\n\n\n\n\n\n\nFigure 4.4: Association between fertility rates and GDP per capita based on the years 1960, 1975, 1990 and 2005, using facet\n\n\n\n\nNow that we’ve specified the facet argument, we now have four separate graphs that can be properly analysed. In Figure 4.4 we see an increasingly negative relationship between the two variables over time. This observation is congruent with the hypothesis that as GDP per capita increases, fertility decreases.\nThis global trend can be attributed to the increasing proportion of women in the workforce in the mid to late 20th century. As a result of World War II (1939-1945), women took on roles outside the home to compensate for men at war. Despite increased GDP per capita, this may have contributed to reduced fertility (babies per woman) over time. In 1960, a clear disparity among continents is seen. Most European countries’ fertility rates fell below 5, while their GDP per capita increased. Most African countries maintained high fertility rates above 5, but little change is seen in GDP per capita. The Asian continent shows the most variation among countries during that year. Some smaller Asian countries continued to maintain high fertility rates as GDP per capita increased in 1960. However, others displayed a drastic decrease in fertility rates by 1960. The Americas followed a steady decline over the years. By 2005, an overall negative relationship can be seen with most countries’ fertility rates below 5 babies per woman.\n\nfig_bubble_row_2011 &lt;-\n  ggplot(data = filter(gapminder_df, year == 2011)) +\n  aes(\n    x = gdp_per_capita,\n    y = fertility\n  ) +\n  geom_point(alpha = 0.5) +\n  facet_wrap(~ continent, nrow = 1)\n\nfig_bubble_row_2011 \n\n\n\n\n\n\nFigure 4.5: Association between fertility rates and GDP per capita based on continent\n\n\n\n\nIn the graph above, we see an example of separating the single graph into graphs based on continent. It has also been specified to have all graphs appear in one single row through the nrow argument. Very importantly however, this graph is unclear and cannot be used to compare the relationship between fertility and gdp per capita.\n\nfig_bubble_facet_2011 &lt;-\n  ggplot(data = filter(gapminder_df, year == 2011)) +\n  aes(\n    x = gdp_per_capita,\n    y = fertility\n  ) +\n  scale_x_continuous(\n    name = \"GDP per Capita\",\n    trans = \"log10\",\n    labels = scales::comma\n  )  +\n  geom_point(alpha = 0.5) +\n  facet_wrap(~ continent)\n\nfig_bubble_facet_2011 \n\n\n\n\n\n\nFigure 4.6: Association between fertility rates and GDP per capita based on continent not using nrow\n\n\n\n\nIn the next example above, we removed the nrow argument and the system automatically separated the graphs into three columns with two rows. Additionally, we changed the x-axis to a log scale to better interpret gdp per capita. There is a way to determine a relationship between fertility and gdp per capita by continent.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>How to Create a Scatterplot</span>"
    ]
  },
  {
    "objectID": "lessons/01_scatterplots.html#public-health-interpretation",
    "href": "lessons/01_scatterplots.html#public-health-interpretation",
    "title": "\n4  How to Create a Scatterplot\n",
    "section": "\n4.6 Public Health Interpretation",
    "text": "4.6 Public Health Interpretation\nA global negative trend is depicted between GDP per capita and fertility over time. Such changes were due to wars as well as social, cultural and economic changes that incentivize smaller families especially in Asian countries. Most European, American and Asian countries depicted significant decreases in fertility rates over time as GDP per capita increased. On the other hand, African countries remain in the top rank for fertility over the years. These differences are depicted in the population pyramid changes of developed vs developing countries. Public health policies can be tailored to incentivizing increased fertility in developed countries to ensure generation continuity, and effective family planning strategies in developing countries.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>How to Create a Scatterplot</span>"
    ]
  },
  {
    "objectID": "lessons/01_scatterplots.html#conclusion",
    "href": "lessons/01_scatterplots.html#conclusion",
    "title": "\n4  How to Create a Scatterplot\n",
    "section": "\n4.7 Conclusion",
    "text": "4.7 Conclusion\nIn this lesson, the basic functions of ggplot2 package were shown, which can create a scatterplot. There are three layers to the code to make a plot in R: data, aesthetic, and geometric. Within the aesthetic layer, functions can be added such as size and color to analyze more variables. Additionally, facets can split up graphs over a categorical variable, adding another potential variable to analyze in the plot.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>How to Create a Scatterplot</span>"
    ]
  },
  {
    "objectID": "lessons_original/01_skimr.html",
    "href": "lessons_original/01_skimr.html",
    "title": "\n5  Skimr Package\n",
    "section": "",
    "text": "5.1 Introduction\nSkimr is an R package designed to provide summary statistics about variables in data frames, tibbles, data tables and vectors. The function is modifiable where you can add additional variables, which are not a part of default summary function within R. Skimr allows us to quickly assess data quality by feature and type in a quick report. This is a critical step in Data Exploration, where Understanding our data helps us to generate a hypothesis and determine what data analysis are appropriate.\nThis presentation will cover the simplest and most effective ways to explore data in R.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Skimr Package</span>"
    ]
  },
  {
    "objectID": "lessons_original/01_skimr.html#introduction",
    "href": "lessons_original/01_skimr.html#introduction",
    "title": "\n5  Skimr Package\n",
    "section": "",
    "text": "5.1.1 Packages\nTo begin we will upload the packages necessary for the lesson, this includes the following:\n\n\nreadr() to import our data file\n\nknitr() that houses the kable() feature that allows us to construct and customize tables.\n\ntidyverse houses the dyplyrpackage that assists with data manipulation and visualization.\nTheskimrpackage provides a compact summary of the variables in a dataset.\n\n\n# install.packages(\"skimr\")\n# install.packages(\"knitr\")\n# install.packages(\"tidyverse\")\n\n# load all the packages we will need to analyze the data and use the skim\n#   function\nlibrary(skimr)\nlibrary(knitr)\nlibrary(readxl)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n5.1.2 Census Data\nFor this assignment we will be using the Census_2010 dataset. There is no code book associated with the data, making it difficult to provide an accurate description of the variables. The information recorded shows the United States population estimates from the years 2010-2015, as well as relevant variables like net population change, number of births, number of deaths, international and domestic migration. Within the dataframe, there are 3,193 observations and 100 variables.\nThe data can be imported into R from the following link: https://fiudit-my.sharepoint.com/:x:/g/personal/ssinc013_fiu_edu/ESK1A13PstVGtf7HUwNNt68Bnh1YPfH8L-hnvMUxjBuCVw?e=CCwQU9\n\n# import the data\n# census_2010 &lt;- read_csv(\"Data/census_2010.csv\")\ncensus_2010 &lt;- readxl::read_xlsx(\"../data/01_census_2010.xlsx\")\n\n# what are the variables\ncolnames(census_2010) %&gt;% \n  head(n = 10)\n\n [1] \"SUMLEV\"            \"REGION\"            \"DIVISION\"         \n [4] \"STATE\"             \"COUNTY\"            \"STNAME\"           \n [7] \"CTYNAME\"           \"CENSUS2010POP\"     \"ESTIMATESBASE2010\"\n[10] \"POPESTIMATE2010\"",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Skimr Package</span>"
    ]
  },
  {
    "objectID": "lessons_original/01_skimr.html#the-summary-function",
    "href": "lessons_original/01_skimr.html#the-summary-function",
    "title": "\n5  Skimr Package\n",
    "section": "\n5.2 The Summary() Function",
    "text": "5.2 The Summary() Function\nIn R, the most similar function is summary(). The summary() function in R can be used to quickly summarize the values in a data frame or vector.\nThis syntax shows examples of the summary function using both our data set, and a vector:\n\n#| label: Summary-syntax-with-data\n\n# Example using summary function with data\nsummary(census_2010$CENSUS2010POP)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n      82    11299    26424   193387    71404 37253956 \n\n# Example using summary function with vector\n# Define vector\nx &lt;- c(3, 4, 23, 5, 7, 8, 9, 12, 26, 15, 20, 21, NA)\n\n# Summarize values in vector\nsummary(x)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   3.00    6.50   10.50   12.75   20.25   26.00       1 \n\n\nThe summary() function automatically calculates: The minimum value, The value of the 1st quartile (25th percentile), The median value, The value of the 3rd quartile (75th percentile) and The maximum value. Any missing values (NA) in the vector, the summary() function will automatically exclude them when calculating the summary statistics.\nNow, let’s see how skim() compares.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Skimr Package</span>"
    ]
  },
  {
    "objectID": "lessons_original/01_skimr.html#skimr-package",
    "href": "lessons_original/01_skimr.html#skimr-package",
    "title": "\n5  Skimr Package\n",
    "section": "\n5.3 Skimr Package",
    "text": "5.3 Skimr Package\nThe skim() function will generate a summary of the variables in your dataset, including their data type, number of non-missing values, minimum and maximum values, median, mean, standard deviation, and more (Waring et al. 2022).\nThe following syntax ensures that the data is compatible with Skimr functions.\n\nCode# is the summary data a skimr dataframe\nskim(census_2010) %&gt;% \n  is_skim_df() # TRUE\n\n[1] TRUE\nattr(,\"message\")\ncharacter(0)\n\n\nWe can explore the data as a tibble:\n\nCode# use skim to get descriptive statistics of the data\nskim(census_2010) %&gt;% \n  head(n = 10)\n\n\nData summary\n\n\nName\ncensus_2010\n\n\nNumber of rows\n3193\n\n\nNumber of columns\n100\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n8\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\nSTNAME\n0\n1\n4\n20\n0\n51\n0\n\n\nCTYNAME\n0\n1\n4\n33\n0\n1927\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nSUMLEV\n0\n1\n49.84\n1.25\n40\n50\n50\n50\n50\n▁▁▁▁▇\n\n\nREGION\n0\n1\n2.67\n0.81\n1\n2\n3\n3\n4\n▁▆▁▇▂\n\n\nDIVISION\n0\n1\n5.19\n1.97\n1\n4\n5\n7\n9\n▂▇▅▆▃\n\n\nSTATE\n0\n1\n30.26\n15.15\n1\n18\n29\n45\n56\n▃▇▆▆▇\n\n\nCOUNTY\n0\n1\n101.92\n107.63\n0\n33\n77\n133\n840\n▇▁▁▁▁\n\n\nCENSUS2010POP\n0\n1\n193387.05\n1176201.45\n82\n11299\n26424\n71404\n37253956\n▇▁▁▁▁\n\n\nESTIMATESBASE2010\n0\n1\n193396.87\n1176244.25\n82\n11299\n26446\n71491\n37254503\n▇▁▁▁▁\n\n\nPOPESTIMATE2010\n0\n1\n193765.65\n1178710.28\n83\n11275\n26467\n71721\n37334079\n▇▁▁▁▁\n\n\n\n\n\nUsing skimr functions provides a cleaner and more detailed display of the results compared to the summary() function. In this example we are showing the first ten variables in our data set. The data summary tab shows the number of rows and columns, column type frequency and group variables. There is also additional descriptive information like missing values, unique characters.\nThis will be relevant for data cleaning as well as understanding the distribution. Both are critical to determine which statistical analysis would be most appropriate to use for a project.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Skimr Package</span>"
    ]
  },
  {
    "objectID": "lessons_original/01_skimr.html#other-skimr-features",
    "href": "lessons_original/01_skimr.html#other-skimr-features",
    "title": "\n5  Skimr Package\n",
    "section": "\n5.4 Other Skimr Features",
    "text": "5.4 Other Skimr Features\n\n5.4.1 Separate dataframes by type\nThe data frames produced by skim() are wide and sparse, filled with columns that are mostly NA. For that reason, it can be convenient to work with “by type” subsets of the original data frame. These smaller subsets have their NA columns removed.\nFeatures:\n\n\npartition() - Creates a list of smaller data frames. Each entry in the list is a data type from the original dataframe\n\nbind() - Takes the list and rebuilds the original dataframe.\n\nyank() - Extract a subtable from a dataframe with a particular type.\n\nThe following syntax is using partition() to separate the large census_df.\n\nCode# split the character and numeric data\nseparate_df &lt;- partition(skim(census_2010))\n# check only the character data\nseparate_df$character\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\nSTNAME\n0\n1\n4\n20\n0\n51\n0\n\n\nCTYNAME\n0\n1\n4\n33\n0\n1927\n0\n\n\n\n\nCode# create summary statistics for only numeric variables\nnumeric_separate_df &lt;- separate_df[2]\n# pull out the desired summary statistics in the nested list\nhead(numeric_separate_df$numeric[\"mean\"]) %&gt;% \n  kable(digits = 1) \n\n\n\nmean\n\n\n\n49.8\n\n\n2.7\n\n\n5.2\n\n\n30.3\n\n\n101.9\n\n\n193387.1\n\n\n\n\n\nThe following syntax is using bind() to combine the smaller character and numeric lists into the desired df.\n\nCode# combine the character and numeric data\nhead(bind(separate_df))\n\n\nData summary\n\n\nName\ncensus_2010\n\n\nNumber of rows\n3193\n\n\nNumber of columns\n100\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\nSTNAME\n0\n1\n4\n20\n0\n51\n0\n\n\nCTYNAME\n0\n1\n4\n33\n0\n1927\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nSUMLEV\n0\n1\n49.84\n1.25\n40\n50\n50\n50\n50\n▁▁▁▁▇\n\n\nREGION\n0\n1\n2.67\n0.81\n1\n2\n3\n3\n4\n▁▆▁▇▂\n\n\nDIVISION\n0\n1\n5.19\n1.97\n1\n4\n5\n7\n9\n▂▇▅▆▃\n\n\nSTATE\n0\n1\n30.26\n15.15\n1\n18\n29\n45\n56\n▃▇▆▆▇\n\n\n\n\nCode# confirm that the bound table is the same as the original skimmed table\nidentical(bind(separate_df), skim(census_2010)) \n\n[1] TRUE\n\n\nThe following syntax is using yank() to extract a specific table eg.character to examine.\n\nCode# Extract character data\nyank(skim(census_2010), \"character\")\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\nSTNAME\n0\n1\n4\n20\n0\n51\n0\n\n\nCTYNAME\n0\n1\n4\n33\n0\n1927\n0\n\n\n\n\n\n\n5.4.2 Skimr with Dplyr\nSkimr functions can be used in combination with Dplyr functions to examine specific variables within the census dataset.\nThe following example used skim() with filter() to display the variable CENSUS2010POP. The dataframe was further customized to display variable name and data type using select().\n\nCode# use dplyr functions on the statistics summary table\ncensus_filter &lt;- skim(census_2010) %&gt;% \n  filter(skim_variable == \"CENSUS2010POP\")\ncensus_filter\n\n\nData summary\n\n\nName\ncensus_2010\n\n\nNumber of rows\n3193\n\n\nNumber of columns\n100\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\nCENSUS2010POP\n0\n1\n193387\n1176201\n82\n11299\n26424\n71404\n37253956\n▇▁▁▁▁\n\n\n\nCodecensus_select &lt;- skim(census_2010) %&gt;% \n  select(skim_type, skim_variable)\nhead(census_select)\n\n# A tibble: 6 × 2\n  skim_type skim_variable\n  &lt;chr&gt;     &lt;chr&gt;        \n1 character STNAME       \n2 character CTYNAME      \n3 numeric   SUMLEV       \n4 numeric   REGION       \n5 numeric   DIVISION     \n6 numeric   STATE        \n\n\nYou can also customize the output of the skim() function by using various arguments. For example, you can use the numeric argument to specify which variables should be treated as numeric variables, or use the ranges argument to specify custom ranges for variables.\nUsing skim() in combination with mutate() we will compute a new variable to add to our skim dataframe.\n\nCode# create a new variable calculate the change in birth rate from 2010 to 2011\ncensus_2010 %&gt;% \n  # new variable\n  mutate(net_birth = BIRTHS2011 - BIRTHS2010) %&gt;% \n  # move the variable to the beginning of the dataset\n  relocate(net_birth, .after = CENSUS2010POP) %&gt;% \n  # summary statistics table\n  skim() %&gt;% \n  # only the first fifteen variables\n  head(n = 15) %&gt;% \n  # change the formatting \n  kable(digit = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_type\nskim_variable\nn_missing\ncomplete_rate\ncharacter.min\ncharacter.max\ncharacter.empty\ncharacter.n_unique\ncharacter.whitespace\nnumeric.mean\nnumeric.sd\nnumeric.p0\nnumeric.p25\nnumeric.p50\nnumeric.p75\nnumeric.p100\nnumeric.hist\n\n\n\ncharacter\nSTNAME\n0\n1\n4\n20\n0\n51\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\ncharacter\nCTYNAME\n0\n1\n4\n33\n0\n1927\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nnumeric\nSUMLEV\n0\n1\nNA\nNA\nNA\nNA\nNA\n49.84\n1.25\n40\n50\n50\n50\n50\n▁▁▁▁▇\n\n\nnumeric\nREGION\n0\n1\nNA\nNA\nNA\nNA\nNA\n2.67\n0.81\n1\n2\n3\n3\n4\n▁▆▁▇▂\n\n\nnumeric\nDIVISION\n0\n1\nNA\nNA\nNA\nNA\nNA\n5.19\n1.97\n1\n4\n5\n7\n9\n▂▇▅▆▃\n\n\nnumeric\nSTATE\n0\n1\nNA\nNA\nNA\nNA\nNA\n30.26\n15.15\n1\n18\n29\n45\n56\n▃▇▆▆▇\n\n\nnumeric\nCOUNTY\n0\n1\nNA\nNA\nNA\nNA\nNA\n101.92\n107.63\n0\n33\n77\n133\n840\n▇▁▁▁▁\n\n\nnumeric\nCENSUS2010POP\n0\n1\nNA\nNA\nNA\nNA\nNA\n193387.05\n1176201.45\n82\n11299\n26424\n71404\n37253956\n▇▁▁▁▁\n\n\nnumeric\nnet_birth\n0\n1\nNA\nNA\nNA\nNA\nNA\n1870.12\n11792.85\n-3\n96\n232\n639\n386443\n▇▁▁▁▁\n\n\nnumeric\nESTIMATESBASE2010\n0\n1\nNA\nNA\nNA\nNA\nNA\n193396.87\n1176244.25\n82\n11299\n26446\n71491\n37254503\n▇▁▁▁▁\n\n\nnumeric\nPOPESTIMATE2010\n0\n1\nNA\nNA\nNA\nNA\nNA\n193765.65\n1178710.28\n83\n11275\n26467\n71721\n37334079\n▇▁▁▁▁\n\n\nnumeric\nPOPESTIMATE2011\n0\n1\nNA\nNA\nNA\nNA\nNA\n195251.40\n1189647.76\n90\n11277\n26417\n72387\n37700034\n▇▁▁▁▁\n\n\nnumeric\nPOPESTIMATE2012\n0\n1\nNA\nNA\nNA\nNA\nNA\n196744.52\n1200508.37\n81\n11195\n26362\n72496\n38056055\n▇▁▁▁▁\n\n\nnumeric\nPOPESTIMATE2013\n0\n1\nNA\nNA\nNA\nNA\nNA\n198200.69\n1211123.45\n89\n11180\n26519\n72222\n38414128\n▇▁▁▁▁\n\n\nnumeric\nPOPESTIMATE2014\n0\n1\nNA\nNA\nNA\nNA\nNA\n199754.09\n1222669.36\n87\n11121\n26483\n72257\n38792291\n▇▁▁▁▁\n\n\n\n\n\n\n5.4.3 Adding Variables\n\nbase - An sfl that sets skimmers for all column types.\nappend - Whether the provided options should be in addition to the defaults already in skim. Default is TRUE.\n\nAs mentioned, skim() is designed to display default statistics, however you can use this function to change the summary statistics that it returns.\nskim_with() is type closure: a function that returns adds a new variable to the table. This lets you have several skimming functions in a single R session, but it also means that you need to assign the return of skim_with() before you can use it.\nYou assign values within skim_with() by using the sfl() helper (skimr function list). It identifies which skimming functions you want to remove, by setting them to NULL. Assign an sfl to each column type that you wish to modify.\nFor example, we will add the following variables to the dataframe: median, min, max, IQR, length.\n\nCodemy_skim &lt;- skim_with(\n  numeric = sfl(median, min, max, IQR),\n  character = sfl(length), \n  append = TRUE\n)\n\n# add new variables into the summary table\ncensus_2010 %&gt;% \n  my_skim() %&gt;% \n  head(n = 10)\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n3193\n\n\nNumber of columns\n100\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n8\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\nlength\n\n\n\nSTNAME\n0\n1\n4\n20\n0\n51\n0\n3193\n\n\nCTYNAME\n0\n1\n4\n33\n0\n1927\n0\n3193\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\nmedian\nmin\nmax\nIQR\n\n\n\nSUMLEV\n0\n1\n49.84\n1.25\n40\n50\n50\n50\n50\n▁▁▁▁▇\n50\n40\n50\n0\n\n\nREGION\n0\n1\n2.67\n0.81\n1\n2\n3\n3\n4\n▁▆▁▇▂\n3\n1\n4\n1\n\n\nDIVISION\n0\n1\n5.19\n1.97\n1\n4\n5\n7\n9\n▂▇▅▆▃\n5\n1\n9\n3\n\n\nSTATE\n0\n1\n30.26\n15.15\n1\n18\n29\n45\n56\n▃▇▆▆▇\n29\n1\n56\n27\n\n\nCOUNTY\n0\n1\n101.92\n107.63\n0\n33\n77\n133\n840\n▇▁▁▁▁\n77\n0\n840\n100\n\n\nCENSUS2010POP\n0\n1\n193387.05\n1176201.45\n82\n11299\n26424\n71404\n37253956\n▇▁▁▁▁\n26424\n82\n37253956\n60105\n\n\nESTIMATESBASE2010\n0\n1\n193396.87\n1176244.25\n82\n11299\n26446\n71491\n37254503\n▇▁▁▁▁\n26446\n82\n37254503\n60192\n\n\nPOPESTIMATE2010\n0\n1\n193765.65\n1178710.28\n83\n11275\n26467\n71721\n37334079\n▇▁▁▁▁\n26467\n83\n37334079\n60446",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Skimr Package</span>"
    ]
  },
  {
    "objectID": "lessons_original/01_skimr.html#conclusion",
    "href": "lessons_original/01_skimr.html#conclusion",
    "title": "\n5  Skimr Package\n",
    "section": "\n5.5 Conclusion",
    "text": "5.5 Conclusion\nOverall, Skimr is a useful package for quickly summarizing the variables in a dataset and gaining insights into its structure and content.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Skimr Package</span>"
    ]
  },
  {
    "objectID": "lessons_original/01_skimr.html#references",
    "href": "lessons_original/01_skimr.html#references",
    "title": "\n5  Skimr Package\n",
    "section": "\n5.6 References",
    "text": "5.6 References\n\n\n\n\nWaring, Elin, Michael Quinn, Amelia McNamara, Eduardo Arino de la Rubia, Hao Zhu, and Shannon Ellis. 2022. Skimr: Compact and Flexible Summaries of Data. https://docs.ropensci.org/skimr/.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Skimr Package</span>"
    ]
  },
  {
    "objectID": "lessons/01_table1.html",
    "href": "lessons/01_table1.html",
    "title": "\n6  Demographics Table With Table1\n",
    "section": "",
    "text": "6.1 Introduction\nIn most scientific research journals, the first included table is often referred to as Table1. It is a table that presents descriptive statistics of baseline characteristics of the study population stratified by exposure. This package makes it fairly straightforward to produce such a table using R. Table1 includes descriptive statistics for the total study sample, with the rows (explanatory variables) consisting of the key study variables that are often included in the final analysis1. Then within the columns (outcome of interest/response variable), you will find cells given as an (%) for categorical variables, whereas a mean, SD, or the median will be provided for continuous variables. Additionally, there will be a total column provided which can help in the assessment of the overall sample.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Demographics Table With Table1</span>"
    ]
  },
  {
    "objectID": "lessons/01_table1.html#necessary-packages",
    "href": "lessons/01_table1.html#necessary-packages",
    "title": "\n6  Demographics Table With Table1\n",
    "section": "\n6.2 Necessary Packages",
    "text": "6.2 Necessary Packages\nThe htmlTable package allows for the usage of the table1() function to create a table 1, while also making life easy when attempting to copy this table into a Word document.\nThe boot package was created to aid in performing bootstrapping analysis. With it comes numerous data sets, specifically clinical trial data sets to make this possible. However, there is no code book provided within the package when the data is downloaded as a csv file. This is a link on Github that explains and elaborates on every data within the package itself2.\n\n#install.packages(\"htmlTable\")\n#install.packages(\"boot\")\n\n# Load libraries\nlibrary(htmlTable)\nlibrary(table1)\nlibrary(boot)",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Demographics Table With Table1</span>"
    ]
  },
  {
    "objectID": "lessons/01_table1.html#data-source-and-description",
    "href": "lessons/01_table1.html#data-source-and-description",
    "title": "\n6  Demographics Table With Table1\n",
    "section": "\n6.3 Data source and description",
    "text": "6.3 Data source and description\nToday, we will be using the melanoma data set which consists of malignant melanoma measurements of patients. Each patient had their tumor surgically removed between the years of 1962 and 1977 at the Department of Plastic Surgery, University Hospital of Odense located in Denamrk. Each surgery consisted of the complete removal of the tumor with an additional removal of about 2.5cm of the surrounding skin. When this was completed, the thickness of the tumor was recorded along with the physical appearance of ulceration vs no ulceration, as it is an important prognostic indication of those with a thick/ulcerated tumor to have an increased chance of death as a consequence of melanoma.\n\ndata(melanoma, package = \"boot\")\nmelanoma_data &lt;- melanoma\n\n#Now that we loaded the raw data set, we will conduct a visual exploration before wrangling #the data and applying any functions, while also considering the requirements involved in #the construction of a table1.\n\nsummary(melanoma_data)\n\n      time          status          sex              age             year     \n Min.   :  10   Min.   :1.00   Min.   :0.0000   Min.   : 4.00   Min.   :1962  \n 1st Qu.:1525   1st Qu.:1.00   1st Qu.:0.0000   1st Qu.:42.00   1st Qu.:1968  \n Median :2005   Median :2.00   Median :0.0000   Median :54.00   Median :1970  \n Mean   :2153   Mean   :1.79   Mean   :0.3854   Mean   :52.46   Mean   :1970  \n 3rd Qu.:3042   3rd Qu.:2.00   3rd Qu.:1.0000   3rd Qu.:65.00   3rd Qu.:1972  \n Max.   :5565   Max.   :3.00   Max.   :1.0000   Max.   :95.00   Max.   :1977  \n   thickness         ulcer      \n Min.   : 0.10   Min.   :0.000  \n 1st Qu.: 0.97   1st Qu.:0.000  \n Median : 1.94   Median :0.000  \n Mean   : 2.92   Mean   :0.439  \n 3rd Qu.: 3.56   3rd Qu.:1.000  \n Max.   :17.42   Max.   :1.000",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Demographics Table With Table1</span>"
    ]
  },
  {
    "objectID": "lessons/01_table1.html#cleaning-the-data-to-create-a-model-data-frame",
    "href": "lessons/01_table1.html#cleaning-the-data-to-create-a-model-data-frame",
    "title": "\n6  Demographics Table With Table1\n",
    "section": "\n6.4 Cleaning the data to create a model data frame",
    "text": "6.4 Cleaning the data to create a model data frame\nLet us now explore the type of variables within the data set.\n\ntypeof(melanoma_data$status) \n\n[1] \"double\"\n\n\nWe will first provide a basic table1 to illustrate how the function works. Currently, all the variables are in numeric/double formats, however for the creation of a basic table1, it is of importance to convert the dependent/response variable of interest to reflect categories (factor).\nOur main variable of interest (dependent/response) is the status. According to the code book found in Github, status is coded into three levels that indicate the patients status at the end of the study. Level 1 indicates that they had died from melanoma, Level 2 indicates that they were still alive at the conclusion of the study, and Level 3 indicates that they had died from causes unrelated to their melanoma. As such, we will factor the “status” variable into three levels. With this in mind, let us go ahead and convert melanoma into a factor variable with three levels. For ease of analysis we will use 2 = “Alive” as the reference level. This can be done in two ways:\n\nAlthough more time consuming, it is highly recommended that beginners utilize the function as.factor() and then utilize the recode_factor() function to minimize the errors.\nWhen you become more skilled and are able to understand how the factor function works, it is possible to do everything in one step with the factor() function. In this function you can put levels and labels all in one function instead of having to break it up into more than one function.\n\nFor our example we will use as.factor then recode_factor() using 2 = “Alive” as our reference group.\n\nmelanoma_data$status &lt;-\n  as.factor(melanoma_data$status)\n\n# print the first six observations\nhead(melanoma_data$status)\n\n[1] 3 3 2 3 1 1\nLevels: 1 2 3\n\n# Recode\nmelanoma_data$status &lt;- recode_factor(\n  melanoma_data$status, \n  \"2\" = \"Alive\", # this is the reference group\n  \"1\" = \"Died from melanoma\",\n  \"3\" = \"Non-Melanoma death\"\n)\n\n# Print the first six observations\nhead(melanoma_data$status)\n\n[1] Non-Melanoma death Non-Melanoma death Alive              Non-Melanoma death\n[5] Died from melanoma Died from melanoma\nLevels: Alive Died from melanoma Non-Melanoma death\n\n\nAs you can see in the variable levels, “Alive” is the reference level. It is extremely important to pick a reference level to lay the foundation of the table along with highlighting the outcome of interest of your hypothesis. In summary, this lays the foundation of a well organized table.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Demographics Table With Table1</span>"
    ]
  },
  {
    "objectID": "lessons/01_table1.html#creation-of-basic-table-1",
    "href": "lessons/01_table1.html#creation-of-basic-table-1",
    "title": "\n6  Demographics Table With Table1\n",
    "section": "\n6.5 Creation of basic table 1",
    "text": "6.5 Creation of basic table 1\nNow that our main variable of interest is a factor with three levels, we will run a basic table1 with the independent/explanatory variables of interest: sex, age, ulcer, and thickness.\nRecall that the explanatory variables of interest are still in “double” formats. Conveniently, to analyze data before the independent variables are converted to factors and labeled, the table1 provides the ability to highlight level results. This only applies for independent variables that are in numeric/double formats in which each number represents a group. For instance 0 although is a number format we know it has a group meaning such as male.\nFor the independent variables, if they have factors in the front, it provides the number of cases (aka observations). If they are a continuous variable, we will get the mean, the SD, the minimum and the maximum amounts.\n\nbasic_table1 &lt;- table1( \n  ~ factor(sex) + age + factor(ulcer) + thickness | status, \n  data = melanoma_data\n)\n\nbasic_table1\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlive(N=134)\nDied from melanoma(N=57)\nNon-Melanoma death(N=14)\nOverall(N=205)\n\n\n\nfactor(sex)\n\n\n\n\n\n\n0\n91 (67.9%)\n28 (49.1%)\n7 (50.0%)\n126 (61.5%)\n\n\n1\n43 (32.1%)\n29 (50.9%)\n7 (50.0%)\n79 (38.5%)\n\n\nage\n\n\n\n\n\n\nMean (SD)\n50.0 (15.9)\n55.1 (17.9)\n65.3 (10.9)\n52.5 (16.7)\n\n\nMedian [Min, Max]\n52.0 [4.00, 84.0]\n56.0 [14.0, 95.0]\n65.0 [49.0, 86.0]\n54.0 [4.00, 95.0]\n\n\nfactor(ulcer)\n\n\n\n\n\n\n0\n92 (68.7%)\n16 (28.1%)\n7 (50.0%)\n115 (56.1%)\n\n\n1\n42 (31.3%)\n41 (71.9%)\n7 (50.0%)\n90 (43.9%)\n\n\nthickness\n\n\n\n\n\n\nMean (SD)\n2.24 (2.33)\n4.31 (3.57)\n3.72 (3.63)\n2.92 (2.96)\n\n\nMedian [Min, Max]\n1.36 [0.100, 12.9]\n3.54 [0.320, 17.4]\n2.26 [0.160, 12.6]\n1.94 [0.100, 17.4]\n\n\n\n\n\n\n\nNote that the table1 package uses a familiar formula interface, where the variables to include in the table are separated by ‘+’ symbols, the “stratification” variable (which creates the columns) appears to the right of a “conditioning” symbol ‘|’, and the data argument specifies a data.frame that contains the variables in the formula.\nIf we do not put factor for a grouped variable then the following will happen:\n\nwrong_table1 &lt;- table1(\n  ~ sex + age + ulcer + thickness | status, \n  data = melanoma_data\n)\n\nwrong_table1\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlive(N=134)\nDied from melanoma(N=57)\nNon-Melanoma death(N=14)\nOverall(N=205)\n\n\n\nsex\n\n\n\n\n\n\nMean (SD)\n0.321 (0.469)\n0.509 (0.504)\n0.500 (0.519)\n0.385 (0.488)\n\n\nMedian [Min, Max]\n0 [0, 1.00]\n1.00 [0, 1.00]\n0.500 [0, 1.00]\n0 [0, 1.00]\n\n\nage\n\n\n\n\n\n\nMean (SD)\n50.0 (15.9)\n55.1 (17.9)\n65.3 (10.9)\n52.5 (16.7)\n\n\nMedian [Min, Max]\n52.0 [4.00, 84.0]\n56.0 [14.0, 95.0]\n65.0 [49.0, 86.0]\n54.0 [4.00, 95.0]\n\n\nulcer\n\n\n\n\n\n\nMean (SD)\n0.313 (0.466)\n0.719 (0.453)\n0.500 (0.519)\n0.439 (0.497)\n\n\nMedian [Min, Max]\n0 [0, 1.00]\n1.00 [0, 1.00]\n0.500 [0, 1.00]\n0 [0, 1.00]\n\n\nthickness\n\n\n\n\n\n\nMean (SD)\n2.24 (2.33)\n4.31 (3.57)\n3.72 (3.63)\n2.92 (2.96)\n\n\nMedian [Min, Max]\n1.36 [0.100, 12.9]\n3.54 [0.320, 17.4]\n2.26 [0.160, 12.6]\n1.94 [0.100, 17.4]\n\n\n\n\n\n\n\nAs you can see above, we have the incorrect values provided of the explanatory variables. For example, in the variable of sex, we expect to see the number of individuals who identify as male or female, but instead we observe the mean, which is not a proper descriptive statistic as sex is a categorical variable.\nTo avoid this issue as well as problems in other procedures (like logistic regressions), it is crucial that we remember to factor the variables before we run any function. But because we don’t have nice labels for the variables and categories, it doesn’t look great. To improve things, we can create factors with descriptive labels for the categorical variables (sex and ulcer), label each variable the way we want, and specify units for the continuous variables (age and thickness). According to the code book, the patient’s sex: 1 = male, 0 = female, and ulcer is an indicator of ulceration : 1 = present, 0 = absent. We also specify that the overall column to be labeled “Total” and be positioned on the left, and add a caption and footnote:\n\nmelanoma_data$sex &lt;- as.factor(melanoma_data$sex)\n\n# print the first six observations\nhead(melanoma_data$sex)\n\n[1] 1 1 1 0 1 1\nLevels: 0 1\n\n# Recode\nmelanoma_data$sex &lt;- recode_factor(\n  melanoma_data$sex, \n  \"0\" = \"Female\",\n  \"1\" = \"Male\"\n)\n\n# Print the first six observations\nhead(melanoma_data$sex)\n\n[1] Male   Male   Male   Female Male   Male  \nLevels: Female Male\n\n\n\ntypeof(melanoma_data$ulcer)\n\n[1] \"double\"\n\nmelanoma_data$ulcer &lt;- as.factor(melanoma_data$ulcer)\n\n# print the first six observations\nhead(melanoma_data$ulcer)\n\n[1] 1 0 0 0 1 1\nLevels: 0 1\n\n# Recode\nmelanoma_data$ulcer &lt;- recode_factor(\n  melanoma_data$ulcer, \n  \"0\" = \"Absent\",\n  \"1\" = \"Present\"\n)\n\n# Print the first six observations\nhead(melanoma_data$ulcer)\n\n[1] Present Absent  Absent  Absent  Present Present\nLevels: Absent Present\n\n\nIn addition, we need to add units to the two continuous variables age and thickness. According to the code book, age is the patient’s age measured in years and thickness corresponds to the tumor’s thickness in millimeters (mm). The package table1 provides an easy way to demonstrate measurement information:\n\nunits(melanoma_data$age) &lt;- \"years\"\nunits(melanoma_data$thickness) &lt;- \"mm\"\n\nAdditionally, for visual and descriptive purposes, the function table1 is able to easily provide labels for the variables that will be shown in the final table using the label() function. Also, (caption \\&lt;-) provides a title for the table and (footnote \\&lt;-) provides any footnote information.\n\nlabel(melanoma_data$sex) &lt;- \"Sex\"\nlabel(melanoma_data$age) &lt;- \"Age\"\nlabel(melanoma_data$ulcer) &lt;- \"Ulceration\"\nlabel(melanoma_data$thickness) &lt;-\"Thickness*\"\n\ncaption_char &lt;- \"Table 1. Melanoma Dataset Descriptive Statistics\"\nfootnote_char &lt;- \"*Also known as Breslow thickness\"\n\nBelow, we can demonstrate the final table1 layout. As you can see, you no longer use factor() in front of the variable as we already factorized it in the previous steps.\n\ntable1(\n  ~ sex + age + ulcer + thickness | status, \n  data = melanoma_data,\n  overall = c(left = \"Total\"), \n  caption = caption_char, \n  footnote = footnote_char\n)\n\n\nTable 1. Melanoma Dataset Descriptive Statistics\n\n\n\nTotal(N=205)\nAlive(N=134)\nDied from melanoma(N=57)\nNon-Melanoma death(N=14)\n\n\n*Also known as Breslow thickness\n\n\n\nSex\n\n\n\n\n\n\nFemale\n126 (61.5%)\n91 (67.9%)\n28 (49.1%)\n7 (50.0%)\n\n\nMale\n79 (38.5%)\n43 (32.1%)\n29 (50.9%)\n7 (50.0%)\n\n\nAge (years)\n\n\n\n\n\n\nMean (SD)\n52.5 (16.7)\n50.0 (15.9)\n55.1 (17.9)\n65.3 (10.9)\n\n\nMedian [Min, Max]\n54.0 [4.00, 95.0]\n52.0 [4.00, 84.0]\n56.0 [14.0, 95.0]\n65.0 [49.0, 86.0]\n\n\nUlceration\n\n\n\n\n\n\nAbsent\n115 (56.1%)\n92 (68.7%)\n16 (28.1%)\n7 (50.0%)\n\n\nPresent\n90 (43.9%)\n42 (31.3%)\n41 (71.9%)\n7 (50.0%)\n\n\nThickness* (mm)\n\n\n\n\n\n\nMean (SD)\n2.92 (2.96)\n2.24 (2.33)\n4.31 (3.57)\n3.72 (3.63)\n\n\nMedian [Min, Max]\n1.94 [0.100, 17.4]\n1.36 [0.100, 12.9]\n3.54 [0.320, 17.4]\n2.26 [0.160, 12.6]",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Demographics Table With Table1</span>"
    ]
  },
  {
    "objectID": "lessons/01_table1.html#changing-the-tables-appearance",
    "href": "lessons/01_table1.html#changing-the-tables-appearance",
    "title": "\n6  Demographics Table With Table1\n",
    "section": "\n6.6 Changing the table’s appearance",
    "text": "6.6 Changing the table’s appearance\nThe default style of table1 uses an Arial font, and resembles the booktabs style commonly used in LaTeX. While this default style is not ugly, inevitably there will be a desire to customize the visual appearance of the table (fonts, colors, gridlines, etc). The package provides a limited number of built-in options for changing the style, while further customization can be achieved in R Markdown documents using CSS.3\n\n6.6.1 Using built-in styles\nThe package includes a limited number of built-in styles including:\n\nzebra: alternating shaded and unshaded rows (zebra stripes)\ngrid: show all grid lines\nshade: shade the header row(s) in gray\ntimes: use a serif font\n\nThese styles can be selected using the topclass argument of table1. Some examples follow:\n\ntable1(~ sex + age + ulcer + thickness | status, \n       data = melanoma_data,\n       overall = c(left = \"Total\"), \n       caption = caption_char, \n       footnote = footnote_char, \n       topclass=\"Rtable1-zebra\"\n)\n\n\nTable 1. Melanoma Dataset Descriptive Statistics\n\n\n\nTotal(N=205)\nAlive(N=134)\nDied from melanoma(N=57)\nNon-Melanoma death(N=14)\n\n\n*Also known as Breslow thickness\n\n\n\nSex\n\n\n\n\n\n\nFemale\n126 (61.5%)\n91 (67.9%)\n28 (49.1%)\n7 (50.0%)\n\n\nMale\n79 (38.5%)\n43 (32.1%)\n29 (50.9%)\n7 (50.0%)\n\n\nAge (years)\n\n\n\n\n\n\nMean (SD)\n52.5 (16.7)\n50.0 (15.9)\n55.1 (17.9)\n65.3 (10.9)\n\n\nMedian [Min, Max]\n54.0 [4.00, 95.0]\n52.0 [4.00, 84.0]\n56.0 [14.0, 95.0]\n65.0 [49.0, 86.0]\n\n\nUlceration\n\n\n\n\n\n\nAbsent\n115 (56.1%)\n92 (68.7%)\n16 (28.1%)\n7 (50.0%)\n\n\nPresent\n90 (43.9%)\n42 (31.3%)\n41 (71.9%)\n7 (50.0%)\n\n\nThickness* (mm)\n\n\n\n\n\n\nMean (SD)\n2.92 (2.96)\n2.24 (2.33)\n4.31 (3.57)\n3.72 (3.63)\n\n\nMedian [Min, Max]\n1.94 [0.100, 17.4]\n1.36 [0.100, 12.9]\n3.54 [0.320, 17.4]\n2.26 [0.160, 12.6]\n\n\n\n\n\n\ntable1(~ sex + age + ulcer + thickness | status, \n       data = melanoma_data,\n       overall = c(left = \"Total\"), \n       caption = caption_char, \n       footnote = footnote_char, \n       topclass=\"Rtable1-grid\"\n)\n\n\nTable 1. Melanoma Dataset Descriptive Statistics\n\n\n\nTotal(N=205)\nAlive(N=134)\nDied from melanoma(N=57)\nNon-Melanoma death(N=14)\n\n\n*Also known as Breslow thickness\n\n\n\nSex\n\n\n\n\n\n\nFemale\n126 (61.5%)\n91 (67.9%)\n28 (49.1%)\n7 (50.0%)\n\n\nMale\n79 (38.5%)\n43 (32.1%)\n29 (50.9%)\n7 (50.0%)\n\n\nAge (years)\n\n\n\n\n\n\nMean (SD)\n52.5 (16.7)\n50.0 (15.9)\n55.1 (17.9)\n65.3 (10.9)\n\n\nMedian [Min, Max]\n54.0 [4.00, 95.0]\n52.0 [4.00, 84.0]\n56.0 [14.0, 95.0]\n65.0 [49.0, 86.0]\n\n\nUlceration\n\n\n\n\n\n\nAbsent\n115 (56.1%)\n92 (68.7%)\n16 (28.1%)\n7 (50.0%)\n\n\nPresent\n90 (43.9%)\n42 (31.3%)\n41 (71.9%)\n7 (50.0%)\n\n\nThickness* (mm)\n\n\n\n\n\n\nMean (SD)\n2.92 (2.96)\n2.24 (2.33)\n4.31 (3.57)\n3.72 (3.63)\n\n\nMedian [Min, Max]\n1.94 [0.100, 17.4]\n1.36 [0.100, 12.9]\n3.54 [0.320, 17.4]\n2.26 [0.160, 12.6]\n\n\n\n\n\n\ntable1(~ sex + age + ulcer + thickness | status, \n       data = melanoma_data,\n       overall = c(left = \"Total\"), \n       caption = caption_char, \n       footnote = footnote_char, \n       topclass=\"Rtable1-grid Rtable1-shade Rtable1-times\"\n)\n\n\nTable 1. Melanoma Dataset Descriptive Statistics\n\n\n\nTotal(N=205)\nAlive(N=134)\nDied from melanoma(N=57)\nNon-Melanoma death(N=14)\n\n\n*Also known as Breslow thickness\n\n\n\nSex\n\n\n\n\n\n\nFemale\n126 (61.5%)\n91 (67.9%)\n28 (49.1%)\n7 (50.0%)\n\n\nMale\n79 (38.5%)\n43 (32.1%)\n29 (50.9%)\n7 (50.0%)\n\n\nAge (years)\n\n\n\n\n\n\nMean (SD)\n52.5 (16.7)\n50.0 (15.9)\n55.1 (17.9)\n65.3 (10.9)\n\n\nMedian [Min, Max]\n54.0 [4.00, 95.0]\n52.0 [4.00, 84.0]\n56.0 [14.0, 95.0]\n65.0 [49.0, 86.0]\n\n\nUlceration\n\n\n\n\n\n\nAbsent\n115 (56.1%)\n92 (68.7%)\n16 (28.1%)\n7 (50.0%)\n\n\nPresent\n90 (43.9%)\n42 (31.3%)\n41 (71.9%)\n7 (50.0%)\n\n\nThickness* (mm)\n\n\n\n\n\n\nMean (SD)\n2.92 (2.96)\n2.24 (2.33)\n4.31 (3.57)\n3.72 (3.63)\n\n\nMedian [Min, Max]\n1.94 [0.100, 17.4]\n1.36 [0.100, 12.9]\n3.54 [0.320, 17.4]\n2.26 [0.160, 12.6]\n\n\n\n\n\nNote that the style name needs to be preceded by the prefix Rtable1-. Multiple styles can be applied in combination by separating them with a space.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Demographics Table With Table1</span>"
    ]
  },
  {
    "objectID": "lessons/01_table1.html#conclusion",
    "href": "lessons/01_table1.html#conclusion",
    "title": "\n6  Demographics Table With Table1\n",
    "section": "\n6.7 Conclusion",
    "text": "6.7 Conclusion\nIn conclusion, table1 is one of the most utilized tools in the scientific research field. Understanding how to use the table1 package in R can be of benefit to many. It is important to note that this presentation is just a brief summary with what is possible with this package. For example, you can add extra columns to the table, other than descriptive statistics. This can be accomplished using the extra.col option. In addition, you can also stratify the response variable to highlight two of the responses, like dead or alive in our example.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Demographics Table With Table1</span>"
    ]
  },
  {
    "objectID": "lessons/01_table1.html#references",
    "href": "lessons/01_table1.html#references",
    "title": "\n6  Demographics Table With Table1\n",
    "section": "References",
    "text": "References\n\n\n\n\n1. \nHayes-Larson E, Kezios KL, Mooney SJ, Lovasi G. Who is in this study, anyway? Guidelines for a useful Table 1. Journal of Clinical Epidemiology [Internet] 2019;114:125–32. Available from: http://dx.doi.org/10.1016/j.jclinepi.2019.06.011\n\n\n\n2. \nA. C. Davison, D. V. Hinkley. Bootstrap methods and their applications [Internet]. Cambridge: Cambridge University Press; 1997. Available from: doi:10.1017/CBO9780511802843\n\n\n\n3. \nCohen J. Statistical power analysis for the behavioral sciences. 2nd ed. Hillsdale, N.J: L. Erlbaum Associates; 1988.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Demographics Table With Table1</span>"
    ]
  },
  {
    "objectID": "lessons/01_gtsummary.html",
    "href": "lessons/01_gtsummary.html",
    "title": "\n7  Table by gtsummary\n",
    "section": "",
    "text": "7.1 Packages for this Lesson\n# Installing Required Packages\n# install.packages(\"public.ctn0094data\")\n# install.packages(\"tidyverse\")\n# install.packages(\"gtsummary\")\n\n# Loading Required Packages\nlibrary(public.ctn0094data)\nlibrary(tidyverse)\nlibrary(gtsummary)\nlibrary(dplyr) # for re-coding",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Table by gtsummary</span>"
    ]
  },
  {
    "objectID": "lessons/01_gtsummary.html#introduction-to-gtsummary",
    "href": "lessons/01_gtsummary.html#introduction-to-gtsummary",
    "title": "\n7  Table by gtsummary\n",
    "section": "\n7.2 Introduction to ‘gtsummary’",
    "text": "7.2 Introduction to ‘gtsummary’\nThe gtsummary package is useful mainly for creating publication-ready tables (i.e.demographic table, simple summary table, contingency-table, regression table, etc.). The best feature of this package is it can automatically detect if the data is continuous, dichotomous or categorical, and which descriptive statistics needs to apply.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Table by gtsummary</span>"
    ]
  },
  {
    "objectID": "lessons/01_gtsummary.html#data-source-and-description",
    "href": "lessons/01_gtsummary.html#data-source-and-description",
    "title": "\n7  Table by gtsummary\n",
    "section": "\n7.3 Data Source and Description",
    "text": "7.3 Data Source and Description\nThe public.ctn0094data package provides harmonized and normalized data sets from the CTN-0094 clinical trial. These data sets describe the experiences of care-seeking individuals suffering from opioid use disorder (OUD). The trial is part of the Clinical Trials Network (CTN) protocol number 0094, funded by the US National Institute of Drug Abuse (NIDA). It is used by the NIDA to develop, validate, refine, and deliver new treatment options to patients.\nIn this lesson, I used the demographics, and fagerstrom data sets from the public.ctn0094data package to demonstrate the gtsummary function. The demographics part contains the demographic variables such as age, sex, race, marital status etc. The fagerstrom part contains data on smoking habit (smoker/non-smoker, Fagerstrom Test for Nicotine Dependence Score (ranging from 0 to 10) ~ FTND, Number of cigarettes smoked per day.). The FTND is a questionnaire that assesses the physical dependence of adults on nicotine. The test uses yes/no questions scored from 0 to 1 and multiple-choice questions scored from 0 to 3, and the total score ranges from 0 to 10. The higher the score, the more intense the patient’s nicotine dependence is. The score categories are: 8+: High dependence, 7–5: Moderate dependence, 4–3: Low to moderate dependence and 0–2: Low dependence.\n\n# Searching suitable data sets: You can skip \ndata(package = \"public.ctn0094data\")\n#data(demographics, package = \"public.ctn0094data\")\n#names(demographics)\n#data(fagerstrom, package = \"public.ctn0094data\")\n#names(fagerstrom)\n#table(fagerstrom$ftnd)",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Table by gtsummary</span>"
    ]
  },
  {
    "objectID": "lessons/01_gtsummary.html#creating-model-data-frames",
    "href": "lessons/01_gtsummary.html#creating-model-data-frames",
    "title": "\n7  Table by gtsummary\n",
    "section": "\n7.4 Creating Model Data Frames",
    "text": "7.4 Creating Model Data Frames\nThe demographics and fagerstrom data sets within the public.ctn0094data package were joined by ID (who variable) and a new dta frame smoking_df is created.\n\n# Joining data sets: \nsmoking_df &lt;- demographics %&gt;% \n  left_join(fagerstrom, by = \"who\")",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Table by gtsummary</span>"
    ]
  },
  {
    "objectID": "lessons/01_gtsummary.html#demographic-table-with-tbl_summary-function",
    "href": "lessons/01_gtsummary.html#demographic-table-with-tbl_summary-function",
    "title": "\n7  Table by gtsummary\n",
    "section": "\n7.5 Demographic Table with tbl_summary Function",
    "text": "7.5 Demographic Table with tbl_summary Function\n\n7.5.1 Creating Table 1: Demographic Characteristic\nIn order to create a basic demographic table, I will now select which variables I want to show in the table and then use the tbl_summary function to create the table. I am also adding the description of the variables I included in my table.\n\n\nage: an integer variable that indicates the Age of the patient.\n\nrace: a factor variable with levels ‘Black’, ‘Other Refused/missing’, and ‘White’, which represents the Self-reported race of the patient.\n\neducation: a factor variable denotes the Education level at intake, with levels ‘HS/GED’ for high school graduate or equivalent, ‘Less than HS’ for less than high school education, ‘More than HS’ for some education beyond high school, and ‘Missing’ if the information is not provided.\n\nis_male: a factor variable with levels ‘No’ and ‘Yes’, describing the Sex (not gender) of the patient, where ‘Yes’ indicates male.\n\nmarital: a factor variable indicating the Marital status at intake, with levels ‘Married or Partnered’, ‘Never married’, ‘Separated/Divorced/Widowed’, and ‘Not answered’ if the question was not asked during intake.\n\nis_smoker: a factor indicating whether the patient is a smoker or not. Levels include “No” (not a smoker) and “Yes” (a smoker).\n\n\n# Selecting variables in a new data frame `table_1df` for table 1\ntable_1df &lt;- smoking_df %&gt;% \n  select(age, race, education, is_male, marital, is_smoker)\n\n# Table 1\ntable_1 &lt;- table_1df  %&gt;% tbl_summary()\n\ntable_1\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nN = 3,5601\n\n\n\n\nage\n34 (27, 45)\n\n\n    Unknown\n208\n\n\nrace\n\n\n\n    Black\n365 (10%)\n\n\n    Other\n506 (14%)\n\n\n    Refused/missing\n58 (1.6%)\n\n\n    White\n2,631 (74%)\n\n\neducation\n\n\n\n    HS/GED\n691 (39%)\n\n\n    Less than HS\n352 (20%)\n\n\n    More than HS\n724 (41%)\n\n\n    Unknown\n1,793\n\n\nis_male\n2,351 (66%)\n\n\n    Unknown\n4\n\n\nmarital\n\n\n\n    Married or Partnered\n329 (19%)\n\n\n    Never married\n1,028 (59%)\n\n\n    Separated/Divorced/Widowed\n394 (23%)\n\n\n    Unknown\n1,809\n\n\nis_smoker\n2,631 (85%)\n\n\n    Unknown\n460\n\n\n\n\n1 Median (IQR); n (%)\n\n\n\n\n\n\n\n7.5.2 Customizing Table 1: Changing the Label\nI am using label function to change the label of all variables. Other customization will be shown in the next contingency table.\n\n# Changing the Label\n\ntable_1 &lt;-\n  table_1df %&gt;% \n  tbl_summary(\n    label = list(\n      age = \"Age\",\n      race = \"Race\",\n      education = \"Education level\",\n      is_male = \"Male\",\n      marital = \"Marital status\",\n      is_smoker = \"Smoker\"\n    )\n  )\n\ntable_1\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nN = 3,5601\n\n\n\n\nAge\n34 (27, 45)\n\n\n    Unknown\n208\n\n\nRace\n\n\n\n    Black\n365 (10%)\n\n\n    Other\n506 (14%)\n\n\n    Refused/missing\n58 (1.6%)\n\n\n    White\n2,631 (74%)\n\n\nEducation level\n\n\n\n    HS/GED\n691 (39%)\n\n\n    Less than HS\n352 (20%)\n\n\n    More than HS\n724 (41%)\n\n\n    Unknown\n1,793\n\n\nMale\n2,351 (66%)\n\n\n    Unknown\n4\n\n\nMarital status\n\n\n\n    Married or Partnered\n329 (19%)\n\n\n    Never married\n1,028 (59%)\n\n\n    Separated/Divorced/Widowed\n394 (23%)\n\n\n    Unknown\n1,809\n\n\nSmoker\n2,631 (85%)\n\n\n    Unknown\n460\n\n\n\n\n1 Median (IQR); n (%)",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Table by gtsummary</span>"
    ]
  },
  {
    "objectID": "lessons/01_gtsummary.html#contingency-table-with-tbl_summary-function",
    "href": "lessons/01_gtsummary.html#contingency-table-with-tbl_summary-function",
    "title": "\n7  Table by gtsummary\n",
    "section": "\n7.6 Contingency Table with tbl_summary Function",
    "text": "7.6 Contingency Table with tbl_summary Function\n\n7.6.1 Creating Table 2: Demographic Variables by Smoking Status\nI will now show the table 1 demographic variables by smoking habit status (is_smoker, Yes = smoker and No = non-smokers)\n\n# Contingency table \ntable_2 &lt;- table_1df %&gt;% tbl_summary(by = is_smoker) \n\ntable_2\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nNo, N = 4691\n\n\nYes, N = 2,6311\n\n\n\n\nage\n36 (28, 47)\n33 (26, 44)\n\n\n    Unknown\n7\n79\n\n\nrace\n\n\n\n\n    Black\n46 (9.8%)\n259 (9.8%)\n\n\n    Other\n68 (14%)\n376 (14%)\n\n\n    Refused/missing\n1 (0.2%)\n18 (0.7%)\n\n\n    White\n354 (75%)\n1,978 (75%)\n\n\neducation\n\n\n\n\n    HS/GED\n104 (34%)\n531 (41%)\n\n\n    Less than HS\n23 (7.5%)\n290 (22%)\n\n\n    More than HS\n179 (58%)\n488 (37%)\n\n\n    Unknown\n163\n1,322\n\n\nis_male\n336 (72%)\n1,724 (66%)\n\n\nmarital\n\n\n\n\n    Married or Partnered\n75 (25%)\n236 (18%)\n\n\n    Never married\n152 (50%)\n775 (59%)\n\n\n    Separated/Divorced/Widowed\n77 (25%)\n293 (22%)\n\n\n    Unknown\n165\n1,327\n\n\n\n\n1 Median (IQR); n (%)\n\n\n\n\n\n\n\n7.6.2 Removing Missing Data\nIf I do not want to show the missing data in my table, I will use missing = \"no\".\n\n# Removing Missing Data\ntable_2nm &lt;- table_1df %&gt;% tbl_summary(by = is_smoker,\n                                   missing = \"no\") \ntable_2nm\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nNo, N = 4691\n\n\nYes, N = 2,6311\n\n\n\n\nage\n36 (28, 47)\n33 (26, 44)\n\n\nrace\n\n\n\n\n    Black\n46 (9.8%)\n259 (9.8%)\n\n\n    Other\n68 (14%)\n376 (14%)\n\n\n    Refused/missing\n1 (0.2%)\n18 (0.7%)\n\n\n    White\n354 (75%)\n1,978 (75%)\n\n\neducation\n\n\n\n\n    HS/GED\n104 (34%)\n531 (41%)\n\n\n    Less than HS\n23 (7.5%)\n290 (22%)\n\n\n    More than HS\n179 (58%)\n488 (37%)\n\n\nis_male\n336 (72%)\n1,724 (66%)\n\n\nmarital\n\n\n\n\n    Married or Partnered\n75 (25%)\n236 (18%)\n\n\n    Never married\n152 (50%)\n775 (59%)\n\n\n    Separated/Divorced/Widowed\n77 (25%)\n293 (22%)\n\n\n\n\n1 Median (IQR); n (%)\n\n\n\n\n\n\n\n7.6.3 Applying Statistical Tests\nI will use add_p function to show the statistical analysis. This will automatically detect if data in each variable is continuous, dichotomous or categorical, and apply the appropriate descriptive statistics accordingly.\n\n# Adding p-value\ntable_2 &lt;- table_1df %&gt;% tbl_summary(by = is_smoker,\n                                   missing = \"no\") %&gt;% \n  add_p()\n\ntable_2\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nNo, N = 4691\n\n\nYes, N = 2,6311\n\n\np-value2\n\n\n\n\nage\n36 (28, 47)\n33 (26, 44)\n&lt;0.001\n\n\nrace\n\n\n0.8\n\n\n    Black\n46 (9.8%)\n259 (9.8%)\n\n\n\n    Other\n68 (14%)\n376 (14%)\n\n\n\n    Refused/missing\n1 (0.2%)\n18 (0.7%)\n\n\n\n    White\n354 (75%)\n1,978 (75%)\n\n\n\neducation\n\n\n&lt;0.001\n\n\n    HS/GED\n104 (34%)\n531 (41%)\n\n\n\n    Less than HS\n23 (7.5%)\n290 (22%)\n\n\n\n    More than HS\n179 (58%)\n488 (37%)\n\n\n\nis_male\n336 (72%)\n1,724 (66%)\n0.010\n\n\nmarital\n\n\n0.006\n\n\n    Married or Partnered\n75 (25%)\n236 (18%)\n\n\n\n    Never married\n152 (50%)\n775 (59%)\n\n\n\n    Separated/Divorced/Widowed\n77 (25%)\n293 (22%)\n\n\n\n\n\n\n1 Median (IQR); n (%)\n\n\n\n2 Wilcoxon rank sum test; Fisher’s exact test; Pearson’s Chi-squared test\n\n\n\n\n\n\n\nNote: The footnote 2 shows all the statistical tests applied to this table. It can be understandable from the table that for categorical variable it applied Pearson’s Chi-squared test, for continuous non-normal distributed variable it applied Wilcoxon rank sum test; and for small sample data, it applied Fisher’s exact test. It would be great to see different footnotes for each of the test next to each p-value, however, I did not find a way to do that.\n\n7.6.4 Customizing Table 2(a)\nI will now customize the table 2 to show total number and overall number and show missing values by using the following functions:\n\n# Adding total and overall number \ntable_2 &lt;- table_1df %&gt;% tbl_summary(by = is_smoker,\n                                   label = list(\n                                     age = \"Age\",\n                                     race = \"Race\",\n                                     education = \"Education level\",\n                                     is_male = \"Male\",\n                                     marital = \"Marital status\"\n                                   ),\n                                   missing_text = \"(Missing)\"\n                                  ) %&gt;% \n  add_p() %&gt;%\n  add_n() %&gt;%\n  add_overall() \n\ntable_2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nN\n\nOverall, N = 3,1001\n\n\nNo, N = 4691\n\n\nYes, N = 2,6311\n\n\np-value2\n\n\n\n\nAge\n3,014\n34 (27, 45)\n36 (28, 47)\n33 (26, 44)\n&lt;0.001\n\n\n    (Missing)\n\n86\n7\n79\n\n\n\nRace\n3,100\n\n\n\n0.8\n\n\n    Black\n\n305 (9.8%)\n46 (9.8%)\n259 (9.8%)\n\n\n\n    Other\n\n444 (14%)\n68 (14%)\n376 (14%)\n\n\n\n    Refused/missing\n\n19 (0.6%)\n1 (0.2%)\n18 (0.7%)\n\n\n\n    White\n\n2,332 (75%)\n354 (75%)\n1,978 (75%)\n\n\n\nEducation level\n1,615\n\n\n\n&lt;0.001\n\n\n    HS/GED\n\n635 (39%)\n104 (34%)\n531 (41%)\n\n\n\n    Less than HS\n\n313 (19%)\n23 (7.5%)\n290 (22%)\n\n\n\n    More than HS\n\n667 (41%)\n179 (58%)\n488 (37%)\n\n\n\n    (Missing)\n\n1,485\n163\n1,322\n\n\n\nMale\n3,100\n2,060 (66%)\n336 (72%)\n1,724 (66%)\n0.010\n\n\nMarital status\n1,608\n\n\n\n0.006\n\n\n    Married or Partnered\n\n311 (19%)\n75 (25%)\n236 (18%)\n\n\n\n    Never married\n\n927 (58%)\n152 (50%)\n775 (59%)\n\n\n\n    Separated/Divorced/Widowed\n\n370 (23%)\n77 (25%)\n293 (22%)\n\n\n\n    (Missing)\n\n1,492\n165\n1,327\n\n\n\n\n\n\n1 Median (IQR); n (%)\n\n\n\n2 Wilcoxon rank sum test; Fisher’s exact test; Pearson’s Chi-squared test\n\n\n\n\n\n\n\n\n7.6.5 Customizing Table 2(b)\nI will now customize the title, caption and header and made the variable names bold of table 2 by using the following functions:\n\n# Adding title, caption and header \ntable_2 &lt;- table_1df %&gt;% tbl_summary(by = is_smoker,\n                                   label = list(\n                                     age = \"Age\",\n                                     race = \"Race\",\n                                     education = \"Education level\",\n                                     is_male = \"Male\",\n                                     marital = \"Marital status\"\n                                   ),\n                                   missing_text = \"(Missing)\"\n                                  ) %&gt;% \n  add_p() %&gt;%\n  add_n() %&gt;%\n  add_overall() %&gt;%\n  bold_labels() %&gt;%\n  modify_caption(\"Table 2. Demographic characteristics according to smoking status\") %&gt;%\n  modify_header(label ~ \"**Demographic characteristics**\") %&gt;%\n  modify_spanning_header(c(\"stat_1\", \"stat_2\") ~ \"**Smoking status**\") \n  \ntable_2\n\n\n\n\n\nTable 2. Demographic characteristics according to smoking status\n\n\n\n\n\n\n\n\n\n\nDemographic characteristics\nN\n\nOverall, N = 3,1001\n\nSmoking status\n\np-value2\n\n\n\n\nNo, N = 4691\n\n\nYes, N = 2,6311\n\n\n\n\n\nAge\n3,014\n34 (27, 45)\n36 (28, 47)\n33 (26, 44)\n&lt;0.001\n\n\n    (Missing)\n\n86\n7\n79\n\n\n\nRace\n3,100\n\n\n\n0.8\n\n\n    Black\n\n305 (9.8%)\n46 (9.8%)\n259 (9.8%)\n\n\n\n    Other\n\n444 (14%)\n68 (14%)\n376 (14%)\n\n\n\n    Refused/missing\n\n19 (0.6%)\n1 (0.2%)\n18 (0.7%)\n\n\n\n    White\n\n2,332 (75%)\n354 (75%)\n1,978 (75%)\n\n\n\nEducation level\n1,615\n\n\n\n&lt;0.001\n\n\n    HS/GED\n\n635 (39%)\n104 (34%)\n531 (41%)\n\n\n\n    Less than HS\n\n313 (19%)\n23 (7.5%)\n290 (22%)\n\n\n\n    More than HS\n\n667 (41%)\n179 (58%)\n488 (37%)\n\n\n\n    (Missing)\n\n1,485\n163\n1,322\n\n\n\nMale\n3,100\n2,060 (66%)\n336 (72%)\n1,724 (66%)\n0.010\n\n\nMarital status\n1,608\n\n\n\n0.006\n\n\n    Married or Partnered\n\n311 (19%)\n75 (25%)\n236 (18%)\n\n\n\n    Never married\n\n927 (58%)\n152 (50%)\n775 (59%)\n\n\n\n    Separated/Divorced/Widowed\n\n370 (23%)\n77 (25%)\n293 (22%)\n\n\n\n    (Missing)\n\n1,492\n165\n1,327\n\n\n\n\n\n\n1 Median (IQR); n (%)\n\n\n\n2 Wilcoxon rank sum test; Fisher’s exact test; Pearson’s Chi-squared test\n\n\n\n\n\n\n\n\n7.6.6 Customizing Table 2(c)\nHere, I am keeping only those customization that I prefer to have in my final table 2.\n\n# Final table\ntable_2 &lt;- table_1df %&gt;% tbl_summary(by = is_smoker,\n                                   label = list(\n                                     age = \"Age\",\n                                     race = \"Race\",\n                                     education = \"Education level\",\n                                     is_male = \"Male\",\n                                     marital = \"Marital status\"\n                                   ),\n                                   missing = \"no\"\n                                  ) %&gt;% \n  add_p() %&gt;%\n  #add_n() %&gt;%\n  #add_overall() %&gt;%\n  bold_labels() %&gt;%\n  modify_caption(\"Table 2. Demographic characteristics according to smoking status\") %&gt;%\n  modify_header(label ~ \"**Characteristics**\") %&gt;%\n  modify_spanning_header(c(\"stat_1\", \"stat_2\") ~ \"**Smoking Status**\") %&gt;%\n  modify_footnote(all_stat_cols() ~ \"Median (IQR) for Age; n (%) for all other variables\") \n  \ntable_2\n\n\n\n\n\nTable 2. Demographic characteristics according to smoking status\n\n\n\n\n\n\n\n\nCharacteristics\nSmoking Status\n\np-value2\n\n\n\n\nNo, N = 4691\n\n\nYes, N = 2,6311\n\n\n\n\n\nAge\n36 (28, 47)\n33 (26, 44)\n&lt;0.001\n\n\nRace\n\n\n0.8\n\n\n    Black\n46 (9.8%)\n259 (9.8%)\n\n\n\n    Other\n68 (14%)\n376 (14%)\n\n\n\n    Refused/missing\n1 (0.2%)\n18 (0.7%)\n\n\n\n    White\n354 (75%)\n1,978 (75%)\n\n\n\nEducation level\n\n\n&lt;0.001\n\n\n    HS/GED\n104 (34%)\n531 (41%)\n\n\n\n    Less than HS\n23 (7.5%)\n290 (22%)\n\n\n\n    More than HS\n179 (58%)\n488 (37%)\n\n\n\nMale\n336 (72%)\n1,724 (66%)\n0.010\n\n\nMarital status\n\n\n0.006\n\n\n    Married or Partnered\n75 (25%)\n236 (18%)\n\n\n\n    Never married\n152 (50%)\n775 (59%)\n\n\n\n    Separated/Divorced/Widowed\n77 (25%)\n293 (22%)\n\n\n\n\n\n\n1 Median (IQR) for Age; n (%) for all other variables\n\n\n\n2 Wilcoxon rank sum test; Fisher’s exact test; Pearson’s Chi-squared test\n\n\n\n\n\n\n\n\n7.6.7 Interpretation of Table 2\nInterpreting the variable Education level:\nNull Hypothesis (H₀): There is no association between education level and smoking status.\nAlternative Hypothesis (H₁): There is an association between education level and smoking status.\nSince the p-value is less than 0.001, we reject the null hypothesis. This indicates that there is a statistically significant association between education level and smoking status. However, to understand the nature of this association (whether education level affects smoking status or vice versa), further analysis would be needed.\n\n7.6.8 Missing value distribution in Table 2\nWe often want to see the missing value distribution among the the demographic variables. For example, we want to see the missing value distribution for the smoking status variable. First, we need to re-code the NA into a new category for is_smoker variable and recreate the table.\n\n7.6.8.1 Missing value data creation\n\n# Recoding `is_smoker` variable into `is_smoker_new`\ntable_1df &lt;- table_1df %&gt;% \n  mutate(is_smoker_new = ifelse(is.na(is_smoker), 99, is_smoker))  # converting all NA to 99\n\n# Convert into factor\ntable_1df$is_smoker_new &lt;- factor(table_1df$is_smoker_new,\n                                  levels = c(1, 2, 99),\n                                  labels = c(\"No\", \"Yes\", \"Missing\"))\n\n# New data frame \ntable_1df_new &lt;- table_1df %&gt;% \n  select(age, race, education, is_male, marital, is_smoker_new)\n\n\n7.6.8.2 Missing value table creation\n\n# Final table\ntable_2miss &lt;- table_1df_new %&gt;% tbl_summary(by = is_smoker_new,\n                                   label = list(\n                                     age = \"Age\",\n                                     race = \"Race\",\n                                     education = \"Education level\",\n                                     is_male = \"Male\",\n                                     marital = \"Marital status\"\n                                   ),\n                                   missing = \"no\"\n                                  ) %&gt;% \n  add_p() %&gt;%\n  #add_n() %&gt;%\n  #add_overall() %&gt;%\n  bold_labels() %&gt;%\n  modify_caption(\"Table 2. Demographic characteristics according to smoking status\") %&gt;%\n  modify_header(label ~ \"**Characteristics**\") %&gt;%\n  modify_spanning_header(c(\"stat_1\", \"stat_2\", \"stat_3\") ~ \"**Smoking Status**\") %&gt;%\n  modify_footnote(all_stat_cols() ~ \"Median (IQR) for Age; n (%) for all other variables\") \n  \ntable_2miss\n\n\n\n\n\nTable 2. Demographic characteristics according to smoking status\n\n\n\n\n\n\n\n\n\nCharacteristics\nSmoking Status\n\np-value2\n\n\n\n\nNo, N = 4691\n\n\nYes, N = 2,6311\n\n\nMissing, N = 4601\n\n\n\n\n\nAge\n36 (28, 47)\n33 (26, 44)\n39 (29, 47)\n&lt;0.001\n\n\nRace\n\n\n\n&lt;0.001\n\n\n    Black\n46 (9.8%)\n259 (9.8%)\n60 (13%)\n\n\n\n    Other\n68 (14%)\n376 (14%)\n62 (13%)\n\n\n\n    Refused/missing\n1 (0.2%)\n18 (0.7%)\n39 (8.5%)\n\n\n\n    White\n354 (75%)\n1,978 (75%)\n299 (65%)\n\n\n\nEducation level\n\n\n\n&lt;0.001\n\n\n    HS/GED\n104 (34%)\n531 (41%)\n56 (37%)\n\n\n\n    Less than HS\n23 (7.5%)\n290 (22%)\n39 (26%)\n\n\n\n    More than HS\n179 (58%)\n488 (37%)\n57 (38%)\n\n\n\nMale\n336 (72%)\n1,724 (66%)\n291 (64%)\n0.019\n\n\nMarital status\n\n\n\n&lt;0.001\n\n\n    Married or Partnered\n75 (25%)\n236 (18%)\n18 (13%)\n\n\n\n    Never married\n152 (50%)\n775 (59%)\n101 (71%)\n\n\n\n    Separated/Divorced/Widowed\n77 (25%)\n293 (22%)\n24 (17%)\n\n\n\n\n\n\n1 Median (IQR) for Age; n (%) for all other variables\n\n\n\n2 Kruskal-Wallis rank sum test; Pearson’s Chi-squared test",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Table by gtsummary</span>"
    ]
  },
  {
    "objectID": "lessons/01_gtsummary.html#regression-table-with-tbl_regression-function",
    "href": "lessons/01_gtsummary.html#regression-table-with-tbl_regression-function",
    "title": "\n7  Table by gtsummary\n",
    "section": "\n7.7 Regression Table with tbl_regression() Function",
    "text": "7.7 Regression Table with tbl_regression() Function\n\n7.7.1 Creating Regression Model\nHere, we are creating a logistic regression model where smoking status is the response variable, education is exploratory variable and age, race and sex are considered as confounders.\n\n# Building the Multivariable logistic model\nm1 &lt;- glm(is_smoker ~  education + age + race + is_male, \n          table_1df, \n          family = binomial)\n\n# View raw model results\nsummary(m1)$coefficients\n\n                         Estimate  Std. Error     z value     Pr(&gt;|z|)\n(Intercept)            3.50434562 0.389242905  9.00297879 2.196748e-19\neducationLess than HS  1.01143171 0.252724965  4.00210447 6.278157e-05\neducationMore than HS -0.60886151 0.144410039 -4.21619932 2.484542e-05\nage                   -0.04564764 0.006417912 -7.11253757 1.139285e-12\nraceOther             -0.24842217 0.315210858 -0.78811425 4.306299e-01\nraceRefused/missing    0.39602359 1.124629178  0.35213704 7.247355e-01\nraceWhite             -0.01922531 0.251971208 -0.07629961 9.391807e-01\nis_maleYes            -0.39712021 0.147363550 -2.69483338 7.042384e-03\n\n\n\n7.7.2 Creating Table 3: Regression Table\nHere, I am using tbl_regression function to see the regression results in the table. The exponentiate = TRUE shows the data as Odds Ratio after exponentiation of the beta values.\n\n# Creating Regression Table \ntable_3 &lt;- tbl_regression(m1, exponentiate = TRUE)\n\ntable_3\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nOR1\n\n\n95% CI1\n\np-value\n\n\n\neducation\n\n\n\n\n\n    HS/GED\n—\n—\n\n\n\n    Less than HS\n2.75\n1.71, 4.61\n&lt;0.001\n\n\n    More than HS\n0.54\n0.41, 0.72\n&lt;0.001\n\n\nage\n0.96\n0.94, 0.97\n&lt;0.001\n\n\nrace\n\n\n\n\n\n    Black\n—\n—\n\n\n\n    Other\n0.78\n0.42, 1.44\n0.4\n\n\n    Refused/missing\n1.49\n0.23, 29.4\n0.7\n\n\n    White\n0.98\n0.59, 1.59\n&gt;0.9\n\n\nis_male\n\n\n\n\n\n    No\n—\n—\n\n\n\n    Yes\n0.67\n0.50, 0.89\n0.007\n\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n7.7.3 Customizing Table 3\nHere, I have customized the table 3 by using functions I applied in table 1.\n\n# Customizing Regression Table \ntable_3 &lt;- tbl_regression(m1, exponentiate = TRUE,\n                           label = list(\n                             age = \"Age\",\n                             race = \"Race\",\n                             education = \"Education level\",\n                             is_male = \"Male\"\n                             ),\n                          missing = \"no\"\n                          ) %&gt;% \n  bold_labels() %&gt;%\n  bold_p(t = 0.10) %&gt;%  \n  italicize_levels() %&gt;%\n  modify_caption(\"Table 3. Logistic Regression for smoking status as response varialbe (n=3014)\")\n\ntable_3\n\n\n\n\n\nTable 3. Logistic Regression for smoking status as response varialbe (n=3014)\n\n\n\n\n\n\n\nCharacteristic\n\nOR1\n\n\n95% CI1\n\np-value\n\n\n\nEducation level\n\n\n\n\n\n    HS/GED\n—\n—\n\n\n\n    Less than HS\n2.75\n1.71, 4.61\n&lt;0.001\n\n\n    More than HS\n0.54\n0.41, 0.72\n&lt;0.001\n\n\nAge\n0.96\n0.94, 0.97\n&lt;0.001\n\n\nRace\n\n\n\n\n\n    Black\n—\n—\n\n\n\n    Other\n0.78\n0.42, 1.44\n0.4\n\n\n    Refused/missing\n1.49\n0.23, 29.4\n0.7\n\n\n    White\n0.98\n0.59, 1.59\n&gt;0.9\n\n\nMale\n\n\n\n\n\n    No\n—\n—\n\n\n\n    Yes\n0.67\n0.50, 0.89\n0.007\n\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n7.7.4 Interpreting Table 3\nInterpreting the variable Education level:\nFor individuals with less than high school education, the odds of being a smoker are 2.75 times higher compared to those with HS/GED, after adjusting for age, race, and sex.\nConversely, for individuals with more than high school education, the odds of being a smoker are 0.54 times lower compared to those with HS/GED, after adjusting for age, race, and sex.\nInterpreting the variable Age:\nFor each unit increase in age, the odds of being a smoker decrease by a factor of 0.96 (or 4%), after adjusting for education, race, and sex.\nIn R, for interpreting categorical variables, reference level is selected by alphabetic order, therefore, the HS/GED is selected as reference level (H), next one is Less than HS (L) and then More than HS (M).\n\n7.7.5 Changing the Reference Level in Table 3\nOften, we need to change the reference level as per our analysis need or aim of the study. We can select the specific reference level and run the table 3. First step is to check if the variable is in factor format. If it is not in factor format, we need to convert it into factor. Next, we can use the following codes to refer and use in table 3.\n\n7.7.5.1 New Model with New Reference Level\nHere I am creating model 2 (m2) wit the new reference as Less than HS for the education variable.\n\n# Check factor format\nstr(table_1df$education) # It shows that it is in factor format.\n\n Factor w/ 3 levels \"HS/GED\",\"Less than HS\",..: 3 3 3 3 NA 1 3 NA 1 3 ...\n\n# Building the glm model with specific reference level for education  = \"Less than HS\".\nm2 &lt;- glm(is_smoker ~  relevel(factor(education), ref = \"Less than HS\")  + age + race + is_male, \n          table_1df, \n          family = binomial)\n\n# View raw model results\nsummary(m2)$coefficients\n\n                                                                Estimate\n(Intercept)                                                   4.51577733\nrelevel(factor(education), ref = \"Less than HS\")HS/GED       -1.01143171\nrelevel(factor(education), ref = \"Less than HS\")More than HS -1.62029322\nage                                                          -0.04564764\nraceOther                                                    -0.24842217\nraceRefused/missing                                           0.39602359\nraceWhite                                                    -0.01922531\nis_maleYes                                                   -0.39712021\n                                                              Std. Error\n(Intercept)                                                  0.436459823\nrelevel(factor(education), ref = \"Less than HS\")HS/GED       0.252724965\nrelevel(factor(education), ref = \"Less than HS\")More than HS 0.244106320\nage                                                          0.006417912\nraceOther                                                    0.315210858\nraceRefused/missing                                          1.124629178\nraceWhite                                                    0.251971208\nis_maleYes                                                   0.147363550\n                                                                 z value\n(Intercept)                                                  10.34637575\nrelevel(factor(education), ref = \"Less than HS\")HS/GED       -4.00210447\nrelevel(factor(education), ref = \"Less than HS\")More than HS -6.63765370\nage                                                          -7.11253757\nraceOther                                                    -0.78811425\nraceRefused/missing                                           0.35213704\nraceWhite                                                    -0.07629961\nis_maleYes                                                   -2.69483338\n                                                                 Pr(&gt;|z|)\n(Intercept)                                                  4.346284e-25\nrelevel(factor(education), ref = \"Less than HS\")HS/GED       6.278157e-05\nrelevel(factor(education), ref = \"Less than HS\")More than HS 3.187156e-11\nage                                                          1.139285e-12\nraceOther                                                    4.306299e-01\nraceRefused/missing                                          7.247355e-01\nraceWhite                                                    9.391807e-01\nis_maleYes                                                   7.042384e-03\n\n\n\n7.7.5.2 Creating and Customizing New Table 3 with New Reference Level\nHere, I have created the new table 3 for m2 model and customized it accordingly.\n\n# Customizing Regression Table \ntable_3n &lt;- tbl_regression(m2, exponentiate = TRUE,  # Creating the table\n                           label = list(\n                             age = \"Age\",\n                             race = \"Race\",\n                             education = \"Education level\",\n                             is_male = \"Male\"\n                             ),\n                          missing = \"no\"\n                          ) %&gt;% \n  bold_labels() %&gt;%\n  bold_p(t = 0.10) %&gt;%  \n  italicize_levels() %&gt;%\n  modify_caption(\"Table 3. Logistic Regression for smoking status as response varialbe (n=3014)\")\n\ntable_3n\n\n\n\n\n\nTable 3. Logistic Regression for smoking status as response varialbe (n=3014)\n\n\n\n\n\n\n\nCharacteristic\n\nOR1\n\n\n95% CI1\n\np-value\n\n\n\nrelevel(factor(education), ref = \"Less than HS\")\n\n\n\n\n\n    Less than HS\n—\n—\n\n\n\n    HS/GED\n0.36\n0.22, 0.59\n&lt;0.001\n\n\n    More than HS\n0.20\n0.12, 0.31\n&lt;0.001\n\n\nAge\n0.96\n0.94, 0.97\n&lt;0.001\n\n\nRace\n\n\n\n\n\n    Black\n—\n—\n\n\n\n    Other\n0.78\n0.42, 1.44\n0.4\n\n\n    Refused/missing\n1.49\n0.23, 29.4\n0.7\n\n\n    White\n0.98\n0.59, 1.59\n&gt;0.9\n\n\nMale\n\n\n\n\n\n    No\n—\n—\n\n\n\n    Yes\n0.67\n0.50, 0.89\n0.007\n\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Table by gtsummary</span>"
    ]
  },
  {
    "objectID": "lessons/01_gtsummary.html#conclusion-take-home-message",
    "href": "lessons/01_gtsummary.html#conclusion-take-home-message",
    "title": "\n7  Table by gtsummary\n",
    "section": "\n7.8 Conclusion (Take Home Message)",
    "text": "7.8 Conclusion (Take Home Message)\n\nWe can use gtsummary package for creating publication-ready tables.\nThe tbl_summary() and the tbl_regression() are the frequently used functions in this package.\nMultiple other functions can be used to customize the table and can address the journal requirements.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Table by gtsummary</span>"
    ]
  },
  {
    "objectID": "02_header_one-sample.html",
    "href": "02_header_one-sample.html",
    "title": "One-Sample Tests",
    "section": "",
    "text": "Text Outline\nThis text covers the basics of:",
    "crumbs": [
      "One-Sample Tests"
    ]
  },
  {
    "objectID": "02_header_one-sample.html#text-outline",
    "href": "02_header_one-sample.html#text-outline",
    "title": "One-Sample Tests",
    "section": "",
    "text": "Exploring data\nOne-sample tests\nTwo-sample tests\nLinear regression and ANOVA\nGeneralized linear models\nSome special topics\nMaybe a bit about power calculations…",
    "crumbs": [
      "One-Sample Tests"
    ]
  },
  {
    "objectID": "02_header_one-sample.html#part-outline",
    "href": "02_header_one-sample.html#part-outline",
    "title": "One-Sample Tests",
    "section": "Part Outline",
    "text": "Part Outline\nThis part of the text contains the chapters on various one-sample statistical tests:\n\n\\(Z\\)-test\nPaired \\(t\\)-test\nPaired Wilcoxon test\nTransformations to Normality\nMcNemar’s Test\nFisher’s Exact Test\nChi-Square Goodness of Fit\nBootstrapped Confidence Intervals",
    "crumbs": [
      "One-Sample Tests"
    ]
  },
  {
    "objectID": "lessons/02_z-test_one_prop.html",
    "href": "lessons/02_z-test_one_prop.html",
    "title": "\n8  Z-Test for One Proportion\n",
    "section": "",
    "text": "8.1 Introduction to One-Sample \\(Z\\)-Tests\nThe one-sample \\(Z\\)-test is used to compare a sample proportion to a population proportion.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Z-Test for One Proportion</span>"
    ]
  },
  {
    "objectID": "lessons/02_z-test_one_prop.html#mathematical-definition-of-the-one-sample-z-test",
    "href": "lessons/02_z-test_one_prop.html#mathematical-definition-of-the-one-sample-z-test",
    "title": "\n8  Z-Test for One Proportion\n",
    "section": "\n8.2 Mathematical definition of the One-Sample \\(Z\\)-Test",
    "text": "8.2 Mathematical definition of the One-Sample \\(Z\\)-Test\nConsider a sample of size \\(n\\) with binary values (such as “true” or “false”). Let \\(p_{s}\\) and \\(p_{E}\\) be the observed sample and expected (population) proportions, respectively. The formula to calculate the \\(z\\) statistic is\n\\[\nz \\equiv \\frac{\n  p_s - p_E\n}{\n  \\sqrt{\n    \\frac{1}{n}p_s(1 - p_s)\n  }\n}.\n\\]",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Z-Test for One Proportion</span>"
    ]
  },
  {
    "objectID": "lessons/02_z-test_one_prop.html#data-source-and-description",
    "href": "lessons/02_z-test_one_prop.html#data-source-and-description",
    "title": "\n8  Z-Test for One Proportion\n",
    "section": "\n8.3 Data source and description",
    "text": "8.3 Data source and description\nWe will use the CTN-0094 data set, a data set of harmonized clinical trials for opioid use disorder. The full database is in public.ctn0094data::, engineered features are in public.ctn0094extra::, and clinical trial outcomes (wrangled dependent variables) are in CTNote::. We will install all three packages, but only use CTNote:: for now.\n\n# install.packages(\"public.ctn0094data\")\n# install.packages(\"public.ctn0094extra\")\n# install.packages(\"CTNote\")\n\nlibrary(CTNote)\nlibrary(tidyverse)",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Z-Test for One Proportion</span>"
    ]
  },
  {
    "objectID": "lessons/02_z-test_one_prop.html#cleaning-the-data-to-create-a-model-data-frame",
    "href": "lessons/02_z-test_one_prop.html#cleaning-the-data-to-create-a-model-data-frame",
    "title": "\n8  Z-Test for One Proportion\n",
    "section": "\n8.4 Cleaning the data to create a model data frame",
    "text": "8.4 Cleaning the data to create a model data frame\nBecause our method requires only one sample, we have very little work to do. We will use the Kosten et al. (1993) definition of opioid abstinence, provided in the data set outcomesCTN0094 as the column kosten1993_isAbs.\n\n# What do the values look like?\nsummary(outcomesCTN0094$kosten1993_isAbs)\n\n   Mode   FALSE    TRUE \nlogical    2158    1402 \n\n# How many samples are there?\nnrow(outcomesCTN0094)\n\n[1] 3560\n\n\nThere are 3560 logical values, and TRUE indicates that the trial participant achieved abstinence according to the definition used in Kosten et al. (1993).",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Z-Test for One Proportion</span>"
    ]
  },
  {
    "objectID": "lessons/02_z-test_one_prop.html#assumptions-of-the-one-sample-z-test",
    "href": "lessons/02_z-test_one_prop.html#assumptions-of-the-one-sample-z-test",
    "title": "\n8  Z-Test for One Proportion\n",
    "section": "\n8.5 Assumptions of the One-Sample \\(Z\\)-Test",
    "text": "8.5 Assumptions of the One-Sample \\(Z\\)-Test\nTo use a one-sample \\(Z\\)-test, we make the following assumptions:\n\nThe data are from a random sample\nEach observation in the data are independent\nNeither the sample proportion nor population proportions are “extreme”; usually we apply this method if these proportions are between 5% and 95%.\nThe data can be described as “successes” and “failures”, and there are at least 10 samples in each category.\n\nIf these assumptions hold, then \\[\nz \\sim N(0, 1).\n\\]",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Z-Test for One Proportion</span>"
    ]
  },
  {
    "objectID": "lessons/02_z-test_one_prop.html#checking-the-assumptions-with-plots",
    "href": "lessons/02_z-test_one_prop.html#checking-the-assumptions-with-plots",
    "title": "\n8  Z-Test for One Proportion\n",
    "section": "\n8.6 Checking the assumptions with plots",
    "text": "8.6 Checking the assumptions with plots\n\n8.6.1 Independence and Randomness\nBecause the samples were collected at random via an FDA approved clinical trial protocol, we assume that all the participants were randomly selected and are independent of each other.\n\n8.6.2 “Extreme” Proportions\nAccording to Ling et al. (2020), the 12-month abstinence proportion of all 533 participants in their study was 40.5 percent. As we can see here, our abstinence rates are 39.4. Neither these proportions are smaller than 5% or greater than 95%.\n\n(pExpected &lt;- 0.508 * (425/533))\n\n[1] 0.4050657\n\n# Count the number of TRUE values\n(nAbstinent &lt;- sum(outcomesCTN0094$kosten1993_isAbs))\n\n[1] 1402\n\n\n\n8.6.3 Type and Counts of Data\nWe observe binary data, and we see at least 10 successes and at least 10 failures.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Z-Test for One Proportion</span>"
    ]
  },
  {
    "objectID": "lessons/02_z-test_one_prop.html#code-to-run-a-one-sample-z-test",
    "href": "lessons/02_z-test_one_prop.html#code-to-run-a-one-sample-z-test",
    "title": "\n8  Z-Test for One Proportion\n",
    "section": "\n8.7 Code to run a One-Sample \\(Z\\)-Test",
    "text": "8.7 Code to run a One-Sample \\(Z\\)-Test\nNow that we have checked our assumptions, we can perform the one-sample \\(Z\\)-test for proportions.\n\nprop.test(\n  x = nAbstinent,\n  n = nrow(outcomesCTN0094),\n  p = pExpected\n)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  nAbstinent out of nrow(outcomesCTN0094), null probability pExpected\nX-squared = 1.8218, df = 1, p-value = 0.1771\nalternative hypothesis: true p is not equal to 0.4050657\n95 percent confidence interval:\n 0.3777537 0.4101176\nsample estimates:\n        p \n0.3938202",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Z-Test for One Proportion</span>"
    ]
  },
  {
    "objectID": "lessons/02_z-test_one_prop.html#brief-interpretation-of-the-output",
    "href": "lessons/02_z-test_one_prop.html#brief-interpretation-of-the-output",
    "title": "\n8  Z-Test for One Proportion\n",
    "section": "\n8.8 Brief interpretation of the output",
    "text": "8.8 Brief interpretation of the output\nThe 95% confidence interval contains the population proportion, so we fail to reject the hypothesis that the patients from these clinical trials achieve different abstinence rates than the general population.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Z-Test for One Proportion</span>"
    ]
  },
  {
    "objectID": "lessons/02_wilcoxon_one_samp.html",
    "href": "lessons/02_wilcoxon_one_samp.html",
    "title": "\n9  Wilcoxon Signed Rank Test for One Sample\n",
    "section": "",
    "text": "9.1 Introduction to Wilcoxson Signed Rank Test\nThe one-sample Wilcoxson Signed Rank Test is used to compare a sample proportion to a population proportion.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Wilcoxon Signed Rank Test for One Sample</span>"
    ]
  },
  {
    "objectID": "lessons/02_wilcoxon_one_samp.html#mathematical-definition-of-the-wilcoxson-signed-rank-test",
    "href": "lessons/02_wilcoxon_one_samp.html#mathematical-definition-of-the-wilcoxson-signed-rank-test",
    "title": "\n9  Wilcoxon Signed Rank Test for One Sample\n",
    "section": "\n9.2 Mathematical definition of the Wilcoxson Signed Rank Test",
    "text": "9.2 Mathematical definition of the Wilcoxson Signed Rank Test\nLet’s assume that we have one sample of size \\(n\\), \\(x_1, x_2, \\ldots, x_n\\), which cannot be approximated by a normal distribution. Because of this, we are no longer comparing \\(\\bar{x}\\) to \\(\\mu\\), but we are instead asking if the sample median is equal to a population median, \\(M\\). For more detail, see the maths here: https://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test.\nHere are the steps to calculate this test statistic manually:\n\nSubtract the population median from each sample: \\(x^*_i := x_1 - M\\)\n\nTake the absolute value of the shifted samples, \\(|x^*_i|\\).\nRank these absolute values.\nMultiply the signs of the shifted samples by the ranks of the absolute values.\nSum these products and compare them to a normal distribution with mean 0 and \\(\\sigma^2 = \\frac{1}{6}(2n+1)(n+1)n\\). (We will not explain the maths here to show why this can be approximately normal, or why this is the estimated variance.)\n\nConsider a simple example: we want to ask if the number of people visiting a local clinic per hour is different from the county median of 2.9 visits per hour. Here is a small sample of simulated (non-normal) data:\n\nset.seed(123)\n\nN &lt;- 15\nnClinicVisits &lt;- rpois(n = N, lambda = 4)\n\n# Plot the data and visually compare to the county median.\nhist(nClinicVisits)\nabline(v = 2.9, lwd = 2, col = \"red\")\n\n\n\n\n\n\n\nNow let’s go through our steps:\n\nsteps_df &lt;- tibble::tibble(x = nClinicVisits)\n\n# 1. shift the sample by the population median\nsteps_df$xStar &lt;- steps_df$x - 2.9\n\n# 2. absolute value\nsteps_df$absXStar &lt;- abs(steps_df$xStar)\n\n# 3. ranks\nsteps_df$xRank &lt;- rank(steps_df$absXStar)\n\n# 4. signs x ranks\nsteps_df$signRank &lt;- sign(steps_df$xStar) * steps_df$xRank\n\n# Inspect our steps\nsteps_df\n\n# A tibble: 15 × 5\n       x  xStar absXStar xRank signRank\n   &lt;int&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1     3  0.100    0.100   1.5      1.5\n 2     6  3.1      3.1    11.5     11.5\n 3     3  0.100    0.100   1.5      1.5\n 4     6  3.1      3.1    11.5     11.5\n 5     7  4.1      4.1    13.5     13.5\n 6     1 -1.9      1.9     9       -9  \n 7     4  1.1      1.1     6        6  \n 8     7  4.1      4.1    13.5     13.5\n 9     4  1.1      1.1     6        6  \n10     4  1.1      1.1     6        6  \n11     8  5.1      5.1    15       15  \n12     4  1.1      1.1     6        6  \n13     5  2.1      2.1    10       10  \n14     4  1.1      1.1     6        6  \n15     2 -0.9      0.9     3       -3  \n\n\nNow we can calculate the Wilcoxon Signed Rank test statisic and compare it to its asymptotic \\(p\\)-value.\n\n# 5. Compare sum to normal distribution and calculate the p-value\noneTailP &lt;- pnorm(\n  q = sum(steps_df$signRank),\n  mean = 0,\n  sd = sqrt((2 * N + 1) * (N + 1) * N / 6)\n)\n(1 - oneTailP) / 2\n\n[1] 0.001601623\n\n\nHow does this compare to the exact distribution \\(p\\)-value?\n\nwilcox.test(x = nClinicVisits, mu = 2.9)\n\nWarning in wilcox.test.default(x = nClinicVisits, mu = 2.9): cannot compute\nexact p-value with ties\n\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  nClinicVisits\nV = 108, p-value = 0.00672\nalternative hypothesis: true location is not equal to 2.9",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Wilcoxon Signed Rank Test for One Sample</span>"
    ]
  },
  {
    "objectID": "lessons/02_wilcoxon_one_samp.html#data-source-and-description",
    "href": "lessons/02_wilcoxon_one_samp.html#data-source-and-description",
    "title": "\n9  Wilcoxon Signed Rank Test for One Sample\n",
    "section": "\n9.3 Data source and description",
    "text": "9.3 Data source and description\nNow that we have seen how the test works, we will apply it to a real data scenario. We will use gene-level \\(p\\)-values from the Golub and Van Loan (1999) data set from the R package multtest:: (https://rdrr.io/bioc/multtest/man/golub.html); the original is a data set of data set of gene expression values for leukemia, but we have gene-specific \\(p\\)-values from a gene-level hypothesis test. We created these \\(p\\)-values in the script R/create_golub_data_20240523.R, but they do not represent any real analysis results.\n\nlibrary(tidyverse)",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Wilcoxon Signed Rank Test for One Sample</span>"
    ]
  },
  {
    "objectID": "lessons/02_wilcoxon_one_samp.html#cleaning-the-data-to-create-a-model-data-frame",
    "href": "lessons/02_wilcoxon_one_samp.html#cleaning-the-data-to-create-a-model-data-frame",
    "title": "\n9  Wilcoxon Signed Rank Test for One Sample\n",
    "section": "\n9.4 Cleaning the data to create a model data frame",
    "text": "9.4 Cleaning the data to create a model data frame\nBecause our method requires only one sample, we have very little work to do. We import the data set of \\(p\\)-values.\n\ngolub_pVals_num &lt;- readRDS(file = \"../data/02_golub_pVals_20240523.rds\")\n\nThere are 3051 \\(p\\)-values. The null hypothesis would be that there is no statistically significant effects in the data, so the distribution of these \\(p\\)-values should be a Uniform distribution. Our hypothesis is that the population mean is then 0.5 (the average value of a Uniform distribution).",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Wilcoxon Signed Rank Test for One Sample</span>"
    ]
  },
  {
    "objectID": "lessons/02_wilcoxon_one_samp.html#assumptions-of-the-wilcoxson-signed-rank-test",
    "href": "lessons/02_wilcoxon_one_samp.html#assumptions-of-the-wilcoxson-signed-rank-test",
    "title": "\n9  Wilcoxon Signed Rank Test for One Sample\n",
    "section": "\n9.5 Assumptions of the Wilcoxson Signed Rank Test",
    "text": "9.5 Assumptions of the Wilcoxson Signed Rank Test\nTo use a one-sample Wilcoxson Signed Rank Test, we make the following assumptions:\n\nThe data are from a random sample\nEach observation in the data are independent\nThe values can be “ranked” (this assumption gets fuzzy when you have discrete data, because it’s possible to get ties or values that are exactly 0 in those cases)\n\nIf these assumptions hold, then the test statistic is asymptotically normal. If your data has lots of zeros or equal values (which would result in tied ranks), then use this method with caution.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Wilcoxon Signed Rank Test for One Sample</span>"
    ]
  },
  {
    "objectID": "lessons/02_wilcoxon_one_samp.html#checking-the-assumptions",
    "href": "lessons/02_wilcoxon_one_samp.html#checking-the-assumptions",
    "title": "\n9  Wilcoxon Signed Rank Test for One Sample\n",
    "section": "\n9.6 Checking the assumptions",
    "text": "9.6 Checking the assumptions\n\n9.6.1 Independence and Randomness\nThese are gene-level \\(p\\)-values, so we do not have “independence”. However, because this is a pedagogical example, we will take a random sample of these genes to test (and this random sample should be independent enough, but we have no guarantee of this).\n\n# Create random sample of genes to test\nset.seed(20150516)\ngene_sample &lt;- sample(\n  x = golub_pVals_num,\n  size = 200,\n  replace = FALSE\n)\n\nWhat does the data distribution look like?\n\nhist(gene_sample)\n\n\n\n\n\n\n\nRemember, this is a “fake” analysis (all 38 samples in this data are leukemia cases, and I tested one half against the other—there should absolutely NOT be any real biological signal in this data).\n\n9.6.2 Type of Data\nThese values are \\(p\\)-values, so they can be ranked.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Wilcoxon Signed Rank Test for One Sample</span>"
    ]
  },
  {
    "objectID": "lessons/02_wilcoxon_one_samp.html#code-to-run-a-wilcoxson-signed-rank-test",
    "href": "lessons/02_wilcoxon_one_samp.html#code-to-run-a-wilcoxson-signed-rank-test",
    "title": "\n9  Wilcoxon Signed Rank Test for One Sample\n",
    "section": "\n9.7 Code to run a Wilcoxson Signed Rank Test",
    "text": "9.7 Code to run a Wilcoxson Signed Rank Test\nNow that we have checked our assumptions, we can perform the Wilcoxson Signed Rank Test on random samples of the genes to test if they have an average value of 0.5.\n\nwilcox.test(\n  x = gene_sample,\n  mu = 0.5, # average from all theoretical p-values under H0\n  alternative = \"less\" # H1: random p-values &lt; 0.5\n)\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  gene_sample\nV = 5859, p-value = 1.584e-07\nalternative hypothesis: true location is less than 0.5",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Wilcoxon Signed Rank Test for One Sample</span>"
    ]
  },
  {
    "objectID": "lessons/02_wilcoxon_one_samp.html#brief-interpretation-of-the-output",
    "href": "lessons/02_wilcoxon_one_samp.html#brief-interpretation-of-the-output",
    "title": "\n9  Wilcoxon Signed Rank Test for One Sample\n",
    "section": "\n9.8 Brief interpretation of the output",
    "text": "9.8 Brief interpretation of the output\nThe \\(p\\)-value for this test is less than 0.05, so we reject the hypothesis that the average gene-specific \\(p\\)-value for this set of results is greater than or equal to 0.5 (the theoretical average of \\(p\\)-values under the null hypothesis).",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Wilcoxon Signed Rank Test for One Sample</span>"
    ]
  },
  {
    "objectID": "lessons/02_transformations.html",
    "href": "lessons/02_transformations.html",
    "title": "\n10  Transformations to Normality\n",
    "section": "",
    "text": "10.1 Introduction\nThe pattern of values obtained when a variable is measured in a large number of individuals is called a distribution. Distributions can be broadly classified as normal and non-normal. The normal distribution is also called ‘Gaussian distribution’ as it was first described by K.F. Gauss. This chapter outlines the process of transforming data to achieve a normal distribution in R. Parametric methods, such as t-tests and ANOVA, require that the dependent (outcome) variable is approximately normally distributed within each group being compared. When the normality assumption is not satisfied, transforming the data can correct the non-normal distributions. For t-tests and ANOVA, it is sufficient to transform the dependent variable. However, for linear regression, transformations may be applied to the independent variable, the dependent variable, or both to achieve a linear relationship between variables and ensure homoscedasticity.\nHere are the libraries we will use for this material:\nlibrary(ggplot2)\nlibrary(gridExtra)\nlibrary(GGally)\nlibrary(moments)\nlibrary(knitr)\nlibrary(MASS)",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Transformations to Normality</span>"
    ]
  },
  {
    "objectID": "lessons/02_transformations.html#when-to-apply-transformations-to-normality",
    "href": "lessons/02_transformations.html#when-to-apply-transformations-to-normality",
    "title": "\n10  Transformations to Normality\n",
    "section": "\n10.2 When to Apply Transformations to Normality",
    "text": "10.2 When to Apply Transformations to Normality\nOne of the critical assumptions of statistical hypothesis testing is that the data are samples from a normal distribution. Therefore, it is essential to identify whether distributions are skewed or normal. There are several straightforward methods to detect skewness. Firstly, if the mean is less than twice the standard deviation, the distribution is likely skewed. Additionally, in a normally distributed population, the mean and standard deviation of the samples are independent. This characteristic can be used to detect skewness; if the standard deviation increases as the mean increases across groups from a population, the distribution is skewed. Beyond these simple methods, normality can be verified using statistical tests such as the Shapiro-Wilk test, the Kolmogorov-Smirnov test, and the Anderson-Darling test. Additionally, the moments package in R can be used to calculate skewness quantitatively. The skewness is determined using the third standardized moment, providing a measure of the asymmetry of the data distribution. If skewness is identified, efforts should be made to transform the data to achieve a normal distribution. This transformation is crucial for applying robust parametric tests in the analysis.\nTransformations can also be employed to facilitate comparison and interpretation. A classical example of a variable commonly reported after logarithmic transformation is the hydrogen ion concentration (pH). Another instance where transformation aids in data comparison is the logarithmic transformation of a dose-response curve. When plotted, the dose-response relationship is curvilinear; however, plotting the response against the logarithm of the dose (log dose-response plot) results in an elongated S-shaped curve. The middle portion of this curve forms a straight line, making it easier to compare two straight lines by measuring their slopes than to compare two curves. Thus, transformation can significantly enhance data comparison.\nIn summary, transformations can be applied to normalize data distribution or to simplify interpretation and comparison.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Transformations to Normality</span>"
    ]
  },
  {
    "objectID": "lessons/02_transformations.html#types-of-transformations-to-normality",
    "href": "lessons/02_transformations.html#types-of-transformations-to-normality",
    "title": "\n10  Transformations to Normality\n",
    "section": "\n10.3 Types of Transformations to Normality",
    "text": "10.3 Types of Transformations to Normality\nOften, the transformation that normalizes the distribution also equalizes the variance. While there are several types of transformations available, such as logarithmic, square root, reciprocal, cube root, and Box-Cox, the first three are the most commonly used. Among the transformations discussed in this section, the logarithmic transformation is the most often used. The following guidelines can help in selecting the appropriate method of transformation:\n\n10.3.1 Logarithmic Transformation\nIf the standard deviation is proportional to the mean, the distribution is positively skewed, making logarithmic transformation ideal. Note that when using a log transformation, a constant should be added to all values to ensure they are positive before transformation. The log tranformation is \\[\ny' = \\log(y + c),\n\\] where \\(y\\) is the original value and \\(c\\) is a constant added to ensure all values are positive.\n\n\n\n\n\n\n\n\n\n\n\n10.3.2 Square Root Transformation\nWhen the variance is proportional to the mean, square root transformation is preferred. This is particularly applicable to variables measured as counts, such as the number of malignant cells in a microscopic field or the number of deaths from swine flu. The square root transformation is: \\[\ny' = \\sqrt{y}.\n\\]\n\n\n\n\n\n\n\n\n\n\n\n10.3.3 Arithmetic Reciprocal Transformation\nIf the observations are truncated on the right (such as often the case for academic grade distributions), then one preliminary transformation is to “reverse” the values by subtracting each value from the maximum of all observed values (or from the maximum possible value for observations on a defined scale). This operation “flips” the data distribution from having a heavy left tail to having a heavy right tail, which allows us to perform a secondary transformation (such as a log or square root). This transformation is: \\[\ny' = \\max(y) - y.\n\\]\n\n\n\n\n\n\n\n\nNOTE: now that the data are right-skewed, other transformations can be applied as usual.\n\n10.3.4 Geometric Reciprocal Transformation\nIf the standard deviation is proportional to the mean squared, a reciprocal transformation is appropriate. This is typically used for highly variable quantities, such as serum creatinine levels. Note that this transformation requires all values to be positive or all values to be negative before applying it. \\[\ny' = \\frac{1}{y \\pm c},\n\\] where \\(y\\) is the original value and \\(c\\) is a constant added (subtracted) to ensure all values are positive (negative).\n\n\n\n\n\n\n\n\n\n\n\n10.3.5 Box-Cox Transformation\nThe Box-Cox transformation is a family of power transformations that can be used to stabilize variance and make the data more closely conform to a normal distribution, especially when the best power transformation (e.g., square root, logarithmic) is uncertain. By estimating an optimal parameter \\(\\lambda\\) from the data, the Box-Cox transformation tailors the transformation to the specific dataset’s needs. The transformation is defined as:\n\\[\ny(\\lambda) = \\begin{cases}\n  \\frac{y^\\lambda - 1}{\\lambda} & \\text{if } \\lambda \\neq 0 \\\\\n  \\log(y) & \\text{if } \\lambda = 0\n\\end{cases}\n\\] Here, \\(\\lambda\\) is a parameter that is estimated from the data. The Box-Cox transformation is particularly useful because it includes many of the other transformations (such as the logarithmic and square root transformations) as special cases.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Transformations to Normality</span>"
    ]
  },
  {
    "objectID": "lessons/02_transformations.html#examples-of-transformations-to-normality",
    "href": "lessons/02_transformations.html#examples-of-transformations-to-normality",
    "title": "\n10  Transformations to Normality\n",
    "section": "\n10.4 Examples of Transformations to Normality",
    "text": "10.4 Examples of Transformations to Normality\n\n10.4.1 Data Source and Description\nThe USJudgeRatings dataset is a built-in dataset in R that contains ratings of 43 judges in the US Superior Court. The ratings are based on the evaluations from lawyers who have had cases before these judges. The dataset includes multiple variables that represent different aspects of the judges’ performance.\n\n10.4.1.1 Variables in the Dataset\n\n\nCONT: Judicial “controlling” or authoritative nature.\n\nINTG: Judicial integrity.\n\nDMNR: Judicial demeanor.\n\nDILG: Judicial diligence.\n\nCFMG: Case flow management.\n\nDECI: Judicial decision-making.\n\nPREP: Judicial preparation.\n\nFAMI: Familiarity with the law.\n\nORAL: Oral skills.\n\nWRIT: Written skills.\n\nPHYS: Physical ability.\n\nRTEN: Willingness to follow trends.\n\nThis dataset is useful for analyzing various performance metrics of judges and can be used to explore relationships between different aspects of judicial performance. In the following examples, we’ll consider two variables:\n\n\nCONT: Number of contacts of lawyer with judge. Positively skewed.\n\nPHYS: Physical ability. Negatively skewed\n\n10.4.2 Loading the Data\n\n# Load the USJudgeRatings dataset\ndata(\"USJudgeRatings\")\ndf &lt;- USJudgeRatings\n\n# Display the first few rows of the dataset\nhead(df)\n\n               CONT INTG DMNR DILG CFMG DECI PREP FAMI ORAL WRIT PHYS RTEN\nAARONSON,L.H.   5.7  7.9  7.7  7.3  7.1  7.4  7.1  7.1  7.1  7.0  8.3  7.8\nALEXANDER,J.M.  6.8  8.9  8.8  8.5  7.8  8.1  8.0  8.0  7.8  7.9  8.5  8.7\nARMENTANO,A.J.  7.2  8.1  7.8  7.8  7.5  7.6  7.5  7.5  7.3  7.4  7.9  7.8\nBERDON,R.I.     6.8  8.8  8.5  8.8  8.3  8.5  8.7  8.7  8.4  8.5  8.8  8.7\nBRACKEN,J.J.    7.3  6.4  4.3  6.5  6.0  6.2  5.7  5.7  5.1  5.3  5.5  4.8\nBURNS,E.B.      6.2  8.8  8.7  8.5  7.9  8.0  8.1  8.0  8.0  8.0  8.6  8.6\n\n\n\n10.4.3 Visualizations of CONT and PHYS Variables\n\nggplot(df) + \n  aes(x = CONT) + \n  scale_x_continuous(limits = c(3, 12))+\n  labs(title = \"Density Plot of CONT\", x = \"CONT\", y = \"Density\") +\n  geom_density(fill = \"lightgray\") +\n  stat_function(\n    fun = dnorm,\n    args = list(mean = mean(df$CONT), sd = sd(df$CONT)),\n    color = \"red\",\n    linetype = \"dashed\"\n  ) \n\n\n\n\n\n\nFigure 10.1: Distribution of CONT Variable\n\n\n\n\n\nggplot(df) +\n  aes(x = PHYS) + \n  scale_x_continuous(limits = c(3, 12)) +\n  labs(title = \"Density Plot of PHYS\", x = \"PHYS\", y = \"Density\") +\n  geom_density(fill = \"lightgray\") +\n  stat_function(\n    fun = dnorm,\n    args = list(mean = mean(df$PHYS), sd = sd(df$PHYS)),\n    color = \"red\",\n    linetype = \"dashed\"\n  )\n\n\n\n\n\n\nFigure 10.2: Distribution of PHYS Variable\n\n\n\n\n\n10.4.4 Summary Statistics for CONT and PHYS Variables\n\n# Get the summary statistics for CONT and PHYS variables; note that the\n#   summary() function returns a named numeric vector, so to preserve the names\n#   we transform this vector to a matrix first (before creating the data frame).\nsummary_df &lt;- data.frame(\n  CONT = as.matrix(summary(df$CONT)),\n  PHYS = as.matrix(summary(df$PHYS))\n)\n\n# Display summary statistics as a table\nkable(summary_df)\n\n\nTable 10.1: Summary Statistics for CONT and PHYS Variables\n\n\n\n\n\nCONT\nPHYS\n\n\n\nMin.\n5.700000\n4.700000\n\n\n1st Qu.\n6.850000\n7.700000\n\n\nMedian\n7.300000\n8.100000\n\n\nMean\n7.437209\n7.934884\n\n\n3rd Qu.\n7.900000\n8.500000\n\n\nMax.\n10.600000\n9.100000\n\n\n\n\n\n\n\n\n\n10.4.5 Skewness and Kurtosis for CONT and PHYS Variables\n\n# Calculate skewness and kurtosis for CONT and PHYS variables with moments:: \nskewness_vals &lt;- sapply(df[, c(\"CONT\", \"PHYS\")], moments::skewness)\nkurtosis_vals &lt;- sapply(df[, c(\"CONT\", \"PHYS\")], moments::kurtosis)\n\n# Create a data frame to display skewness and kurtosis\nskew_kurt_df &lt;- data.frame(\n  Skewness = skewness_vals,\n  Kurtosis = kurtosis_vals\n)\n\n# Display the data frame as a table\nkable(skew_kurt_df)\n\n\nTable 10.2: Skewness and Kurtosis for CONT and PHYS Variables\n\n\n\n\n\nSkewness\nKurtosis\n\n\n\nCONT\n1.085973\n4.729637\n\n\nPHYS\n-1.558215\n5.408086\n\n\n\n\n\n\n\n\n\n10.4.6 Visualizations of Transformed CONT and PHYS Variables\nWe will first apply a natural log transformation to the “controlling/authoritarian” variable.\n\n# Apply log transformation to CONT variable\ndf$LOG_CONT &lt;- log(df$CONT)\n\n# Plot density of log-transformed CONT variable\nggplot(df) + \n  aes(x = LOG_CONT) +\n  geom_density(fill = \"lightgray\") +\n  labs(\n    title = \"Density Plot of Log-Transformed CONT\",\n    x = \"Log-Transformed CONT\",\n    y = \"Density\"\n  ) +\n  stat_function(\n    fun = dnorm,\n    args = list(mean = mean(df$LOG_CONT), sd = sd(df$LOG_CONT)),\n    color = \"red\",\n    linetype = \"dashed\"\n  )\n\n\n\n\n\n\nFigure 10.3: Distribution of log transformed CONT Variable\n\n\n\n\nNow we will apply a Box-Cox transformation to the “physical ability” variable.\n\n# Apply Box-Cox transformation to PHYS using MASS:: package\nbc &lt;- MASS::boxcox(df$PHYS ~ 1, lambda = seq(-5, 5, 0.1), plotit = TRUE)\nlambda &lt;- bc$x[which.max(bc$y)]\ndf$BOX_COX_PHYS &lt;- (df$PHYS^lambda - 1) / lambda\n\n\n\n\n\n\nFigure 10.4: Distribution of optimal lambda determined by the boxcox function\n\n\n\n\n\n# Plot density of Box-Cox transformed PHYS variable\nggplot(df) +\n  aes(x = BOX_COX_PHYS) +\n  labs(\n    title = \"Density Plot of Box-Cox Transformed PHYS\",\n    x = \"Box-Cox Transformed PHYS\",\n    y = \"Density\"\n  ) + \n  geom_density(fill = \"lightgray\") +\n  stat_function(\n    fun = dnorm,\n    args = list(mean = mean(df$BOX_COX_PHYS), sd = sd(df$BOX_COX_PHYS)),\n    color = \"red\",\n    linetype = \"dashed\"\n  )\n\n\n\n\n\n\nFigure 10.5: Distribution of Box-Cox transformed PHYS Variable\n\n\n\n\n\n10.4.7 Skewness and Kurtosis for Transformed CONT and PHYS Variables\n\n# Calculate skewness and kurtosis for transformed variables\nskewness_vals_trans &lt;- sapply(df[, c(\"LOG_CONT\", \"BOX_COX_PHYS\")], skewness)\nkurtosis_vals_trans &lt;- sapply(df[, c(\"LOG_CONT\", \"BOX_COX_PHYS\")], kurtosis)\n\n# Create a data frame to display skewness and kurtosis for transformed variables\nskew_kurt_df_trans &lt;- data.frame(\n  Skewness = skewness_vals_trans,\n  Kurtosis = kurtosis_vals_trans\n)\n\n# Display the data frame as a table\nkable(skew_kurt_df_trans)\n\n\nTable 10.3: Skewness and Kurtosis for Transformed Variables (LOG_CONT and BOX_COX_PHYS)\n\n\n\n\n\nSkewness\nKurtosis\n\n\n\nLOG_CONT\n0.6555572\n3.758254\n\n\nBOX_COX_PHYS\n-0.3813574\n2.501916",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Transformations to Normality</span>"
    ]
  },
  {
    "objectID": "lessons/02_transformations.html#results",
    "href": "lessons/02_transformations.html#results",
    "title": "\n10  Transformations to Normality\n",
    "section": "\n10.5 Results",
    "text": "10.5 Results\nThe application of log and Box-Cox transformations has effectively improved the normality of the CONT and PHYS variables, respectively. For the CONT variable, the original distribution exhibited a positive skewness of 1.086 and a kurtosis of 4.730, indicating a right-skewed distribution with heavy tails and a sharp peak. The log transformation reduced the skewness to 0.656 and the kurtosis to 3.758, demonstrating a significant move towards normality, though the distribution still retains some right-skewness and heavier tails compared to a normal distribution. The PHYS variable originally had a negative skewness of -1.558 and a kurtosis of 5.408, reflecting a left-skewed distribution with heavy tails and a pronounced peak. Following the Box-Cox transformation, the skewness was reduced to -0.381 and the kurtosis to 2.502. These results indicate that the transformed PHYS distribution is much closer to normality, with reduced skewness and lighter tails, achieving a more symmetric distribution. In summary, the transformations have substantially mitigated the skewness and kurtosis of both variables, enhancing their suitability for statistical analyses that assume normality. This adjustment ensures more reliable and valid results in subsequent analyses.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Transformations to Normality</span>"
    ]
  },
  {
    "objectID": "lessons/02_transformations.html#conclusion-and-discussion",
    "href": "lessons/02_transformations.html#conclusion-and-discussion",
    "title": "\n10  Transformations to Normality\n",
    "section": "\n10.6 Conclusion and Discussion",
    "text": "10.6 Conclusion and Discussion\nThe transformations applied to the CONT and PHYS variables demonstrate the effectiveness of data transformation techniques in improving the normality of distributions. By addressing skewness and kurtosis, transformations like the log and Box-Cox methods help in stabilizing variance and making data more symmetric. This enhancement is crucial for statistical analyses that rely on the assumption of normality, ensuring more accurate and reliable results. Overall, the use of appropriate transformations is a vital step in data preprocessing, significantly enhancing the suitability of data for various analytical procedures and improving the robustness of statistical inferences. However, caution is warranted in the interpretation of results after transformation. Transformed data can sometimes complicate the understanding of results and their real-world implications, as the transformed scale may not directly relate to the original measurements. It is essential to back-transform results when interpreting findings to ensure they are meaningful and relevant to the original context.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Transformations to Normality</span>"
    ]
  },
  {
    "objectID": "lessons/02_transformations.html#references",
    "href": "lessons/02_transformations.html#references",
    "title": "\n10  Transformations to Normality\n",
    "section": "\n10.7 References",
    "text": "10.7 References\n\nManikandan S. (2010). Data transformation. Journal of pharmacology & pharmacotherapeutics, 1(2), 126–127. https://doi.org/10.4103/0976-500X.72373\nWest R. M. (2022). Best practice in statistics: The use of log transformation. Annals of clinical biochemistry, 59(3), 162–165. https://doi.org/10.1177/00045632211050531\nLee D. K. (2020). Data transformation: a focus on the interpretation. Korean journal of anesthesiology, 73(6), 503–508. https://doi.org/10.4097/kja.20137",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Transformations to Normality</span>"
    ]
  },
  {
    "objectID": "lessons_original/02_fisher_exact_test.html",
    "href": "lessons_original/02_fisher_exact_test.html",
    "title": "\n11  Fisher’s Exact Test\n",
    "section": "",
    "text": "11.1 Introduction",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fisher's Exact Test</span>"
    ]
  },
  {
    "objectID": "lessons_original/02_fisher_exact_test.html#introduction",
    "href": "lessons_original/02_fisher_exact_test.html#introduction",
    "title": "\n11  Fisher’s Exact Test\n",
    "section": "",
    "text": "Fisher’s exact test is an independent test used to determine if there is a relationship between categorical (non-parametric) variables with a small sample size.\nUsed to assess whether proportions of one variable are different among values of another table.\nUses (hypergeometric) marginal distribution to derive exact p-values which are not approximated, which are also somewhat conservative.\nThe rules of Chi distribution do not apply when the frequency count is &lt;5 for more than 20% of the cells in a contingency table (Bower 2003).\nData is easily manipulated by using a contingency table.\n\n\n11.1.1 Assumptions\n\nAssumes that the individual observations are independent.\nAssumes that the row and column totals are fixed or conditioned.\nThe variables are categorical and randomly sampled.\nObservations are count data.\n\n11.1.2 Hypotheses\nThe hypotheses of Fisher’s exact test are similar to Chi-square test:\nNull hypothesis:\\((H_0)\\) There is no relationship between the categorical variables, the variables are independent.\nAlternative hypothesis: \\((H_1)\\) There is a relationship between the categorical variables, the variables are dependent.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fisher's Exact Test</span>"
    ]
  },
  {
    "objectID": "lessons_original/02_fisher_exact_test.html#fishers-exact-test-equation",
    "href": "lessons_original/02_fisher_exact_test.html#fishers-exact-test-equation",
    "title": "\n11  Fisher’s Exact Test\n",
    "section": "\n11.2 Fisher’s Exact Test Equation",
    "text": "11.2 Fisher’s Exact Test Equation\nFisher’s exact test for a one-tailed p-value is calculated using the following formula:\n\\[   p = {(a+b)!(c+d)!(a+c)!(b+d)! \\over a! b! c! d! n!} \\] - n = population size/ total frequency\n- a + b = “successes” values in the contingency table\n- a + c = sample size / draws from the population\n- a = sample successes\n\n11.2.1 Formula description\nthis test is usually used as a one-tailed test but it can also be used as a two tailed test as well, \\(a\\),\\(b\\),\\(c\\), and \\(d\\) are the individual frequencies on the 2x2 contingency table and \\(n\\) is our total frequency. This particular test is used to obtain the probability of the combination of frequencies that we can actually obtain.\n\n11.2.2 What is a contingency table?\nThis is a table that shows the distribution of a variable in the rows and columns. Sometimes referred to as a 2x2 table. They are useful in summarizing categorical variables. The table() function is used to create a contingency table in R. When the variables of interest are summarized in a contingency table it is easier to run the Fisher’s Exact test.\n\n11.2.2.1 Example: Creating a contingency table\nLets say we have information on the gender of participants in a clinical trial and the type of drug administered to them we can create the following contingency table for further analysis.\n\n# Example R code to create a contingency table\n\n# Creating a data frame\n df &lt;- data.frame(\n   \"Drug\" = c(\"Drug A\", \"Drug B\", \"Drug A\"),\n   \"Gender\" = c(\"Male\", \"Male\", \"Female\")\n )\n \n# Creating contingency table using table()\nctable &lt;- table(df)\nprint(ctable)\n\n        Gender\nDrug     Female Male\n  Drug A      1    1\n  Drug B      0    1\n\n\n\n11.2.3 Performing Fisher’s Exact Test in R\nWe will need to install the ggstatplot package to visualize the statistical results.\n\n# install.packages(\"ggstatplot\") \n# install.packages(\"summarytools\")\n# install.packages(\"gmodels\")\n# install.packages(tidyverse)\n\n\n11.2.4 Data Source: GMP2017\nFor this example we will be using the Greater Manchester Police’s UK stop and search data from 2017(December) sourced from the Sage Research Methods Dataset Part 2 (https://methods.sagepub.com/dataset/fishers-exact-gmss-2017). This data has information on stop and search events, gender and ethnicity. For this example we would like to access whether there is a significant relationship between gender and stop and search events (having controlled drugs vs harmful weapons)?\n\nGMP17 &lt;- read.csv(\n  \"../data/02_dataset-gmss-2017-subset1_jittered_20240503.csv\"\n)\n\n\n11.2.5 Load in libraries\n\nlibrary(gmodels)\nlibrary(ggstatsplot)\nlibrary(gt)\nlibrary(gtsummary)\nlibrary(katex)\nlibrary(tidyverse)\n\n\n11.2.6 Descriptive summary\n\nCodehead(GMP17)\n\n  Gender Ethnicity ObjectSearch\n1      1         1            1\n2      1         1           -9\n3      1         1            1\n4      1         1            1\n5      1         1           -9\n6      1         1            1\n\nCodestr(GMP17)\n\n'data.frame':   186 obs. of  3 variables:\n $ Gender      : int  1 1 1 1 1 1 1 1 1 -9 ...\n $ Ethnicity   : int  1 1 1 1 1 1 2 1 1 1 ...\n $ ObjectSearch: int  1 -9 1 1 -9 1 1 1 -9 -9 ...\n\nCode# determining the number of rows\nNROW(GMP17)\n\n[1] 186\n\n\n\n11.2.7 Assessing frequencies to answer research question\nFor this analysis we will use the Gender variable and the ObjectSearch variable\n\n# Dropping the Ethnicity variable to remain with variables of interest for for the 2x2 table\n\nnewGMP17 &lt;-GMP17[ -c(2) ]\n \nhead(newGMP17)\n\n  Gender ObjectSearch\n1      1            1\n2      1           -9\n3      1            1\n4      1            1\n5      1           -9\n6      1            1\n\n\nThe data contains missing values categorized as -9 that we need to drop and we need to rename our variables based on the data dictionary provided https://methods.sagepub.com/dataset/download/fishers-exact-gmss-2017/guide/codebook.\n\n# Exclude rows that have missing data in both variables\nnewGMP17_nom &lt;- subset(newGMP17, Gender &gt; 0)\nnewGMP17_nom2 &lt;- subset(newGMP17_nom, ObjectSearch  &gt; 0)\nsummary(newGMP17_nom2)\n\n     Gender       ObjectSearch  \n Min.   :1.000   Min.   :1.000  \n 1st Qu.:1.000   1st Qu.:1.000  \n Median :1.000   Median :1.000  \n Mean   :1.052   Mean   :1.267  \n 3rd Qu.:1.000   3rd Qu.:2.000  \n Max.   :2.000   Max.   :2.000  \n\nnrow(newGMP17_nom2)\n\n[1] 116\n\n\n\n# Renaming the Gender variable based on data dictionary\nnewGMP17_nom2$Gender &lt;- recode_factor(\n  newGMP17_nom2$Gender,\n  \"1\" = \"Male\",\n  \"2\" = \"Female\"\n)\n\n# Renaming the Gender variable based on data dictionary\nnewGMP17_nom2$ObjectSearch &lt;- recode_factor(\n  newGMP17_nom2$ObjectSearch,\n  \"1\" = \"Controlled_Drugs\",\n  \"2\" = \"Harmful_Objects\"\n)\n\n\n# Creating the contingency table for subset data\ncGMP17 = table(newGMP17_nom2)\nprint(cGMP17)\n\n        ObjectSearch\nGender   Controlled_Drugs Harmful_Objects\n  Male                 83              27\n  Female                2               4\n\n\n\n11.2.8 Visualizing data using mosaic plot\n\nwe can use the mosaic plot to represent the data.\n\n\nmosaicplot(\n  cGMP17,\n  main = 'Mosaic Plot',\n  color = TRUE\n)\n\n\n\n\n\n\n\n\n11.2.9 Running the Fisher’s exact test using fisher.test()\nWhat if we just run a Chi-square test?\nUsing our GMP17 dataset we can try to run a Chi-square test instead of the Fisher’s Exact test and see what happens.\nThe R output gives us a warning that the Chi Square is not appropriate hence we should use another test in this case the Fisher’s Exact Test.\n\nchisq.test(cGMP17)$expected\n\nWarning in chisq.test(cGMP17): Chi-squared approximation may be incorrect\n\n\n        ObjectSearch\nGender   Controlled_Drugs Harmful_Objects\n  Male          80.603448       29.396552\n  Female         4.396552        1.603448\n\n\n\n11.2.10 Running the test\n\n# running the fisher's exact test\n\ntest &lt;- fisher.test(cGMP17)\ntest\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  cGMP17\np-value = 0.04297\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n  0.8133673 70.2637501\nsample estimates:\nodds ratio \n  6.030297 \n\n\nUsing the gt summary to view results.\n\nnewGMP17_nom2 |&gt; \n  tbl_summary(by = Gender) |&gt; \n  add_p() |&gt; \n  add_overall()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nOverall, N = 1161\n\n\nMale, N = 1101\n\n\nFemale, N = 61\n\n\np-value2\n\n\n\n\nObjectSearch\n\n\n\n0.043\n\n\n    Controlled_Drugs\n85 (73%)\n83 (75%)\n2 (33%)\n\n\n\n    Harmful_Objects\n31 (27%)\n27 (25%)\n4 (67%)\n\n\n\n\n\n\n1 n (%)\n\n\n\n2 Fisher’s exact test\n\n\n\n\n\n\n\n\n11.2.11 Interpretation of results\nThe most important test statistic is the p - value therefore we can retrieve the specific result using the following code;\n\ntest$p.value \n\n[1] 0.04297268\n\n\nOdds ratio = 6.33, 95% CI = 0.85-73.59], we reject the null hypothesis (p &lt; 0.05) and conclude that there is a strong association between the two categorical independent variables (gender and object search events)\nTherefore the odds ratio indicates that the odds of having controlled drugs at a stop and search is 6.33 times as likely for males compared to females. In other words, males are more likely of having controlled drugs at a stop and search than females.\n\n11.2.12 Visualizing statistical results with plots using ggstatsplot\n\nwe download the ggsattsplot package to visualize the results in a plot.\n\n\n# Fisher's exact test \n\ntest &lt;- fisher.test(cGMP17)\n\n# combine plot and statistical test with ggbarstats\n\nggbarstats(\n  newGMP17_nom2, Gender, ObjectSearch,\n  results.subtitle = FALSE,\n  subtitle = paste0(\n    \"Fisher's exact test\", \", p-value = \",\n    ifelse(test$p.value &lt; 0.001, \"&lt; 0.001\", round(test$p.value, 3))\n  )\n)\n\n\n\n\n\n\n\nFrom the plot, it is clear that the proportion of males among object search events is higher compared to females, suggesting that there is a relationship between the two variables.\nThis is confirmed thanks to the p-value displayed in the subtitle of the plot. As previously, we reject the null hypothesis and we conclude that the variables gender and stop and search events are not independent (p-value = 0.038).",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fisher's Exact Test</span>"
    ]
  },
  {
    "objectID": "lessons_original/02_fisher_exact_test.html#what-if-we-have-more-than-two-levels",
    "href": "lessons_original/02_fisher_exact_test.html#what-if-we-have-more-than-two-levels",
    "title": "\n11  Fisher’s Exact Test\n",
    "section": "\n11.3 What if we have more than two levels?",
    "text": "11.3 What if we have more than two levels?\nUsing the drug example used previously lets say we have 3 drugs ‘Drug A, Drug B or Drug C’ and we want to see if there is any relationship with gender ‘Male/Female’.\n\n# Creating a data frame\ndf &lt;- data.frame (\n  \"Drug\" = c(\"Drug A\", \"Drug B\", \"Drug A\", \"Drug C\", \"Drug C\"),\n  \"Gender\" = c(\"Male\", \"Male\", \"Female\", \"Female\", \"Female\")\n)\n \n# Creating contingency table using table()\nctable &lt;- table(df)\nprint(ctable)\n\n        Gender\nDrug     Female Male\n  Drug A      1    1\n  Drug B      0    1\n  Drug C      2    0\n\n\n\n# Running the Fisher's Exact test for the 3x2 table\nfisher.test(ctable)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  ctable\np-value = 0.6\nalternative hypothesis: two.sided\n\n\nThe p-value is non-significant [p = 0.6], we fail to reject the null hypothesis (p &lt; 0.05) and conclude that there is no association between the drug treatments and gender. If the results had been significant we would have gone ahead and conducted a post hoc analysis using pairwise_fisher_test to asses each combination.\nSummary\nThis article describe the assumptions and hypotheses of the Fisher’s Exact test. It also provides examples on how it can be applied.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fisher's Exact Test</span>"
    ]
  },
  {
    "objectID": "lessons_original/02_fisher_exact_test.html#references",
    "href": "lessons_original/02_fisher_exact_test.html#references",
    "title": "\n11  Fisher’s Exact Test\n",
    "section": "\n11.4 References",
    "text": "11.4 References\n\nBower, Keith M. 2003. “When to Use Fisher’s Exact Test.” In American Society for Quality, Six Sigma Forum Magazine, 2:35–37. 4.\nMcCrum-Gardner, Evie. 2008. “Which Is the Correct Statistical Test to Use?” British Journal of Oral and Maxillofacial Surgery 46 (1): 38–41.\nWong KC. Chi squared test versus Fisher’s exact test. Hong Kong Med J. 2011 Oct;17(5):427\nPatil, I. (2021). Visualizations with statistical details: The ‘ggstatsplot’ approach. Journal of Open Source Software, 6(61), 3167, doi:10.21105/joss.03167\nZach Bobbit. (2021). Fisher’s Exact Test: Definition, Formula, and Example",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fisher's Exact Test</span>"
    ]
  },
  {
    "objectID": "03_header_two-sample.html",
    "href": "03_header_two-sample.html",
    "title": "Two-Sample Tests",
    "section": "",
    "text": "Text Outline\nThis text covers the basics of:",
    "crumbs": [
      "Two-Sample Tests"
    ]
  },
  {
    "objectID": "03_header_two-sample.html#text-outline",
    "href": "03_header_two-sample.html#text-outline",
    "title": "Two-Sample Tests",
    "section": "",
    "text": "Exploring data\nOne-sample tests\nTwo-sample tests\nLinear regression and ANOVA\nGeneralized linear models\nSome special topics\nMaybe a bit about power calculations…",
    "crumbs": [
      "Two-Sample Tests"
    ]
  },
  {
    "objectID": "03_header_two-sample.html#part-outline",
    "href": "03_header_two-sample.html#part-outline",
    "title": "Two-Sample Tests",
    "section": "Part Outline",
    "text": "Part Outline\nThis part of the text contains the chapters on various two-sample statistical tests:\n\n\\(t\\)-test\nWelch’s \\(t\\)-test\nMann-Whitney \\(U\\) test\nCochran’s \\(Q\\) test\n\\(\\chi^2\\) Test for Independence",
    "crumbs": [
      "Two-Sample Tests"
    ]
  },
  {
    "objectID": "lessons_original/03_two_sample_ttest.html",
    "href": "lessons_original/03_two_sample_ttest.html",
    "title": "\n12  Two sample \\(t\\)-test\n",
    "section": "",
    "text": "12.1 Two sample \\(t\\)-test\nThis is also called the independent sample t test. It is used to see whether the unknown population means of two groups are equal or different. This test requires one variable which can be the exposure x and another variable which can be the outcome y. If you have more than two groups then analysis of variance (ANOVA) will be more suitable. If data is nonparametric then an alternative test to use would be the Mann Whitney U test or a permutation test.(cressie1986?).\nThere are two types of t tests, the first being the Student’s t test, which assumes the variance of the two groups is equal, the second being the Welch’s t test (default in R), which assumes the variance in the two groups is different.\nIn this article we will be discussing the Student’s t test.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Two sample $t$-test</span>"
    ]
  },
  {
    "objectID": "lessons_original/03_two_sample_ttest.html#two-sample-t-test",
    "href": "lessons_original/03_two_sample_ttest.html#two-sample-t-test",
    "title": "\n12  Two sample \\(t\\)-test\n",
    "section": "",
    "text": "12.1.1 Assumptions\n\nMeasurements for one observation do not affect measurements for any other observation (assumes independence).\nData in each group must be obtained via a random sample from the population.\nData in each group are normally distributed.\nData values are continuous.\nThe variances for the two independent groups are equal in the Student’s t test.\nThere should be no significant outliers.\n\n12.1.2 Hypotheses\n\n\\((H_0)\\): the mean of group A \\((m_A)\\) is equal to the mean of group B \\((m_B)\\)- two tailed test,\n\\((H_0)\\): \\((m_A)\\ge (m_B)\\)- one tailed test.\n\n\\((H_0)\\): \\((m_A)\\le (m_B)\\)- one tailed test.\nThe corresponding alternative hypotheses would be as follows:\n\n\n\n\n\\((H_1)\\): \\((m_A)\\neq(m_B)\\)- two tailed test.\n\n\\((H_1)\\): \\((m_A)&lt;(m_B)\\)- one tailed test.\n\n\\((H_1)\\): \\((m_A)&gt; (m_B)\\)- one tailed test.\n\n12.1.3 Statistical hypotheses formula\nFor the Student’s t test which assumes equal variance the following is how the |t| statistic may be calculated using groups A and B as examples:\n\\(t ={ {m_{A} - m_{B}} \\over \\sqrt{ {S^2 \\over n_{A} } + {S^2 \\over n_{B}}   }}\\)\nThis can be described as the sample mean difference divided by the sample standard deviation of the sample mean difference where:\n\\(m_A\\) and \\(m_B\\) are the mean values of A and B,\n\\(n_A\\) and \\(n_B\\) are the seize of group A and B,\n\\(S^2\\) is the estimator for the pooled variance,\nwith the degrees of freedom (df) = \\(n_A + n_B - 2\\),\nand \\(S^2\\) is calculated as follows:\n\\(S^2 = { {\\sum{ (x_A-m_{A})^2} + \\sum{ (x_B-m_{B})^2}} \\over {n_{A} + n_{B} - 2 }}\\)\nResults for both Students t test and Welch’s t test are usually similar unless the group sizes and standard deviations are different.\nWhat if the data is not independent?\nIf the data is not independent such as paired data in the form of matched pairs which are correlated, we use the paired t test. This test checks whether the means of two paired groups are different from each other. It’s usually used in clinical trial studies with a “before and after” or case control studies with matched pairs. For this test we only assume the difference of each pair to be normally distributed (the paired groups are the ones important for analysis) unlike the independent t test which assumes that data from both samples are independent and variances are equal.(fralick?)\n\n12.1.4 Example\n\n12.1.4.1 Prerequisites\n\n\ntidyverse: data manipulation and visualization.\n\nrstatix: providing pipe friendly R functions for easy statistical analyses.\n\ncar: providing variance tests.\n\n\n#install.packages(\"ggstatplot\") \n#install.packages(\"car\")\n#install.packages(\"rstatix\")\n#install.packages(tidyVerse)\n\n\n12.1.4.2 Dataset\nThis example dataset sourced from kaggle was obtained from surveys of students in Math and Portuguese classes in secondary school. It contains demographic information on gender, social and study information.(cortez2008?)\n\n# load relevant libraries\nlibrary(rcompanion)\nlibrary(car)\nlibrary (gt)\nlibrary(gtsummary)\nlibrary(ggpubr)\nlibrary(rstatix)\nlibrary(tidyverse)\n\n\n# load the dataset\nstu_math &lt;- read_csv(\"../data/03_student-mat.csv\")\n\nRows: 395 Columns: 33\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (17): school, sex, address, famsize, Pstatus, Mjob, Fjob, reason, guardi...\ndbl (16): age, Medu, Fedu, traveltime, studytime, failures, famrel, freetime...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nChecking the data\n\n# check the data\nglimpse(stu_math)\n\nRows: 395\nColumns: 33\n$ school     &lt;chr&gt; \"GP\", \"GP\", \"GP\", \"GP\", \"GP\", \"GP\", \"GP\", \"GP\", \"GP\", \"GP\",…\n$ sex        &lt;chr&gt; \"F\", \"F\", \"F\", \"F\", \"F\", \"M\", \"M\", \"F\", \"M\", \"M\", \"F\", \"F\",…\n$ age        &lt;dbl&gt; 18, 17, 15, 15, 16, 16, 16, 17, 15, 15, 15, 15, 15, 15, 15,…\n$ address    &lt;chr&gt; \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\",…\n$ famsize    &lt;chr&gt; \"GT3\", \"GT3\", \"LE3\", \"GT3\", \"GT3\", \"LE3\", \"LE3\", \"GT3\", \"LE…\n$ Pstatus    &lt;chr&gt; \"A\", \"T\", \"T\", \"T\", \"T\", \"T\", \"T\", \"A\", \"A\", \"T\", \"T\", \"T\",…\n$ Medu       &lt;dbl&gt; 4, 1, 1, 4, 3, 4, 2, 4, 3, 3, 4, 2, 4, 4, 2, 4, 4, 3, 3, 4,…\n$ Fedu       &lt;dbl&gt; 4, 1, 1, 2, 3, 3, 2, 4, 2, 4, 4, 1, 4, 3, 2, 4, 4, 3, 2, 3,…\n$ Mjob       &lt;chr&gt; \"at_home\", \"at_home\", \"at_home\", \"health\", \"other\", \"servic…\n$ Fjob       &lt;chr&gt; \"teacher\", \"other\", \"other\", \"services\", \"other\", \"other\", …\n$ reason     &lt;chr&gt; \"course\", \"course\", \"other\", \"home\", \"home\", \"reputation\", …\n$ guardian   &lt;chr&gt; \"mother\", \"father\", \"mother\", \"mother\", \"father\", \"mother\",…\n$ traveltime &lt;dbl&gt; 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 3, 1, 2, 1, 1, 1, 3, 1, 1,…\n$ studytime  &lt;dbl&gt; 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 3, 1, 2, 3, 1, 3, 2, 1, 1,…\n$ failures   &lt;dbl&gt; 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0,…\n$ schoolsup  &lt;chr&gt; \"yes\", \"no\", \"yes\", \"no\", \"no\", \"no\", \"no\", \"yes\", \"no\", \"n…\n$ famsup     &lt;chr&gt; \"no\", \"yes\", \"no\", \"yes\", \"yes\", \"yes\", \"no\", \"yes\", \"yes\",…\n$ paid       &lt;chr&gt; \"no\", \"no\", \"yes\", \"yes\", \"yes\", \"yes\", \"no\", \"no\", \"yes\", …\n$ activities &lt;chr&gt; \"no\", \"no\", \"no\", \"yes\", \"no\", \"yes\", \"no\", \"no\", \"no\", \"ye…\n$ nursery    &lt;chr&gt; \"yes\", \"no\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes…\n$ higher     &lt;chr&gt; \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"ye…\n$ internet   &lt;chr&gt; \"no\", \"yes\", \"yes\", \"yes\", \"no\", \"yes\", \"yes\", \"no\", \"yes\",…\n$ romantic   &lt;chr&gt; \"no\", \"no\", \"no\", \"yes\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\"…\n$ famrel     &lt;dbl&gt; 4, 5, 4, 3, 4, 5, 4, 4, 4, 5, 3, 5, 4, 5, 4, 4, 3, 5, 5, 3,…\n$ freetime   &lt;dbl&gt; 3, 3, 3, 2, 3, 4, 4, 1, 2, 5, 3, 2, 3, 4, 5, 4, 2, 3, 5, 1,…\n$ goout      &lt;dbl&gt; 4, 3, 2, 2, 2, 2, 4, 4, 2, 1, 3, 2, 3, 3, 2, 4, 3, 2, 5, 3,…\n$ Dalc       &lt;dbl&gt; 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1,…\n$ Walc       &lt;dbl&gt; 1, 1, 3, 1, 2, 2, 1, 1, 1, 1, 2, 1, 3, 2, 1, 2, 2, 1, 4, 3,…\n$ health     &lt;dbl&gt; 3, 3, 3, 5, 5, 5, 3, 1, 1, 5, 2, 4, 5, 3, 3, 2, 2, 4, 5, 5,…\n$ absences   &lt;dbl&gt; 6, 4, 10, 2, 4, 10, 0, 6, 0, 0, 0, 4, 2, 2, 0, 4, 6, 4, 16,…\n$ G1         &lt;dbl&gt; 5, 5, 7, 15, 6, 15, 12, 6, 16, 14, 10, 10, 14, 10, 14, 14, …\n$ G2         &lt;dbl&gt; 6, 5, 8, 14, 10, 15, 12, 5, 18, 15, 8, 12, 14, 10, 16, 14, …\n$ G3         &lt;dbl&gt; 6, 6, 10, 15, 10, 15, 11, 6, 19, 15, 9, 12, 14, 11, 16, 14,…\n\n\nIn total there are 395 observations and 33 variables. We will drop the variables we do not need and keep the variables that will help us answer the following: Is there a difference between boys and girls in math final grades?\n\\(H_0\\): There is no statistical difference between the final grades between boys and girls.\n\\(H_1\\): There is a statistically significant difference in the final grades between the two groups.\n\n# creating a subset of the data \nmath = subset(stu_math, select= c(sex,G3))\nglimpse(math)\n\nRows: 395\nColumns: 2\n$ sex &lt;chr&gt; \"F\", \"F\", \"F\", \"F\", \"F\", \"M\", \"M\", \"F\", \"M\", \"M\", \"F\", \"F\", \"M\", \"…\n$ G3  &lt;dbl&gt; 6, 6, 10, 15, 10, 15, 11, 6, 19, 15, 9, 12, 14, 11, 16, 14, 14, 10…\n\n\nSummary statistics- the dependent variable is continuous (grades=G3) and the independent variable is character but binary (sex).\n\n# summarizing our data\n summary(math)\n\n     sex                  G3       \n Length:395         Min.   : 0.00  \n Class :character   1st Qu.: 8.00  \n Mode  :character   Median :11.00  \n                    Mean   :10.42  \n                    3rd Qu.:14.00  \n                    Max.   :20.00  \n\n\nWe see that data ranges from 0-20 with 0 being people who were absent and could not take the test therefore missing data. We remove these 0 values before running the t test. However other models should be considered such as the zero inflated model to differentiate those who truly got a 0 and those who were not present to take test.\n\n# creating a boxplot to visualize the data with no outliers\nmath2 = subset(math, G3&gt;0)\nboxplot(G3 ~ sex,data=math2)\n\n\n\n\n\n\n\nVisualizing the data- we can use histograms and box lots to visualize the data to check for outliers and distribution thus checking for normality.\n\n# Histograms for data by groups \n\nmale = math2$G3[math2$sex == \"M\"]\nfemale = math2$G3[math2$sex == \"F\"]\n\n# plotting distribution for males\nplotNormalHistogram(\n  male, \n  breaks= 20,\n  xlim=c(0,20),\n  main=\"Distribution of the grades for males \", \n  xlab= \"Math Grades\"\n  )\n\n\n\n\n\n\n\nFinal grades for males seem to be normally distributed from 0-20. Data is approximately normal because we have a large amount of bins.\n\n# plotting distribution for females\nplotNormalHistogram(\n  female, breaks= 20,\n  xlim=c(0,20),\n  main=\"Distribution of the grades for females \", \n  xlab= \"Math Grades\"\n  )\n\n\n\n\n\n\n\nFinal grades for females also appear to be normally distributed. The final score across both is almost evenly distributed. However there seem to be a significant number of individuals who failed the test (grade=0).\n\n# plotting bar plot to see the distribution in sample size\nsample_size = table(math2$sex)\nbarplot(sample_size,main= \"Distribution of sample size by sex\")\n\n\n\n\n\n\n\nThe bar graph shows that there are slightly more females in the sample than males.\nIdentifying outliers\n\n# creating a boxplot to visualize the outliers (G3=0)\nboxplot(G3 ~ sex,data=math2)\n\n\n\n\n\n\n\nThe box plot shows us that there are no outliers as these have been removed in terms of people who had a score of 0. This score is not truly reflective of the performance between boys and girls as a grade of 0 may represent absentia or other reasons for the test not been taken. Therefore we opt to drop the outliers. We will compare to see if this decision affects the mean which appears similar from the above plot.\n\n# finding the mean for the groups with outliers\nmean(math$G3[math$sex==\"F\"])\n\n[1] 9.966346\n\nmean(math$G3[math$sex==\"M\"])\n\n[1] 10.91444\n\n# finding the mean for the groups without outliers\nmean(math2$G3[math2$sex==\"F\"])\n\n[1] 11.20541\n\nmean(math2$G3[math2$sex==\"M\"])\n\n[1] 11.86628\n\n\nThe mean has increased slightly and the difference decreased after removing the outliers but the distribution is still the same.\nCheck the equality of variances (homogeneity)\nWe can use the Levene’s test or the Bartlett’s test to check for homogeneity of variances. The former is in the car library and the later in the rstatix library. If the variances are homogeneous the p value will be greater than 0.05.\nOther tests include F test 2 sided, Brown-Forsythe and O’Brien but we shall not cover these.\n\n# running the Bartlett's test to check equal variance\nbartlett.test(G3~sex, data=math2)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  G3 by sex\nBartlett's K-squared = 0.12148, df = 1, p-value = 0.7274\n\n# running the Levene's test to check equal variance\nmath2 %&gt;% levene_test(G3~sex)\n\n# A tibble: 1 × 4\n    df1   df2 statistic     p\n  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1     1   355     0.614 0.434\n\n\nThe p value is greater than 0.05 suggesting there is no difference between the variances of the two groups.\n\n12.1.4.3 Assessment\n\nData is continuous(G3)\nData is normally distributed\nData is independent (males and females distinct not the same individual)\nNo significant outliers\nThere are equal variances\n\nAs the assumptions are met we go ahead to perform the Student’s \\(t\\)-test.\n\n12.1.4.4 Performing the two-sample \\(t\\)-test\n\nSince the default is the Welch t test we use the \\(\\color{blue}{\\text{var.eqaul = TRUE }}\\) code to signify a Student’s t test. There is a t.test() function in stats package and a t_test() in the rstatix package. For this analysis we use the rstatix method as it comes out as a table.\n\n# perfoming the two sample t test\nstat.test &lt;- math2 %&gt;% \n  t_test(G3~ sex, var.equal=TRUE) %&gt;%\n  add_significance()\nstat.test\n\n# A tibble: 1 × 9\n  .y.   group1 group2    n1    n2 statistic    df      p p.signif\n  &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   \n1 G3    F      M        185   172     -1.94   355 0.0531 ns      \n\n\n\nstat.test$statistic\n\n        t \n-1.940477 \n\n\nThe results are represented as follows;\n\ny - dependent variable\ngroup1, group 2 - compared groups(independent variables)\ndf - degrees of freedom\np - p value\n\ngtsummary table of results\n\n math2 |&gt; \n  tbl_summary(\n    by = sex,\n    statistic =\n      list(\n        all_continuous() ~ \"{mean} ({sd})\",\n        all_dichotomous() ~ \"{p}%\")\n    ) |&gt; \n   add_n() |&gt; \n  add_overall() |&gt; \n  add_difference()\n\n\n\n\n\n\nCharacteristic\nN\n\nOverall, N = 3571\n\n\nF, N = 1851\n\n\nM, N = 1721\n\n\nDifference2\n\n\n95% CI2,3\n\n\np-value2\n\n\n\nG3\n357\n11.5 (3.2)\n11.2 (3.2)\n11.9 (3.3)\n-0.66\n-1.3, 0.01\n0.053\n\n\n\n\n1 Mean (SD)\n\n\n\n2 Welch Two Sample t-test\n\n\n\n3 CI = Confidence Interval\n\n\n\n\n\n\n\nInterpretation of results\nFor the two sample t test with t(355) = -1.940477, p = 0.0531, the p value is greater than our alpha of 0.05 , we fail to reject the null hypothesis and conclude that there is no statistical difference between the means of the two groups. There is no difference in final grades between boys and girls. (A significant |t| would be 1.96 or greater).\nEffect size\nCohen’s d can be an used as an effect size statistic for the two sample t test. It is the difference between the means of each group divided by the pooled standard deviation.\n\\(d= {m_A-m_B \\over SD_pooled}\\)\nIt ranges from 0 to infinity, with 0 indicating no effect where the means are equal. 0.5 means that the means differ by half the standard deviation of the data and 1 means they differ by 1 standard deviation. It is divided into small, medium or large using the following cut off points.\n\nsmall 0.2-&lt;0.5\nmedium 0.5-&lt;0.8\nlarge &gt;=0.8\n\nFor the above test the following is how we can find the effect size;\n\n#perfoming cohen's d\nmath2 %&gt;% \n  cohens_d(G3~sex,var.equal = TRUE)\n\n# A tibble: 1 × 7\n  .y.   group1 group2 effsize    n1    n2 magnitude\n* &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;ord&gt;    \n1 G3    F      M       -0.206   185   172 small    \n\n\nThe effect size is small d= -0.20.\nIn conclusion, a two-samples t-test showed that the difference was not statistically significant, t(355) = -1.940477, p &lt; 0.0531, d = -0.20; where, t(355) is shorthand notation for a t-statistic that has 355 degrees of freedom and d is Cohen’s d. We can conclude that the females mean final grade is greater than males final grade (d= -0.20) but this result is not significant.\nWhat if it is one tailed t test?\nUse the \\(\\color{blue}{\\text{alternative =}}\\) option to determine if one group is \\(\\color{blue}{\\text{\"less\"}}\\) or \\(\\color{blue}{\\text{\"greater\"}}\\). For example if we want to see whether the final grades for females are greater than males we can use the following code:\n\n# perfoming the one tailed two sample t test\nstat.test &lt;- math2 %&gt;% \n  t_test(G3~ sex, var.equal=TRUE, alternative = \"greater\") %&gt;%\n  add_significance()\nstat.test\n\n# A tibble: 1 × 9\n  .y.   group1 group2    n1    n2 statistic    df     p p.signif\n  &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   \n1 G3    F      M        185   172     -1.94   355 0.973 ns      \n\n\nThe p value is greater than 0.05 (p=0.973), we fail to reject the null hypothesis. We conclude that the final grades for females are not significantly greater than for males.\nWhat about running the paired sample t test?\nWe can simply add the syntax \\(\\color{blue}{\\text{paired= TRUE}}\\) to our t_test() to run the analysis for matched pairs data.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Two sample $t$-test</span>"
    ]
  },
  {
    "objectID": "lessons_original/03_two_sample_ttest.html#conclusion",
    "href": "lessons_original/03_two_sample_ttest.html#conclusion",
    "title": "\n12  Two sample \\(t\\)-test\n",
    "section": "\n12.2 Conclusion",
    "text": "12.2 Conclusion\nThis article covers the Student’s t test and how we run it in R. It also shows how we find the effect size and how we can conclude the results.\n\n12.2.1 References",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Two sample $t$-test</span>"
    ]
  },
  {
    "objectID": "lessons_original/03_two_sample_mann_whitney.html",
    "href": "lessons_original/03_two_sample_mann_whitney.html",
    "title": "\n13  Mann-Whittney-U Test Example\n",
    "section": "",
    "text": "13.1 Introduction\nOverall goal: Identify whether the distribution of two groups significantly differs.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Mann-Whittney-U Test Example</span>"
    ]
  },
  {
    "objectID": "lessons_original/03_two_sample_mann_whitney.html#introduction",
    "href": "lessons_original/03_two_sample_mann_whitney.html#introduction",
    "title": "\n13  Mann-Whittney-U Test Example\n",
    "section": "",
    "text": "Mann Whitney U test, also known as the Wilcoxon Rank-Sum test, is commonly used to compare the means or medians of two independent groups with the assumption that the at least one group is not normally distributed and when sample size is small.\n\nThe Welch U test should be used when there exists signs of skewness and variance of heterogeneity.fagerland2009?\n\n\n\n\nIt is useful for numerical/continuous variables.\n\nFor example, if researchers want to compare two different groups’ age or height (continuous variables) in a study with non-normally distributed data.sundjaja2023?\n\n\n\nWhen conducting this test, aside from reporting the p-value, the spread, and the shape of the data should be described.hart2001?\n\n\n\n13.1.1 Hypotheses\nNull Hypothesis (H0): Distribution1 = Distribution2\n\nMean/Median Ranks of two levels are equalchiyau2020?\n\n\nAlternate Hypothesis (H1): Distribution1 ≠ Distribution2\n\nMean/Median Ranks of two levels are significantly differentchiyau2020?\n\n\n\n13.1.1.1 Mathematical Equation\n\\(U_1 = n_1n_2 + \\frac{n_1 \\cdot (n_1 + 1)}{2} - R_1\\)\n\\(U_2 = n_1n_2 + \\frac{n_2 \\cdot (n_2 + 1)}{2} - R_2\\)\nWhere:\n\n\n\\(U_1\\) and \\(U_2\\) represent the test statistics for two groups;(Male & Female).\n\n\\(R_1\\) and \\(R_2\\) represent the sum of the ranks of the observations for two groups.\n\n\\(n_1\\) and \\(n_2\\) are the sample sizes for two groups.\n\n13.1.1.2 Basic criteria\n\nSamples are independent: Each dependent variable must be related to only one independent variable.\nThe response variable is ordinal or continuous.\nAt least one variable is not normally distributed.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Mann-Whittney-U Test Example</span>"
    ]
  },
  {
    "objectID": "lessons_original/03_two_sample_mann_whitney.html#performing-mann-whitney-u-test-in-r",
    "href": "lessons_original/03_two_sample_mann_whitney.html#performing-mann-whitney-u-test-in-r",
    "title": "\n13  Mann-Whittney-U Test Example\n",
    "section": "\n13.2 Performing Mann-Whitney U Test in R",
    "text": "13.2 Performing Mann-Whitney U Test in R\n\n13.2.1 Data\nIn this example, we will perform the Mann-Whitney U Test using wave 8 (2012-2013) data of a longitudinal epidemiological study titled Hispanic Established Populations For the Epidemiological Study of Elderly (HEPESE).\nThe HEPESE provides data on risk factors for mortality and morbidity in Mexican Americans in order to contrast how these factors operate differently in non-Hispanic White Americans, African Americans, and other major ethnic groups.The data is publicly available and can be obtained from the University of Michigan website.kyriakoss.markides2016?\nUsing this data, we want to explore whether there are significant gender differences in age when Type 2 diabetes mellitus (T2DM) is diagnosed. Type 2 diabetes is a chronic disease condition that has affected 37 million people living in the United States. Type 2 diabetes is the eighth leading cause of death and disability in US. Type 2 diabetes generally occurs among adults aged 45 or older although, young adults and children are also diagnosed with it these days. Diabetes and its complications are preventable when following proper lifestyles and timely medications. 1 in 5 of US people don’t know they have diabetes.national2020?\nResearch has shown that men are more likely to develop type 2 diabetes while women are more likely to experience complications, including heart and kidney disease.meissner2021?\nIn this report, we want to test whether there are significant differences in age at which diabetes is diagnosed among males and females.\nDependent Response Variable\nageAtDx = Age_Diagnosed = Age at which diabetes is diagnosed.\nIndependent Variable\nisMale = Gender\nResearch Question:\nDoes the age at which diabetes is diagnosed significantly differ among Men and Women?\nNull Hypothesis (H0): Mean rank of age at which diabetes is diagnosed is equal among men and women.\nAlternate Hypothesis (H1): Mean rank of age at which diabetes is diagnosed is not equal among men and women.\n\n13.2.2 Packages\n\ngmodels: It helps to compute and display confidence intervals (CI) for model estimates.warnes2022?\nDescTools: It provides tools for basic statistics e.g. to compute Median CI for an efficient data description.andrisignorell2023?\nggplot2: It helps to create Boxplots.\nqqplotr: It helps to create QQ plot.\ndplyr: It is used to manipulate data and provide summary statistics.\n\nhaven: It helps to import spss data into r.\nDependencies = TRUE : It indicates that while installing packages, it must also install all dependencies of the specified package.\n\n\n\n# install.packages(\"gmodels\", dependencies = TRUE)\n# install.packages(\"car\", dependencies = TRUE)\n# install.packages(\"DescTools\", dependencies = TRUE)\n# install.packages(\"ggplot2\", dependencies = TRUE)\n# install.packages(\"qqplotr\", dependencies = TRUE)\n# install.packages(\"gtsummary\", dependencies = TRUE)\n\nLoading Library\n\nsuppressPackageStartupMessages(library(haven))\nsuppressPackageStartupMessages(library(ggpubr))\nsuppressPackageStartupMessages(library(gmodels))\nsuppressPackageStartupMessages(library(DescTools))\nsuppressPackageStartupMessages(library(ggplot2))\nsuppressPackageStartupMessages(library(qqplotr))\nsuppressPackageStartupMessages(library(gtsummary))\nsuppressPackageStartupMessages(library(tidyverse))\n\nData Importing\n\n# Mann_W_U &lt;- read_sav(\"data\\\\36578-0001-Data.sav\")\nMann_W_U &lt;- read_csv(\"../data/03_HEPESE_synthetic_20240510.csv\")\n\nRows: 744 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (1): ageAtDx\nlgl (1): isMale\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n13.2.3 Data Exploration\n\n# str(Mann_W_U)\nstr(Mann_W_U$isMale)\n\n logi [1:744] FALSE FALSE FALSE TRUE FALSE TRUE ...\n\nstr(Mann_W_U$ageAtDx)\n\n num [1:744] 87 70 68 60 55 33 38 65 50 68 ...\n\n\nAfter inspecting the data, we found that values of our dependent and independent variable values are in character form. We want them to be numerical and categorical, respectively. First, we will convert dependent variable into numerical form and our independent variable into categorical. Next, we will recode the factors as male and female. Also for ease, we will rename our dependent and independent variable.\n\n# convert to number and factor\nMann_W_U$ageAtDx &lt;- as.numeric(Mann_W_U$ageAtDx)\nclass(Mann_W_U$ageAtDx)\n\n[1] \"numeric\"\n\nMann_W_U$isMale &lt;- as_factor(Mann_W_U$isMale)\nclass(Mann_W_U$isMale)\n\n[1] \"factor\"\n\n\nThe next step is to calculate some of the descriptive data to give us a better idea of the data that we are dealing with. This can be done using the summarise function.\nDescriptive Data\n\nDes &lt;- \n Mann_W_U %&gt;% \n select(isMale, ageAtDx) %&gt;% \n group_by(isMale) %&gt;%\n summarise(\n   n = n(),\n   mean = mean(ageAtDx, na.rm = TRUE),\n   sd = sd(ageAtDx, na.rm = TRUE),\n   stderr = sd/sqrt(n),\n   LCL = mean - qt(1 - (0.05 / 2), n - 1) * stderr,\n   UCL = mean + qt(1 - (0.05 / 2), n - 1) * stderr,\n   median = median(ageAtDx, na.rm = TRUE),\n   min = min(ageAtDx, na.rm = TRUE), \n   max = max(ageAtDx, na.rm = TRUE),\n   IQR = IQR(ageAtDx, na.rm = TRUE),\n   LCLmed = MedianCI(ageAtDx, na.rm = TRUE)[2],\n   UCLmed = MedianCI(ageAtDx, na.rm = TRUE)[3]\n )\n\nDes\n\n# A tibble: 2 × 13\n  isMale     n  mean    sd stderr   LCL   UCL median   min   max   IQR LCLmed\n  &lt;fct&gt;  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 FALSE    455  67.6  14.1  0.661  66.3  68.9     70    18    93    19     68\n2 TRUE     289  67.1  15.1  0.886  65.3  68.8     70    20    94    18     68\n# ℹ 1 more variable: UCLmed &lt;dbl&gt;\n\n\n\nn: The number of observations for each gender.\nmean: The mean age when diabetes is diagnosed for each gender.\nsd: The standard deviation of each gender.\nstderr: The standard error of each gender level.  That is the standard deviation / sqrt (n).\nLCL, UCL: The upper and lower confidence intervals of the mean.  This values indicates the range at which we can be 95% certain that the true mean falls between the lower and upper values specified for each gender group assuming a normal distribution. \nmedian: The median value for each gender.\nmin, max: The minimum and maximum value for each gender.\nIQR: The interquartile range of each gender. That is the 75th percentile –  25th percentile.\nLCLmed, UCLmed: The 95% confidence interval for the median.\n\nVisual exploration of data\nThe next step is to visualize the data. This can be done using different functions under the ggplot package.\n1) Box plot\n\nggplot(\n Mann_W_U, \n aes(\n   x = isMale, \n   y = ageAtDx, \n   fill = isMale\n )\n) +\n stat_boxplot(\n   geom = \"errorbar\", \n   width = 0.5\n ) +\n geom_boxplot(\n   fill = \"light blue\"\n ) + \n stat_summary(\n   fun.y = mean, \n   geom = \"point\", \n   shape = 10, \n   size = 3.5, \n   color = \"black\"\n ) + \n ggtitle(\n   \"Boxplot of Gender\"\n ) + \n theme_bw() + \n theme(\n   legend.position = \"none\"\n )\n\nWarning: The `fun.y` argument of `stat_summary()` is deprecated as of ggplot2 3.3.0.\nℹ Please use the `fun` argument instead.\n\n\n\n\n\n\n\n\n2) QQ plot\n\nlibrary(conflicted)\nconflict_prefer(\"stat_qq_line\", \"qqplotr\", quiet = TRUE)\n\n\n# Perform QQ plots by group\nQQ_Plot &lt;- \nggplot(\n data = Mann_W_U, \n aes(\n   sample = ageAtDx, \n   color = isMale, \n   fill = isMale\n )\n) +\n stat_qq_band(\n   alpha = 0.5, \n   conf = 0.95, \n   qtype = 1, \n   bandType = \"boot\"\n ) +\n stat_qq_line(\n   identity = TRUE\n ) +\n stat_qq_point(\n   col = \"black\"\n ) +\n facet_wrap(\n   ~ isMale, scales = \"free\"\n ) +\n labs(\n   x = \"Theoretical Quantiles\", \n   y = \"Sample Quantiles\"\n ) + theme_bw()\n\nQQ_Plot\n\n\n\n\n\n\n\n\nstat_qq_line: Draws a reference line based on the data quantiles.\n\nStat_qq_band: Draws confidence bands based on three methods; “pointwise”/“boot”,“Ks” and “ts”.\n\n\"pointwise\" constructs simultaneous confidence bands based on the normal distribution;\n\"boot\" creates pointwise confidence bands based on a parametric boostrap;\n\"ks\" constructs simultaneous confidence bands based on an inversion of the Kolmogorov-Smirnov test;\n\"ts\" constructs tail-sensitive confidence bandsaldor-noiman2013?\n\n\n\nStat_qq_Point: It is a modified version of ggplot: : stat_qq with some parameters adjustments and a new option to detrend the points.\n3) Histogram\nA histogram is the most commonly used graph to show frequency distributions.\n\nHist &lt;- \n ggplot(\nMann_W_U,\naes(\n  x = ageAtDx,\n  fill = isMale\n)\n  ) +\ngeom_histogram() +\nfacet_wrap(~ isMale) \n\nHist\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n3b) Density curve in Histogram\n\n\nA density curve gives us a good idea of the “shape” of a distribution, including whether or not a distribution has one or more “peaks” of frequently occurring values and whether or not the distribution is skewed to the left or the right.zach2020?\n\nggplot(\n  Mann_W_U, \n  aes(\n    x = ageAtDx,\n    fill = isMale\n  )\n) + \n  geom_density() +\n  labs(\n    x = \"Age When diabetes is diagnosed\",\n    y = \"Density\",\n    fill = \"Gender\",\n    title = \"A Density Plot of Age when diabetes is diagnosed\",\n    caption = \"Data Source: HEPESE Wave 8 (ICPSR 36578)\"\n  ) +\n  facet_wrap(~isMale)\n\n\n\n\n\n\n\nThe density curve provided us idea that our data do not have bell shaped distribution and it is slightly skewed towards left.\n4) Statistical test for normality\n\n   Mann_W_U %&gt;%\n group_by(\n   isMale\n ) %&gt;%\n summarise(\n   `W Stat` = shapiro.test(ageAtDx)$statistic,\n    p.value = shapiro.test(ageAtDx)$p.value,\n   options(scipen = 999)\n )\n\n# A tibble: 2 × 4\n  isMale `W Stat`  p.value `options(scipen = 999)`\n  &lt;fct&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;named list&gt;           \n1 FALSE     0.959 6.50e-10 &lt;dbl [1]&gt;              \n2 TRUE      0.937 9.99e-10 &lt;dbl [1]&gt;              \n\n\nInterpretation\nFrom the above table, we see that the value of the Shapiro-Wilk Test is 0.0006 and 0.000002 which are both less than 0.05, therefore we have enough evidence to reject the null hypothesis and confirm that the data significantly deviate from a normal distribution.\n\n13.2.4 Mann Whitney U Test\n\nresult &lt;- wilcox.test(\n  ageAtDx ~ isMale, \n  data = Mann_W_U, \n  na.rm = TRUE, \n  # paired = FALSE, \n  exact = FALSE, \n  conf.int = TRUE\n)\n\ntest_statistics &lt;- result$statistic\n\np_values &lt;- result$p.value\n\nmethod_used &lt;- result$method\n\nresult_df &lt;- \n  data.frame(\n    Test_Statistic = test_statistics,\n    P_Value = p_values,\n    Method = method_used\n  )\n\ntbl &lt;- \n  tbl_df(\n    data = result_df\n  ) \n\nWarning: `tbl_df()` was deprecated in dplyr 1.0.0.\nℹ Please use `tibble::as_tibble()` instead.\n\ntbl\n\n# A tibble: 1 × 3\n  Test_Statistic P_Value Method                                           \n           &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                                            \n1          66178   0.880 Wilcoxon rank sum test with continuity correction",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Mann-Whittney-U Test Example</span>"
    ]
  },
  {
    "objectID": "lessons_original/03_two_sample_mann_whitney.html#results",
    "href": "lessons_original/03_two_sample_mann_whitney.html#results",
    "title": "\n13  Mann-Whittney-U Test Example\n",
    "section": "\n13.3 Results",
    "text": "13.3 Results\nThe mean age at which diabetes is diagnosed is not significantly different in males (69 years old) and females (66 years old). The Mann-Whitney U-Test showed that this difference is not statistically significant at 0.05 level of significance because statistical p value (p=.155) is greater than critical value (p=0.05). The test statistic is W = 5040.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Mann-Whittney-U Test Example</span>"
    ]
  },
  {
    "objectID": "lessons_original/03_two_sample_mann_whitney.html#conclusion",
    "href": "lessons_original/03_two_sample_mann_whitney.html#conclusion",
    "title": "\n13  Mann-Whittney-U Test Example\n",
    "section": "\n13.4 Conclusion",
    "text": "13.4 Conclusion\nFrom the above result, we can conclude that gender does not play a significant role in the age at which one is diagnosed with diabetes. Diabetes is the 8th leading cause of death and disability in the US, and 1 in 5 US adults are currently unaware of their diabetes condition. This urges the need for increased policy efforts towards timely diabetes testing and diagnosing. Although previous research has suggested that there are gender based differences in diabetes related severity of inquiries,kautzky-willer2016? Our findings suggest that this difference is not due to age and can be due to other gender based differences such as willingness to seek medical care, underlying health issues etc. We found that there is no need for gender-based approaches to interventions aimed at increasing diabetes surveillance, and efforts should focus on targeting the population as a whole.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Mann-Whittney-U Test Example</span>"
    ]
  },
  {
    "objectID": "lessons_original/03_two_sample_mann_whitney.html#references",
    "href": "lessons_original/03_two_sample_mann_whitney.html#references",
    "title": "\n13  Mann-Whittney-U Test Example\n",
    "section": "References",
    "text": "References",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Mann-Whittney-U Test Example</span>"
    ]
  },
  {
    "objectID": "lessons_original/03_two_sample_bootstrap_conf_int.html",
    "href": "lessons_original/03_two_sample_bootstrap_conf_int.html",
    "title": "14  Bootstrap Confidence Intervals",
    "section": "",
    "text": "14.1 Bootstrap\nWhat is bootstrapping?\nBootstrapping is a technique from Efron (1979) that is built on a simple idea: if the data we have is a sample from a population, why don’t we sample from our own data to make more samples? Now, because we don’t have access to any new data, we’re going to take samples of our data set with replacement.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Bootstrap Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "lessons_original/03_two_sample_bootstrap_conf_int.html#bootstrap",
    "href": "lessons_original/03_two_sample_bootstrap_conf_int.html#bootstrap",
    "title": "14  Bootstrap Confidence Intervals",
    "section": "",
    "text": "14.1.1 When to use bootstrapping?\nThe purpose of bootstrapping is to increase the sample size for our analysis when the sample we have been given is small.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Bootstrap Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "lessons_original/03_two_sample_bootstrap_conf_int.html#packages",
    "href": "lessons_original/03_two_sample_bootstrap_conf_int.html#packages",
    "title": "14  Bootstrap Confidence Intervals",
    "section": "14.2 Packages",
    "text": "14.2 Packages\npalmerpenguins - The dataset we will using in this Lesson.\ntidyverse - includes the packages to be used for data wrangling.\nboot - The R package boot generates bootstrap samples in R.\n\n# install.packages(\"palmerpenguins\")\n# install.packages(\"boot\")\n\nlibrary(palmerpenguins)\nlibrary(boot)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Bootstrap Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "lessons_original/03_two_sample_bootstrap_conf_int.html#penguins",
    "href": "lessons_original/03_two_sample_bootstrap_conf_int.html#penguins",
    "title": "14  Bootstrap Confidence Intervals",
    "section": "14.3 Penguins",
    "text": "14.3 Penguins\nThis Data was collected and made available by Dr. Kristen Gorman and the Palmer Station, Antarctica. The palmerpenguins package contains two datasets. One is called penguins, and is a simplified version of the raw data.\nPenguins includes measurements for penguin species, island in Palmer Archipelago, size (flipper length, body mass, bill dimensions), and sex. The full dataset contains A tibble with 344 rows and 8 variables(Horst, Hill, and Gorman 2020).\n\nResearch Question: What is the difference in flipper length of the Adelie penguin from two different Islands?\n\n\n# load the data\ndata(penguins)\n\n# see the structure of the data\nstr(penguins)\n\ntibble [344 × 8] (S3: tbl_df/tbl/data.frame)\n $ species          : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ island           : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ bill_length_mm   : num [1:344] 39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ...\n $ bill_depth_mm    : num [1:344] 18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ...\n $ flipper_length_mm: int [1:344] 181 186 195 NA 193 190 181 195 193 190 ...\n $ body_mass_g      : int [1:344] 3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ...\n $ sex              : Factor w/ 2 levels \"female\",\"male\": 2 1 1 NA 1 2 1 2 NA NA ...\n $ year             : int [1:344] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ...\n\n\nFirst we created empty vectors to list and store data points.\nOur for loop will organize the data for two of the islands where Adelie Penguins reside - Torgersen and Biscoe\nFor simplicity, we excluded the island of Dream because their population size was much larger compared to Torgersen and Biscoe populations.\n\n\nCode\nnew_penguins_df &lt;- \n  filter(penguins, species == \"Adelie\", island != \"Dream\") %&gt;% \n  select(species, island, flipper_length_mm) %&gt;% \n  arrange(island, .by_group = TRUE) %&gt;% \n  drop_na()\n\nstr(new_penguins_df)\n\n\ntibble [95 × 3] (S3: tbl_df/tbl/data.frame)\n $ species          : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ island           : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ flipper_length_mm: int [1:95] 174 180 189 185 180 187 183 187 172 180 ...\n\n\n\n14.3.1 Distribution\nBoxplots and histograms will be useful to understand the distribution of the data.\nOur data is not normal based on the distribution.\n\n\nCode\n# check the boxplot of the data\nboxplot(\n  new_penguins_df$flipper_length_mm ~ new_penguins_df$island, las = 1, \n  ylab = \"Flipper Length (mm)\",\n  xlab = \"Island\",\n  main = \"Flipper Length by Island\"\n)\n\n\n\n\n\n\n\n\n\nCode\n# check the histogram of the data\nhist(\n  x = new_penguins_df$flipper_length_mm,\n  main = \"Distribution of Flipper Length (mm)\",\n  xlab = \"Flipper Length\"\n)\n\n\n\n\n\n\n\n\n\n\n\n14.3.2 Bootstrapping Test\nWe need the difference in means in order to conduct our permutation test. We will test whether the difference is significant so that we can reject the null. This indicates that there is a different in flipper length among the same species that come from different islands.\n\n\nCode\n# set a seed so that our random results can be replicated by other people:\nset.seed(20150516)\n\n# take a random re-sample of the data that is the *same size*\nN &lt;- length(new_penguins_df$flipper_length_mm)\n\n# a random sample:\nsample(new_penguins_df$flipper_length_mm, size = N, replace = TRUE)\n\n\n [1] 184 192 198 195 195 176 188 183 184 193 199 198 184 190 198 195 195 199 193\n[20] 197 198 189 197 188 189 199 200 190 183 198 194 190 191 196 189 195 198 197\n[39] 191 184 198 180 195 186 193 193 191 195 190 198 189 181 197 196 182 200 188\n[58] 184 202 189 197 186 181 195 181 191 185 193 196 185 192 199 186 196 180 190\n[77] 190 195 197 193 191 181 195 190 186 189 192 187 190 195 195 182 172 194 181\n\n\nCode\n# number of bootstrap samples\nB_int &lt;- 10000\n\n# create a list of these thousands of samples \nbootstrapSamples_ls &lt;- map(\n  .x = 1:B_int,\n  .f = ~{\n    sample(new_penguins_df$flipper_length_mm, size = N, replace = TRUE)\n  }\n)\n\n# subset of the random samples \nbootstrapSamples_ls[1:3]\n\n\n[[1]]\n [1] 183 190 189 188 181 198 181 172 187 189 189 193 180 197 191 190 196 191 195\n[20] 181 193 190 190 186 188 195 190 197 198 190 180 198 194 188 195 191 203 199\n[39] 190 189 195 186 189 199 202 197 189 190 194 190 181 190 190 181 186 196 174\n[58] 185 174 202 191 184 181 184 193 190 190 190 191 196 189 195 195 198 193 190\n[77] 197 184 186 188 193 190 191 195 198 180 191 185 189 192 183 192 199 186 195\n\n[[2]]\n [1] 187 194 187 189 184 188 187 187 184 197 193 191 187 189 190 172 187 186 180\n[20] 193 191 195 195 180 184 189 197 191 187 186 186 187 184 188 190 193 198 190\n[39] 195 198 184 197 195 195 195 198 194 191 198 197 198 186 194 195 189 186 181\n[58] 180 191 180 191 193 196 191 202 191 187 181 199 172 181 191 195 195 194 198\n[77] 191 191 190 192 190 199 195 193 195 197 188 181 190 185 186 191 174 193 195\n\n[[3]]\n [1] 191 196 203 195 185 195 193 186 186 202 186 203 187 180 185 186 192 202 186\n[20] 192 200 195 184 185 195 193 199 190 189 185 181 181 188 197 181 190 188 185\n[39] 187 184 184 195 199 186 200 186 192 195 190 182 189 191 203 193 195 191 191\n[58] 199 195 198 187 191 195 190 190 187 189 192 186 199 193 190 187 181 190 191\n[77] 190 190 183 193 190 197 181 190 187 198 187 190 200 184 190 184 186 191 193\n\n\n\n\n14.3.3 Building Confidence Intervals for Various Statistics: Example 1\n\n# The Sample Mean\nbootMeans_num &lt;-\n  bootstrapSamples_ls %&gt;%\n  # the map_dbl() function takes in a list and returns an atomic vector of type\n  #   double (numeric)\n  map_dbl(mean)\n\n# a normally distributed histogram using the samples from bootstrapping\nhist(bootMeans_num)\n\n\n\n\n\n\n\n# 95% confidence interval?\nquantile(bootMeans_num, probs = c(0.025, 0.975))\n\n    2.5%    97.5% \n188.7682 191.3684 \n\n\n\n\n14.3.4 Building Confidence Intervals for Various Statistics: Example 2\nSource: https://www.geeksforgeeks.org/bootstrap-confidence-interval-with-r-programming/\n\n# Custom function to find correlation between the bill length and depth \ncorr.fun &lt;- function(data, idx) {\n  \n# vector of indices that the boot function uses\n  df &lt;- data[idx, ]\n\n# Find the spearman correlation between\n# the 3rd (length) and 5th (depth) columns of dataset\n  cor(df[, 3], df[, 4], method = 'spearman')\n}\n\n# Setting the seed for reproducability of results\nset.seed(42)\n\n# Calling the boot function with the dataset\nbootstrap &lt;- boot(iris, corr.fun, R = 1000)\n\n# Display the result of boot function\nbootstrap\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = iris, statistic = corr.fun, R = 1000)\n\n\nBootstrap Statistics :\n     original       bias    std. error\nt1* 0.9376668 -0.002717295 0.009436212\n\n# Plot the bootstrap sampling distribution using ggplot\nplot(bootstrap)\n\n\n\n\n\n\n\n# Function to find the bootstrap CI\nboot.ci(\n  boot.out = bootstrap,\n    type = \"perc\"\n)\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 1000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = bootstrap, type = \"perc\")\n\nIntervals : \nLevel     Percentile     \n95%   ( 0.9142,  0.9519 )  \nCalculations and Intervals on Original Scale",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Bootstrap Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "lessons_original/03_two_sample_bootstrap_conf_int.html#conclusion",
    "href": "lessons_original/03_two_sample_bootstrap_conf_int.html#conclusion",
    "title": "14  Bootstrap Confidence Intervals",
    "section": "14.4 Conclusion",
    "text": "14.4 Conclusion\nThe bootstrapping method is useful when working with data with relatively small samples in order to increase the sample size and normality of the data.\n\n\n\n\nHorst, Allison Marie, Alison Presmanes Hill, and Kristen B Gorman. 2020. Palmerpenguins: Palmer Archipelago (Antarctica) Penguin Data. https://doi.org/10.5281/zenodo.3960218.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Bootstrap Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "lessons_original/03_two_sample_chi_sq_independence.html",
    "href": "lessons_original/03_two_sample_chi_sq_independence.html",
    "title": "15  Chi-Squared Independence",
    "section": "",
    "text": "15.1 Introduction\nThe Chi-square test of independence (also known as the Pearson Chi-square test, or simply the Chi-square) is one of the most useful statistics for testing hypotheses when the variables are nominal. It is a non-parametric tool that does not require equality of variances among the study groups or homoscedasticity in the data.(mchugh2013?)\nBeing a non-parametric test tool, Chi-square test can be used when any one of the following conditions pertains to the data:(mchugh2013?)",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Chi-Squared Independence</span>"
    ]
  },
  {
    "objectID": "lessons_original/03_two_sample_chi_sq_independence.html#introduction",
    "href": "lessons_original/03_two_sample_chi_sq_independence.html#introduction",
    "title": "15  Chi-Squared Independence",
    "section": "15.2 Assumptions(mchugh2013?)",
    "text": "Variables are nominal or ordinal.\nThe frequency count is &gt;5 for more than 80% of the cells in a contingency table.\nThe sample sizes of the study groups may be of equal size or unequal size but samples do not have equal variance.\nThe original data were measured at an interval or ratio level, but violate one of the following assumptions of a parametric test:\n\nThe distribution of the data was seriously skewed or kurtotic.\nThe data violate the assumptions of equal variance or homoscedasticity.\nFor any reasons , the continuous data were collapsed into a small number of categories, and thus the data are no longer interval or ratio.(miller1982?)\n\n15.2 Assumptions(mchugh2013?)\n\nThe data in the cells should be frequencies, or counts of cases rather than % or some other transformation of the data.\nThe levels (or categories) of the variables are mutually exclusive.\nEach subject may contribute data to one and only one cell in the χ2. If, for example, the same subjects are tested over time such that the comparisons are of the same subjects at Time 1, Time 2, Time 3, etc., then χ2 may not be used.\nThe study groups must be independent. This means that a different test must be used if the two groups are related. For example, a different test must be used if the researcher’s data consists of paired samples, such as in studies in which a parent is paired with his or her child.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Chi-Squared Independence</span>"
    ]
  },
  {
    "objectID": "lessons_original/03_two_sample_chi_sq_independence.html#hypotheses",
    "href": "lessons_original/03_two_sample_chi_sq_independence.html#hypotheses",
    "title": "15  Chi-Squared Independence",
    "section": "15.3 Hypotheses",
    "text": "15.3 Hypotheses\nNull Hypothesis (H0): Outcome variable is independent of type of exposure variables. There is no significant difference in the association of group A/B with outcome variable.\nAlternate Hypothesis: (H1) Outcome variable varies significantly depending upon the type of exposure variable. There is a significant difference in the association of group A/B with outcome variable.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Chi-Squared Independence</span>"
    ]
  },
  {
    "objectID": "lessons_original/03_two_sample_chi_sq_independence.html#chi-squared-independence-test-equation",
    "href": "lessons_original/03_two_sample_chi_sq_independence.html#chi-squared-independence-test-equation",
    "title": "15  Chi-Squared Independence",
    "section": "15.4 Chi-Squared Independence Test Equation",
    "text": "15.4 Chi-Squared Independence Test Equation\n\\(\\chi^2 = \\sum \\frac{{(O_{ij} - E_{ij})^2}}{{E_{ij}}}\\)\nWhere:\n\n\\(\\chi^2\\) The Chi-Squared test statistic.\n\\(\\sum\\chi^2\\) Formula instructions to sum all the cell Chi-square values.\n\\(O_{ij}\\) Observed (the actual frequency in each cell (i, j) of the contingency table.\n\\(E_{ij}\\) Expected frequency in cell (i, j) calculated below.\n\\(\\chi^2{i-j}\\) i-j is the correct notation to represent all the cells, from the first cell (i) to the last cell(j).\n\nCalculating Expected Value\n\\(E = \\frac{M{r} * M{c}}n\\)\nWhere:\n\n\\(E\\) represents the cell expected value,\n\\(M{r}\\) represents the row marginal for that cell,\n\\(M{c}\\) represents the column marginal for that cell, and\n\\(n\\) represents the total sample size.\n\n\n15.4.1 Formula Description\n\nThe first step in calculating a χ2 is to calculate the sum of each row, and the sum of each column. These sums are called the “marginals” and there are row marginal values and column marginal values. \nThe second step is to calculate the expected values for each cell. In the Chi-square statistic, the “expected” values represent an estimate of how the cases would be distributed if there were no effect of exposure variables.\nThen third step is to compute the \\(\\chi^2\\) with above formula.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Chi-Squared Independence</span>"
    ]
  },
  {
    "objectID": "lessons_original/03_two_sample_chi_sq_independence.html#performing-chi-squared-independence-test-in-r",
    "href": "lessons_original/03_two_sample_chi_sq_independence.html#performing-chi-squared-independence-test-in-r",
    "title": "15  Chi-Squared Independence",
    "section": "15.5 Performing Chi-Squared Independence Test in R",
    "text": "15.5 Performing Chi-Squared Independence Test in R\nThe first step is to load the required packages that will allow us to conduct the test statistic.\n\n# install.package(\"openintro\")\n# install.package(\"gtsummary\")\n# install.packages(\"rstatix\")\n# install.packages(\"vcd\")\n# install.package(\"tidyverse\")\n\n\nlibrary(openintro)  # for data\n\nLoading required package: airports\n\n\nLoading required package: cherryblossom\n\n\nLoading required package: usdata\n\nlibrary(gtsummary)  # for tables\n\n#BlackLivesMatter\n\nlibrary(vcd)        # for mosaic plot\n\nLoading required package: grid\n\nlibrary(rstatix)    # for post hoc tests\n\n\nAttaching package: 'rstatix'\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\nlibrary(tidyverse)  # for data wrangling and visualization\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks rstatix::filter(), stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n15.5.1 Data Source\nThe openintro package contains data sets used in open-source textbooks such as Introduction to Modern Statistics (1st Ed). (mineçetinkaya-rundel2023?) It is often used for teaching purposes and create examples for how to run various test statistics and functions using R. This package can be installed using the install.packages(\"openintro\") feature. You can also find more information about this package here.\nFor the purposes of this presentations we will be using the diabetes2 dataset found within this package.\nIn the data there are 699 diabetes patients. Each of the 699 patients in the experiment were randomized to one of the following treatments: (1) continued treatment with metformin (coded as met), (2) formin combined with rosiglitazone (coded as rosi), or or (3) a lifestyle-intervention program (coded as lifestyle).Three treatments were compared to test their relative efficacy (effectiveness) in treating Type 2 Diabetes in patients aged 10-17 who were being treated with metformin. The primary outcome was lack of glycemic control (or not); lacking glycemic control means the patient still needed insulin, which is not the preferred outcome for a patient.(todaystudygroup2012?)\n\nstr(diabetes2)\n\ntibble [699 × 2] (S3: tbl_df/tbl/data.frame)\n $ treatment: Factor w/ 3 levels \"lifestyle\",\"met\",..: 2 3 3 1 2 1 1 3 3 2 ...\n $ outcome  : Factor w/ 2 levels \"failure\",\"success\": 2 1 2 2 2 2 2 2 2 1 ...\n\nprint(diabetes2)\n\n# A tibble: 699 × 2\n   treatment outcome\n * &lt;fct&gt;     &lt;fct&gt;  \n 1 met       success\n 2 rosi      failure\n 3 rosi      success\n 4 lifestyle success\n 5 met       success\n 6 lifestyle success\n 7 lifestyle success\n 8 rosi      success\n 9 rosi      success\n10 met       failure\n# ℹ 689 more rows\n\n\n\n\n15.5.2 Contingency Tables\nThe first step in a Chi-Squared Independence Test involves creating a contingency table that is used to calculate the expected frequencies for each variable. This will help us summarize the data and show the distribution of the variables. This is done using the table() function as seen in the code below.\n\n# Create the table\ndiabetes_table &lt;- table(\n  diabetes2$outcome,\n  diabetes2$treatment\n)\n\nprint(diabetes_table)\n\n         \n          lifestyle met rosi\n  failure       109 120   90\n  success       125 112  143\n\n\n\n\n15.5.3 Mosaic Plots\nYou can also use a mosaic plot to visualize the data better. Our data is in interger format so we first need to reformat it into factor form. This can we done with the code below.\n\n#reformat treatment\ndiabetes2$treatment &lt;- \n  as.factor(diabetes2$treatment)\n\n#print\nhead(diabetes2$treatment)\n\n[1] met       rosi      rosi      lifestyle met       lifestyle\nLevels: lifestyle met rosi\n\n#recode treatment\ndiabetes2$treatment &lt;- \n  recode_factor(\n    diabetes2$treatment,\n            \"lifestyle\" = \"Lifestyle\",\n            \"met\" = \"Metform\",\n            \"rosi\" = \"Rosiglitazone Plus Metformin\"\n)\n\n#print\nhead(diabetes2$treatment)\n\n[1] Metform                      Rosiglitazone Plus Metformin\n[3] Rosiglitazone Plus Metformin Lifestyle                   \n[5] Metform                      Lifestyle                   \nLevels: Lifestyle Metform Rosiglitazone Plus Metformin\n\n#reformat outcome\ndiabetes2$outcome &lt;- \n  as.factor(diabetes2$outcome)\n\n#print\nhead(diabetes2$outcome)\n\n[1] success failure success success success success\nLevels: failure success\n\n#recode\ndiabetes2$outcome &lt;- \n  recode_factor(\n    diabetes2$outcome,\n            \"failure\" = \"Failure\",\n            \"success\" = \"Success\"\n)\n\n#print\nhead(diabetes2$outcome)\n\n[1] Success Failure Success Success Success Success\nLevels: Failure Success\n\n\nNext, we can create the mosaic plot using the mosaic() function.\n\n# Creating the mosaic plot\nmosaic(\n  ~ treatment + outcome, \n       data = diabetes2,\n          highlighting = \"outcome\", \n          highlighting_fill = c(\"lightgrey\", \"black\"),\n          main = \"Outcome of Interventions for Type 2 Diabetes\",\n          gp_varnames = gpar(fontsize = 14, fontface = 2),\n          gp_labels = gpar(fontsize = 10)\n)\n\n\n\n\n\n\n\n\n\n\n15.5.4 Running the Chi-Squared Test\nThe next step is to run our actual test statistic. This is done using the chisq.test() function as seen in the code below.The correct argument is used to indicate whether to apply continuity correction when computing the test. We are setting this to TRUE since we are dealing with a 2x2 contingency table where we have two categorical variables, each with two levels.\n\nchi_sq_result &lt;- chisq.test(diabetes_table, \n                           correct = TRUE)\n\nprint(chi_sq_result)\n\n\n    Pearson's Chi-squared test\n\ndata:  diabetes_table\nX-squared = 8.1645, df = 2, p-value = 0.01687\n\n\n\n15.5.4.1 Tabulating the chisq output in a publishable format using gt_summary\n\ntable1 &lt;-   \n  tbl_summary(\n    diabetes2,\n    by = treatment\n) %&gt;% \n  add_p() %&gt;%\n  modify_caption(\"Results of Chi Square Test\") %&gt;%\n  bold_labels()\n\n\ntable1\n\n\n\n\n\n\nResults of Chi Square Test\n\n\n\n\n\n\n\n\n\nCharacteristic\nLifestyle, N = 2341\nMetform, N = 2321\nRosiglitazone Plus Metformin, N = 2331\np-value2\n\n\n\n\noutcome\n\n\n\n\n\n\n0.017\n\n\n    Failure\n109 (47%)\n120 (52%)\n90 (39%)\n\n\n\n\n    Success\n125 (53%)\n112 (48%)\n143 (61%)\n\n\n\n\n\n1 n (%)\n\n\n2 Pearson’s Chi-squared test\n\n\n\n\n\n\n\n\n\n\n\n\n15.5.5 Test Options\nThe Chi-Squared Independence test statistic has various options in R. A brief description of those options is summarized in the table below.\n\n\n\n\n\n\n\n\nOption\nDescription\nExample\n\n\n\n\nx\nA numeric vector or matrix. x and y can also both be factors.\nx &lt;- matrix(c(10, 20, 30, 40), nrow = 2)\n\n\ncorrect\nA logical indicating whether to apply continuity correction – This is done when the expected frequencies in the contingency table are small (&lt;5).\ncorrect &lt;- TRUE\n\n\np\nA vector of probabilities of the same length as x. An error is given if any entry of p is negative.\np &lt;- c(0.4, 0.6)\n\n\nrescale.p\nA logical scalar; if TRUE then p is rescaled (if necessary) to sum to 1. If rescale.p is FALSE, and p does not sum to 1, an error is given.\nrescale.p &lt;- FALSE\n\n\nsimulate.p.value\nA logical indicating whether to compute p-values by Monte Carlo simulation.\nsimulate.p.value &lt;- FALSE\n\n\nB\nAn integer specifying the number of replicates used in the Monte Carlo test.*\nB &lt;- 1000\n\n\n\n*The Monte Carlo test is a technique that involves simulation to estimate the p-value or test statistic for hypothesis testing. This is used when using complex models and exact p-values cannot be calculated or when the distribution assumptions are violated. (christianp.robert2010?)",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Chi-Squared Independence</span>"
    ]
  },
  {
    "objectID": "lessons_original/03_two_sample_chi_sq_independence.html#interpretation",
    "href": "lessons_original/03_two_sample_chi_sq_independence.html#interpretation",
    "title": "15  Chi-Squared Independence",
    "section": "15.6 Interpretation",
    "text": "15.6 Interpretation\nOur results give us a chi-squared test statistic of 8.1645 with a p-value of 0.017. Since p value is smaller than critical p value (0.05), we have enough evidence to reject the null hypothesis and conclude that there is a strong association between the type of treatment on Type 2 diabetes. However, we dont’t know which treatment option is significantly different so we are going to do a post hoc test in below code chunk.\n\n15.6.1 Post hoc test\n\npost_hoc_test &lt;- pairwise_prop_test(diabetes_table)\n\npost_hoc_test\n\n# A tibble: 3 × 5\n  group1    group2       p  p.adj p.adj.signif\n* &lt;chr&gt;     &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;       \n1 lifestyle met    0.309   0.309  ns          \n2 lifestyle rosi   0.1     0.2    ns          \n3 met       rosi   0.00606 0.0182 *",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Chi-Squared Independence</span>"
    ]
  },
  {
    "objectID": "lessons_original/03_two_sample_chi_sq_independence.html#conclusion",
    "href": "lessons_original/03_two_sample_chi_sq_independence.html#conclusion",
    "title": "15  Chi-Squared Independence",
    "section": "15.7 Conclusion",
    "text": "15.7 Conclusion\nFrom our post hoc test result we see that there is a statistically significant difference between Met and Rosi. Given our data, we can conclude that Rosi is the better treatment option for the population tested.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Chi-Squared Independence</span>"
    ]
  },
  {
    "objectID": "04_header_anova-and-regression.html",
    "href": "04_header_anova-and-regression.html",
    "title": "ANOVA and Regression",
    "section": "",
    "text": "Text Outline\nThis text covers the basics of:",
    "crumbs": [
      "ANOVA and Regression"
    ]
  },
  {
    "objectID": "04_header_anova-and-regression.html#text-outline",
    "href": "04_header_anova-and-regression.html#text-outline",
    "title": "ANOVA and Regression",
    "section": "",
    "text": "Exploring data\nOne-sample tests\nTwo-sample tests\nANOVA and linear regression\nGeneralized linear models\nSome special topics\nMaybe a bit about power calculations…",
    "crumbs": [
      "ANOVA and Regression"
    ]
  },
  {
    "objectID": "04_header_anova-and-regression.html#part-outline",
    "href": "04_header_anova-and-regression.html#part-outline",
    "title": "ANOVA and Regression",
    "section": "Part Outline",
    "text": "Part Outline\nThis part of the text contains the chapters on various ways to perform ANOVA and linear regression:\n\nOne-Way ANOVA\nTwo-way ANOVA\nWelch’s ANOVA\nKruskal-Wallace Test\nTukey HSD Post-Hoc Test\nRepeated Measures ANOVA\nRandom Intercept Models\nCorrelation Matrices and Covariances\nMultiple Regression (linear)\nPolynomial regression",
    "crumbs": [
      "ANOVA and Regression"
    ]
  },
  {
    "objectID": "lessons_original/04_anova_two_way.html",
    "href": "lessons_original/04_anova_two_way.html",
    "title": "\n16  Two-Way ANOVA\n",
    "section": "",
    "text": "16.1 Two-way ANOVA\nA two-way ANOVA is used to estimate how the mean of a quantitative variable changes according to the levels of two categorical variables.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Two-Way ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_anova_two_way.html#two-way-anova",
    "href": "lessons_original/04_anova_two_way.html#two-way-anova",
    "title": "\n16  Two-Way ANOVA\n",
    "section": "",
    "text": "It is used to determine how independent (grouping) variables, in combination, affect a dependent (response) variable.\nGrouping variables are also called factors. Levels are the categories of each component.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Two-Way ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_anova_two_way.html#how-does-it-work",
    "href": "lessons_original/04_anova_two_way.html#how-does-it-work",
    "title": "\n16  Two-Way ANOVA\n",
    "section": "\n16.2 How does it work?",
    "text": "16.2 How does it work?\nThe F test is used in ANOVA to determine statistical significance. The F test compares the variance in each group mean to the overall variance in the dependent variable because it is a group-wise comparison test.\nWhen the variance is higher between groups than within the groups, the F test value will be greater, and therefore a higher likelihood that the difference observed is real and not due to chance.\nThere are three null hypotheses tested while doing a two-way ANOVA:\n\nThere is no difference between the group means at any level of the first independent variable.\nThere is no difference between the group means at any level of the second independent variable.\nThe effect of one independent variable does not depend on the effect of the other independent variable (also viewed as no interaction effect),\n\n\n16.2.1 Assumptions needed\nThere are certain assumptions that must be considered before using a two-way ANOVA.\n\nHomogenuity of variance: The variances for each group should be roughly equal.\nIndependence of observations: The observations in each group are independent of each other and the observations within groups were obtained by a random sample.\nNormally distributed: The response variable is approximately normally distributed for each group.\n\nBelow we will see an example on how to test for these assumptions and how to conduct a two-way ANOVA test once we know they have been met.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Two-Way ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_anova_two_way.html#two-way-anova-table",
    "href": "lessons_original/04_anova_two_way.html#two-way-anova-table",
    "title": "\n16  Two-Way ANOVA\n",
    "section": "\n16.3 Two-Way ANOVA Table",
    "text": "16.3 Two-Way ANOVA Table\n\n\n\n\n\n\n\n\n\nSource of Variation\nSum of Squares\nDegrees of freedom\nMean Squares\nF value\n\n\n\nFactor A\n\\(SS_A\\)\n\\(k-1\\)\n\\(MS_A\\)\n\\(F_A\\)\n\n\nFactor B\n\\(SS_B\\)\n\\(l-1\\)\n\\(MS_B\\)\n\\(F_B\\)\n\n\nInteraction AB\n\\(SS_{AB}\\)\n\\((k-1)(l-1)\\)\n\\(MS_{AB}\\)\n\\(F_{AB}\\)\n\n\nError\n\\(SS_E\\)\n\\(kl(m-1)\\)\n\\(MS_E\\)\n\n\n\nTotal\n\\(SS_T\\)\n\\(klm-1\\)\n\n\n\n\n\nWhere\n\n\\[ MS_E := \\frac{SS_E}{kl(m-1)} \\]\n\\[ MS_A := \\frac{SS_A}{k-1} \\text{ and } F_A := \\frac{MS_A}{MS_E} \\]\n\\[ MS_B := \\frac{SS_B}{l-1} \\text{ and } F_B := \\frac{MS_B}{MS_E} \\]\n\\[ MS_{AB} := \\frac{SS_{AB}}{(k-1)(l-1)} \\text{ and } F_{AB} := \\frac{MS_{AB}}{MS_E} \\]\n\nWe explain these components as:\n\n\n\\(SS_A\\): Factor \\(A\\) main effect sums of squares with associated df \\(k-1\\)\n\n\n\\(SS_B\\): Factor \\(B\\) main effect sums of squares, with associated df \\(l-1\\)\n\n\n\\(SS_{AB}\\): interaction sum of squares, with associated df \\((k-1)(l-1)\\)\n\n\n\\(SS_E\\): error sum of squares with associated df \\(kl(m-1)\\)\n\n\n\\(SS_T\\): Total sums of squares, associated with df \\(klm-1\\)\n\n\n\n16.3.1 Example\nFor this example, an agricultural crop yield dataset was sourced from Scribbr.\nThe dataset contains:\n\nType of fertilizer (1,2,3)\nPlanting density (1 = low, 2 = high)\nBlock number in the field (1,2,3,4)\n\nThis two-way ANOVA will examine whether the type of fertilizer and planting density (independent variables) have an effect on the average crop yield (dependent variable).\n\n16.3.1.1 Loading Libraries and Data\n\n# needed to create presentation ready table \nlibrary(gt)\n# needed to create presentation ready table \nlibrary(tidymodels)\n# needed to create AIC table to compare all three models \nlibrary(AICcmodavg)\nlibrary(ggplot2)\nlibrary(tidyverse)\n\ncrop_data_df &lt;- read_csv(\"../data/04_crop_data.csv\")\n\n\n16.3.1.2 Data Exploration\n\n# show first six rows of the dataset\nhead(crop_data_df)\n\n# A tibble: 6 × 4\n  density block fertilizer yield\n    &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;\n1       1     1          1  177.\n2       2     2          1  178.\n3       1     3          1  176.\n4       2     4          1  178.\n5       1     1          1  177.\n6       2     2          1  177.\n\n# overview of summary statistics\nsummary(crop_data_df)\n\n    density        block        fertilizer     yield      \n Min.   :1.0   Min.   :1.00   Min.   :1    Min.   :175.4  \n 1st Qu.:1.0   1st Qu.:1.75   1st Qu.:1    1st Qu.:176.5  \n Median :1.5   Median :2.50   Median :2    Median :177.1  \n Mean   :1.5   Mean   :2.50   Mean   :2    Mean   :177.0  \n 3rd Qu.:2.0   3rd Qu.:3.25   3rd Qu.:3    3rd Qu.:177.4  \n Max.   :2.0   Max.   :4.00   Max.   :3    Max.   :179.1  \n\n\n\n16.3.1.2.1 Boxplots\n\nplot_fertilizer &lt;- \n  ggplot(data = crop_data_df) +\n    aes(\n      x = as.factor(fertilizer),\n      y = yield,\n      color = as.factor(fertilizer) \n    ) +\n  labs(\n    x = \"Fertilizer Type\",\n    y = \"Crop Yield\"\n  ) +\n  geom_boxplot()\n\nplot_fertilizer\n\n\n\n\n\n\nplot_density &lt;- \n  ggplot(data = crop_data_df) +\n    aes(\n      x = as.factor(density),\n      y = yield,\n      color = as.factor(density)\n    ) +\n  labs(\n    x = \"Planting Density\",\n    y = \"Crop Yield\"\n  ) +\n  geom_boxplot()\n\nplot_density\n\n\n\n\n\n\n\n\n16.3.1.3 Null\n\nH01: There is no statistical difference in average yield for any fertilizer type\nH02: There is no difference in average yield between either planting density\nH03: The effect of fertilizer type on average yield is not dependent on the effect of planting density - no interaction effect\n\n16.3.1.4 Alternative\n\nH11: There is a difference in the average yield for fertilizer types\nH12: There is a difference in the average yield based on planting density\nH13: There is an interaction effect between planting density and fertilizer type average yield\n\n16.3.1.5 Performing the two-way ANOVA\nFirst we want to test without interaction between the two independent variables for our first model. The code below is\n\n# converting into a factor for post-hoc assessment \ncrop_data_df$fertilizer &lt;- as.factor(crop_data_df$fertilizer)\ncrop_data_df$density &lt;- as.factor(crop_data_df$density)\n\n# performing the two-way ANOVA\ntwo_way &lt;- \n  aov(yield ~ fertilizer + density, data = crop_data_df)\n\n# creating a tidy table using gt\ntable_1 &lt;- two_way %&gt;% \n  tidy() %&gt;% \n  gt()\n\n# customizing table \ntable_1 |&gt;\n   tab_header(\n      title = \"ANOVA Results\",\n      subtitle = \"Two-way ANOVA for yield\"\n    )\n\n\n\n\n\n\n\nANOVA Results\n\n\nTwo-way ANOVA for yield\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nfertilizer\n2\n6.068047\n3.0340233\n9.073123\n0.0002532992\n\n\ndensity\n1\n5.121681\n5.1216812\n15.316179\n0.0001741418\n\n\nResiduals\n92\n30.764505\n0.3343968\nNA\nNA\n\n\n\n\n\n\n\nWe also want to test a model that shows interaction between the two independent variables.\n\n# performing the two-way ANOVA with interaction \ninteraction &lt;- \n  aov(yield ~ fertilizer * density, data = crop_data_df)\n\n# creating a tidy table using gt\ntable_int &lt;- interaction %&gt;% \n  tidy() %&gt;% \n  gt()\n\n# customizing table \ntable_int |&gt;\n   tab_header(\n      title = \"ANOVA Results\",\n      subtitle = \"Two-way ANOVA for yield with interaction term\"\n    )\n\n\n\n\n\n\n\nANOVA Results\n\n\nTwo-way ANOVA for yield with interaction term\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nfertilizer\n2\n6.0680466\n3.0340233\n9.0010522\n0.0002731890\n\n\ndensity\n1\n5.1216812\n5.1216812\n15.1945174\n0.0001864075\n\n\nfertilizer:density\n2\n0.4278183\n0.2139091\n0.6346053\n0.5325000914\n\n\nResiduals\n90\n30.3366866\n0.3370743\nNA\nNA\n\n\n\n\n\n\n\nThe p-value for both independent variables are less than 0.05, therefore we reject the null hypotheses (H01 and H02).\nThe p value for the interaction term is greater than 0.05, hence we fail to reject the null hypothesis (H03). Hence not much variation can be explained by the interaction term.\n\n16.3.1.6 Blocking Variable - 3rd Model\nThe crops were planted in across various blocks that may differ in other factors such as sunlight, moisture etc. This could possibly lead to confounding. Therefore it is important to control for the effect of differences between blocks by adding the third variable to our tests.\n\n# performing two-way anova with interaction and blocking\nblocking &lt;- \n  aov(yield ~ fertilizer * density + block, data = crop_data_df)\n\n# creating a tidy table using gt \ntable_block &lt;- blocking %&gt;% \n  tidy() %&gt;% \n  gt()\n\n# customizes gt table\ntable_block |&gt;\n   tab_header(\n      title = \"Two- way ANOVA Results\",\n      subtitle = \"with interaction term & blocking variable\"\n    )\n\n\n\n\n\n\n\nTwo- way ANOVA Results\n\n\nwith interaction term & blocking variable\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nfertilizer\n2\n6.0680466\n3.0340233\n9.0459852\n0.0002652607\n\n\ndensity\n1\n5.1216812\n5.1216812\n15.2703680\n0.0001813274\n\n\nblock\n1\n0.4860877\n0.4860877\n1.4492776\n0.2318360383\n\n\nfertilizer:density\n2\n0.4278183\n0.2139091\n0.6377732\n0.5308657318\n\n\nResiduals\n89\n29.8505990\n0.3354000\nNA\nNA\n\n\n\n\n\n\n\nFor the block variable the sum of squares is low and the p value is greater than 0.05. Hence not much information is added to the model. The sum of square for both independent variables also remain unchanged.\n\n16.3.1.7 Determining the Best Fit Model\nThe Akaike information criterion (AIC) can be used to determine the best model. AIC balances the variation explained by the number of parameters used to calculate the information value of each model. The lower the AIC value, the more information explained.\n\nmodel_set &lt;- list(two_way, interaction, blocking)\nmodel_names &lt;- c(\"two_way\", \"interaction\", \"blocking\")\n \ngt_fmt &lt;-\n  aictab(model_set, modnames = model_names) \n\ngt_print &lt;- \n  gt(gt_fmt)\n\ngt_print\n\n\n\n\n\n\nModnames\nK\nAICc\nDelta_AICc\nModelLik\nAICcWt\nLL\nCum.Wt\n\n\n\ntwo_way\n5\n173.8562\n0.000000\n1.0000000\n0.75476252\n-81.59474\n0.7547625\n\n\ninteraction\n7\n177.1178\n3.261693\n0.1957638\n0.14775516\n-80.92256\n0.9025177\n\n\nblocking\n8\n177.9496\n4.093464\n0.1291563\n0.09748232\n-80.14722\n1.0000000\n\n\n\n\n\n\n\nAs shown in this table, the two_way model is the best fit for our crop data analysis.\n\n16.3.2 Checking for homoscedasticity\n\npar(mfrow=c(2,2))\nplot(two_way)\n\n\n\n\n\n\npar(mfrow=c(1,1))\n\nThe output of the residual means shows no large outliers that could possibly skew the data. Hence we can assume equal variances. The Q-Q plot also depicts normality.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Two-Way ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_anova_two_way.html#post-hoc-testing-tukey-hsd",
    "href": "lessons_original/04_anova_two_way.html#post-hoc-testing-tukey-hsd",
    "title": "\n16  Two-Way ANOVA\n",
    "section": "\n16.4 Post-hoc testing (Tukey HSD)",
    "text": "16.4 Post-hoc testing (Tukey HSD)\nIn order to determine how the levels differ from one another the Tukey’s Honestly-Significant-difference test can be used.\n\ntukey_crop &lt;- TukeyHSD(two_way)  \n  \ntukey_crop %&gt;% \n  tidy %&gt;% \n  gt() %&gt;% \n  tab_header(\n      title = \"Tukey Multiple Comparisons of means\",\n      subtitle = \"for fertilizer & density\"\n  )\n\n\n\n\n\n\n\nTukey Multiple Comparisons of means\n\n\nfor fertilizer & density\n\n\nterm\ncontrast\nnull.value\nestimate\nconf.low\nconf.high\nadj.p.value\n\n\n\n\nfertilizer\n2-1\n0\n0.1761687\n-0.16822506\n0.5205625\n0.4452958212\n\n\nfertilizer\n3-1\n0\n0.5991256\n0.25473179\n0.9435194\n0.0002218678\n\n\nfertilizer\n3-2\n0\n0.4229569\n0.07856306\n0.7673506\n0.0119381379\n\n\ndensity\n2-1\n0\n0.4619560\n0.22752045\n0.6963916\n0.0001741423\n\n\n\n\n\n\n\nThis table shows of pairwise differences between each level in the independent variables. Comparisons with p values less than 0.05 are termed significant;\n\nfertilizer types 1 and 3\nfertilizer types 2 and 3\nplanting density groups (binary)\n\n\nplot(tukey_crop, las = 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOnly the fertilizer comparison of type 1 & 2 confidence interval includes 0; no significant statistical difference\n\n\n16.4.1 Summary Chart of Crop yield data\n\nanova_plot &lt;- \n  ggplot(crop_data_df,\n    aes(\n      x = density, \n      y = yield, \n      group = fertilizer, \n      color = fertilizer\n    )\n  ) +\n  geom_point(\n    cex = 1.5, \n    pch = 1.0, \n    position = position_jitter(w = 0.1, h = 0.1)\n  )\n\nanova_plot &lt;- anova_plot + \n  stat_summary(fun.data = 'mean_se', geom = 'errorbar', width = 0.2, color = \"grey50\") +\n  stat_summary(fun.data = 'mean_se', geom = 'pointrange') \n\nanova_plot &lt;- anova_plot +\n  facet_wrap(~ fertilizer)\n\nanova_plot &lt;- anova_plot +\n  theme_classic() +\n  labs(title = \"Crop yield averages based on fertilizer types and planting density\",\n      x = \"Planting density (1 = low density, 2 = high density)\",\n      y = \"Yield Average\")\n\nanova_plot",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Two-Way ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_anova_two_way.html#conclusions",
    "href": "lessons_original/04_anova_two_way.html#conclusions",
    "title": "\n16  Two-Way ANOVA\n",
    "section": "\n16.5 Conclusions",
    "text": "16.5 Conclusions\nThere is a statistically-significant difference in average crop yield by both the fertilizer type and planting density variables with F values of 9.018 (p &lt; 0.001) and 15.316 (p &lt; 0.001) respectively. The interaction between these two terms was not significant.\nThe Tukey post-hoc test showed significant pairwise differences between fertilizer types 1 & 3, and 2 & 3. It also depicted significant differences between the two types of planting densities (low & high).",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Two-Way ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_anova_kruskal_wallis.html",
    "href": "lessons_original/04_anova_kruskal_wallis.html",
    "title": "17  Non-Parametric ANOVA: The Kruskal-Wallis Test",
    "section": "",
    "text": "17.1 Introduction\nThe Kruskal-Wallis test (H-test) is a hypothesis test for multiple independent samples, which is used when the assumptions for a one factor analysis of variance are violated. In other word, it is the non-parametric alternative to the One Way ANOVA. Non-parametric means that the data does not follow normal distribution. It is sometimes called the one-way ANOVA on ranks, as the ranks of the data values are used in the test rather than the actual data points.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Non-Parametric ANOVA: The Kruskal-Wallis Test</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_anova_kruskal_wallis.html#introduction",
    "href": "lessons_original/04_anova_kruskal_wallis.html#introduction",
    "title": "17  Non-Parametric ANOVA: The Kruskal-Wallis Test",
    "section": "",
    "text": "17.1.1 Mathematical Equation\n\\[H = \\frac{{n-1}}{{n}}\\sum^k_{i=1}\\frac{n_i({\\bar{R_i} - E})^2}{{σ^2_R}}\\]\nWhere:\n\n\\(H\\) is the Kruskal-Wallis test statistic,\n\\(n\\) is the total number of observations,\n\\(R_i\\) is the sum of ranks for each group,\n\\(E\\) is the expected value of the sum of ranks under the null hypothesis.\n\\(σ^2_R\\) is the square of standard deviation of Rank sum.\n\nEquation for Expected Rank\n\\[E = \\frac{{n+1}}{{2}}\\]\nWhere:\n-n represents total number of observations.\nEquation for Rank Mean for group i\n\\[R_i = \\frac{{\\sum{R}}}{{n_g}}\\]\nWhere:\n\nR_i represents mean rank for \\(i^{th}\\) group,\n\\(\\sum{R}\\) represents sum of ranks in \\(i^{th}\\) group,\n\\(n_g\\) represents number of observation in \\(i^{th}\\) group.\n\nExample\n\n\n\nAssigning ranks/ E and mean rank calculated/ ready for H calculation\n\n\n\n\n17.1.2 Assumptions\n1. Ordinal or Continuous Response Variable – the response variable should be an ordinal or continuous variable.\n2. Independence – the observations in each group need to be independent of each other.\n3. Sample Size and distribution – each group must have a sample size of 5 or more and the distributions in each group need to have a similar shape but groups does not follow normal distribution.\n\n\n17.1.3 Hypothesis\nThe test determines whether two or more independent groups have same central tendency.\n\nH0: population rank sum average are equal for independent group and therefore come from same population.\nH1: population rank sum average are significantly different for at-least two or more independent group and therefore come from different population.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Non-Parametric ANOVA: The Kruskal-Wallis Test</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_anova_kruskal_wallis.html#running-kruskal-wallis-in-r",
    "href": "lessons_original/04_anova_kruskal_wallis.html#running-kruskal-wallis-in-r",
    "title": "17  Non-Parametric ANOVA: The Kruskal-Wallis Test",
    "section": "17.2 Running Kruskal-Wallis in R",
    "text": "17.2 Running Kruskal-Wallis in R\n\n17.2.1 Packages\n\n# install.packages(\"FSA\") # Houses dunnTest for pair wise comparison\n# install.packages(\"ggpubr\")  # For density plot and for creating and customizing 'ggplot2'- based publication ready plots\n# install.packages(\"ggstatplot\") # Houses gbetweenstats() function that allows building a combination of box and violin plots along with                                        statistical details.\n# install.packages(\"tidyverse\") # For wrangling and tidying the data\n# install.packages(\"MultNonParam\")\n\nlibrary(MultNonParam)\n\nLoading required package: ICSNP\n\n\nLoading required package: mvtnorm\n\n\nLoading required package: ICS\n\nsuppressPackageStartupMessages(library(ggpubr))   \nsuppressPackageStartupMessages(library(ggstatsplot))\nsuppressPackageStartupMessages(library(FSA))       \nsuppressPackageStartupMessages(library(gtsummary))\nsuppressPackageStartupMessages(library(tidyr))     \nsuppressPackageStartupMessages(library(tidyverse)) \n\n\n\n17.2.2 Data\nAs an example we will manually create a data, details of which can be found Here.\nThe data represents antibody production after receiving a vaccine. A hospital administered one of three different vaccines - A, B, or C to 6 individuals per group and measured the antibody presence (\\(\\mu\\)g/mL) in their blood after a chosen time period. The data is as follows: The goal of this exercise will be to determine how the three vaccines performed compared to each other. Essentially, we are looking to determine if the antibody data for each vaccine originates from the same distribution. The sample size is small and normal distribution cannot be assumed. Therefore, we will be conducting the Kruskal-Wallis test.\nNull Hypothesis (H0): The vaccines induce equal amounts of antibody production. (all three groups originate from the same distribution and have the same median)\nAlternative Hypothesis (H1): At least one vaccine induces different amount of antibodies to be produced.(at least one group originates from a different distribution and has a different median)\n\n# Creating dataframe for antibodies produced (in $\\mu$g/mL$) by three different vaccines;\n\nA &lt;- c(1232, 751, 339, 848, 447, 542)\nB &lt;- c(302, 57, 521, 278, 176, 201)\nC &lt;- c(839, 342, 473, 1128, 242, 475)\n\ndf &lt;- data.frame(A, B, C)\n\ndf_tidy &lt;- pivot_longer(\n  data = df,\n  cols = c(\"A\", \"B\", \"C\"),\n  names_to = \"Vaccines\",\n  values_to = \"Antibody\"\n)\n\ndf_tidy_sorted &lt;- \n  df_tidy %&gt;% \n  arrange(Vaccines)\n\ndf_tidy_sorted\n\n# A tibble: 18 × 2\n   Vaccines Antibody\n   &lt;chr&gt;       &lt;dbl&gt;\n 1 A            1232\n 2 A             751\n 3 A             339\n 4 A             848\n 5 A             447\n 6 A             542\n 7 B             302\n 8 B              57\n 9 B             521\n10 B             278\n11 B             176\n12 B             201\n13 C             839\n14 C             342\n15 C             473\n16 C            1128\n17 C             242\n18 C             475\n\nvaccine_efficacy = df_tidy_sorted\n\n\nstr(vaccine_efficacy)\n\ntibble [18 × 2] (S3: tbl_df/tbl/data.frame)\n $ Vaccines: chr [1:18] \"A\" \"A\" \"A\" \"A\" ...\n $ Antibody: num [1:18] 1232 751 339 848 447 ...\n\n\n\nvaccine_efficacy$Vaccines &lt;- as_factor(vaccine_efficacy$Vaccines)\n\nstr(vaccine_efficacy)\n\ntibble [18 × 2] (S3: tbl_df/tbl/data.frame)\n $ Vaccines: Factor w/ 3 levels \"A\",\"B\",\"C\": 1 1 1 1 1 1 2 2 2 2 ...\n $ Antibody: num [1:18] 1232 751 339 848 447 ...\n\n\n\n\n17.2.3 Computing summary statistics by group\nThe first step is to inspect the data and calculate a summary of statistics. This can be done by using the summarise function.\n\ngroup_by(vaccine_efficacy, Vaccines) %&gt;%\n  summarise(\n    count = n(),\n    mean = mean(Antibody, na.rm = TRUE),\n    sd = sd(Antibody, na.rm = TRUE),\n    median = median(Antibody, na.rm = TRUE),\n    IQR = IQR(Antibody, na.rm = TRUE)\n  )\n\n# A tibble: 3 × 6\n  Vaccines count  mean    sd median   IQR\n  &lt;fct&gt;    &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 A            6  693.  325.   646.  353 \n2 B            6  256.  156.   240.  114.\n3 C            6  583.  335.   474   373.\n\n\n\n\n17.2.4 Box Plot\nThe next step will be to visualize the dataset using a box plot. This will allow us to estimate differences in distribution.\n\nvaccine_efficacy %&gt;% \n  ggplot(aes(Vaccines, Antibody)) + \n  geom_boxplot() +\n  ggtitle(\"Vaccine Efficacy\") +\n  xlab(\"Vaccines\") + ylab(\"Antibodies\")\n\n\n\n\n\n\n\n\nBased on the box plot, we see that there is similarity in distribution of A and C while B looks to be different. We can also add the individual data points and connect the boxes to visually see the density distribution and compare with normal distribution for each vaccines.\n\n17.2.4.1 Adding error bars: mean_se\n\nggline(vaccine_efficacy, x = \"Vaccines\", y = \"Antibody\",\n       add = c(\"mean_se\", \"jitter\"),\n       order = c(\"A\", \"B\", \"C\"),\n       ylab = \"Antibody\", xlab = \"Vaccines\")\n\n\n\n\n\n\n\n\n\n\n17.2.4.2 Density plot with overlaid normal plot\nNext, we want to create a density plot to further visualize the data and compare it to what a normal distribution of these data should look like. This can be done by using the ggdensity function as seen below.\n\n# Plot the distribution of the antibodies for vaccine \nggdensity(df$A, fill = \"lightgray\", title = \"Distribution plot of Antibody Production for Vaccine A\") +\n  scale_x_continuous() +\n  xlab(\"A\") +\n  stat_overlay_normal_density(color = \"red\", linetype = \"dashed\")\n\n\n\n\n\n\n\n\n\n# Plot the distribution of the antibodies for vaccine \nggdensity(df$B, fill = \"lightgray\", title = \"Distribution plot of Antibody Production for Vaccine B\") +\n  scale_x_continuous() +\n  xlab(\"B\") +\n  stat_overlay_normal_density(color = \"red\", linetype = \"dashed\")\n\n\n\n\n\n\n\n\n\n# Plot the distribution of the antibodies for vaccine \nggdensity(df$C, fill = \"lightgray\", title = \"Distribution plot of Antibody Production for Vaccine C\") +\n  scale_x_continuous() +\n  xlab(\"C\") +\n  stat_overlay_normal_density(color = \"red\", linetype = \"dashed\")\n\n\n\n\n\n\n\n\nFrom these density plots, we see that our data is not normally distributed and distribution shape for two vaccines looks similar while one vaccine deviates. As our data is not normally distributed and has small sample size, we will now perform Kruskal-Wallis test to find out whether there are any significant differences between the three vaccines in terms of their efficacy (antibodies production in the body).\n\n\n\n17.2.5 Kruskal-Wallis Test\nThe Kruskal-Wallis test can be done in R using the kruskal.test function as seen below.\n\nresult &lt;- kruskal.test(Antibody ~ Vaccines, data = vaccine_efficacy)\n\nprint(result)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  Antibody by Vaccines\nKruskal-Wallis chi-squared = 7.2982, df = 2, p-value = 0.02601\n\n\n\n\n17.2.6 Tabulating the result\n\ntable1 &lt;-   \n  tbl_summary(\n    vaccine_efficacy,\n    by = Vaccines,\n) %&gt;% \n  add_p() %&gt;%\n  modify_caption(\"Antibody Production of Different Vaccines\") %&gt;%\n  bold_labels()\n\n\ntable1\n\n\n\n\n\n\nAntibody Production of Different Vaccines\n\n\nCharacteristic\nA, N = 61\nB, N = 61\nC, N = 61\np-value2\n\n\n\n\nAntibody\n647 (471, 824)\n240 (182, 296)\n474 (375, 748)\n0.026\n\n\n\n1 Median (IQR)\n\n\n2 Kruskal-Wallis rank sum test\n\n\n\n\n\n\n\n\n\n\n\n17.2.7 Interpretation\nFrom the Kruskal-Wallis test, we get that our test statistic is 26.63 with p-value 0.026, which is smaller than our level of significance 0.05. This gives us enough evidence to reject the null hypothesis. Therefore, we conclude that there is a significant difference in the efficacy of at least two of the three vaccines.\n\n17.2.7.1 Post-hoc-Test\nThe Kruskal-Wallis test helps to determine whether at least two groups differ from each other but it does not specify where in which groups the significance lies. We need to conduct a post-hoc test for this. For this purpose, the Dunn test is the appropriate nonparametric test for the pairwise multiple comparison. We will use Holm adjustment method for multiple comparison. You can read about various adjustment methods for multiple comparison herechen2017?\n\npair_wise_compare &lt;- dunnTest(Antibody~Vaccines, \n                              data = vaccine_efficacy,\n                              method = \"holm\"\n  \n)\n\npair_wise_compare\n\nDunn (1964) Kruskal-Wallis multiple comparison\n\n\n  p-values adjusted with the Holm method.\n\n\n  Comparison          Z     P.unadj     P.adj\n1      A - B  2.5955427 0.009444166 0.0283325\n2      A - C  0.6488857 0.516412268 0.5164123\n3      B - C -1.9466571 0.051575864 0.1031517\n\n\nWhen looking at the adjusted p-values in the last column for each pairwise comparison, we can see that only the A-B vaccine comparison has a p-value that is less than our level of significance of 0.05. Therefore, we conclude that there is significant difference in vaccine A-B while there is no significant difference between vaccines A-C, and B-C.\n\n\n\n17.2.8 Alternative method\nA very good alternative for performing a Kruskal-Wallis and the post-hoc tests in R is with the ggbetweenstats() function from the {ggstatsplot} package: It provides a combination of box and violin plots along with jittered data points for between-subjects designs with statistical details included in the plot as a subtitle.\n\nggbetweenstats(\n  data = vaccine_efficacy,\n  x = Vaccines,\n  y = Antibody,\n  type = \"nonparametric\", # ANOVA or Kruskal-Wallis\n  plot.type = \"box\",\n  pairwise.comparisons = TRUE,\n  pairwise.display = \"significant\",\n  centrality.type = \"nonparametric\", # It displays median for non parametric data by default.\n  bf.message = FALSE # Logical that decides whether to display Bayes Factor in favor of the null hypothesis. This argument is relevant only for parametric test\n)\n\n\n\n\n\n\n\n\nThis method has the advantage that all necessary statistical results are displayed directly on the plot. It also provides a more efficient and concise code.\nThe results of the Kruskal-Wallis test are shown in the subtitle above the plot (the p-value is after p =). Moreover, the results of the post-hoc test are displayed between each group via accolades, and the boxplots allow to visualize the distribution for each species.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Non-Parametric ANOVA: The Kruskal-Wallis Test</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_anova_kruskal_wallis.html#conclusion",
    "href": "lessons_original/04_anova_kruskal_wallis.html#conclusion",
    "title": "17  Non-Parametric ANOVA: The Kruskal-Wallis Test",
    "section": "17.3 Conclusion",
    "text": "17.3 Conclusion\nIn conclusion, the Kruskal-Wallace test is a non-parametric hypothesis test that can be used to determine if there are significant differences between two or more groups using the ranks of the data values. The first step involves visualizing the data to confirm it violates the rules of normality. Next, you conduct the Kruskal-Wallis test to determine if there are significant differences. Finally, you run a post-hoc test to calculate pairwise comparisons and determine which specific groups are significantly different.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Non-Parametric ANOVA: The Kruskal-Wallis Test</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_anova_kruskal_wallis.html#references",
    "href": "lessons_original/04_anova_kruskal_wallis.html#references",
    "title": "17  Non-Parametric ANOVA: The Kruskal-Wallis Test",
    "section": "References",
    "text": "References",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Non-Parametric ANOVA: The Kruskal-Wallis Test</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_anova_random_intercept.html",
    "href": "lessons_original/04_anova_random_intercept.html",
    "title": "18  Random Intercept Model",
    "section": "",
    "text": "18.1 Libraries Used\nlibrary(gt)        # to make table outputs presentation/publication ready. \nlibrary(broom)     # clean output.\nlibrary(knitr)     # to make tables. \nlibrary(gtsummary) # extension to gt\nlibrary(lattice)   # It is a powerful data visualization for multivariate data\nlibrary(lme4)      # Fit linear and generalized linear mixed-effects models\nlibrary(arm)       # Data Analysis Using Regression and Multilevel models\nlibrary(tidyverse) # for cleaning wrangling data",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Random Intercept Model</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_anova_random_intercept.html#what-is-a-random-intercept-model",
    "href": "lessons_original/04_anova_random_intercept.html#what-is-a-random-intercept-model",
    "title": "18  Random Intercept Model",
    "section": "18.2 What is a Random Intercept model",
    "text": "18.2 What is a Random Intercept model\nBefore talking about a random intercept model, let’s understand why they are necessary and important in the real world by discussing a variance component model first. This will make sense as we go along in this lecture.\n\n18.2.1 Variance component model\nWe are familiar with a fixed level of a factor or variable. Which means that the factor level in an experiment is the only thing we are interested. For example, let’s say we are interested in measuring the difference in resistance resulting from putting identical resistors to three different temperatures for a period of 24 hours. Let’s say we have three different groups, and each of these three different groups have a sample size of 5. So each of the three treatment groups was replicated 5 times.\n\n\n\nLevel 1\nLevel 2\nLevel 3\n\n\n\n\n6.9\n8.3\n8.0\n\n\n5.4\n6.8\n10.5\n\n\n5.8\n7.8\n8.1\n\n\n4.6\n9.2\n6.9\n\n\n4.0\n6.5\n9.3\n\n\nmean\nmean\nmean\n\n\n5.34\n7.72\n8.56\n\n\n\nIn this example, the level of the temperature is considered fixed meaning, the three temperatures were the only ones that we are interested in. This is called a fixed effects model.\na fixed effect model is a statistical model in which the parameters are fixed or non-random. This can also be referred to a regression model, in which group mean are “fixed” (non-random) or in simpler, terms something that is “fixed” in analysis is constant like sex assigned at birth or ethnicity.\n\\[ y_i = \\beta_0 + X_i\\beta_i + \\alpha_u + \\epsilon_i \\]\nNow, let’s say we want to look at different levels of factors that were chosen because of random sampling, like number of operators working that day, lot batches, days etc. So in this case we are now regarding factors not related to themselves (variables) but we are now trying to represent all possible levels that these factors may take, the appropriate model is now a random effects model.\nfitting these random effects models are important because we want to obtain estimates of different contributions that experimental factors make to the variability of our data! (we can represent this as the variance) this is what is called variance component\n\n\n18.2.2 Why this is relevant\nWell, a variance component model helps us see how much variance in our response at the different levels. But what if you are interested in seeing the effects of the explanatory variables? Or, what if your observations are NOT randomly sampled from simple random sample but instead from a cluster or a multi-level sampling design? Random intercept models or random effects models are important.\n\n\n18.2.3 Example 1: School level data\nLet’s say we have some data on exam results of students within a school and we use a variance component model and see that 15% of the variance is at the school level. Like for example, differing school districts, differing school policies etc. However, is it fair to really say that 15% of the variance in example scores is caused by schools? you could also say that maybe that part of the variance could be cause by the students being different themselves as well before taking the exam.\nIn this case, it might be important to control for the previous exams the students took, so you can look at the variance that is due to the things that happened when the students were at that school.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Random Intercept Model</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_anova_random_intercept.html#fitting-a-single-level-regression-model",
    "href": "lessons_original/04_anova_random_intercept.html#fitting-a-single-level-regression-model",
    "title": "18  Random Intercept Model",
    "section": "18.3 Fitting a single-level regression model",
    "text": "18.3 Fitting a single-level regression model\nwhen we want to control for something (like previous exams students took) we can fit a single-level regression model that looks something like this:\n\\[y_1 = \\beta_0 + \\beta_1x_i + e_i\\] where\n\n\\(y_1\\) is your dependent variable\n\\(\\beta_0\\) is your intercept and\n\\(\\beta_1\\) is your slope parameter (which is also your slope treatment effect).\nand \\(e_i\\) is your random error\n\nWhen you have clustered data fitting this model causes problems. Clustered data is data where you observation or participants are related. Like exam results for students within a school, height of children within a family etc.\nif we try to fit this clustered data:\n\nour standard errors will be wrong.\nthis single level data model doesn’t show up how much variation is at the school level and how much much of the variation is at the student level.\n\nSo fitting this type of data in this regression we wont know how much of an effect the school level has on the exam score, after controlling for the previous score.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Random Intercept Model</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_anova_random_intercept.html#solution-fitting-a-random-intercept-model",
    "href": "lessons_original/04_anova_random_intercept.html#solution-fitting-a-random-intercept-model",
    "title": "18  Random Intercept Model",
    "section": "18.4 Solution: Fitting a Random Intercept model",
    "text": "18.4 Solution: Fitting a Random Intercept model\nSo what we can do is combine the variance component and single-level regression model to build a random intercept model. So this random intercept model has 2 random terms. the level one random term: \\(e_{ij} \\sim N(0, \\sigma_e^2)\\) and the \\(N(0, \\sigma_u^2)\\) and has two parts:\n\na fixed part\na random part\n\n\\[ y_{ij} = \\overbrace{\\beta_0 + \\beta_1X_{ij}}^{\\text{fixed part}} + \\underbrace{u_j + e_{ij}}_{\\text{random part}}\\] where the fixed parts includes our parameters that we estimate as our coefficients, and the random part is the parameter we estimate as the variance \\(e_{ij} \\sim N(0, \\sigma_e^2)\\) and the \\(N(0, \\sigma_u^2)\\) and these are allowed to vary and \\(u_j\\) and \\(e_{ij}\\) are normally distributed.\nwhere:\n\n\\(y_{ij}\\) is your dependent variable at \\(i\\) individual and \\(j\\) level\n\\(N(0, \\sigma_u^2)\\) is the measurement at the school level\nand \\(e_{ij} \\sim N(0, \\sigma_e^2)\\) is the measurement at the student level\nand \\(i\\) subscript is for the students\nand \\(j\\) is the school subscript\n\nwe can also write this equation like so:\n\\[ Y_{ij} = \\mu + b_i + \\varepsilon_{ij} \\]\nwhere\n\n\\(Y_{ij}\\) is your dependent outcome of intested for a subject \\(i\\) at school \\(j\\)\n\\(\\mu\\) is the population average mean\n\\(b_i\\) is the random students effects (you have a random effect for every student)\n\\(\\varepsilon_{ij}\\) is your random error.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Random Intercept Model</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_anova_random_intercept.html#final-key-points",
    "href": "lessons_original/04_anova_random_intercept.html#final-key-points",
    "title": "18  Random Intercept Model",
    "section": "18.5 Final key points",
    "text": "18.5 Final key points\n\nrandom intercept models are used for answering questions about clustered data, and at different levels. For example, what is the relationship between exam scores at 11 and at age 16? how much variation is there between students progress from 11 to 16 at the school level?\n\\(b_i\\) is the error associated with the students.\n\\(\\varepsilon_{ij}\\) is the random error.\nfor a random intercept model, each individual will have a random intercept, but the sample slope.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Random Intercept Model</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_anova_random_intercept.html#assumptions-of-a-random-effects-model",
    "href": "lessons_original/04_anova_random_intercept.html#assumptions-of-a-random-effects-model",
    "title": "18  Random Intercept Model",
    "section": "18.6 Assumptions of a random effects model:",
    "text": "18.6 Assumptions of a random effects model:\n\nunobserved cluster effects is not correlated with observed variables (all \\(u_{ij}\\) terms are not correlated with the your predictors.)\nthe within and between effects are the same.\nyour error term is independent with your constant term.\nyou have homoscedasticity\n\\(b_i\\) and \\(\\varepsilon\\) are independent of each other",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Random Intercept Model</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_anova_random_intercept.html#hypothesis-of-a-random-effects-model",
    "href": "lessons_original/04_anova_random_intercept.html#hypothesis-of-a-random-effects-model",
    "title": "18  Random Intercept Model",
    "section": "18.7 hypothesis of a random effects model:",
    "text": "18.7 hypothesis of a random effects model:\nhypothesis testing for a random effects model runs as follows:\n\\[ H_0: \\sigma^2_u = 0\\] \\[H_1: \\sigma^2_u \\not = 0\\] the null hypothesis states that if \\(\\sigma^2_u\\) is true, then the random component is not needed in this model. so you can fit a single level regression model. to do this, you would can do a likelihood ratio test comparing the two model to see if sigma is significant. In other words, seeing if there is no difference in intercepts. If there is NO difference in intercepts (or the slopes are similar), then a random intercept model or random component is not needed.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Random Intercept Model</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_anova_random_intercept.html#example-2-planktonic-larval-duration-pld",
    "href": "lessons_original/04_anova_random_intercept.html#example-2-planktonic-larval-duration-pld",
    "title": "18  Random Intercept Model",
    "section": "18.8 Example 2: Planktonic larval duration (PLD)",
    "text": "18.8 Example 2: Planktonic larval duration (PLD)\nthis is example is from O’Connor et al (2007). A brief intro on this study, temperature is important for the development of organisms. In marine species, temperature is very important and linked to growth. So the time spent as a planktonic larvae can have associations on mortality and regulation on the species. Previous research has looked at the association between species comparison but not within species comparisons. What if we are interest in within and between species variation?\n\n18.8.1 load PLD data\n\nPLD &lt;- read_table(\"../data/04_PLD.txt\")\n\nI am curious about the structure of this data and how it briefly looks.\n\n#strcuture \nstr(PLD)\n\nspc_tbl_ [214 × 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ phylum : chr [1:214] \"Annelida\" \"Annelida\" \"Annelida\" \"Annelida\" ...\n $ species: chr [1:214] \"Circeus.spirillum\" \"Circeus.spirillum\" \"Circeus.spirillum\" \"Hydroides.elegans\" ...\n $ D.mode : chr [1:214] \"L\" \"L\" \"L\" \"P\" ...\n $ temp   : num [1:214] 5 10 15 15 20 25 30 5 10 17 ...\n $ pld    : num [1:214] 16 16 4 8 6.5 5 3.2 15 9.5 4.5 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   phylum = col_character(),\n  ..   species = col_character(),\n  ..   D.mode = col_character(),\n  ..   temp = col_double(),\n  ..   pld = col_double()\n  .. )\n\n# just the top - seeing how it looks\nhead(PLD)\n\n# A tibble: 6 × 5\n  phylum   species           D.mode  temp   pld\n  &lt;chr&gt;    &lt;chr&gt;             &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 Annelida Circeus.spirillum L          5  16  \n2 Annelida Circeus.spirillum L         10  16  \n3 Annelida Circeus.spirillum L         15   4  \n4 Annelida Hydroides.elegans P         15   8  \n5 Annelida Hydroides.elegans P         20   6.5\n6 Annelida Hydroides.elegans P         25   5  \n\n# brief summary\nsummary(PLD)\n\n    phylum            species             D.mode               temp      \n Length:214         Length:214         Length:214         Min.   : 2.50  \n Class :character   Class :character   Class :character   1st Qu.:12.28  \n Mode  :character   Mode  :character   Mode  :character   Median :20.00  \n                                                          Mean   :18.59  \n                                                          3rd Qu.:25.00  \n                                                          Max.   :32.00  \n      pld        \n Min.   :  1.00  \n 1st Qu.: 11.00  \n Median : 19.30  \n Mean   : 26.28  \n 3rd Qu.: 33.45  \n Max.   :129.00  \n\n\nI am curious about how this would look just plotting the variable pld or planktonic larvae duration and the temperature. So i am interested in seeing how the temperature is associated with their their survival duration.\n\n# how to plot in base R \nplot(pld ~ temp, data = PLD,\n     xlab = \"temperature (C)\",\n     ylab = \"PLD survival (Days)\",\n     pch = 16)\n# add lm line in base R\nabline(lm(pld ~ temp, data = PLD))\n\n\n\n\n\n\n\n\nyou can also do this in ggplot plot like so:\n\nggplot(data = PLD) +\n  aes(y = pld, x = temp) +\n  stat_smooth(method = \"lm\") +\n  geom_point() +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n18.8.2 Fit first model: Linear model\nLet’s first fit a linear model and check any assumptions. Why are we fitting a linear model first? It might be important to check the standard error of this model and compare to the next model, that might be a better fit later on.\n\n# fitting linear model first \n\nLinearModel_1 &lt;- lm(pld ~ temp, data = PLD)\n\nsummary(LinearModel_1)\n\n\nCall:\nlm(formula = pld ~ temp, data = PLD)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-44.158 -11.351  -0.430   7.684  93.884 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  54.8390     3.7954  14.449  &lt; 2e-16 ***\ntemp         -1.5361     0.1899  -8.087 4.65e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 20.36 on 212 degrees of freedom\nMultiple R-squared:  0.2358,    Adjusted R-squared:  0.2322 \nF-statistic:  65.4 on 1 and 212 DF,  p-value: 4.652e-14\n\n\nWe can fit this output in a gtsummary to make it nicer looking:\n\nLinearModel_1 |&gt; \n  tbl_regression()\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI1\np-value\n\n\n\n\ntemp\n-1.5\n-1.9, -1.2\n&lt;0.001\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\n\nI am interested in checking out visually, the equal variance (homoscedasticity) and so i will plot a a base residual graph:\n\n# better to do a scatter plot \nLinearModel_res &lt;- resid(LinearModel_1)\n\n# plot residual \nplot(PLD$temp, LinearModel_res,\n     ylab = \"residuals\",\n     xlab = \"temp\",\n     main = \"Residual graph\"\n     )\nabline(0,0)\n\n\n\n\n\n\n\n\nJust upon visual observation, it seems like this assumption may be violated so i think it might be important to do some transformations.\n\n\n18.8.3 Log transformation\n\nLinearMode_2Log &lt;- lm(log(pld) ~ log(temp), data= PLD)\n\nsummary(LinearMode_2Log)\n\n\nCall:\nlm(formula = log(pld) ~ log(temp), data = PLD)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0768 -0.3956  0.1802  0.5461  1.9656 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   4.6946     0.3128  15.011  &lt; 2e-16 ***\nlog(temp)    -0.6308     0.1093  -5.771 2.77e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8236 on 212 degrees of freedom\nMultiple R-squared:  0.1358,    Adjusted R-squared:  0.1317 \nF-statistic: 33.31 on 1 and 212 DF,  p-value: 2.767e-08\n\n\n\n\n18.8.4 residual of new log transformed graph\n\n# better to do a scatter plot \nLinearModel2_res &lt;- resid(LinearMode_2Log)\n\n# plot residual \nplot(PLD$temp, LinearModel2_res,\n     ylab = \"residuals(log)\",\n     xlab = \"temp(log)\",\n     main = \"Residual graph (log transformation)\"\n     )\nabline(0,0)\n\n\n\n\n\n\n\n\nA bit better! Now i kinda want to see the original plot i plotted with PLD and temperature:\n\nplot(log(pld) ~ log(temp), data = PLD,\n     xlab = \"Temperature in C\",\n     ylab = \"PLD in days\")\nabline(LinearMode_2Log)\n\n\n\n\n\n\n\n\nin ggplot you can use the facet_wrap() function to separate by phylum:\n\nggplot(data = PLD) +\n  aes(x = log(temp), y = log(pld)) +\n  geom_point() +\n  stat_smooth(method = \"lm\") +\n  facet_wrap(~phylum) +\n  theme_classic()",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Random Intercept Model</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_anova_random_intercept.html#fitting-a-random-intercept-model-random-intercept-same-slope",
    "href": "lessons_original/04_anova_random_intercept.html#fitting-a-random-intercept-model-random-intercept-same-slope",
    "title": "18  Random Intercept Model",
    "section": "18.9 Fitting a random intercept model (random intercept, same slope)",
    "text": "18.9 Fitting a random intercept model (random intercept, same slope)\nI am interested in seeing if the overall temperature and the PLD relationship is similar among species, but not the same. We are interested in plotting a mixed effects model with a random intercept but fixed/same slope. I am only interested in the species-specific plot for now with the phylum Mollusca.\n\n# filter to only mollusca\n\nMollusca_subset &lt;- \n  PLD |&gt; \n  filter(phylum == \"Mollusca\")\n\nggplot(data = Mollusca_subset) +\n  aes(x = log(pld), y = log(temp)) +\n  geom_point() +\n  labs(x = \"Log(temperature)\",\n       y = \"Log(PLD)\") + \n  stat_smooth(method = \"lm\", formula = \"y ~ x\", se = FALSE, fullrange = TRUE) +\n  theme_classic() +\n  facet_wrap(~ species)\n\n\n\n\n\n\n\n\n\n18.9.1 fitting model\nWe can use the library lme4 to fit a model of a linear regression with a random effect\n\n# creating log -transformed variables \nMollusca_subset$log_pld &lt;- log(Mollusca_subset$pld)\nMollusca_subset$log_temp &lt;- log(Mollusca_subset$temp)\n\n# mixed model with random intercept only \nRandIntModel_Mollusca &lt;- lmer(log_pld ~ log_temp + (1 | species), data = Mollusca_subset)\n\nsummary(RandIntModel_Mollusca)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: log_pld ~ log_temp + (1 | species)\n   Data: Mollusca_subset\n\nREML criterion at convergence: 43.7\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.37222 -0.32686 -0.09382  0.43821  2.34871 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n species  (Intercept) 0.86457  0.9298  \n Residual             0.03309  0.1819  \nNumber of obs: 44, groups:  species, 16\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)   7.1728     0.5284  13.575\nlog_temp     -1.5175     0.1592  -9.531\n\nCorrelation of Fixed Effects:\n         (Intr)\nlog_temp -0.896\n\n\n\nthe fixed effects: section in the output is the estimate for the fixed slope, and the grand mean of the intercept. in the fixed effects section the intercept here is the random effect, and you can see this\nin the Random effects section in this output, in this case our random effect was specified in the the Names sections, which tells us the parameter is the intercept.\nthe Group section tells us we have a random intercept for each species. We also have the variance and standard deviation for the random effects (as well as the residuals)\n\nSince we have a random effect at the individual level, we can subset this section out so it is clear to see that these organism will have a random intercept and fixed slope.\n\n# subset of the coefficients for random intercept and fixed slop\n\ncoef(RandIntModel_Mollusca)$species\n\n                      (Intercept) log_temp\nChlamys.hastata          7.612748 -1.51751\nCrassostrea.virginica    7.592515 -1.51751\nCrepidula.fornicata.     8.045609 -1.51751\nCrepidula.plana          8.063220 -1.51751\nHaliotis.asinina         5.807247 -1.51751\nHaliotis.fulgens         6.083069 -1.51751\nHaliotis.sorenseni.      6.688210 -1.51751\nMactra.solidissima       7.589660 -1.51751\nMopalia.muscosa          7.061086 -1.51751\nMytilus.edulis           7.141801 -1.51751\nNassarius.obsoletus      7.370159 -1.51751\nOstrea.lurida            6.967626 -1.51751\nPerna.viridis            8.144314 -1.51751\nStrombus.gigas           8.048942 -1.51751\nTivela.mactroides        7.677603 -1.51751\nTonicella.lineata        4.871674 -1.51751\n\n\nSo now we can see that in the Mollusca subset, we have all random intercepts for individual specifies, but the same slope.\n\n\n18.9.2 Inter class correlation coefficient (ICC)\nfor a random intercept model, we can run a diagnostic called the inter-class correlation coefficient (ICC), which lets us know how much group specific information is available for the random effect. this is somewhat similar to the ANOVA, in which it looks at the variability within groups compared to the variability between groups. Low ICC means that observation within group don’t really cluster.\n\\[ ICC = {\\sigma^2_{\\alpha} \\over \\sigma^2 + \\sigma^2_\\alpha} \\]\n\n# creating data frame\nvar &lt;- as.data.frame(VarCorr(RandIntModel_Mollusca))\n\n#check our data frame\nvar\n\n       grp        var1 var2       vcov     sdcor\n1  species (Intercept) &lt;NA&gt; 0.86457141 0.9298233\n2 Residual        &lt;NA&gt; &lt;NA&gt; 0.03309183 0.1819116\n\n#ICC equation\nICC &lt;- var$vcov[1] / (var$vcov[1] + var$vcov[2])\n\n# ICC value \nICC\n\n[1] 0.9631356\n\n\nIn our model, the \\(\\sigma^2_{\\alpha}\\) is 0.8645 (also the vcov part) and the \\(\\sigma^2\\) is 0.033. so once we do the mathematics we get 0.9631. Which is the proportion of the total variance in Y that is accounted for by clustering. This is a high value and therefore, suggesting we have within-group variability, so it might be good we are running this random effects model.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Random Intercept Model</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_anova_random_intercept.html#interpretation-of-results",
    "href": "lessons_original/04_anova_random_intercept.html#interpretation-of-results",
    "title": "18  Random Intercept Model",
    "section": "18.10 Interpretation of results",
    "text": "18.10 Interpretation of results\n\nsummary(RandIntModel_Mollusca)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: log_pld ~ log_temp + (1 | species)\n   Data: Mollusca_subset\n\nREML criterion at convergence: 43.7\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.37222 -0.32686 -0.09382  0.43821  2.34871 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n species  (Intercept) 0.86457  0.9298  \n Residual             0.03309  0.1819  \nNumber of obs: 44, groups:  species, 16\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)   7.1728     0.5284  13.575\nlog_temp     -1.5175     0.1592  -9.531\n\nCorrelation of Fixed Effects:\n         (Intr)\nlog_temp -0.896\n\n\nInterpretation: Summary of this PLD data includes information about the random effects. Here we can see that the column groups shows the random effect variable. in the name section, you can see that the random effect is our intercept. so we have the variation due to the species. in the Residuals section, this is the variation that cannot be explained by the model (the error). As you will notice our Standard error is smaller compared to the ordinary regression we ran in the previous one. Standard error for this model is 0.15 and the previous standard error for the first model we ran was 0.18.\nSo 0.86 / 0.86 + 0.03 = 0.96 , so the difference between between species can explain 96% of the variance that is is left over after the variance is explained by our fixed effect. since the random effects of the species explain most.\nthere is a very long description on the why the lmer() function doesn’t include the p-value that can be found here.\ninterpretation of temp variable for the fixed part, we can interpret this parameter the same as a single-level regression model, so \\(\\beta_1\\) is the increase/decrease in response for 1 unit increase/decrease in \\(x.\\) In other words, for one unit increase in the degrees of temperature, there is a -1.5 decrease in Planktonic larval duration. (or, as the temperature increase, the plankton duration is lower.)",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Random Intercept Model</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_anova_random_intercept.html#conclusion",
    "href": "lessons_original/04_anova_random_intercept.html#conclusion",
    "title": "18  Random Intercept Model",
    "section": "18.11 Conclusion",
    "text": "18.11 Conclusion\nIn this lecture you learned about the importance of a random intercept model, when it is appropriate to use a random intercept model, the difference between an ordinary single-level model, and a random intercept model, the assumptions of the random intercept model, hypothesis testing for the variation, the Interclass correlation coefficient (ICC) and finally, how to interpret results from the fixed part and the random part of a random intercept model.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Random Intercept Model</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_anova_random_intercept.html#references",
    "href": "lessons_original/04_anova_random_intercept.html#references",
    "title": "18  Random Intercept Model",
    "section": "18.12 References",
    "text": "18.12 References\n\nAbedin, Jaynal, and Kishor Kumar Das. 2015. Data Manipulation with r. Packt Publishing Ltd.\nAnnesley, Thomas M. 2010. “Bars and Pies Make Better Desserts Than Figures.” Clinical Chemistry 56 (9): 1394–1400.\nAnscombe, Francis J. 1973. “Graphs in Statistical Analysis.” The American Statistician 27 (1): 17–21.\nBorer, Elizabeth T, Eric W Seabloom, Matthew B Jones, and Mark Schildhauer. 2009. “Some Simple Guidelines for Effective Data Management.” The Bulletin of the Ecological Society of America 90 (2): 205–14.\nBorghi, John, Stephen Abrams, Daniella Lowenberg, Stephanie Simms, and John Chodacki. 2018. “Support Your Data: A Research Data Management Guide for Researchers.” Research Ideas and Outcomes 4: e26439.\nBroman, Karl W, and Kara H Woo. 2018. “Data Organization in Spreadsheets.” The American Statistician 72 (1): 2–10.\nChamberlin, Thomas C. 1890. “The Method of Multiple Working Hypotheses.” Science 15 (366): 92–96.\nsupplementary material for Tom Snijders and Roel Bosker textbook - Shjders, T Bosker R, 1999. Multilevel analysis: an introduction to basic and advanced mltilevel modeling, London, Sage, including updates and corrections data set examples http://stat.gamma.rug.nl/multilevel.htm\nUniversity of Bristol, Random Intercept model, (2018). http://www.bristol.ac.uk/cmm/learning/videos/random-intercepts.html\nMidway, S. (2019). “Data Analysis in R.” https://bookdown.org/steve_midway/DAR/random-effects.html",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Random Intercept Model</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_correlation.html",
    "href": "lessons_original/04_correlation.html",
    "title": "\n19  Correlation and Covariance Matrices\n",
    "section": "",
    "text": "19.1 Introduction\nOne of the general goals of statistics is to understand if there is a relationship between two variables.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Correlation and Covariance Matrices</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_correlation.html#introduction",
    "href": "lessons_original/04_correlation.html#introduction",
    "title": "\n19  Correlation and Covariance Matrices\n",
    "section": "",
    "text": "Covariance measures the degree to which the deviation of one variable (X) from its mean is related to the deviation of another variable (Y) from its mean. In other words, covariance measures the joint variability of two random variables or how these increase or decrease in relation with each other.\n\nFor instance, if greater values of one variable tend to correspond with greater values of another variable, this suggests positive covariance.\nCovariance can have both positive and negative values.\n\n\nA Covariance Matrix shows the covariance between different variables of a data set.\nCorrelation tells us both the strength and the direction of this relationship by listing the Correlation Coefficient (“Pearson”, “Spearman”, or “Kendall”) for the pair as measure of association.\n\nCorrelation matrix allows for the exploration of the correlation among the multiple variables in a data set. It proves this information through a table listing the correlation coefficients (r) for each relationship between pair of variables in the data set.\n\nCorrelation is best used for multiple variables that express a linear relationship with one another.\nCorrelation between two variables highlight how a change in one variable impacts a change in another variable.\n\n\n\n\nIn sum, a covariance and a correlation matrix sound similar because covariance is a measure of correlation, while correlation is a scaled version of covariance. This means correlation is a special case of covariance which can be achieved when the data is in standardized form.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Correlation and Covariance Matrices</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_correlation.html#about-covariance-and-correlation",
    "href": "lessons_original/04_correlation.html#about-covariance-and-correlation",
    "title": "\n19  Correlation and Covariance Matrices\n",
    "section": "\n19.2 About Covariance and Correlation",
    "text": "19.2 About Covariance and Correlation\n\n19.2.1 Equations, properties, and uses\nMathematical equations, properties, and uses\nRemember the equation for the variance for one variable\n\\[\nvar(x) = \\frac{1}{n-1}{\\sum_{i = 1}^{n}{(x_i - \\bar{x})^2}}\n\\]\nCovariance: Relation between two variables where, (x_i are the values of the X-variable), (y_i* are the values of the Y-variable), (x_bar is the mean of the X-variable), (y_bar* is the mean of the Y-variable), and (n are the number of data points)\n\\[\nCov(x,y) = \\frac{1}{n-1}{\\sum_{i = 1}^{n}{(x_i - \\bar{x})(y_i-\\bar{y})}}\n\\]\n\n19.2.1.1 Covariance Properties\n\n\n(x, y) &gt; 0: the covariance for both variables is positive or moving in the same direction.\n\n(x, y) &lt; 0: the covariance for both variables is negative or moving in the opposite direction.\n\n(x, y) = 0: there is no relationship between the variables.\n\nCovariance Matrix: Covariance between variables in a data set\n\\[\nC = \\frac{1}{n-1}{\\sum_{i = 1}^{n}{(X_i - \\bar{X})(X_i-\\bar{X})}^T}\n\\]\nThe general formula to represent a covariance matrix\n\\[\n\\begin{bmatrix} Var(x_{1}) & ... & Cov(x_{1},x_{n})\\\\ : &. & :\\\\ :& \\: \\: \\: \\: \\: \\: \\: \\: \\: \\: .& :\\\\ Cov(x_{n},x_{1}) & ... & Var(x_{n}) \\end{bmatrix}.\n\\]\nUses of a covariance matrix\n\nCalculate the Mahalanobis distance.,\nKalman filters.,\nGaussian mixture models.\nPCA (principal component analysis)\n\n\n\n\n\n\n\nThis is of importance and allows us to see the relationship between covariance and correlation in this simplified equation for the Pearson Correlation Coefficient, r. [video explaining standarization and mathematical properties of r]: \n\ntilestats2021?\n\n\n\n\\[\nr = \\frac {Cov(x,y)}{(\\sqrt{var(x)})(\\sqrt{var(y)})}\n\\]\n\n19.2.1.2 Correlation Matrix\n\n\nSample correlation matrix\n\nCorrelation Properties: direction\n\n\nr = 1: A perfect positive correlation.\n\nr = 0: Zero or no correlation.\n\nr = -1: A perfect negative relationship. where,\n\nThe sign of the correlation coefficient indicates the direction of the association.\n\n\n\nCorrelation Properties: strength\n\n.1 &lt; | r | &lt; .3 (small / weak correlation).\n.3 &lt; | r | &lt; .5 (medium / moderate correlation).\n.5 &lt; | r | (large / strong correlation).\n\nThe magnitude of the correlation coefficient indicates the strength of the association.\n\n\n\nCorrelation uses\n\nTo summarize the correlation among various variables of a data set.\nTo perform regression testing (like multicollinarity)\nBasis of other analyses (like regression models)\n\nCorrelation, r and Covariance\n\n\nVisual contrast\n\nWe will concentrate on the Correlation Matrix which provides the Pearson Correlation Coefficient or (its variations for non-parametric data such as, spearman and kendall).",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Correlation and Covariance Matrices</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_correlation.html#data-set-used",
    "href": "lessons_original/04_correlation.html#data-set-used",
    "title": "\n19  Correlation and Covariance Matrices\n",
    "section": "\n19.3 Data set used",
    "text": "19.3 Data set used\nWe will use the R data set “mtcars” to provide examples on developing a Covariance Matrix and on checking the assumptions for and development of a Correlation Matrix using the pearson correlation coefficient.\n\n# Load data\ndata(\"mtcars\")\n# Print sample\nhead(mtcars) %&gt;% \n  kable(\n    format = \"markdown\",\n    digits = 2,\n    caption = \"The mtcars data set\"\n  )\n\n\nThe mtcars data set\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\nMazda RX4\n21.0\n6\n160\n110\n3.90\n2.62\n16.46\n0\n1\n4\n4\n\n\nMazda RX4 Wag\n21.0\n6\n160\n110\n3.90\n2.88\n17.02\n0\n1\n4\n4\n\n\nDatsun 710\n22.8\n4\n108\n93\n3.85\n2.32\n18.61\n1\n1\n4\n1\n\n\nHornet 4 Drive\n21.4\n6\n258\n110\n3.08\n3.21\n19.44\n1\n0\n3\n1\n\n\nHornet Sportabout\n18.7\n8\n360\n175\n3.15\n3.44\n17.02\n0\n0\n3\n2\n\n\nValiant\n18.1\n6\n225\n105\n2.76\n3.46\n20.22\n1\n0\n3\n1",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Correlation and Covariance Matrices</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_correlation.html#covariance-matrix-procedure",
    "href": "lessons_original/04_correlation.html#covariance-matrix-procedure",
    "title": "\n19  Correlation and Covariance Matrices\n",
    "section": "\n19.4 Covariance Matrix Procedure",
    "text": "19.4 Covariance Matrix Procedure\nThe procedure to develop a covariance matrix uses the cov() function. It takes the data frame as an argument and returns the covariance matrix as result. It is as follows:\n\nconflicts_prefer(rstatix::select)\n\n[conflicted] Will prefer rstatix::select over any other package.\n\nmtcars2 &lt;- mtcars %&gt;%\n  select(1, 3, 4, 5, 6, 7)\nmtcars2_cov &lt;- cov(mtcars2)\nkable(mtcars2_cov,\n      caption = \"Covariance of mtcars using the Pearson type\"\n      )\n\n\nCovariance of mtcars using the Pearson type\n\n\n\n\n\n\n\n\n\n\n\nmpg\ndisp\nhp\ndrat\nwt\nqsec\n\n\n\nmpg\n36.324103\n-633.09721\n-320.73206\n2.1950635\n-5.1166847\n4.5091492\n\n\ndisp\n-633.097208\n15360.79983\n6721.15867\n-47.0640192\n107.6842040\n-96.0516815\n\n\nhp\n-320.732056\n6721.15867\n4700.86694\n-16.4511089\n44.1926613\n-86.7700806\n\n\ndrat\n2.195063\n-47.06402\n-16.45111\n0.2858814\n-0.3727207\n0.0871407\n\n\nwt\n-5.116685\n107.68420\n44.19266\n-0.3727207\n0.9573790\n-0.3054816\n\n\nqsec\n4.509149\n-96.05168\n-86.77008\n0.0871407\n-0.3054816\n3.1931661\n\n\n\n\n\n\n\nOnly the “continuous” variables of mtcars were included in this covariance matrix because the default type of procedure is “Pearson”.\nThe printout of the covariance matrix is not easy to read or as useful as the correlation matrix which has several functions that allow for visual and organizational displays of information.\nSome of the results are the following:\n\n\n\nThe values along the diagonals of the matrix are simply the variances of each product.\nThe variance of mpg is 36.32\nThe variance of disp is 15360.8\nThe other values in the matrix represent the covariances between the various products, for example:\nThe covariance between mpg and disp is -633.10, this is a negative covariance indicating that vehicles that are fuel efficient or have high miles per galon have lower levels of displacement (smaller engines, less powerful).\nThe covariance between hp and wt is 44.19, this is a positive covariance indicating that as horse power increases, so does the weight of a vehicle.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Correlation and Covariance Matrices</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_correlation.html#the-correlation-matrix-procedure",
    "href": "lessons_original/04_correlation.html#the-correlation-matrix-procedure",
    "title": "\n19  Correlation and Covariance Matrices\n",
    "section": "\n19.5 The Correlation Matrix Procedure",
    "text": "19.5 The Correlation Matrix Procedure\nThe Correlation Matrix procedure is of emphasis because it provides information on the linear relationship among variables (strength and direction). This analysis is of importance to build Regression and multivariate models, as it assists in:\n\n1. The observation of patterns and associations in the data.\n2. To conduct or as the basis of other analyses (linear regression models)\n3. To conduct diagnostic checks (multicollinearity)\n\n\n19.5.1 Hypothesis testing in correlations\nThe bivariate Pearson Correlation produces a sample correlation coefficient, r, which measures the strength and direction of linear relationships between pairs of continuous variables in a sample. By extension, the Pearson Correlation evaluates whether there is statistical evidence for a linear relationship among the same pairs of variables in the population, represented by a population correlation coefficient, ρ (“rho”), which is unknown. The sample pearson correlation coefficient, r, is the estimate of the unknown population coefficient (ρ). Therefore,\n\nThe hypothesis test lets us decide whether the value of the population correlation coefficient ρ is “close to zero” or “significantly different from zero”. We decide this based on the sample correlation coefficient r and the sample size n .\n\nTwo-tailed significance test:\nThe null hypothesis for a two-tailed significance test of a correlation is: \\[\nH_0: ρ = 0\n\\] rho is zero or approximately zero. This means that the relationship in the population is equal to 0; (“there is no association”)\nThe alternative hypothesis for a two-tailed significance test of a correlation is: \\[\nH_1: ρ ≠ 0\n\\] rho is not zero, which means that the relationship in the population is not equal to 0; (“there is an association in the population”)\nOne-tailed significance test:\nThe null hypothesis for a one-tailed significance test of a correlation is: \\[\nH_0: ρ = 0\n\\] which means that the relationship in the population is equal to 0; (“there is no association”)\nThe alternative hypothesis for a one-tailed significance test of a correlation where ρ &gt; 0 is: \\[\nH_1: ρ &gt; 0\n\\] which means that there is a positive relationship in the population.\nOR\nThe alternative hypothesis for a one-tailed significance test of a correlation where ρ &lt; 0 is: \\[\nH_1: ρ &lt; 0\n\\] which means that there is a negative relationship in the population.\nThese hypotheses are in terms of ρ.\n\n19.5.2 Assumptions of a Correlation Matrix:\n1. The two variables should be measured at the interval or ratio level (e.i., continuous).\na.  If interested in the relationship between two categorical variables or ranks (categorical is misleading) you can use \"Spearman\" as the type of correlation. But if you have two binary variables (i.e. living/dead, black/white, success/failure), then use a Phi Correlation coefficient.\n\nb.  If interested in a mix, one can use various functions like \"mixedCor\" in the `psych` package.\n\nc.  If interested in how the levels of a variable like male or female affect the correlation of two variables, we conduct a partial correlation which allow us to control for a third or more other variables using the function \"pcor.test\" in the `ppcor` package.\n2. There should exist a linear relationship between the two variables.\n3. Variables should be roughly normally distributed.\n4. Each observation in the dataset should have a pair of values.\n5. There should be no extreme outliers in the dataset.\n\n19.5.3 Checking Assumptions\n\n19.5.3.0.1 Level of Measurement\nUsing the skimr package, we will get data set information such as the distribution (histogram) of, and the type/level of measurement for the variables in the “mtcars” data set.\n\nskim(mtcars)\n\n\nData summary\n\n\nName\nmtcars\n\n\nNumber of rows\n32\n\n\nNumber of columns\n11\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n11\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nmpg\n0\n1\n20.09\n6.03\n10.40\n15.43\n19.20\n22.80\n33.90\n▃▇▅▁▂\n\n\ncyl\n0\n1\n6.19\n1.79\n4.00\n4.00\n6.00\n8.00\n8.00\n▆▁▃▁▇\n\n\ndisp\n0\n1\n230.72\n123.94\n71.10\n120.83\n196.30\n326.00\n472.00\n▇▃▃▃▂\n\n\nhp\n0\n1\n146.69\n68.56\n52.00\n96.50\n123.00\n180.00\n335.00\n▇▇▆▃▁\n\n\ndrat\n0\n1\n3.60\n0.53\n2.76\n3.08\n3.70\n3.92\n4.93\n▇▃▇▅▁\n\n\nwt\n0\n1\n3.22\n0.98\n1.51\n2.58\n3.33\n3.61\n5.42\n▃▃▇▁▂\n\n\nqsec\n0\n1\n17.85\n1.79\n14.50\n16.89\n17.71\n18.90\n22.90\n▃▇▇▂▁\n\n\nvs\n0\n1\n0.44\n0.50\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▆\n\n\nam\n0\n1\n0.41\n0.50\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▆\n\n\ngear\n0\n1\n3.69\n0.74\n3.00\n3.00\n4.00\n4.00\n5.00\n▇▁▆▁▂\n\n\ncarb\n0\n1\n2.81\n1.62\n1.00\n2.00\n2.00\n4.00\n8.00\n▇▂▅▁▁\n\n\n\n\n\nThe skimr function nicely displays the data. It indicates that all the variables are currently in numeric format but based on the descriptive information and on the information provided by ?mtcars, only 6 variables meet the level of measurement assumption (interval, ratio), we will then create a data frame from mtcars named cars_df where we leave these 6 variables as continuous double/numeric and categorize the other 5 variables. We present the summary using the gtsummary package.\n\ncars_df &lt;- within(mtcars, {\n   vs &lt;- factor(vs, labels = c(\"V\", \"S\"))\n   am &lt;- factor(am, labels = c(\"automatic\", \"manual\"))\n   cyl  &lt;- ordered(cyl)\n   gear &lt;- ordered(gear)\n   carb &lt;- ordered(carb)\n})\n\ncars_df %&gt;%\n  gtsummary::tbl_summary(\n    label = list(\n      mpg ~ \"Miles/(US) gallon\",\n      cyl ~ \"Number of cylinders\",\n      disp ~ \"Displacement (cu.in.)\",\n      hp ~  \"Gross horsepower\",\n      drat ~ \"Rear axle ratio\",\n      wt ~  \"Weight (1000 lbs)\",\n      qsec ~ \"1/4 mile time\",\n      vs ~  \"Engine (0 = V-shaped, 1 = straight)\",\n      am ~ \"Transmission (0 = automatic, 1 = manual)\",\n      gear ~ \"Number of forward gears\"),\n    type = all_continuous() ~ \"continuous2\",\n    statistic = all_continuous() ~ c(\n      \"{median} ({p25}, {p75})\",\n      \"{min}, {max}\"\n    )\n  ) %&gt;%\n  add_n() %&gt;%\n  bold_labels() %&gt;%\n  modify_header(label ~ \"**Variable**\")\n\n\n\n\n\n\n\n\n\n\n\nVariable\nN\n\nN = 321\n\n\n\n\nMiles/(US) gallon\n32\n\n\n\n    Median (IQR)\n\n19.2 (15.4, 22.8)\n\n\n    Range\n\n10.4, 33.9\n\n\nNumber of cylinders\n32\n\n\n\n    4\n\n11 (34%)\n\n\n    6\n\n7 (22%)\n\n\n    8\n\n14 (44%)\n\n\nDisplacement (cu.in.)\n32\n\n\n\n    Median (IQR)\n\n196 (121, 326)\n\n\n    Range\n\n71, 472\n\n\nGross horsepower\n32\n\n\n\n    Median (IQR)\n\n123 (97, 180)\n\n\n    Range\n\n52, 335\n\n\nRear axle ratio\n32\n\n\n\n    Median (IQR)\n\n3.70 (3.08, 3.92)\n\n\n    Range\n\n2.76, 4.93\n\n\nWeight (1000 lbs)\n32\n\n\n\n    Median (IQR)\n\n3.33 (2.58, 3.61)\n\n\n    Range\n\n1.51, 5.42\n\n\n1/4 mile time\n32\n\n\n\n    Median (IQR)\n\n17.71 (16.89, 18.90)\n\n\n    Range\n\n14.50, 22.90\n\n\nEngine (0 = V-shaped, 1 = straight)\n32\n\n\n\n    V\n\n18 (56%)\n\n\n    S\n\n14 (44%)\n\n\nTransmission (0 = automatic, 1 = manual)\n32\n\n\n\n    automatic\n\n19 (59%)\n\n\n    manual\n\n13 (41%)\n\n\nNumber of forward gears\n32\n\n\n\n    3\n\n15 (47%)\n\n\n    4\n\n12 (38%)\n\n\n    5\n\n5 (16%)\n\n\ncarb\n32\n\n\n\n    1\n\n7 (22%)\n\n\n    2\n\n10 (31%)\n\n\n    3\n\n3 (9.4%)\n\n\n    4\n\n10 (31%)\n\n\n    6\n\n1 (3.1%)\n\n\n    8\n\n1 (3.1%)\n\n\n\n\n1 n (%)\n\n\n\n\n\n\nNow we will examine only the 6 variables meeting the measurement assumption.\n\n19.5.3.1 Linear Relationship\n\npairs(\n  ~mpg + disp + hp + drat + wt + qsec,\n  data = cars_df,\n  main=\"Simple Scatterplot Matrix\")\n\n\n\n\n\n\n\nBased on this simple scatterplot matrix, all variables seem to follow a linear relationship, therefore, we will examine the normality and outliers assumption of these variables.\n\n19.5.3.2 Normality distribution and extreme outliers\n\npar(mfrow = c(2, 3))\n\nqqnorm(cars_df$mpg,\n       main = \"MPG Q-Q plot\",\n       ylab = \"Miles per Gallon\")\nqqline(cars_df$mpg)\n\nqqnorm(cars_df$disp, \n         main = \"Disp Q-Q plot\",\n         ylab = \"Displacement\")\nqqline(cars_df$disp)\n\nqqnorm(cars_df$hp, \n       main = \"hp Q-Q plot\",\n       ylab = \"Gross horsepower\")\nqqline(cars_df$hp)\n\nqqnorm(cars_df$drat, \n       main = \"drat Q-Q plot\",\n       ylab = \"Rear axle ratio\")\nqqline(cars_df$drat)\n\nqqnorm(cars_df$wt, \n       main = \"wt Q-Q plot\",\n       ylab = \"Weight\")\nqqline(cars_df$wt)\n\nqqnorm(cars_df$qsec,\n       main = \"qsec Q-Q plot\",\n       ylab = \"1/4 Mile Time\")\nqqline(cars_df$qsec)\n\n\n\n\n\n\n# A combination of ggqqplot and plot_annotation can also be used for the qq plot chart creation\n\n\npar(mfrow = c(2, 3))\n\nMPG_b &lt;- boxplot(cars_df$mpg,\n                 main = \"MPG boxplot\",\n                 ylab = \"Miles per Gallon\")\n\nDISP_b &lt;- boxplot(cars_df$disp,\n                  main = \"Disp boxplot\",\n                  ylab = \"Displacement\")\n\nHP_b &lt;- boxplot(cars_df$hp,\n                main = \"hp boxplot\",\n                ylab = \"Gross HP\")\n\nDRAT_b &lt;- boxplot(cars_df$drat,\n                  main = \"drat boxplot\",\n                  ylab = \"drat\")\n\nWT_b &lt;- boxplot(cars_df$wt,\n                main = \"wt boxplot\",\n                ylab = \"Weight\")\n\nqsec_b &lt;- boxplot(cars_df$qsec,\n       main = \"qsec boxplot\",\n       ylab = \"1/4 Mile Time\")\n\n\n\n\n\n\n\n\npar(mfrow = c(2, 3))\n\nMPG_h &lt;- hist(cars_df$mpg,\n              main = \"MPG Histogram\",\n              xlab = \"Miles per Gallon\")\n\nDISP_h &lt;- hist(cars_df$disp,\n               main = \"disp Histogram\", \n               xlab = \"Displacement\")\n\nHP_h &lt;- hist(cars_df$hp,\n             main = \"hp Histogram\",\n             xlab = \"Gross HP\")\n\nDRAT_h &lt;- hist(cars_df$drat,\n               main = \"drat Histogram\",\n               xlab = \"drat\")\n\nWT_h &lt;- hist(cars_df$wt,\n             main = \"wt Histogram\",\n             xlab = \"Weight\")\n\nQSEC_h &lt;- hist(cars_df$qsec,\n          main = \"qsec Histogram\",\n          ylab = \"1/4 Mile Time\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant procedures\n\n\n\n\nWe will run the correlation matrix using a “Pearson Correlation” for these data without resolving or reviewing some of the issues found with non-normality and extreme outliers for procedural and visual methods used to develop correlation matrices. However,\n\n\nNormality of variables: The qqplots, boxplots, and histograms visually hightlight that most of the variables have non-normality issues, for some of the variables, the issues with non-normality might not be too extreme for use in a pearson correlation. However, for three of the variables (displacement (disp), horse power (hp), and weight (wt)), the violation of non-normality might require one of the following strategies because non normally distributed data inflate the significance of “r” and outliers can have great influence on Pearson’ corrrelations as it might result in misleading results (like no association or very low association):\n\n\nStrategies to assist non normal distributions: Data transformations (such as log), Spearman rho correlation, Kendall tau’s correlation.\n\n\n\nPresence of outliers The qqplots, and boxplots visually hightlight that most of the outliers present should be further analyzed as these can have great influence on Pearson’ corrrelations because they might provide misleading results/associations (like no association or very low association):\n\n\nStrategies for outliers: Removal (if it is a confirmed measurement failure or a collection error), The use of Spearman correlation, adjustment/revisions.\n\n\n\n\n\n\n\n\n19.5.4 Computation of a Correlation Matrix\nThe function in R to create a basic correlation matrix is cor(). This function uses “pearson” as the default coefficient/method. The other two methods (for ranked correlation matrices) are “kendall” or “spearman”.\nIf the data set has missing values, one can ask (use =) for “everything”, “all.obs”, “complete.obs”, “na.or.complete”, or “pairwise.complete.obs”. The easiest and most commonly used approach is “use = complete.obs” in which missing values are handled by case wise deletion (and if there are no complete cases, that gives an error).\nOur data set (mtcars2) is composed of only the 6 “continuous” variables of the data set ’mtcars”, and does not have missing values, therefore we can use the default method of “pearson” and do not need to specify a use for missing values.\n\ncars_cor &lt;- round(cor(mtcars2, method = \"pearson\"),2)\nkable(cars_cor,\n      caption = \"Basic correlation matrix\")\n\n\nBasic correlation matrix\n\n\nmpg\ndisp\nhp\ndrat\nwt\nqsec\n\n\n\nmpg\n1.00\n-0.85\n-0.78\n0.68\n-0.87\n0.42\n\n\ndisp\n-0.85\n1.00\n0.79\n-0.71\n0.89\n-0.43\n\n\nhp\n-0.78\n0.79\n1.00\n-0.45\n0.66\n-0.71\n\n\ndrat\n0.68\n-0.71\n-0.45\n1.00\n-0.71\n0.09\n\n\nwt\n-0.87\n0.89\n0.66\n-0.71\n1.00\n-0.17\n\n\nqsec\n0.42\n-0.43\n-0.71\n0.09\n-0.17\n1.00\n\n\n\n\n\nWe can also display the lower triangle of the pearson coefficient matrix by using the upper.tri() function.\n\nupper&lt;-cars_cor\nupper[upper.tri(cars_cor)]&lt;-\"\"\nupper&lt;-as.data.frame(upper)\nkable(upper,\n      caption = \"Lower triangle of matrix correlation\")\n\n\nLower triangle of matrix correlation\n\n\nmpg\ndisp\nhp\ndrat\nwt\nqsec\n\n\n\nmpg\n1\n\n\n\n\n\n\n\ndisp\n-0.85\n1\n\n\n\n\n\n\nhp\n-0.78\n0.79\n1\n\n\n\n\n\ndrat\n0.68\n-0.71\n-0.45\n1\n\n\n\n\nwt\n-0.87\n0.89\n0.66\n-0.71\n1\n\n\n\nqsec\n0.42\n-0.43\n-0.71\n0.09\n-0.17\n1\n\n\n\n\n\nAnother way (easier) to compute and visualize the lower triangle of a correlation matrix could be accomplished by using the function datasummary_correlation() in the modelsummary package.\n\ndatasummary_correlation(mtcars2)\n\n \n\n  \n    \n\ntinytable_ti4g6leuj0stfuqw1z98\n\n\n      \n\n \n                mpg\n                disp\n                hp\n                drat\n                wt\n                qsec\n              \n\n\nmpg \n                  1   \n                  .   \n                  .   \n                  .   \n                  .   \n                  .\n                \n\ndisp\n                  -.85\n                  1   \n                  .   \n                  .   \n                  .   \n                  .\n                \n\nhp  \n                  -.78\n                  .79 \n                  1   \n                  .   \n                  .   \n                  .\n                \n\ndrat\n                  .68 \n                  -.71\n                  -.45\n                  1   \n                  .   \n                  .\n                \n\nwt  \n                  -.87\n                  .89 \n                  .66 \n                  -.71\n                  1   \n                  .\n                \n\nqsec\n                  .42 \n                  -.43\n                  -.71\n                  .09 \n                  -.17\n                  1\n                \n\n\n\n\n    \n\n\nHowever, this table is difficult to interpret as p-values for the pearson correlation coefficients are not included. The package Hmisc has functions available to provide p-values and to format the results of a correlation matrix. The function rcorr() provides the p-values of a correlation matrix. It is necessary to convert the data frame to a matrix to run this procedure:\n\ncars_cor_p &lt;- rcorr(as.matrix(mtcars2))\n\ncars_cor_p\n\n       mpg  disp    hp  drat    wt  qsec\nmpg   1.00 -0.85 -0.78  0.68 -0.87  0.42\ndisp -0.85  1.00  0.79 -0.71  0.89 -0.43\nhp   -0.78  0.79  1.00 -0.45  0.66 -0.71\ndrat  0.68 -0.71 -0.45  1.00 -0.71  0.09\nwt   -0.87  0.89  0.66 -0.71  1.00 -0.17\nqsec  0.42 -0.43 -0.71  0.09 -0.17  1.00\n\nn= 32 \n\n\nP\n     mpg    disp   hp     drat   wt     qsec  \nmpg         0.0000 0.0000 0.0000 0.0000 0.0171\ndisp 0.0000        0.0000 0.0000 0.0000 0.0131\nhp   0.0000 0.0000        0.0100 0.0000 0.0000\ndrat 0.0000 0.0000 0.0100        0.0000 0.6196\nwt   0.0000 0.0000 0.0000 0.0000        0.3389\nqsec 0.0171 0.0131 0.0000 0.6196 0.3389       \n\n\nAnother way in which a correlation matrix could be constructed displaying p-values is with the function corr.test in the psych package. The default alpha = 0.05 and the default method is “pearson”. Both could be changed to for instance, alpha = 0.01 and method = “spearman”. However, the R documentation for this function indicates that non parametric method calculation is slow.\n\nmtcars2_p &lt;- corr.test(mtcars2)$p\n\nkable(mtcars2_p)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmpg\ndisp\nhp\ndrat\nwt\nqsec\n\n\n\nmpg\n0.0000000\n0.0000000\n0.0000020\n0.0001243\n0.0000000\n0.0525761\n\n\ndisp\n0.0000000\n0.0000000\n0.0000009\n0.0000478\n0.0000000\n0.0525761\n\n\nhp\n0.0000002\n0.0000001\n0.0000000\n0.0499439\n0.0002487\n0.0000478\n\n\ndrat\n0.0000178\n0.0000053\n0.0099888\n0.0000000\n0.0000478\n0.6777366\n\n\nwt\n0.0000000\n0.0000000\n0.0000415\n0.0000048\n0.0000000\n0.6777366\n\n\nqsec\n0.0170820\n0.0131440\n0.0000058\n0.6195826\n0.3388683\n0.0000000\n\n\n\n\n\nWe can use the package `rstatix to: 1. Create a pearson coefficient correlation matrix with the function cor_mat() 2. Create a matrix of p-values (95% significance is default) with the function cor_pmat(), but p-values are not adjusted or rounded so it is not the best display 3. Create a visual representation of the matrix\n\n# Correlation matrix between all variables\nmtcars2_corr &lt;- mtcars2 %&gt;% \n  cor_mat()\nmtcars2_corr\n\n# A tibble: 6 × 7\n  rowname   mpg  disp    hp   drat    wt   qsec\n* &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 mpg      1    -0.85 -0.78  0.68  -0.87  0.42 \n2 disp    -0.85  1     0.79 -0.71   0.89 -0.43 \n3 hp      -0.78  0.79  1    -0.45   0.66 -0.71 \n4 drat     0.68 -0.71 -0.45  1     -0.71  0.091\n5 wt      -0.87  0.89  0.66 -0.71   1    -0.17 \n6 qsec     0.42 -0.43 -0.71  0.091 -0.17  1    \n\nmtcars2_corr_p &lt;- mtcars2 %&gt;% \n  cor_pmat()\nmtcars2_corr_p\n\n# A tibble: 6 × 7\n  rowname      mpg     disp           hp       drat        wt       qsec\n  &lt;chr&gt;      &lt;dbl&gt;    &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 mpg     0        9.38e-10 0.000000179  0.0000178  1.29e- 10 0.0171    \n2 disp    9.38e-10 0        0.0000000714 0.00000528 1.22e- 11 0.0131    \n3 hp      1.79e- 7 7.14e- 8 0            0.00999    4.15e-  5 0.00000577\n4 drat    1.78e- 5 5.28e- 6 0.00999      0          4.78e-  6 0.62      \n5 wt      1.29e-10 1.22e-11 0.0000415    0.00000478 2.27e-236 0.339     \n6 qsec    1.71e- 2 1.31e- 2 0.00000577   0.62       3.39e-  1 0         \n\nmtcars2_corr %&gt;%\n  #cor_get_pval() %&gt;%\n  cor_reorder() %&gt;%\n  pull_lower_triangle %&gt;%\n  cor_plot(label = TRUE)\n\n\n\n\n\n\n\nThe rquery.cormat in the corrplot package also provides the results of a correlation matrix showing r, p-values, and a visual representation. It can also provide formated results.\n\n\n\n\n\n\n\n\nThe package corrplot allows us to visually format the correlation matrix so that it can be a full matrix or the bottom or top triangle. Additionally, it can display the correlation coefficients, or another with p-values, or a visual representation of significant values.\n\ntestmtcars2 = cor.mtest(mtcars2, conf.level = 0.95)\n\n\ncorrplot(cars_cor,\n         type = 'full',\n         method = 'circle',\n         addCoef.col = 'black',\n         )\n\n\n\n\n\n\npar(mfrow = c(1, 2))\ncorrplot(cars_cor,\n         type = 'lower',\n         p.mat = testmtcars2$p,\n         number.font = 1,\n         tl.cex = 0.9,\n         pch.cex = 2,\n         insig = 'p-value', \n         sig.level = -1\n         )\ncorrplot(cars_cor,\n         type = 'upper',\n         tl.pos = 'n',\n         p.mat = testmtcars2$p,\n         method = 'color',\n         diag = FALSE, \n         sig.level = c(0.001, 0.01, 0.05),          \n         pch.cex = 0.9,\n         insig = 'label_sig',\n         pch.col = 'grey20'\n)\n\n\n\n\n\n\n\nUsing the add = TRUE call inside the upper triangle corrplot function, one could stack matrices, but this method is complicated and requires adjusting sizes and labeling. To create stack full matrices it is best to use the function corrplot.mixed also in the corrplot package. Sadly, some of the features of corrplot are not available, but you can still have very informative visual outputs.\n\ncorrplot.mixed(cars_cor)\n\n\n\n\n\n\n\nThe following matrix visualization using the function chart.Correlation in the package PerformanceAnalytics provides many of the assumption tests for a correlation. The top triangle is the (absolute) value of the correlation coefficient (the default method is “pearson” but it could be changed to “kendall” or “spearman”) plus the result of the cor.test as stars (this can be changed to other symbols). In the bottom triangle of the matrix are the bivariate scatterplots, with a fitted line. This is a quick way to visualize a correlation matrix coefficient and significant relationships while examining assumptions. A must to run this function is to use the data set developed with the six continuous variables of interest (mtcars2).\n\nchart.Correlation(mtcars2,\n                  histogram = TRUE,\n                  pch = 19)\n\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\nWarning in par(usr): argument 1 does not name a graphical parameter\n\n\n\n\n\n\n\n\n*  The same visualization could be built using the **pairs()** function in base R, but **pairs()** is not very user friendly to construct a correlation matrix.\n*  In addition, the function **pair.panel** in the `psych` package returns something similar. \n    *  Of interest about this function is that it also allows for regression visualization as regression lines and coinfidence intervals could be part of the output.\n\n\n\n\n\n\n\nA simple correlation matrix could be ran using the full data set and then select the variables significantly correlated and then see if any of them meet the assumptions.\nThe cpairs() in the gclus package allows for a plot matrix while ordering and coloring the subplots by correlation.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Correlation and Covariance Matrices</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_correlation.html#other-correlation-types-spearman-and-partial",
    "href": "lessons_original/04_correlation.html#other-correlation-types-spearman-and-partial",
    "title": "\n19  Correlation and Covariance Matrices\n",
    "section": "\n19.6 Other Correlation Types (Spearman and Partial)",
    "text": "19.6 Other Correlation Types (Spearman and Partial)\n\n19.6.1 Spearman (ranking of variables, non-parametric)\n\n# Load data\ndata(\"mtcars\")\ncars_partial &lt;- mtcars\ncars_spearman &lt;- cor.test(mtcars$cyl, mtcars$carb, method = \"spearman\")\n\nWarning in cor.test.default(mtcars$cyl, mtcars$carb, method = \"spearman\"):\nCannot compute exact p-value with ties\n\n# This is simply an example of how a spearman procedure is done. Although we can \n# use continuous variables, the exact p-value cannot be computed with ties. \n# Therefore, it is better to use ranked data in order to compute the exact p-value. \n\n\n19.6.2 Partial Correlation\nThe Partial correlation is the correlation of two variables while controlling for a third or more other variables. The partial correlation coefficient is a measure of the strength of the linear relationship between two variables after entirely controlling for the effects of other variables.\nTo conduct the partial correlation test\n\n# partial correlation between \"mpg\" and \"wt\"  given \"am\"\npcor.test(cars_partial$mpg,cars_partial$wt,cars_partial$am)\n\n    estimate      p.value statistic  n gp  Method\n1 -0.7835341 1.867415e-07 -6.790807 32  1 pearson\n\n\nThis is the calculation of the correlation between miles per gallon and weight while controlling the variable “am” = Transmission (0 = automatic, 1 = manual).",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Correlation and Covariance Matrices</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_correlation.html#example-of-r-in-examining-the-relationship-in-a-pair-of-variables-using-the-pearson-correlation-coefficient-type-of-correlation",
    "href": "lessons_original/04_correlation.html#example-of-r-in-examining-the-relationship-in-a-pair-of-variables-using-the-pearson-correlation-coefficient-type-of-correlation",
    "title": "\n19  Correlation and Covariance Matrices\n",
    "section": "\n19.7 Example of r in examining the relationship in a pair of variables using the Pearson Correlation Coefficient type of correlation",
    "text": "19.7 Example of r in examining the relationship in a pair of variables using the Pearson Correlation Coefficient type of correlation\nUsing the ggscatter function in the ggpubr package, we could examine the Pearson coefficient and the related p-value for a pair of variables we found of interest in the scatterplot matrix above. We will explore the relationship between the the number of miles per gallon and the weight of a vehicle.\n\n19.7.1 Hypothesis test between mpg and wt\nWe will test if a vehicle’s weight is related to its fuel efficiency.\n\\[\nH_0: ρ = 0,\n\\] \\[\nH_1: ρ ≠ 0\n\\]\n\nggscatter(cars_df, x = \"mpg\", y = \"wt\", \n          add = \"reg.line\",\n          conf.int = TRUE, \n          cor.coef = TRUE, \n          cor.method = \"pearson\",\n          xlab = \"Miles/(US) gallon\",\n          ylab = \"Weight (1000 lbs)\"\n          )\n\n\n\n\n\n\n\nWe can then conduct a Pearson correlation test using the cor.test function.\n\ncars_ct &lt;- cor.test(cars_df$wt, cars_df$mpg, \n                    method = \"pearson\")\ncars_ct\n\n\n    Pearson's product-moment correlation\n\ndata:  cars_df$wt and cars_df$mpg\nt = -9.559, df = 30, p-value = 1.294e-10\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.9338264 -0.7440872\nsample estimates:\n       cor \n-0.8676594 \n\n\nTo test the Hypothesis:\nThe test statistic t is rounded to -10 and the corresponding p-value is 1e-10.\nSince the p-value is less than .05, we have sufficient evidence to say that the correlation between a vehicle’s weight and miles per gallon is statistically significant.\nWe can also test the hypothesis manually:\nUsing a t-test:\nn(ordered pairs) = 32 r(sample correlation coefficient) = -0.868 alpha = 0.05\n\\[\nt = \\frac {r}{\\sqrt{\\frac {1-r^2}{n-2}}}\n\\]\n\\[\nt = \\frac {-0.868}{\\sqrt{\\frac {0.2466}{30}}} = -9.574\n\\]\nThe critical values \\(-t_O = - 2.0423\\) and \\(t_o = 2.0423\\) form the rejection regions for the standardized test statistic. Since t = -9.574 and it is less than \\(-t_O = - 2.0423\\), we reject the null hypothesis and we can conclude that there is enough evidence at the 95% level of significance that there is a significant linear correlation between fuel efficiency and a vehicle’s weight.\nAlternatively, we can use a table of critical values for pearson’s r. In this two-tailed example, with df = 30 and alpha = .05 the critical value is .349, then we can make a decision, |r| = 0.868 is greater than 0.349, which means that we reject the null hypothesis and conclude that there is a relationship between vehicle weight and miles per gallon, r(30) = -0.868, p&lt;0.05\nFurthermore,\n\n\nThe direction of the relationship is negative (i.e., heavier vehicles have lower numbers of miles per gallon or are less fuel efficient), meaning that when one variable increases the other variable decreases.\nThe magnitude, or strength, of the association is approximately strong (5 &lt; | r |).",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Correlation and Covariance Matrices</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_correlation.html#references",
    "href": "lessons_original/04_correlation.html#references",
    "title": "\n19  Correlation and Covariance Matrices\n",
    "section": "\n19.8 References",
    "text": "19.8 References",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Correlation and Covariance Matrices</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_regression_ols.html",
    "href": "lessons_original/04_regression_ols.html",
    "title": "\n20  Ordinary Least Squares Regression\n",
    "section": "",
    "text": "20.1 Ordinary Least Squares (OLS) Regression\nOLS is a “method that allows to find a line that best describes the relationship between one or more predictor variables and a response variable”howtop?, with our end result being:\n\\[\n\\hat{y} = b_0 + b_1x\n\\]\nThe best fitting line is typically calculated utilizing the least squares, which can be visually described as the deviation in the vertical direction.\nNow, we will examine the relationship between a vehicle’s weight (WT) and its miles per gallon or fuel efficiency (MPG).\nIn a previous lesson, we found a Pearson’s correlation coefficient of r(30) = -0.868, p&lt;0.05. Based on this information, we concluded that there is a strong negative relationship between MPG and WT. Where, heavier vehicles are associated with lower miles per gallon. Essentially this means that heavier vehicles are less fuel efficient. We will use the interpretation of this correlation as the basis of building an OLS regression to predict the value of MPG for a vehicle based on its weight. An OLS regression could be described as a common method used in regression analysis due to its efficiency in fitting the best straight line through a set of points. Thus, an OLS regression model gives best approximate of true population regression line as it minimizes the total distance from all of the points to the line.\nThe OLS model could be expressed as: \\[\\hat{y}_i = \\beta_0 + \\beta_1x_i\\]\nThen the OLS regression model line for our example is:\n\\[\\widehat{MPG_i} = \\beta_0 +\\beta_1*WT_i\\]",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Ordinary Least Squares Regression</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_regression_ols.html#ordinary-least-squares-ols-regression",
    "href": "lessons_original/04_regression_ols.html#ordinary-least-squares-ols-regression",
    "title": "\n20  Ordinary Least Squares Regression\n",
    "section": "",
    "text": "The line has the following properties:\n\nThe intercept (\\(\\beta_0\\)), its measure is defined by the units in Y. In our case, the units used in MPG. It is the predicted value of Y (MPG) when X (WT) is zero\n\n\nThe slope (\\(\\beta_1\\)), is the predicted change in Y for a one-unit increase in X. Like the correlation coefficient, it provides information on the relationship between X and Y. But unlike the correlation coefficient (unitless), it highlights the relationship in real terms of units. In our example this would look at how miles per gallon increase or decrease for a one unit increase in a vehicle’s weight (according to the R docummentation for the mtcars data set, weight is provided as a measure per every 1,000 pounds and miles per gallon are provided as Miles/(US) gallon), therefore the units are defined by the Y (MPG) and the X (WT).\n\n\n\nFor example, the data for a vehicle that weighs 2,000 pounds the unit is given as “2”\nFor example, a vehicle that spends one gallon of fuel per every 19 miles is given as “19”\n\n\n\n\n\n20.1.1 How to develop the best fitting line?\n\n\nSample residual/error terms plot\n\nThe best fitting line is one that minimizes errors in prediction or one with the Minimum sum of squared residuals (SSR). For more details, you can watch this video:khanacademy2018?\n\\[\nSSR = \\sum_{i = 1}^{n}{(y_i - \\hat{y_i})^2}\n\\]\n\\(residual_i = y_i - \\hat{y_i}\\)\nIt is important to note that prior to calculating the residuals, we must visualize and examine the data, which was done in the previous example. Then, we must run the regression line. We can utilize lm() to perform the OLS regression which will provide us with the model summary, including the following:\n\nPr(&gt;|t|) Multiple R-Squared Adjusted R-Squared Residual Standard Error F-statistic P-value\n\nOnce the model summary is given, we can then move on to creating the residual plots. When performing this step, we have to check the assumptions of homoscedasticity and normality.\n* Residuals = error terms\n* $$ Residual = observed value - predicted value $$\n* The larger the error term in absolute value, the worse the prediction\n* Squaring residuals solve issues arising from some residuals being negative and some positive.\n\n\nAssumptions\n\nLinearity: Linear relationship between the dependent variable and the independent variables.\nIndependence: The observations must be independent of each other.\nHomoscedasticity: The variance of the residuals should be constant across all levels of the independent variables.\nNormality: The residuals / errors should be normally distributed.\nNo multicollinearity: In the case of multiple regression (2+ independent variables), the independent variables should not be highly correlated with each other.\n\n\n\n\n\n\n\n\n\nBe careful about outliers\n\n\n\nOutliers can influence the estimates of the relationship.\n\n\n\n20.1.2 Example of an OLS regression in R\nIn R, the lm function command allows us to develop an OLS regression.\n\n# model predicting mpg (fuel efficiency) using wt (weight)\nMPGReg &lt;- lm(mpg ~ wt, data = mtcars)\nsummary(MPGReg)\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n\nThe OLS regression model is then:\n\\(\\widehat{MPG_i} = 37.285 - 5.344*WT_i\\)\n\n\nInterpretation: \\(\\beta_0\\)\n\nThe model predicts that vehicles with no weight will have 37.285 miles per gallon, on average.\n\nThis is not a very meaningful intercept as vehicles with “0” weight do not exist. A meaningful intercept can be created by subtracting a constant from the x variable to move the intercept.In R as part of the lm command, this can be done by surrounding the independent variable with I() which applies the function inside and treats it as a new variable. For our example we used the rounded lowest weight of the data (1.5) to predict miles per gallon.\n\nThis procedure does not change the slope of the line\n\n\n\n\n\n\n# model predicting mpg (fuel efficiency) using wt (weight)\nMPGReg2 &lt;- lm(mpg ~ I(wt-1.5), data = mtcars)\nsummary(MPGReg2)\n\n\nCall:\nlm(formula = mpg ~ I(wt - 1.5), data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  29.2684     1.1008  26.589  &lt; 2e-16 ***\nI(wt - 1.5)  -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n\n\nThen, the meaningful intercept model predicts that vehicles with a weight of 1500 pounds have 29.268 miles per gallon, on average.\n\nInterpretation: \\(\\beta_1\\)\n\nThe model predicts that on average, an increase of 1,000 pounds in the weight of a vehicle is associated with a decrease of 5.344 miles per gallon.\n\n\n\n\n# define residuals \nres &lt;- resid(MPGReg)\n\n# produce residual vs. fitted plot \nplot(fitted(MPGReg), res)\n\n# add a horizontal line at 0\nabline(0,0)\n\n\n\n\n\n\n# create Q-Q- plot for residuals\nqqnorm(res)\n\n# add a straight diagonal line to the plot\nqqline(res)\n\n\n\n\n\n\n\nBased on the graph above, it is visually clear that normality may not be met due to some outliers. This means that we must explore our data even deeper as it is possible that transformation of our data utilizing one of the following methods must take place:\n\nLog transformation Square Root Transformation Cube Root Transformation\n\nOnce the data is transformed, we can run the residual plot over again in order to achieve normality. For the sake of this presentation, we are only using an example with known limitations such as non-normality.\n\n20.1.3 Hypothesis testing in OLS regression\nThe null hypothesis in this case would be that the slope is zero indicating no relationship between x and y. Or in our example, we can state that there is no relationship between a vehicle’s weight and its miles per gallon. The alternative hypothesis is then that the slope is not zero.\n\\[\nH_0: \\beta_1 = 0\n\\]\n\\[\nH_1: \\beta_1 ≠ 0\n\\]\nWe can test this hypothesis by using the lm summary printout which provides the p-value for the wt coefficient. This indicates that there is indeed a significant relationship between the weight of the car and its efficiency (miles per gallon used). R provides a t-value for the ‘wt’ coefficient which has a p-value of p &lt; 0.000 as seen below:\n                  Estimate        Std. Error       t value      Pr(&gt;|t|)   \n    wt           -5.3445             0.5591       -9.559       1.29e-10\n\n20.1.4 R-Squared Value\nR-squared is “a measure of how much of the variation in the dependent variable is explained by the independent variables in the model. It ranges from 0 to 1, with higher values indicating a better fit”ordinary?.\nAdjusted R-squared is “similar to R-squared, but it takes into account the number of independent variables in the model. It is a more conservative estimate of the model’s fit, as it penalizes the addition of variables that do not improve the model’s performance”ordinary?.\n\n20.1.5 F-Statistic\nThe F-statistic “tests the overall significance of the model by comparing the variation in the dependent variable explained by the model to the variation not explained by the model. A large F-statistic indicates that the model as a whole is significant”interpre?.\n\n20.1.6 Visual representation\nThe visual representation of this model using ggplot is the following:\n\nggplot(mtcars, aes(x = wt, y = mpg))+\n  geom_point()+\n  geom_smooth(method = \"lm\", se = FALSE)+ #se is option for coinfidence bar\n  labs(x= \"Weight (per 1,000 pounds)\",\n       y = \"Miles per gallon\")+\n  theme_bw()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nAs shown in the figure above, we see the summary statistics represented in a visual manner with the line of best fit. As indicated previously, we see a steep negative correlation between weight of the car and the miles per gallon (efficiency) utilized.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Ordinary Least Squares Regression</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_regression_ols.html#references",
    "href": "lessons_original/04_regression_ols.html#references",
    "title": "\n20  Ordinary Least Squares Regression\n",
    "section": "\n20.2 References",
    "text": "20.2 References",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Ordinary Least Squares Regression</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_regression_mls.html",
    "href": "lessons_original/04_regression_mls.html",
    "title": "\n21  Multiple Linear Regression\n",
    "section": "",
    "text": "21.1 Multiple Linear Regression\nMultiple regression generally explains the relationship between multiple independent or predictor variables and one dependent or criterion variable. A dependent variable is modeled as a function of several independent variables with corresponding coefficients, along with the constant term. Multiple regression requires two or more predictor variables.\nEquation\n\\[\n\\hat{Y}= b_0 +b_1x_1 +b_2x_2 + ... b_nx_n + \\epsilon\n\\]\nwhere\n\\(\\hat{Y}\\) is the predicted or expected value of the dependent variable, and is always numeric and continuous.\n\\(b_0\\) is the value of \\(\\hat{Y}\\) when all of the independent variables \\(x_1 -x_n\\) are equal to zero,\n\\(x_1 -x_n\\) are distinct independent or predictor variables\n\\(b_1 - b_n\\) are the estimated regression coefficients. Each regression coefficient represents the change in \\(\\hat{Y}\\) relative to a one unit change in the respective independent variable.\nIn the multiple regression situation, \\(b_1\\), for example, is the change in \\(\\hat{Y}\\) relative to a one unit change in \\(x_1\\), holding all other independent variables constant (i.e., when the remaining independent variables are held at the same value or are fixed).\nAssumptions",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_regression_mls.html#multiple-linear-regression",
    "href": "lessons_original/04_regression_mls.html#multiple-linear-regression",
    "title": "\n21  Multiple Linear Regression\n",
    "section": "",
    "text": "Independence: The Y-values are statistically independent of each other as well as the errors. As with simple linear regression, this assumption is violated if several Y observations are made on the same subject.\nLinearity: The mean value of Y for each specific combination of values of the independent variables (\\(X_1, X_2...X_n\\)) is a linear function of the intercept and parameters (\\(\\beta_0, \\beta_1,...\\))\nHomoscedasticity: The variance of Y is the same for any fixed combination of independent variables.\nNormal Distribution: The residual values are normally distributed with mean zero.\nMulticollinearity: Multicollinearity cannot exist among the predictors (the variables are not correlated)\n\n\n21.1.1 Packages\n\nlibrary(ggplot2)\nlibrary(gt)\nlibrary(car)\n\nLoading required package: carData\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n\n\n✔ broom        1.0.5      ✔ rsample      1.2.1 \n✔ dials        1.2.1      ✔ tibble       3.2.1 \n✔ dplyr        1.1.4      ✔ tidyr        1.3.1 \n✔ infer        1.0.7      ✔ tune         1.2.1 \n✔ modeldata    1.3.0      ✔ workflows    1.1.4 \n✔ parsnip      1.2.1      ✔ workflowsets 1.1.0 \n✔ purrr        1.0.2      ✔ yardstick    1.3.1 \n✔ recipes      1.0.10     \n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ dplyr::recode()  masks car::recode()\n✖ purrr::some()    masks car::some()\n✖ recipes::step()  masks stats::step()\n• Dig deeper into tidy modeling with R at https://www.tmwr.org\n\nlibrary(lmtest)\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.5\n✔ lubridate 1.9.3     ✔ stringr   1.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ readr::col_factor() masks scales::col_factor()\n✖ purrr::discard()    masks scales::discard()\n✖ dplyr::filter()     masks stats::filter()\n✖ stringr::fixed()    masks recipes::fixed()\n✖ dplyr::lag()        masks stats::lag()\n✖ dplyr::recode()     masks car::recode()\n✖ purrr::some()       masks car::some()\n✖ readr::spec()       masks yardstick::spec()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n21.1.2 Data\nThe miles/gallon a car takes (dependent) based on the gross horsepower and weight of the car.\n\ndata(\"mtcars\")\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\n\n21.1.3 Testing the Assumptions\n\n21.1.3.1 Linearity\nIs there a linear relationship between the dependent variable and each of the independent variables?\n\n# is there a linear relationship between MPG and HP?\nggplot(data = mtcars) +\n  aes(\n    x = hp,\n    y = mpg\n  ) +\n  geom_point()\n\n\n\n\n\n\n# is there a linear relationship between MPG and WT?\nggplot(data = mtcars) +\n  aes(\n    x = wt,\n    y = mpg\n  ) +\n  geom_point()\n\n\n\n\n\n\n\n\n21.1.3.2 Multicollinearity\nAre the variables correlated? If the VIF score &gt; 10 then there is multicollinearity.\n\nCodemod &lt;- lm(mpg ~ hp + wt, data = mtcars)\n\n# checking the VIF of both predictors\nvif(mod)\n\n      hp       wt \n1.766625 1.766625 \n\nCode# checking the correlation between the predictors\ncor(mtcars$hp, mtcars$wt)\n\n[1] 0.6587479\n\n\n\n21.1.3.3 Homoscedasticity\nChecking the residual plots for homoscedasticity. The line below is relatively horizontal.\n\nmtcars_df &lt;- mtcars %&gt;% \n  mutate(res_sqrt = sqrt(abs(rstandard(mod))))\n\nggplot(mtcars_df, aes(fitted(mod), res_sqrt)) +\n  geom_point() +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n21.1.3.4 Normality\nChecking normality via histogram and QQ plot of the residuals.\n\n# histogram\nhist(mtcars_df$res_sqrt)\n\n\n\n\n\n\n# qq plot\nplot(mod, 2)\n\n\n\n\n\n\n# cook's d -&gt; are the outliers in the qq plot driving the relationship in the model? \ncooks.distance(mod)\n\n          Mazda RX4       Mazda RX4 Wag          Datsun 710      Hornet 4 Drive \n       1.589652e-02        5.464779e-03        2.070651e-02        4.724822e-05 \n  Hornet Sportabout             Valiant          Duster 360           Merc 240D \n       2.736184e-04        2.155064e-02        1.255218e-02        1.677650e-02 \n           Merc 230            Merc 280           Merc 280C          Merc 450SE \n       2.188702e-03        1.554996e-03        1.215737e-02        1.423008e-03 \n         Merc 450SL         Merc 450SLC  Cadillac Fleetwood Lincoln Continental \n       1.458960e-04        6.266049e-03        2.786686e-05        1.780910e-02 \n  Chrysler Imperial            Fiat 128         Honda Civic      Toyota Corolla \n       4.236109e-01        1.574263e-01        9.371446e-03        2.083933e-01 \n      Toyota Corona    Dodge Challenger         AMC Javelin          Camaro Z28 \n       2.791982e-02        2.087419e-02        2.751510e-02        9.943527e-03 \n   Pontiac Firebird           Fiat X1-9       Porsche 914-2        Lotus Europa \n       1.443199e-02        5.920440e-04        5.674986e-06        7.353985e-02 \n     Ford Pantera L        Ferrari Dino       Maserati Bora          Volvo 142E \n       8.919701e-03        5.732672e-03        2.720397e-01        5.600804e-03 \n\n# Plot Cook's Distance with a horizontal line at 4/n to see which observations\n# exceed this thresdhold\nn &lt;- nrow(mtcars)\nplot(cooks.distance(mod), main = \"Cooks Distance for Influential Obs\")\nabline(h = 4/n, lty = 2, col = \"steelblue\")\n\n\n\n\n\n\n\nCook’s distance refers to how far, on average, predicted y-values will move if the observation in question is dropped from the data set.\nWe can clearly see that four observation in the dataset exceed the 4/n threshold. Thus, we would identify these two observations as influential data points that have a negative impact on the regression model.\nSince there are very few extreme values according to cook’s d values, we can assume they are not the driving force between the relationship of the predictor and outcome.\nTentative model:\nmpg = \\(b_0\\) + \\(b_1\\times\\)horsepower + \\(b_2\\times\\)weight + \\(\\epsilon\\)\n\n21.1.4 Hypothesis Testing\nDoes the horsepower and weight of a car contribute significantly to the miles per gallon of the car?\n\nWhat is the relationship between horsepower and weight, with miles per gallon of the car?-\n\n\\[\nH_0: \\beta_1 = \\beta_2 = 0\n\\]\n\\[\nH_1: \\beta_1 \\not= \\beta_2 \\not= 0\n\\]\n\nCodecar_model &lt;- lm(mpg ~ hp + wt, data = mtcars_df)\n\n# get the F statistic and p-value\nsummary(car_model) \n\n\nCall:\nlm(formula = mpg ~ hp + wt, data = mtcars_df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.941 -1.600 -0.182  1.050  5.854 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 37.22727    1.59879  23.285  &lt; 2e-16 ***\nhp          -0.03177    0.00903  -3.519  0.00145 ** \nwt          -3.87783    0.63273  -6.129 1.12e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.593 on 29 degrees of freedom\nMultiple R-squared:  0.8268,    Adjusted R-squared:  0.8148 \nF-statistic: 69.21 on 2 and 29 DF,  p-value: 9.109e-12\n\nCode# creating a table of the model\ncar_model %&gt;%\n  tidy() %&gt;% \n  gt()\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n(Intercept)\n37.22727012\n1.59878754\n23.284689\n2.565459e-20\n\n\nhp\n-0.03177295\n0.00902971\n-3.518712\n1.451229e-03\n\n\nwt\n-3.87783074\n0.63273349\n-6.128695\n1.119647e-06\n\n\n\n\n\n\n\nThe first step in interpreting the multiple regression analysis is to examine the F-statistic and the associated p-value, at the bottom of model summary.\nIn our example, it can be seen that p-value of the F-statistic is &lt; 9.109e-12, which is less than 0.05. This means that we can reject the null and accept that either horsepower and/or weight of a car predict the miles per gallon.\nTo see which predictor variables are significant, we can examine the coefficients table, which shows the estimate and the associated t-statistic p-values:\n\nCodesummary(car_model)$coefficient \n\n               Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept) 37.22727012 1.59878754 23.284689 2.565459e-20\nhp          -0.03177295 0.00902971 -3.518712 1.451229e-03\nwt          -3.87783074 0.63273349 -6.128695 1.119647e-06\n\nCodesummary(car_model)$coefficient %&gt;% \n  as_tibble() %&gt;% \n  add_column(Variable = c(\"Intercept\", \"Horsepower\", \"Weight\"), .before = \"Estimate\") %&gt;% \n  gt()\n\n\n\n\n\n\nVariable\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\nIntercept\n37.22727012\n1.59878754\n23.284689\n2.565459e-20\n\n\nHorsepower\n-0.03177295\n0.00902971\n-3.518712\n1.451229e-03\n\n\nWeight\n-3.87783074\n0.63273349\n-6.128695\n1.119647e-06\n\n\n\n\n\n\n\nFull model: mpg = 37.23 + (-0.03)\\(\\times\\)horsepower + (-3.88)\\(\\times\\)weight\nFor a given predictor, the t-statistic value evaluates whether or not there is significant association between the predictor and the outcome variable, that is whether the beta coefficient of the predictor is significantly different from zero.\nIt can be seen that horsepower and weight are significantly associated to changes in mpg.\n\nCode# The confidence interval of the model\nconfint(car_model)\n\n                  2.5 %      97.5 %\n(Intercept) 33.95738245 40.49715778\nhp          -0.05024078 -0.01330512\nwt          -5.17191604 -2.58374544\n\nCodeconfint(car_model) %&gt;% \n  as_tibble() %&gt;% \n  add_column(Variable = c(\"Intercept\", \"Horsepower\", \"Weight\"), .before = \"2.5 %\") %&gt;% \n  gt()\n\n\n\n\n\n\nVariable\n2.5 %\n97.5 %\n\n\n\nIntercept\n33.95738245\n40.49715778\n\n\nHorsepower\n-0.05024078\n-0.01330512\n\n\nWeight\n-5.17191604\n-2.58374544\n\n\n\n\n\n\n\n\n21.1.5 Likelihood Ratio Test\n\\(H_0\\): The full model and the nested model fit the data equally well. Thus, you should use the nested model.\n\\(H_1\\): The full model fits the data significantly better than the nested model. Thus, you should use the full model.\nA likelihood ratio test compares the goodness of fit of two nested regression models.\nA nested model is simply one that contains a subset of the predictor variables in the overall regression model.\nWe could then carry out another likelihood ratio test to determine if a model with only one predictor variable is significantly different from a model with the two predictors:\n\n#fit full model\nmodel_full &lt;- lm(mpg ~ hp + wt, data = mtcars)\n\n#fit reduced model\nmodel_reduced &lt;- lm(mpg ~ hp, data = mtcars)\n\n#perform likelihood ratio test for differences in models\nlrtest(model_full, model_reduced)\n\nLikelihood ratio test\n\nModel 1: mpg ~ hp + wt\nModel 2: mpg ~ hp\n  #Df  LogLik Df  Chisq Pr(&gt;Chisq)    \n1   4 -74.326                         \n2   3 -87.619 -1 26.586   2.52e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#fit reduced model\nmodel_reduced &lt;- lm(mpg ~ wt, data = mtcars)\n\n#perform likelihood ratio test for differences in models\nlrtest(model_full, model_reduced)\n\nLikelihood ratio test\n\nModel 1: mpg ~ hp + wt\nModel 2: mpg ~ wt\n  #Df  LogLik Df  Chisq Pr(&gt;Chisq)    \n1   4 -74.326                         \n2   3 -80.015 -1 11.377  0.0007436 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nOur p-values are less than 0.05 for both Chi-squared values, suggesting that the full model offers significant improvement in fit over the model with just one of the predictors.\n\n21.1.6 Model Accuracy\nIn multiple linear regression, the \\(R^2\\) represents the correlation coefficient between the observed values of the outcome variable (y) and the fitted (i.e., predicted) values of y. For this reason, the value of R will always be positive and will range from zero to one.\nAn \\(R^2\\) value close to 1 indicates that the model explains a large portion of the variance in the outcome variable.\n\nCodesummary(car_model)$adj.r.squared %&gt;% \n  as_tibble() %&gt;% \n  rename(\"Adjusted R Squared\" = value) %&gt;% \n  gt()\n\n\n\n\n\n\nAdjusted R Squared\n\n\n0.8148396\n\n\n\n\n\n\nOur model explains 81% of the variation in mpg values caused by horsepower and weight values.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_regression_mls.html#conclusion",
    "href": "lessons_original/04_regression_mls.html#conclusion",
    "title": "\n21  Multiple Linear Regression\n",
    "section": "\n21.2 Conclusion",
    "text": "21.2 Conclusion\n\navPlots(car_model)\n\n\n\n\n\n\n\nFor multiple linear regression, there are multiple independent variables affecting the outcome. This outcome must be continuous, however, the independent variables can be numeric or categorical.\nThere is an inverse or negative relationship between our predictors and outcome variable. The mpg decreases with heavier weight. Higher horsepower also reduces miles per gallon performance (higher fuel consumption).",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_regression_polynomial.html",
    "href": "lessons_original/04_regression_polynomial.html",
    "title": "\n22  Introduction to Polynomial Regression\n",
    "section": "",
    "text": "22.1 What is a Polynomial Regression?\nPolynomial regression is a type of regression analysis that models the non-linear relationship between the predictor variable(s) and response variable1. It is an extension of simple linear regression that allows for more complex relationships between predictor and response variables1.\nIn simple terms, it allows us to fit a curve to our data instead of a straight line.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Introduction to Polynomial Regression</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_regression_polynomial.html#what-is-a-polynomial-regression",
    "href": "lessons_original/04_regression_polynomial.html#what-is-a-polynomial-regression",
    "title": "\n22  Introduction to Polynomial Regression\n",
    "section": "",
    "text": "22.1.1 When is a Polynomial Regression Used?\nPolynomial regression is useful when the relationship between the independent and dependent variables is nonlinear.\nIt can capture more complex relationships than linear regression, making it suitable for cases where the data exhibits curvature.\n\n22.1.2 Assumptions of Polynomial Regression\n\n\nLinearity: There is a curvilinear relationship between the independent variable(s) and the dependent variable.\n\nIndependence: The predictor variables are independent of each other.\n\nHomoscedasticity: The variance of the errors should be constant across all levels of the independent variable(s).\n\nNormality: The errors should be normally distributed with mean zero and a constant variance.\n\n22.1.3 Mathematical Equation\nConsider independent samples \\(i = 1, \\ldots, n\\). The general formula for a polynomial regression representing the relationship between the response variable (\\(y\\)) and the predictor variable (\\(x\\)) as a polynomial function of degree \\(d\\) is:\n\\[\ny_i = \\beta_0 + \\beta_1x_i + \\beta_2x_i^2 + \\beta_3x_i^3 + ... \\beta_dx_i^d + \\epsilon_i,\n\\]\nwhere:\n\n\n\\(y_i\\) represents the response variable,\n\n\\(x_i\\) represents the predictor variable,\n\n\\(\\beta_0,\\ \\beta_1,\\ \\ldots,\\ \\beta_d\\) are the coefficients to be estimated, and\n\n\\(\\epsilon_i\\) represents the errors.\n\nFor large degree \\(d\\), polynomial regression allows us to produce an extremely non-linear curve. Therefore, it is not common to use \\(d &gt; 3\\) because the larger value of \\(d\\), the more overly flexible polynomial curve becomes, which can lead to overfitting them model to the data.\nThe coefficients in polynomial function can be estimated using least square linear regression because it can be viewed as a standard linear model with predictors \\(x_i, \\,x_i^2, \\,x_i^3, ..., x_i^d\\). Hence, polynomial regression is also known as polynomial linear regression.\n\n22.1.4 Performing a Polynomial Regression in R\n\nStep 0: Load required packages\nStep 1: Load and inspect the data\nStep 2: Visualize the data\nStep 3: Fit the model\nStep 4: Assess Assumptions\nStep 5: Describe model output",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Introduction to Polynomial Regression</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_regression_polynomial.html#lets-practice",
    "href": "lessons_original/04_regression_polynomial.html#lets-practice",
    "title": "\n22  Introduction to Polynomial Regression\n",
    "section": "\n22.2 Let’s Practice!",
    "text": "22.2 Let’s Practice!\nNow let’s go through the steps to perform a polynomial regression in R. We’ll be using the lm() function to fit the polynomial regression model. This function comes standard in base R.\n\n22.2.1 Hypotheses\nFor this example, we are investigating the following:\n\n\nResearch Question: Is there a significant quadratic relationship between the weight of a car (wt) and its miles per gallon (mpg) in the mtcars dataset?\n\nNull hypothesis (\\(H_0\\)): There is no significant relationship between the weight of a car (wt) and its miles per gallon (mpg).\n\nAlternative hypothesis (\\(H_A\\)): There is a significant relationship between the weight of a car (wt) and its miles per gallon (mpg).\n\nIn this case, the null hypothesis assumes that the coefficients of the quadratic polynomial terms are zero, indicating no relationship between the weight of the car and miles per gallon. The alternative hypothesis, on the other hand, suggests that at least one of the quadratic polynomial terms is non-zero, indicating a significant relationship between the weight of the car and miles per gallon.\nBy performing the polynomial regression analysis and examining the model summary and coefficients, we can evaluate the statistical significance of the relationship and determine whether to reject or fail to reject the null hypothesis.\n\n22.2.2 Step 0: Install and load required package\nIn R, we’ll use the lm() function from the base package to perform polynomial regression. Also, since we want to visualize our data, we will be loading the ggplot2 package for use.\n\n# For data visualization purposes\n# install.packages(\"ggplot2\")\nlibrary(ggplot2)\n\n\n22.2.3 Step 1: Load and inspect the data\nFor this example, we will use the built-in mtcars dataset (from the standard R package datasets) which is publicly available and contains information about various car models.\n\n# Load mtcars dataset\ndata(mtcars)\n\n\n# Print the first few rows\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\n\n22.2.4 Step 2: Visualize the data\nBefore fitting a polynomial regression model, it’s helpful to visualize the data to identify any non-linear patterns. For our example, we will use a scatter plot to visualize the relationship between the independent and dependent variables:\n\n# Scatter plot of mpg (dependent variable) vs. wt (independent variable)\nggplot(mtcars) +\n  theme_minimal() +\n  aes(x = wt, y = mpg) + \n  labs(x = \"Weight (lbs/1000)\", y = \"Miles per Gallon\") +\n  geom_point()\n\n\n\n\n\n\n\n\n22.2.5 Step 3: Fit Models\nLet’s create a function so we can build multiple models. We will build a standard linear model and a quadratic model (degrees 1 and 2, respectively).\n\n# Function to fit and evaluate polynomial regression models\nfit_poly_regression &lt;- function(degree) {\n  formula &lt;- as.formula(paste(\"mpg ~ poly(wt, \", degree, \")\"))\n  lm(formula, data = mtcars)\n}\n\n# Fit polynomial regression models with degrees 1 to 2\nmodel_1 &lt;- fit_poly_regression(1)\nmodel_2 &lt;- fit_poly_regression(2)\n\nTo fit a polynomial regression model, we’ll use the lm() function and create polynomial terms using the poly() function. In this example, we’ll fit a standard linear (degree = 1) and a quadratic polynomial (degree = 2) to the mtcars dataset.\n\n22.2.6 Step 4: Assess Assumptions\nBefore we can interpret the model, we have to check the assumptions. We will check these assumptions via plots:\n\nResiduals vs. Fitted values (used to check the linearity assumption),\na Q-Q plot of the Residuals (used to check the normality of the residuals),\na Scale-Location plot (used to check for heteroskedasticity), and\nResiduals vs. Leverage values (identifies overly influential values, if any exist).\n\n\npar(mfrow = c(2, 2))\nplot(model_1, which = c(1, 2, 3, 5))\n\n\n\n\n\n\nplot(model_2, which = c(1, 2, 3, 5))\n\n\n\n\n\n\npar(mfrow = c(1, 1))\n\nWe see that Model 2 (the quadratic one) satisfies the “linearity” assumption, because the red line in the “Residuals vs Fitted” graph is flat. However, the Q-Q plot shows that the residuals are not normally distributed, so we should take additional steps to transform the response feature (such as via a square root or log transformation, or something similar).\n\n22.2.7 Step 5. Describe Model Output\nAlthough we recognize that this model is not correct (because the residuals are not approximately normal), we will give an example of how to interpret this output.\n\nsummary(model_2)\n\n\nCall:\nlm(formula = formula, data = mtcars)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.483 -1.998 -0.773  1.462  6.238 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   20.0906     0.4686  42.877  &lt; 2e-16 ***\npoly(wt, 2)1 -29.1157     2.6506 -10.985 7.52e-12 ***\npoly(wt, 2)2   8.6358     2.6506   3.258  0.00286 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.651 on 29 degrees of freedom\nMultiple R-squared:  0.8191,    Adjusted R-squared:  0.8066 \nF-statistic: 65.64 on 2 and 29 DF,  p-value: 1.715e-11\n\n\n\n22.2.8 Bonus Step: Visualize the Final Model\nFinally, let’s plot the scatter plot with the polynomial regression line to visualize the fit:\n\n# Create a data frame with data points and predictions \nplot_data &lt;- data.frame(\n  wt = mtcars$wt,\n  mpg = mtcars$mpg, \n  mpg_predicted = predict(model_2, newdata = mtcars)\n)\n\n# Scatter plot with the polynomial regression line\nggplot(plot_data) +\n  theme_minimal() + \n  aes(x = wt, y = mpg) + \n  labs(\n    title = \"Scatter Plot with Polynomial Regression Line\",\n    x = \"Weight (wt)\",\n    y = \"Miles per Gallon (mpg)\"\n  ) +\n  geom_point() +\n  geom_line(aes(y = mpg_predicted), color = \"red\", size = 1)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Introduction to Polynomial Regression</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_regression_polynomial.html#further-discussion",
    "href": "lessons_original/04_regression_polynomial.html#further-discussion",
    "title": "\n22  Introduction to Polynomial Regression\n",
    "section": "\n22.3 Further Discussion",
    "text": "22.3 Further Discussion\n\n\nPiecewise polynomials: Instead of fitting a high-degree polynomial over the entire range of X, piece- wise polynomial regression involves fitting separate low-degree polynomials over different regions of X. The coefficients βi differ in different parts of the range of X. The points where the coefficients change are called knots. Using more knots leads to a more flexible piecewise polynomial2.\n\nConstraints and spline: the technique of reduce the number of degree of freedom on piecewise polynomial to produce a continuous and naturally smooth fit model on data2.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Introduction to Polynomial Regression</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_regression_polynomial.html#references",
    "href": "lessons_original/04_regression_polynomial.html#references",
    "title": "\n22  Introduction to Polynomial Regression\n",
    "section": "\n22.4 References",
    "text": "22.4 References\n\nField, A. (2013). Discovering Statistics Using IBM SPSS Statistics. (4th ed.). Sage Publications.\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An Introduction to Statistical Learning with Applications in R. (2nd ed.). Publisher. (pp. 290-300)",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Introduction to Polynomial Regression</span>"
    ]
  },
  {
    "objectID": "05_header_generalized-linear-models.html",
    "href": "05_header_generalized-linear-models.html",
    "title": "Generalized Linear Models",
    "section": "",
    "text": "Text Outline\nThis text covers the basics of:",
    "crumbs": [
      "Generalized Linear Models"
    ]
  },
  {
    "objectID": "05_header_generalized-linear-models.html#text-outline",
    "href": "05_header_generalized-linear-models.html#text-outline",
    "title": "Generalized Linear Models",
    "section": "",
    "text": "Exploring data\nOne-sample tests\nTwo-sample tests\nANOVA and linear regression\nGeneralized linear models\nSome special topics\nMaybe a bit about power calculations…",
    "crumbs": [
      "Generalized Linear Models"
    ]
  },
  {
    "objectID": "05_header_generalized-linear-models.html#part-outline",
    "href": "05_header_generalized-linear-models.html#part-outline",
    "title": "Generalized Linear Models",
    "section": "Part Outline",
    "text": "Part Outline\nThis part of the text contains the chapters on various different models within the GLM family:\n\nGeneralized Linear Models: Binary\nGeneralized Linear Models: Ordered\nGeneralized Linear Models: Count (Poisson)\nGeneralized Linear Models: Count (Negative Binomial)",
    "crumbs": [
      "Generalized Linear Models"
    ]
  },
  {
    "objectID": "lessons_original/05_glm_ordinal_logistic.html",
    "href": "lessons_original/05_glm_ordinal_logistic.html",
    "title": "\n23  Ordinal Logistic Regression\n",
    "section": "",
    "text": "23.1 Introduction to Logistic Regression\nWe are all familiar with the concept of Logistic regression. It is used to analyze data when the outcome variables is categorical. There are three types of logistic regression, Binary logistic regression where the outcome variable is binary (Yes/No), Multinomial logistic regression when the outcome variable is categorical with three or more categories, Ordinal logistic regression where there is a natural ordering among three or more categories of the outcome variableagresti2002?.",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Ordinal Logistic Regression</span>"
    ]
  },
  {
    "objectID": "lessons_original/05_glm_ordinal_logistic.html#introduction-to-logistic-regression",
    "href": "lessons_original/05_glm_ordinal_logistic.html#introduction-to-logistic-regression",
    "title": "\n23  Ordinal Logistic Regression\n",
    "section": "",
    "text": "Types of Logistic Regression\n\n\nBinary LR\nMultinomial LR\nOrdinal LR\n\n\n\nNumber of categories?\nTwo\nThree or more\nThree or more\n\n\nOrdering matters?\nNo\nNo\nYes",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Ordinal Logistic Regression</span>"
    ]
  },
  {
    "objectID": "lessons_original/05_glm_ordinal_logistic.html#what-is-ordinal-logistic-regression",
    "href": "lessons_original/05_glm_ordinal_logistic.html#what-is-ordinal-logistic-regression",
    "title": "\n23  Ordinal Logistic Regression\n",
    "section": "\n23.2 What is Ordinal Logistic Regression?",
    "text": "23.2 What is Ordinal Logistic Regression?\nOrdinal logistic regression is a statistical modeling technique used to investigate relationships between predictor variables and ordered ordinal outcome variables. It extends traditional logistic regression to account for the response variable’s inherent ordering, making it suitable for situations where the outcome has multiple levels with unequal intervals.\nFor example, cases when ordinal logistic regression can be applicable are,\n\nLikelihood of agreement : In a survey the responses to the outcome variable is categorized in multiple levels such as, Strongly Disagree, Disagree, Agree, Strongly Agree.\nSatisfaction level: Measuring satisfaction level of a service on a scale like, “very dissatisfied,” “dissatisfied,” “neutral,” “satisfied,” and “very satisfied.”\nPain Intensity: Patients participating in medical research may be asked to rate the intensity of their pain on a scale ranging from “no pain” to “mild pain,” “moderate pain,” and “severe pain.”",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Ordinal Logistic Regression</span>"
    ]
  },
  {
    "objectID": "lessons_original/05_glm_ordinal_logistic.html#how-to-do-ordinary-logistic-regression-in-r",
    "href": "lessons_original/05_glm_ordinal_logistic.html#how-to-do-ordinary-logistic-regression-in-r",
    "title": "\n23  Ordinal Logistic Regression\n",
    "section": "\n23.3 How to do Ordinary Logistic Regression in R",
    "text": "23.3 How to do Ordinary Logistic Regression in R\nSome popular R packages that perform Ordinal/Ordered Logistic Regression are,\n\n\nMASS package : function polr()\n\n\nordinal package: function clm()\n\n\nrms package: function orm()\n\n\nIn this demonstration I will be using polr() from MASS package to conduct the analysis.",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Ordinal Logistic Regression</span>"
    ]
  },
  {
    "objectID": "lessons_original/05_glm_ordinal_logistic.html#mathematical-formulation-of-a-ordinal-model",
    "href": "lessons_original/05_glm_ordinal_logistic.html#mathematical-formulation-of-a-ordinal-model",
    "title": "\n23  Ordinal Logistic Regression\n",
    "section": "\n23.4 Mathematical Formulation of a Ordinal model",
    "text": "23.4 Mathematical Formulation of a Ordinal model\nLet us assume Y is an outcome variable with levels, \\(l = 1, 2, ... , L\\). According to the MASS package the parameterization of the outcome variable Y with l levels is,\n\\[\n\\ln\\left(\\frac{P(Y\\le l)}{P(Y&gt;l)}\\right) =\n  \\zeta - \\eta_{1}X_{1}- \\eta_{2}X_{2} - \\ldots - \\eta_{k}X_{k}\n\\]\nHere,\n\n\n\\(\\zeta\\) is the intercept representing the log-odds of \\(Y\\) being less than or equal to \\(l\\) when the other covariates are 0 or in there reference level. Ordinal logistic regression model has one intercept for each level of Y and the total number of intercepts is \\(L-1\\).\nIn case of categorical predictors, each coefficient \\(\\eta_{k}\\) is the log of odds ratio comparing the odds of \\(Y\\le l\\) at a level compared to the reference category. Taking exponent of this term we get \\(e^{\\eta_{k}}\\) which is the odds ratio comparing the odds of \\(Y\\le l\\) at a level compared to the reference category.\nIn case of continuous predictors, each coefficient \\(\\eta_{k}\\) is the log of odds ratio comparing the odds of \\(Y\\le l\\) between subjects who differ by 1 unit. Taking exponent of this term we get \\(e^{\\eta_{k}}\\) which is the odds ratio comparing the odds of \\(Y\\le l\\) between subjects who differ by 1 unit.\n\nSimilar to binary logistic regression the left hand side of this equation is the log-odds of a probability. In case of binary logistic regression it is log-odds of probability of an event whereas here we consider the cumulative probability up to and a specified level including that level.\n\n23.4.1 Model Assumptions\nThe key assumptions of Ordinary logistic Regression which ensures the validity of the model are as follows,\n\nThe outcome variable is ordered.\nThe predictor variables are either continuous, categorical, or ordinal.\nThere is no multicollinearity among the predictors.\nProportional odds.",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Ordinal Logistic Regression</span>"
    ]
  },
  {
    "objectID": "lessons_original/05_glm_ordinal_logistic.html#example",
    "href": "lessons_original/05_glm_ordinal_logistic.html#example",
    "title": "\n23  Ordinal Logistic Regression\n",
    "section": "\n23.5 Example",
    "text": "23.5 Example\nTo demonstrate the methods I will be using the arthritis data from multgee package. The data has Rheumatoid self-assessment scores for 302 patients, measured on a five-level ordinal response scale at three follow-up times. The arthritis dataset is in a data frame with 906 observations with the following 7 variables:\n\n\nid: Patient identifier variable.\n\ny: Self-assessment score of rheumatoid arthritis measured on a five-level ordinal response scale, 1 being the lowest.\n\nsex: Coded as (1) for female and (2) for male.\n\nage: Recorded at the baseline.\n\ntrt: Treatment group variable, coded as (1) for the placebo group and (2) for the drug group.\n\nbaseline: Self-assessment score of rheumatoid arthritis at the baseline.\n\ntime: Follow-up time recorded in months.\n\n\n23.5.1 Libraries\nHere are libraries required to run the analysis.\n\n# install.packages(\"multgee\")\n# install.packages(\"pander\")\n# install.packages(\"table1\")\n# install.packages(\"car\")\n# install.packages(\"mltools\")\n# install.packages(\"pomcheckr\")\nlibrary(conflicted)\nlibrary(table1)\nlibrary(multgee)\nlibrary(skimr)\nlibrary(pander)\nlibrary(gtsummary)\nlibrary(car)\nlibrary(mltools)\nlibrary(MASS)\nlibrary(pomcheckr)\n\nconflict_prefer(\"filter\", \"dplyr\", quiet = TRUE)\nconflict_prefer(\"select\", \"dplyr\", quiet = TRUE)\nlibrary(tidyverse)\n\n\n23.5.1.1 Warning\nInstead of installing package MASS to the global environment use MASS::polr() for running the Ordinal Logistic Regression model. As masking it conflicts wirh the select() function for tidyverse and gtsummary().\n\n23.5.2 Exploring data\nLet’s begin by looking at the data.\n\narthritis_df &lt;- \n  multgee::arthritis %&gt;%\n  mutate(\n    y = factor(y, ordered = TRUE),\n    sex = factor(\n      sex,\n      levels = c(1, 2),\n      labels = c(\"Female\", \"Male\")\n    ),\n    treatment = factor(\n      trt,\n      levels = c(\"1\", \"2\"),\n      labels = c(\"Placebo\", \"Drug\")\n    ),\n    baseline = factor(baseline, ordered = TRUE)\n  ) %&gt;%\n  select(\"y\", \"sex\", \"age\", \"treatment\", \"baseline\") %&gt;%\n  drop_na() %&gt;% \n  as_tibble()\n\n\n23.5.2.1 Summary\n\nskim(arthritis_df)\n\n\nData summary\n\n\nName\narthritis_df\n\n\nNumber of rows\n888\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n4\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\ny\n0\n1\nTRUE\n5\n3: 345, 4: 275, 2: 159, 5: 76\n\n\nsex\n0\n1\nFALSE\n2\nMal: 645, Fem: 243\n\n\ntreatment\n0\n1\nFALSE\n2\nDru: 445, Pla: 443\n\n\nbaseline\n0\n1\nTRUE\n5\n3: 407, 2: 215, 4: 166, 1: 67\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\nage\n0\n1\n50.43\n11.09\n21\n42\n54\n60\n66\n▁▃▃▇▇\n\n\n\n\n\n23.5.2.2 Descriptives\n\narthritis_df %&gt;% \n  tbl_summary(by = treatment) \n\n\nTable 23.1: Predictors by treatment group\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nPlacebo, N = 4431\n\n\nDrug, N = 4451\n\n\n\n\ny\n\n\n\n\n    1\n26 (5.9%)\n7 (1.6%)\n\n\n    2\n96 (22%)\n63 (14%)\n\n\n    3\n165 (37%)\n180 (40%)\n\n\n    4\n129 (29%)\n146 (33%)\n\n\n    5\n27 (6.1%)\n49 (11%)\n\n\nsex\n\n\n\n\n    Female\n127 (29%)\n116 (26%)\n\n\n    Male\n316 (71%)\n329 (74%)\n\n\nage\n55 (42, 60)\n53 (42, 59)\n\n\nbaseline\n\n\n\n\n    1\n33 (7.4%)\n34 (7.6%)\n\n\n    2\n105 (24%)\n110 (25%)\n\n\n    3\n207 (47%)\n200 (45%)\n\n\n    4\n83 (19%)\n83 (19%)\n\n\n    5\n15 (3.4%)\n18 (4.0%)\n\n\n\n\n1 n (%); Median (IQR)\n\n\n\n\n\n\n\n\n\n\n23.5.2.3 Plotting Outcome variable (rheumatoid arthritis score)\n\narthritis_df %&gt;% \n  count(y) %&gt;% \n  mutate(prop = n / sum(n)) %&gt;% \n  rename(score = y) %&gt;% \n  ggplot() + \n    aes(x = score, y = prop) +\n    labs(\n      x = \"Rheumatoid Arthritis Score\", \n      y = \"Relative Frequencies (w Obs. Count)\"\n    ) +\n    scale_y_continuous(labels = scales::percent) +\n    geom_col() +\n    geom_text(aes(label = n), vjust = 1.5, color = \"white\")\n\n\n\n\n\n\n\n\n23.5.2.4 Pairs\n\nGGally::ggpairs(arthritis_df)\n\n\n\n\n\n\n\n\n23.5.3 How to use polr()\n\nThe basic structure of the function looks like this (there are other options we don’t list, but we won’t need them):\n\npolr(\n  # Two required arguments\n  formula,\n  data,\n  # Optional stuff\n  weights,\n  subset,\n  na.action,\n  Hess = FALSE,\n  method = \"logistic\"\n)\n\nHere,\n\n\nformula: a formula expression as for regression models, of the form response ~ predictors. The response should be a factor (preferably an ordered factor), which will be interpreted as an ordinal response, with levels ordered as in the factor.\n\ndata: a data frame which contains the variables occurring in formula.\n\nweights: optional case weights in fitting. Defaults to 1.\n\nsubset: expression saying which subset of the rows of the data should be used in the fit. All observations are included by default.\n\nna.action: a function to filter missing data. We removed all the missing values from our data, so we won’t use this.\n\nHess: logical for whether the Hessian (the observed information matrix) should be returned. We will use this if we intend to call summary or variance covariance on the fit.\n\nmethod: \"logistic\", \"probit\", \"loglog\" (log-log), \"cloglog\" (complementary log-log), or \"cauchit\" (corresponding to a Cauchy latent variable). The default option is to use the Logistic link function.\n\n23.5.4 Fitting the model\nUsing this function, let’s fit the POLR model to the data. By default, the Hess option is turned off, so we turn it on so that we can calculate the odds ratios later.\n\nfit_olr_mod &lt;- MASS::polr(y ~ ., data = arthritis_df, Hess = TRUE)\n\nThe output is a bit ugly, so we clean it up for more professional documents using the pander() function (from the pander:: and knitr:: packages).\n\npander(summary(fit_olr_mod))\n\nCall: MASS::polr(formula = y ~ ., data = arthritis_df, Hess = TRUE)\n\nCoeficients\n\n\n\n\n\n\n\n \nValue\nStd. Error\nt value\n\n\n\nsexMale\n0.1513\n0.1377\n1.099\n\n\nage\n-0.01366\n0.005713\n-2.391\n\n\ntreatmentDrug\n0.5454\n0.1255\n4.347\n\n\nbaseline.L\n3.109\n0.2826\n11\n\n\nbaseline.Q\n0.6897\n0.233\n2.96\n\n\nbaseline.C\n0.09577\n0.1796\n0.5334\n\n\nbaseline^4\n-0.1802\n0.1239\n-1.455\n\n\n\n\nIntercepts\n\n\n\n\n\n\n\n \nValue\nStd. Error\nt value\n\n\n\n1|2\n-4.244\n0.3731\n-11.38\n\n\n2|3\n-2.162\n0.339\n-6.378\n\n\n3|4\n-0.2073\n0.3352\n-0.6186\n\n\n4|5\n2.094\n0.3429\n6.108\n\n\n\nResidual Deviance: 2238.917\nAIC: 2260.917\n\n\nThe summary() function called on a polr model object gives us the coefficients, intercepts, their standard errors, and \\(t\\)-statistics.\n\n23.5.5 Odds Ratio\nIn order to get the Odds Ratio and the predictor’s confidence intervals we take the exponential of the coefficient. There is no straight forward way of doing that in R. Below is one way of solving that issue, which uses the confint() function from the multgee:: package.\n\n# Calculate a matrix of the lower and upper confidence intervals\nCI_mat &lt;- confint(fit_olr_mod)\n\n# Combine results and make them \"pretty\"\norResults_df &lt;- tibble(\n  variable = rownames(CI_mat),\n  oddsRatio = exp(fit_olr_mod$coefficients),\n  lower = exp(CI_mat[, 1]),\n  upper = exp(CI_mat[, 2])\n)\n\npander(orResults_df)\n\n\n\n\n\n\n\n\n\nvariable\noddsRatio\nlower\nupper\n\n\n\nsexMale\n1.163\n0.8883\n1.524\n\n\nage\n0.9864\n0.9754\n0.9975\n\n\ntreatmentDrug\n1.725\n1.35\n2.208\n\n\nbaseline.L\n22.41\n12.94\n39.27\n\n\nbaseline.Q\n1.993\n1.266\n3.161\n\n\nbaseline.C\n1.101\n0.7745\n1.567\n\n\nbaseline^4\n0.8351\n0.6548\n1.064\n\n\n\n\n\n\n23.5.6 Interpreting the Results\n\n\nSex: Compared to female participants, Male participants had orResults_df[1, \"oddsRatio\", drop = TRUE] fold higher odds of reporting high score of rheumatoid arthritis.\n\nAge: For 1 year change in age the odds of reporting high rheumatoid arthritis score changes orResults_df[2, \"oddsRatio\", drop = TRUE] times.\n\nTreatment: Compared to the Placebo group participants, the participant who received the drug had orResults_df[3, \"oddsRatio\", drop = TRUE] times higher odds of reporting high score of rheumatoid arthritis.\n\nBaseline score: It appears that we could perhaps collapse the baseline rheumatoid arthritis score into only three levels, because the confidence intervals for the cubic and quartic polynomial components include 1.\n\n23.5.7 Checking Assumptions\nNext we check the key assumptions to verify whether the model is appropriate to use.\n\n23.5.7.1 Multicollinearity\nTwo of our predictors are binary, one predictor is continuous, and one predictor and the response are ordered. Because of this, there are no “standard” functions to calculate the correlation matrix of these predictors. However, we will use indicator encoding to transform the binary predictors, and we will use polynomial encoding to transform the ordered predictor (polynomial encoding represents ordered predictors as a set of polynomial terms with \\(G-1\\) components, where \\(G\\) is the number of categories). Both actions were already done “behind the scenes” by the polr() function, so we simply need to access the “model matrix” object. We do this via the model.matrix() function, but we remove the first column because it represents the intercept.\n\n# Extract the encoded features used by the POLR model, dropping the intercept\nmodel.matrix(fit_olr_mod)[, -1] %&gt;% \n  # calculate the correlation matrix of the predictors (using the \"spearman\" \n  #   option because some of the predictors are binary)\n  cor(method = \"spearman\")\n\n                   sexMale          age treatmentDrug   baseline.L  baseline.Q\nsexMale        1.000000000 -0.004979624   0.029167450 -0.028388501  0.02835442\nage           -0.004979624  1.000000000  -0.041135132 -0.113609702  0.06570578\ntreatmentDrug  0.029167450 -0.041135132   1.000000000 -0.003582072  0.01949695\nbaseline.L    -0.028388501 -0.113609702  -0.003582072  1.000000000 -0.19856768\nbaseline.Q     0.028354417  0.065705778   0.019496947 -0.198567680  1.00000000\nbaseline.C    -0.045464607  0.090816675   0.009951240 -0.573972635 -0.02112136\nbaseline^4    -0.036878173 -0.043472765  -0.015992296  0.306681433 -0.78207212\n               baseline.C  baseline^4\nsexMale       -0.04546461 -0.03687817\nage            0.09081667 -0.04347276\ntreatmentDrug  0.00995124 -0.01599230\nbaseline.L    -0.57397264  0.30668143\nbaseline.Q    -0.02112136 -0.78207212\nbaseline.C     1.00000000 -0.31490991\nbaseline^4    -0.31490991  1.00000000\n\n\nThe correlation is quite low among most of the predictors. However, we see that the quadratic and quartic (.Q and ^4, respectively) components are just under the “let’s worry about this” threshold of 0.8 in absolute value. This suggests to me that the highest two categories of the rheumatoid arthritis score at baseline (4 and 5) can probably be collapsed without losing a lot of information. However, we’ll keep things simple for now and say there is no multicollinearity.\n\n23.5.7.2 Proportional Odds\nOrdinal logistic regression makes the assumption that the relationship between each pair of outcome groups is the same. In other words, ordinal logistic regression assumes that the coefficients describing the relationship between, say, the lowest and all higher categories of the response variable are the same as those describing the relationship between the next lowest and all higher categories, and so on. This assumption can be vefied several ways. Here, I have used a package calledpomcheckr? that generates graphics to check for proportional odds assumption created by UCLA statistical consulting group see more here .\n\n23.5.7.2.1 Graphics to check for proportional odds\n\npomchk &lt;- pomcheck(\n  y ~ sex + age + treatment + baseline,\n  data = arthritis_df\n)\nplot(pomchk)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere the function is calculating the difference in proportion of the categories in the outcome variable and plotting them against each category of the predictors. In the ideal case scenario, the distance between the dots in each line is somewhat equal; if this is true, then the categories should be considered proportional. It appears that the proportional odds assumption is violated here.\nAdd discussion on how to fix the assumption violation.",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Ordinal Logistic Regression</span>"
    ]
  },
  {
    "objectID": "lessons_original/05_glm_ordinal_logistic.html#references",
    "href": "lessons_original/05_glm_ordinal_logistic.html#references",
    "title": "\n23  Ordinal Logistic Regression\n",
    "section": "References",
    "text": "References",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Ordinal Logistic Regression</span>"
    ]
  },
  {
    "objectID": "06_header_special-topics.html",
    "href": "06_header_special-topics.html",
    "title": "Special Topics in Statistical Modelling",
    "section": "",
    "text": "Text Outline\nThis text covers the basics of:",
    "crumbs": [
      "Special Topics in Statistical Modelling"
    ]
  },
  {
    "objectID": "06_header_special-topics.html#text-outline",
    "href": "06_header_special-topics.html#text-outline",
    "title": "Special Topics in Statistical Modelling",
    "section": "",
    "text": "Exploring data\nOne-sample tests\nTwo-sample tests\nANOVA and linear regression\nGeneralized linear models\nSome special topics\nMaybe a bit about power calculations…",
    "crumbs": [
      "Special Topics in Statistical Modelling"
    ]
  },
  {
    "objectID": "06_header_special-topics.html#part-outline",
    "href": "06_header_special-topics.html#part-outline",
    "title": "Special Topics in Statistical Modelling",
    "section": "Part Outline",
    "text": "Part Outline\nThis part of the text contains the chapters on various different special statistical models:\n\nLinear Mixed Effects Models\nStructural Equation Models\nCox Proportional Hazards Regression\n(TBD) Multivariate Methods for Genetics/Genomics\nRidge, LASSO, and Elastic Net Regression",
    "crumbs": [
      "Special Topics in Statistical Modelling"
    ]
  },
  {
    "objectID": "lessons/06_lesson_template_spec_topc.html",
    "href": "lessons/06_lesson_template_spec_topc.html",
    "title": "\n24  The Method\n",
    "section": "",
    "text": "24.1 Introduction to &lt;the method&gt;",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>The Method</span>"
    ]
  },
  {
    "objectID": "lessons/06_lesson_template_spec_topc.html#mathematical-definition-of-the-method",
    "href": "lessons/06_lesson_template_spec_topc.html#mathematical-definition-of-the-method",
    "title": "\n24  The Method\n",
    "section": "\n24.2 Mathematical definition of <the method>",
    "text": "24.2 Mathematical definition of &lt;the method&gt;",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>The Method</span>"
    ]
  },
  {
    "objectID": "lessons/06_lesson_template_spec_topc.html#data-source-and-description",
    "href": "lessons/06_lesson_template_spec_topc.html#data-source-and-description",
    "title": "\n24  The Method\n",
    "section": "\n24.3 Data source and description",
    "text": "24.3 Data source and description",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>The Method</span>"
    ]
  },
  {
    "objectID": "lessons/06_lesson_template_spec_topc.html#cleaning-the-data-to-create-a-model-data-frame",
    "href": "lessons/06_lesson_template_spec_topc.html#cleaning-the-data-to-create-a-model-data-frame",
    "title": "\n24  The Method\n",
    "section": "\n24.4 Cleaning the data to create a model data frame",
    "text": "24.4 Cleaning the data to create a model data frame",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>The Method</span>"
    ]
  },
  {
    "objectID": "lessons/06_lesson_template_spec_topc.html#assumptions-of-the-method",
    "href": "lessons/06_lesson_template_spec_topc.html#assumptions-of-the-method",
    "title": "\n24  The Method\n",
    "section": "\n24.5 Assumptions of <the method>",
    "text": "24.5 Assumptions of &lt;the method&gt;",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>The Method</span>"
    ]
  },
  {
    "objectID": "lessons/06_lesson_template_spec_topc.html#checking-the-assumptions-with-plots",
    "href": "lessons/06_lesson_template_spec_topc.html#checking-the-assumptions-with-plots",
    "title": "\n24  The Method\n",
    "section": "\n24.6 Checking the assumptions with plots",
    "text": "24.6 Checking the assumptions with plots",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>The Method</span>"
    ]
  },
  {
    "objectID": "lessons/06_lesson_template_spec_topc.html#code-to-run-the-method",
    "href": "lessons/06_lesson_template_spec_topc.html#code-to-run-the-method",
    "title": "\n24  The Method\n",
    "section": "\n24.7 Code to run <the method>",
    "text": "24.7 Code to run &lt;the method&gt;",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>The Method</span>"
    ]
  },
  {
    "objectID": "lessons/06_lesson_template_spec_topc.html#code-output",
    "href": "lessons/06_lesson_template_spec_topc.html#code-output",
    "title": "\n24  The Method\n",
    "section": "\n24.8 Code output",
    "text": "24.8 Code output\nNOTE: this section will be created automatically by the Quarto document. You should not create a section specifically for this. When you run the code in the previous section, you will get the output automatically.",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>The Method</span>"
    ]
  },
  {
    "objectID": "lessons/06_lesson_template_spec_topc.html#brief-interpretation-of-the-output",
    "href": "lessons/06_lesson_template_spec_topc.html#brief-interpretation-of-the-output",
    "title": "\n24  The Method\n",
    "section": "\n24.9 Brief interpretation of the output",
    "text": "24.9 Brief interpretation of the output",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>The Method</span>"
    ]
  },
  {
    "objectID": "07_header_power.html",
    "href": "07_header_power.html",
    "title": "Statistical Power and Sample Size Determination",
    "section": "",
    "text": "Text Outline\nThis text covers the basics of:",
    "crumbs": [
      "Statistical Power and Sample Size Determination"
    ]
  },
  {
    "objectID": "07_header_power.html#text-outline",
    "href": "07_header_power.html#text-outline",
    "title": "Statistical Power and Sample Size Determination",
    "section": "",
    "text": "Exploring data\nOne-sample tests\nTwo-sample tests\nANOVA and linear regression\nGeneralized linear models\nSome special topics\nMaybe a bit about power calculations…",
    "crumbs": [
      "Statistical Power and Sample Size Determination"
    ]
  },
  {
    "objectID": "07_header_power.html#part-outline",
    "href": "07_header_power.html#part-outline",
    "title": "Statistical Power and Sample Size Determination",
    "section": "Part Outline",
    "text": "Part Outline\nThis part of the text will eventually contain some examples on statistical power calculations and sample size determination methods for some of the techniques covered in this text.",
    "crumbs": [
      "Statistical Power and Sample Size Determination"
    ]
  },
  {
    "objectID": "lessons_original/07_power_ols.html",
    "href": "lessons_original/07_power_ols.html",
    "title": "\n25  Power Analysis for OLS Regression\n",
    "section": "",
    "text": "25.1 Introduction\nThe power of a hypothesis test is the probability of correctly rejecting the null hypothesis or the probability that the test will correctly support the alternative hypothesis (detecting an effect when there actually is one)1. Then,\n\\[\nPower = 1-\\beta\n\\]\nWhere, \\(\\beta\\) = probability of committing a Type II Error (the probability that we would accept the null hypothesis even if the alternative hypothesis is actually true). Then, by decreasing \\(\\beta\\) power increases [@(pdf)ef].\nPower is mainly influenced by sample size, effect size, and significance level.",
    "crumbs": [
      "Statistical Power and Sample Size Determination",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Power Analysis for OLS Regression</span>"
    ]
  },
  {
    "objectID": "lessons_original/07_power_ols.html#introduction",
    "href": "lessons_original/07_power_ols.html#introduction",
    "title": "\n25  Power Analysis for OLS Regression\n",
    "section": "",
    "text": "High power: large chance of a test detecting a true effect.\nLow power: test only has a small chance of detecting a true effect or that the results are likely to be distorted by random and systematic error.\n\n\n\n\n\nVisual view of beta",
    "crumbs": [
      "Statistical Power and Sample Size Determination",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Power Analysis for OLS Regression</span>"
    ]
  },
  {
    "objectID": "lessons_original/07_power_ols.html#power-analysis-ols-regression",
    "href": "lessons_original/07_power_ols.html#power-analysis-ols-regression",
    "title": "\n25  Power Analysis for OLS Regression\n",
    "section": "\n25.2 Power Analysis: OLS Regression",
    "text": "25.2 Power Analysis: OLS Regression\nFor this power analysis we will use the univariate (simple) OLS regression example of our last presentation examining the relationship between a vehicle’s weight (WT) and its miles per gallon or fuel efficiency (MPG).\nWhen performed, the paired correlation provided us with a pearson’s correlation coefficient of r(30) = -.868, p&lt;0.05, (n = 32). When we ran this regression we got an (\\(R^2\\) = .75) Therefore for the r2 value (effect size) for a power analysis we will begin with an r2 value of .75 and an n = 32 to account for the observations already collected. However, the power analysis should occur before collecting samples so that we can have an appropriate number of observations required for our hypothesized effect size. In our example, we are also assuming that the variables are normally distributed. Based on our correlation analysis, weight likely needs a cubic transformation, This would mean that our model would have three coefficients of interest.\nFormula for a univariate Ordinary Least Squares (OLS) Regression:\n\\[\n\\hat{y}_i = \\beta_0 + \\beta_1x_i\n\\]\nThe OLS regression model line for our example is:\n\\[\n\\widehat{MPG_i} = \\beta_0 +\\beta_1*WT_i\n\\]\nUsing an alpha value of \\(\\alpha\\) = .05 (The probability of a type I error/rejecting a correct \\(H_0\\), we will identify the number of observations or sample size (n) necessary to obtain statistical power (80% or \\(\\beta\\) = 0.20) given various effect sizes. Statistical power in our example identifies the likelihood that a univariate OLS will detect an effect of a certain size if there is one.\nA power analysis is made up of four main components. We will provide estimates for any three of these, as the following functions in r calculate the fourth component.\nWe found three functions in r to conduct power analyses for an OLS regression:\n\nThe pwrss.f.reg function in the pwrss package\nThe pwr.f2.test function in the pwr package\nThe wp.regression function in the WebPower package\n\n\n25.2.1 The pwrss.f.reg function\nWe will start our power analysis using the The pwrss.f.reg function for one predictor in an OLS regression, with our given observations of n = 32 and \\(R^2\\) = .75. Given these values, we are expecting that one variable (WT) explains 75% of the variance in the outcome or Miles per gallon (R2=0.75 or r2 = 0.75 in the code)2.\n\nRegOne_lm &lt;- pwrss.f.reg(\n  r2 = 0.75,\n  k = 1,\n  n = 32,\n  power = NULL,\n  alpha = 0.05\n)\n\n Linear Regression (F test) \n R-squared Deviation from 0 (zero) \n H0: r2 = 0 \n HA: r2 &gt; 0 \n ------------------------------ \n  Statistical power = 1 \n  n = 32 \n ------------------------------ \n Numerator degrees of freedom = 1 \n Denominator degrees of freedom = 30 \n Non-centrality parameter = 96 \n Type I error rate = 0.05 \n Type II error rate = 0 \n\nRegOne_lm$power\n\n[1] 1\n\n\nGiven the information provided, we get 100% power. Our effect size of r2 = 0.75 is considered a large effect provided the following guidelines by Cohen (1988)3\n\\(f^2\\) = 0.02 indicates a small effect;\n\\(f^2\\) = 0.15 indicates a medium effect;\n\\(f^2\\) = 0.35 indicates a large effect.\nWe will use these guidelines to continue our exploration. We will concentrate on a fixed medium effect size. Where, the paired correlation is approximately r = .40 for a medium correlation and for an \\(f^2\\) or effect size of 0.15. using this fixed effect, we will look at various sample sizes to obtain power of 80% or greater given a medium effect size. In our sequence of possible sample sizes, the minimum n = 1 as n &gt; p(p+1)/2 = 1(2)/2 = 1\n\nOLSReg_df &lt;- tibble(n = seq.int(from = 2, to = 99 + 2))\n\nOLSReg_df$power &lt;- map_dbl(\n  .x = OLSReg_df$n,\n  .f = ~{\n    out_ls &lt;- pwrss.f.reg(\n      # \"Effect size\" of a linear model\n      r2 = 0.15,\n      # number of predictors\n      k = 1,\n      # sample size\n      n = .x,\n      power = NULL,\n      alpha = 0.05,\n      # Stop printing messages\n      verbose = FALSE\n    )\n    out_ls$power\n  }\n)\n\nWarning in qf(alpha, df1 = u, df2 = v, lower.tail = FALSE): NaNs produced\n\n\nThe following is the power curve for a fixed effect of f2 = 0.15\n\nggplot(data = OLSReg_df) +\n  theme_bw() +\n  aes(x = n, y = power) +\n  labs(\n    title = \"Omnibus (F-test) Power for Linear Model\",\n    subtitle = \"Fixed Effect Size R2 = 0.15, 1 Predictor\"\n  ) +\n  scale_y_continuous(limits = c(0, 1)) +\n  geom_point() +\n  geom_abline(slope = 0, intercept = 0.8, colour = \"gold\")\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nGiven the graph, we notice that we need an approximate sample size or n of close to 50 to detect a medium effect size in an OLS Regression.\nThe following is a power analysis for a univariate OLS regression given a fixed sample size. We will create a sequence of effect sizes that capture Cohen’s guidelines as well as the effect size of 0.75 of our sample regression. Our fixed n will be n = 32 as the sample.\n\nOLSRegN_df &lt;- tibble(R2 = seq(0, 0.75, length.out = 100))\n\nOLSRegN_df$power &lt;- map_dbl(\n  .x = OLSRegN_df$R2,\n  .f = ~{\n    out2_ls &lt;- pwrss.f.reg(\n      # \"Effect size\" of a linear model\n      r2 = .x,\n      # number of predictors\n      k = 1,\n      # sample size\n      n = 32,\n      power = NULL,\n      alpha = 0.05,\n      # Stop printing messages\n      verbose = FALSE\n    )\n    out2_ls$power\n  }\n)\n\nThe following is the power curve for a fixed sample size of n = 32\n\nggplot(data = OLSRegN_df) +\n  theme_bw() +\n  aes(x = R2, y = power) +\n  labs(\n    title = \"Omnibus (F-test) Power for Linear Model\",\n    subtitle = \"Fixed Sample Size = 32, 1 Predictor\"\n  ) +\n  scale_y_continuous(limits = c(0, 1)) +\n  geom_point() +\n  geom_abline(slope = 0, intercept = 0.8, colour = \"red\")\n\n\n\n\n\n\n\nGiven the graph, we notice that given n = 32, a power of 80% and higher is achieved when the effect size is at least approximately r2 = 0.20.\n\n25.2.2 The pwr.f2.test function\nPower analysis using the pwr.f2.test: where, u = 1, The F numerator degrees of freedom (u=1) or the number of coefficients(independent variables) in the model\nand we will use Cohen’s criteria for effect sizes and first provide analyses for a medium effect size of 0.15 [3]45\n\n# Using Cohen 1988 criteria, where, \n#f2 = 0.02 small effect;\n#f2 = 0.15 medium effect;\n#f2 = 0.35 indicates a large effect\n\n### Fixed Effect size f2 = 0.15###\n# n = 50\npwr.f2.test(\n  u = 1, \n  v = 50 - 1 - 1,\n  f2 = .15, \n  sig.level = 0.05, \n  power = NULL\n  )\n\n\n     Multiple regression power calculation \n\n              u = 1\n              v = 48\n             f2 = 0.15\n      sig.level = 0.05\n          power = 0.7653128\n\n# n = 25\npwr.f2.test(\n  u = 1, \n  v = 25 - 1 - 1,\n  f2 = 0.15, \n  sig.level = 0.05, \n  power = NULL\n  )\n\n\n     Multiple regression power calculation \n\n              u = 1\n              v = 23\n             f2 = 0.15\n      sig.level = 0.05\n          power = 0.4584646\n\n# n = 12\npwr.f2.test(\n  u = 1, \n  v = 12 - 1 - 1,\n  f2 = 0.15, \n  sig.level = 0.05, \n  power = NULL\n  )\n\n\n     Multiple regression power calculation \n\n              u = 1\n              v = 10\n             f2 = 0.15\n      sig.level = 0.05\n          power = 0.2289402\n\n\nNow, we will explore a fixed n = 32\n\n# ES = .02, r = .14\npwr.f2.test(\n  u = 1, \n  v = 32 - 1 - 1,\n  f2 = .02, \n  sig.level = 0.05, \n  power = NULL\n  )\n\n\n     Multiple regression power calculation \n\n              u = 1\n              v = 30\n             f2 = 0.02\n      sig.level = 0.05\n          power = 0.1210661\n\n# ES = 0.15, r = .39\npwr.f2.test(\n  u = 1, \n  v = 32 - 1 - 1,\n  f2 = 0.15, \n  sig.level = 0.05, \n  power = NULL\n  ) \n\n\n     Multiple regression power calculation \n\n              u = 1\n              v = 30\n             f2 = 0.15\n      sig.level = 0.05\n          power = 0.5637733\n\n# ES = .35, r = .59\npwr.f2.test(\n  u = 1, \n  v = 32 - 1 - 1,\n  f2 = 0.35, \n  sig.level = 0.05, \n  power = NULL\n  )\n\n\n     Multiple regression power calculation \n\n              u = 1\n              v = 30\n             f2 = 0.35\n      sig.level = 0.05\n          power = 0.8993357\n\n\nWe will now look at the 3 types of effect sizes given various sample sizes\n\neffect_sizes &lt;- c(0.02, 0.15, 0.35) \nsample_sizes = seq(20, 100, 20)\n\ninput_df &lt;- crossing(effect_sizes,sample_sizes)\nglimpse(input_df)\n\nRows: 15\nColumns: 2\n$ effect_sizes &lt;dbl&gt; 0.02, 0.02, 0.02, 0.02, 0.02, 0.15, 0.15, 0.15, 0.15, 0.1…\n$ sample_sizes &lt;dbl&gt; 20, 40, 60, 80, 100, 20, 40, 60, 80, 100, 20, 40, 60, 80,…\n\nget_power &lt;- function(df){\n  power_result &lt;- pwr.f2.test(\n    u = 1,\n    v = df$sample_sizes - 1 - 1, \n    f2 = df$effect_sizes,\n    )\n  df$power=power_result$power\n  return(df)\n}\n\n# run get_power for each combination of effect size \n# and sample size\n\npower_curves &lt;- input_df %&gt;%\n  do(get_power(.)) %&gt;%\n  mutate(effect_sizes = as.factor(effect_sizes)) \n\n\nggplot(power_curves, \n       aes(x=sample_sizes,\n           y=power, \n           color=effect_sizes)\n       ) + \n  geom_line() + \n  geom_hline(yintercept = 0.8, \n             linetype='dotdash',\n             color = \"purple\")\n\n\n\n\n\n\n\nBased on the graph, if we have an effect size of 0.15, we need approximately 50 or more observations (recall n = v + 1 + 1)\n\n25.2.3 The wp.regression function\nLastly, we use the wp.regression function to examine the appropriate sample size given an effect size of 0.15 to achieve a power of 80% or higher [6]7\n\n# Using webpower \n#p1 = 1\n### Fixed ES = 0.15 ###\nres &lt;- wp.regression(n = seq(20,100,20), \n                     p1 = 1, \n                     f2 = 0.15, \n                     alpha = 0.05, \n                     power = NULL\n                    )\nres\n\nPower for multiple regression\n\n      n p1 p2   f2 alpha     power\n     20  1  0 0.15  0.05 0.3745851\n     40  1  0 0.15  0.05 0.6654126\n     60  1  0 0.15  0.05 0.8389166\n     80  1  0 0.15  0.05 0.9280168\n    100  1  0 0.15  0.05 0.9695895\n\nURL: http://psychstat.org/regression\n\nplot(res,  main = \"Fixed Effect Size = 0.15\")+\nabline(a = .80, b = 0, col = 'steelblue', lwd = 3, lty = 2)\n\n\n\n\n\n\n\ninteger(0)\n\n\nThe results are similar to the previous functions. However, in this function, given an effect size of 0.15, we need an n of close to 60 to achieve 80% power.\n\n# Using webpower \n#p1 = 1\n### Fixed n = 50 ###\nres &lt;- wp.regression(n = 50, \n                     p1 = 1, \n                     f2 = seq(0.00, 0.35, 0.05), \n                     alpha = 0.05, \n                     power = NULL\n                    )\nres\n\nPower for multiple regression\n\n     n p1 p2   f2 alpha     power\n    50  1  0 0.00  0.05 0.0500000\n    50  1  0 0.05  0.05 0.3409707\n    50  1  0 0.10  0.05 0.5914439\n    50  1  0 0.15  0.05 0.7653128\n    50  1  0 0.20  0.05 0.8725329\n    50  1  0 0.25  0.05 0.9337077\n    50  1  0 0.30  0.05 0.9667049\n    50  1  0 0.35  0.05 0.9837529\n\nURL: http://psychstat.org/regression",
    "crumbs": [
      "Statistical Power and Sample Size Determination",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Power Analysis for OLS Regression</span>"
    ]
  },
  {
    "objectID": "lessons_original/07_power_ols.html#references",
    "href": "lessons_original/07_power_ols.html#references",
    "title": "\n25  Power Analysis for OLS Regression\n",
    "section": "References",
    "text": "References\n\n\n\n\n1. \nCohen J. Statistical power analysis for the behavioral sciences. 2nd ed. Hillsdale, N.J: L. Erlbaum Associates; 1988. \n\n\n2. \nA practical guide to statistical power and sample size calculations in r [Internet]. Available from: https://cran.r-project.org/web/packages/pwrss/vignettes/examples.html#3_Linear_Regression_(F_and_t_Tests)\n\n\n\n3. \nCohen J. Statistical power analysis for the behavioral sciences. 2nd ed. Hillsdale, N.J: L. Erlbaum Associates; 1988. \n\n\n4. \nPower in r | [Internet]. Available from: https://blogs.uoregon.edu/rclub/2015/11/10/power-in-r/\n\n\n\n5. \nStatistical power analysis - jacob cohen, 1992 [Internet]. Available from: https://journals.sagepub.com/doi/10.1111/1467-8721.ep10768783\n\n\n\n6. \nZhang Z, Wang L. Advanced statistics using r [Internet]. ISDSA Press; 2017. Available from: https://advstats.psychstat.org/\n\n\n\n7. \nWp.regression: Statistical power analysis for linear regression in WebPower: Basic and advanced statistical power analysis [Internet]. Available from: https://rdrr.io/cran/WebPower/man/wp.regression.html",
    "crumbs": [
      "Statistical Power and Sample Size Determination",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Power Analysis for OLS Regression</span>"
    ]
  }
]