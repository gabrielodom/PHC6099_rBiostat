[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "An R Cookbook for Public Health",
    "section": "",
    "text": "0.1 Source Code for PHC6099 Course Notes\nThis material is for the course “R Computing for Health Sciences”. The course notes are published here: https://gabrielodom.github.io/PHC6099_rBiostat/",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course Materials for PHC 6099: 'R Computing for Health Sciences'</span>"
    ]
  },
  {
    "objectID": "index.html#source-code-for-phc6099-course-notes",
    "href": "index.html#source-code-for-phc6099-course-notes",
    "title": "An R Cookbook for Public Health",
    "section": "",
    "text": "0.1.1 Topics\nThe chapters are:\n\nExploring Data\n\nggplot2:: mosaic plots, histograms, and violin plots\nggplot2:: scatterplots and facets \nskimr::\ntable1::\ngtsummary::\n\nOne-Sample Tests\n\n\\(Z\\)-test\nPaired \\(t\\)-test\nPaired Wilcoxon test\nTransformations to Normality\nMcNemar’s Test\nFisher’s Exact Test\nChi-Square Goodness of Fit\nBootstrapped Confidence Intervals\n\nTwo-Sample Tests\n\n\\(t\\)-test\nWelch’s \\(t\\)-test\nMann-Whitney \\(U\\) test\nCochran’s \\(Q\\) test\n\\(\\chi^2\\) Test for Independence\n\nANOVA and Linear Regression\n\nOne-Way ANOVA\nTwo-way ANOVA\nWelch’s ANOVA\nKruskal-Wallace Test\nTukey HSD Post-Hoc Test\nRepeated Measures ANOVA\nRandom Intercept Models\nCorrelation Matrices and Covariances\nMultiple Regression (linear)\nPolynomial regression\n\nGeneralized Linear Models\n\nGeneralized Linear Models: Binary\nGeneralized Linear Models: Ordered\nGeneralized Linear Models: Count (Poisson)\nGeneralized Linear Models: Count (Negative Binomial)\n\nSpecial Topics\n\nLinear Mixed Effects Models\nStructural Equation Models\nCox Proportional Hazards Regression\n(TBD) Multivariate Methods for Genetics/Genomics\nRidge, LASSO, and Elastic Net Regression\n\nPower Calculations (in progress)\n\n\n\n0.1.2 Lesson Outline\nThis is a shell of a lesson that can be copied and pasted for new lessons (or to edit and clean up existing lessons). If you copy this shell, then change all the headings from level 4 to 2. Replace &lt;the method&gt; with the name of your method, or its abbreviation. The file lessons/00_lesson_template.qmd has a .qmd template with these sections.\n\n0.1.2.1 Introduction to &lt;the method&gt;\n\n\n0.1.2.2 Mathematical definition of &lt;the method&gt;\n\n\n0.1.2.3 Data source and description\n\n\n0.1.2.4 Cleaning the data to create a model data frame\n\n\n0.1.2.5 Assumptions of &lt;the method&gt;\n\n\n0.1.2.6 Checking the assumptions with plots\n\n\n0.1.2.7 Code to run &lt;the method&gt;\n\n\n0.1.2.8 Code output\n\n\n0.1.2.9 Brief interpretation of the output",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course Materials for PHC 6099: 'R Computing for Health Sciences'</span>"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "2  About this Book",
    "section": "",
    "text": "2.1 About these Chapters\nThese are the written lecture materials for the class PHC 6099 at Florida International University’s Stempel College of Public Health. Each of the lessons were written by students, so we don’t guarantee that they are always mathematically/statistically accurate. You should use this material as a simple place to start to use these methods, but always read more about these methods when you use them to give yourself a better understanding of their theoretical foundations. This material should not be used to replace a traditional textbook in applied biostatistics. Here are some rather standard books on applied biostatistics (there are free/cheap versions on the internet for most of these texts, but I trust you to find them yourself):",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>About this Book</span>"
    ]
  },
  {
    "objectID": "about.html#about-these-chapters",
    "href": "about.html#about-these-chapters",
    "title": "2  About this Book",
    "section": "",
    "text": "Biostatistical Analysis. Jerrold H. Zar. https://www.pearson.com/en-us/subject-catalog/p/biostatistical-analysis/P200000006419/9780134995441.\nRegression Modelling Strategies. Frank E. Harrell, Jr. https://link.springer.com/book/10.1007/978-3-319-19425-7\nCategorical Data Analysis. Alan Agresti. https://onlinelibrary.wiley.com/doi/book/10.1002/0471249688",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>About this Book</span>"
    ]
  },
  {
    "objectID": "about.html#getting-help-in-r",
    "href": "about.html#getting-help-in-r",
    "title": "2  About this Book",
    "section": "2.2 Getting Help in R",
    "text": "2.2 Getting Help in R\nThis is the second semester of the “R” course sequence at FIU, so we spend very little time explaining the basics of the R language (the first semester is PHC6701; the text for that class is available here: https://gabrielodom.github.io/PHC6701_r4ds/). If you are new to R, please go back to the previous semester’s material and work through that first. If you want to see how we made this book, the source code and data sets for this book are available here: https://github.com/gabrielodom/PHC6099_rBiostat.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>About this Book</span>"
    ]
  },
  {
    "objectID": "01_header_EDA.html",
    "href": "01_header_EDA.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Text Outline\nThis text covers the basics of:",
    "crumbs": [
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "01_header_EDA.html#text-outline",
    "href": "01_header_EDA.html#text-outline",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Exploring data\nOne-sample tests\nTwo-sample tests\nLinear regression and ANOVA\nGeneralized linear models\nSome special topics\nMaybe a bit about power calculations…",
    "crumbs": [
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "01_header_EDA.html#part-outline",
    "href": "01_header_EDA.html#part-outline",
    "title": "Exploratory Data Analysis",
    "section": "Part Outline",
    "text": "Part Outline\nThis part of the text contains the chapters on the following packages which are useful for exploratory data analysis:\n\nggplot2:: mosaic plots, histograms, and violin plots\nggplot2:: scatterplots and facets\nskimr::\ntable1::\ngtsummary::",
    "crumbs": [
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "lessons/01_mosaic_violin.html",
    "href": "lessons/01_mosaic_violin.html",
    "title": "\n3  Mosaic & Box/Violin Plots\n",
    "section": "",
    "text": "3.1 Packages for this Lesson\n# Installing Required Packages\n# install.packages(\"public.ctn0094data\")\n# install.packages(\"tidyverse\")\n# install.packages(\"ggmosaic\")\n\n# Loading Required Packages\nlibrary(public.ctn0094data)\nlibrary(tidyverse)\nlibrary(ggmosaic)",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Mosaic & Box/Violin Plots</span>"
    ]
  },
  {
    "objectID": "lessons/01_mosaic_violin.html#introduction-to-mosaic-and-boxviolin-plots",
    "href": "lessons/01_mosaic_violin.html#introduction-to-mosaic-and-boxviolin-plots",
    "title": "\n3  Mosaic & Box/Violin Plots\n",
    "section": "\n3.2 Introduction to Mosaic and Box/Violin Plots",
    "text": "3.2 Introduction to Mosaic and Box/Violin Plots\nMosaic, box, and violin plots are useful for visualizing summary statistics.\nA mosaic plot is a special type of stacked bar chart used for two or more categorical variables. The width of the columns is proportional to the number of observations in each level of the variable plotted on the horizontal, or x-axis. The vertical length of the bars is proportional to the number of observations in the second variable within each level of the first variable.\nBox and violin plots are used for continuous variables by group. Box plots display six summary measures (the minimum, first quartile (Q1), median, third quartile (Q3), the interquartile range, and maximum). A violin plot illustrates the distribution of numerical data for one or more level of a categorical variable by combining summary statistics and density of each variable. Each curve corresponds to the respective frequency of data points within each region. A box plot is typically overlaid to provide additional information.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Mosaic & Box/Violin Plots</span>"
    ]
  },
  {
    "objectID": "lessons/01_mosaic_violin.html#data-source-and-description",
    "href": "lessons/01_mosaic_violin.html#data-source-and-description",
    "title": "\n3  Mosaic & Box/Violin Plots\n",
    "section": "\n3.3 Data Source and Description",
    "text": "3.3 Data Source and Description\nThe National Drug Abuse Treatment Clinical Trials Network (CTN) is a means by which medical and specialty treatment providers, treatment researchers, participating patients, and the National Institute on Drug Abuse cooperatively develop, validate, refine, and deliver new treatment options to patients. The CTN 094 demographics and everybody data sets from the public.ctn0094data package were utilized for the following visualizations. CTN 094 is a comprehensive, harmonized and normalized database of treatment data from CTN_0027, CTN_0030, and CTN_0051, where experiences of individuals with opioid use disorder (OUD) who seek care are described.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Mosaic & Box/Violin Plots</span>"
    ]
  },
  {
    "objectID": "lessons/01_mosaic_violin.html#cleaning-the-data-to-create-a-model-data-frame",
    "href": "lessons/01_mosaic_violin.html#cleaning-the-data-to-create-a-model-data-frame",
    "title": "\n3  Mosaic & Box/Violin Plots\n",
    "section": "\n3.4 Cleaning the Data to Create a Model Data Frame",
    "text": "3.4 Cleaning the Data to Create a Model Data Frame\nThe demographics and everybody data sets within the public.ctn0094data package were joined by ID (who variable). Race, age, is_male (gender), and project were selected features for the following visualizations.\n\n# Creating model data frame to include age, race, project, and is_male\n# from demographics and everybody data sets. Joined by subject ID (who)\ndemographics_df &lt;- demographics %&gt;% \n  left_join(everybody, by = \"who\")  %&gt;%\n  select(age, race, project, is_male)",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Mosaic & Box/Violin Plots</span>"
    ]
  },
  {
    "objectID": "lessons/01_mosaic_violin.html#assumptions-with-mosaic-boxviolin-plots",
    "href": "lessons/01_mosaic_violin.html#assumptions-with-mosaic-boxviolin-plots",
    "title": "\n3  Mosaic & Box/Violin Plots\n",
    "section": "\n3.5 Assumptions with Mosaic & Box/Violin Plots",
    "text": "3.5 Assumptions with Mosaic & Box/Violin Plots\nIn mosaic plots, two categorical variables are plotted along the horizontal (x) and vertical (y) axis. Each combination of categories forms a rectangle or tile within the plot.\nIn box and violin plots, a categorical variable is plotted along the horizontal or x-axis, while a continuous variable is plotted along the vertical or y-axis. Violin plots can be limiting if symmetry, skew, or other shape and variability characteristics are different between groups because precise comparison cannot be easily interpreted between density curves. For this reason, violin plots are typically rendered with another overlaid chart type, like box plot quartiles.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Mosaic & Box/Violin Plots</span>"
    ]
  },
  {
    "objectID": "lessons/01_mosaic_violin.html#code-to-run-mosaic-boxviolin-plots-output",
    "href": "lessons/01_mosaic_violin.html#code-to-run-mosaic-boxviolin-plots-output",
    "title": "\n3  Mosaic & Box/Violin Plots\n",
    "section": "\n3.6 Code to Run Mosaic & Box/Violin Plots & output",
    "text": "3.6 Code to Run Mosaic & Box/Violin Plots & output\n\n3.6.1 Mosaic Plots\nIn order to create a Mosaic plot, you must specify what data object you will be using within the ggplot() function. Then you will set aesthetic mapping options within the following geometric object layer: geom_mosaic().\nIn geom_mosaic(), the following aesthetics can be specified:\n\n\nweight: a weighting variable.\n\nx: categorical variable for the x-axis.\n\nSpecified as x = product(var1, var2, ...)\n\nThe product() function is used to extract the values from the categorical variable specified.\n\n\n\nalpha: a variable specifying transparency.\n\nIf the variable is not called in x:, then alpha: will be added in the first position.\n\n\n\nfill: a variable specifying fill color.\n\nIf the variable is not called in x:, then fill: will be added after the optional alpha: variable.\n\n\n\nconds: a variable specifying conditions.\n\nSpecified as conds = product(var1, var2, ...)\n\n\n\n\nThe ordering of the variables is vital as the product plot is created hierarchically.\n\n3.6.1.1 Basic Mosaic Plot\nIn the following example of a basic mosaic plot, we visualize the distribution of Race among CTN Projects 27, 30, and 51.\n\n# Basic Mosaic Plot\nmosaic_basic &lt;- demographics_df %&gt;% \n  ggplot() +\n  geom_mosaic(\n    aes(\n      # geom_mosaic() does not have one-to-one mapping between a variable and the x- \n      # or y-axis. So you must use the product() function when assigning a variable\n      # to the x-axis to account for the variable number of variables.\n      x = product(project),\n      fill = race\n    )\n  ) +\n  labs(\n    y = \"Race\",\n    x = \"Project\",\n    title = \"Mosaic Plot of Race by CTN Project\") +\n  # Specifies default `geom_mosaic` aesthetics, e.g white panel background, \n  # removes grid lines, adjusts widths and heights of rows and columns to \n  # reflect frequencies\n  theme_mosaic() +\n  # Removes legend illustrating Race and respective fill colors\n  theme(legend.position = \"None\")\n  \nmosaic_basic\n\n\n\n\n\n\nFigure 3.1\n\n\n\n\n\n3.6.1.2 More Advanced Mosaic Plot\nIn a more advanced version of a mosaic plot, we can visualize more than 2 categorical variables. The following example utilizes race, project, and ethnicity among CTN Projects 27, 30, and 51.\n\n# Advanced Mosaic Plot\nmosaic_advanced &lt;- demographics_df %&gt;% \n  ggplot() +\n  geom_mosaic(\n    aes(\n      x = product(race, project),\n      fill = is_male\n    )\n  ) +\n  labs(\n    y = \"Race\",\n    x = \"Project\",\n    title = \"Mosaic Plot of Race by Gender and CTN Project\",\n    fill = \"Gender\"\n  ) +\n  scale_fill_manual(\n    labels = c(\"No\" = \"Female\", \"Yes\" = \"Male\"),\n    values = c(\"darkseagreen2\", \"darkslategray3\", \"grey\")\n  ) +\n  theme_mosaic() +\n  # Adjust axis tick labels to 60 degrees and justification to the right\n  # with hjust (horizontal justification) and vjust (vertical justification)\n  theme(axis.text.x = element_text(angle = 60, hjust = 1, vjust = 1))\n  \nmosaic_advanced\n\n\n\n\n\n\nFigure 3.2\n\n\n\n\n\n3.6.2 Box Plots\nIn order to create a box plot, you must specify what data object you will be using within the ggplot() function. Then you will set aesthetic mapping options within the aes() or aesthetic layer. The geom_boxplot() layer specifies the box plot.\nThe following aesthetics are understood by geom_boxplot():\n\n\nx or y: Specifies the categorical variable along the x- or y-axis.\n\nlower or xlower: Specifies the 25th percentile/first quartile.\n\nupper or xupper: Specifies the 75th percentile/third quartile.\n\nmiddle or xmiddle: Specifies the 50th percentile/second quartile/median.\n\nymin or xmin: Specifies the y or x minimum for the plot.\n\nymax or xmax: Specifies the y or x maximum for the plot.\n\nalpha: Specifies a variable to determine transparency.\n\ncolor: Assigns an outline color to respective levels of a specified categorical variable.\n\nfill: Assigns a fill color to respective levels of a specified categorical variable.\n\ngroup: Partitions data by a discrete variable when no other grouping variable is specified, or grouping is incorrectly defaulted by R.\n\nlinetype: Specifies line type of box plot.\n\nlinewidth: Specifies line width of box plot.\n\nshape: Specifies the shape of the (outlier) points.\n\nsize: Specifies the size of the points and text.\n\nweight: Specifies a weight variable.\n\n\n3.6.2.1 Basic Box Plot\nThe following is a basic box plot showing the relationship between one continuous and one categorical variable.\n\n# Box Plot\nbox_basic &lt;- demographics_df %&gt;% \n  ggplot() +\n  aes(x = race, y = age, color = race) +\n  labs(\n    x = \"Race\",\n    y = \"Age\",\n    title = \"Box Plot of Race and Age\",\n    color = \"Race\"\n  ) +\n  # Using width to adjust the width of the boxes\n  geom_boxplot(width = 0.5) +\n  theme(legend.position = \"None\")\n\nbox_basic\n\n\n\n\n\n\nFigure 3.3\n\n\n\n\n\n3.6.2.2 More Advanced Box Plot\nWith geom_box(), you can also specify a additional categorical variable (different from your x and y variables) to break up your plot by that variable. For example, the following plot takes the previous plot of race and age and adds information side-by-side by gender (is_male).\n\n# Box Plot\nbox_advanced &lt;- demographics_df %&gt;% \n  ggplot() +\n  aes(x = race, y = age, color = is_male) +\n  # changing the labels for is_male, and specifying the colors we want\n  scale_color_manual(\n    labels = c(\"No\" = \"Female\", \"Yes\" = \"Male\"),\n    values = c(\"darkorchid4\", \"darkolivegreen4\")\n  ) +\n  labs(\n    x = \"Race\",\n    y = \"Age\",\n    title = \"Box Plot of Race and Age\",\n    color = \"Gender\"\n  ) +\n  # Using width to adjust the width of the boxes\n  geom_boxplot(width = 0.5)\n\nbox_advanced\n\n\n\n\n\n\nFigure 3.4\n\n\n\n\n\n3.6.3 Violin Plot\nIn order to create a Violin plot, you must specify what data object you will be using within the ggplot() function. Then you will set aesthetic mapping options within the aes() or aesthetic layer. The geom_violin() layer specifies the violin plot. An additional call for geom_boxplot() will overlay box quartiles on the violin plot to display summary statistics.\nThe following aesthetics are understood by geom_violin():\n\n\nx: Specifies the categorical variable along the x-axis.\n\ny: Specifies the continuous variable along the y-axis.\n\nalpha: Specifies a variable to determine transparency.\n\ncolor: Assigns an outline color to respective levels of a specified categorical variable.\n\nfill: Assigns a fill color to respective levels of a specified categorical variable.\n\ngroup: Partitions data by a discrete variable when no other grouping variable is specified, or grouping is incorrectly defaulted by R.\n\nlinetype: Specifies line type of violin plot.\n\nlinewidth: Specifies line width of violin plot.\n\nweight: Specifies a weight variable.\n\n\n# Violin Plot\nviolin_basic &lt;- demographics_df %&gt;% \n  ggplot() +\n  aes(x = race, y = age, color = race) +\n  scale_color_manual(\n    values = c(\"coral1\", \"darkgreen\", \"deepskyblue2\", \"darkorchid2\")\n  ) +\n  labs(\n    x = \"Race\",\n    y = \"Age\",\n    title = \"Violin Plot of Race and Age\",\n    subtitle = \"With Summary Information\",\n    color = \"Race\"\n  ) +\n  geom_violin() +\n  geom_boxplot(width = 0.1) +\n  theme(legend.position = \"None\")\n\nviolin_basic                        \n\n\n\n\n\n\nFigure 3.5",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Mosaic & Box/Violin Plots</span>"
    ]
  },
  {
    "objectID": "lessons/01_mosaic_violin.html#brief-interpretation",
    "href": "lessons/01_mosaic_violin.html#brief-interpretation",
    "title": "\n3  Mosaic & Box/Violin Plots\n",
    "section": "\n3.7 Brief Interpretation",
    "text": "3.7 Brief Interpretation\n\n3.7.1 Mosaic Plot\n\nCompared to Project 27 and Project 51, Project 30 had the highest proportion of participants who indicated that their race is ‘White’.\nCompared to Project 30 and Project 51, Project 27 had the highest proportion of participants who indicated that their race is ‘Other’.\nCompared to Project 27 and Project 51, Project 30 has the lowest proportion of participants who indicated that their race is ‘Other’.\n\n3.7.2 Box Plot\n\nParticipants who indicated that their race is ‘Black’ exhibited the highest median age of around 45 years old\nParticipants who indicated that their race is ‘White’ exhibited the lowest median age at approximately 31 years old.\n\n3.7.3 Violin Plot\nThis plot more clearly shows the bimodality of age by race among Black and ‘Other’ participants in CTN. It also shows the skewness of age in the White participants. Specifically:\n\nParticipants who indicated that their race is ‘White’ exhibited peak density around mid-20s compared to those who indicated that their race is ‘Black’, where peak density is exhibited around late-40s.\nParticipants who indicated that their race is ‘White’ had the lowest median age at approximately 31 years old, where participants who indicated that their race is ‘Black’ had the highest median age at approximately 45 years old.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Mosaic & Box/Violin Plots</span>"
    ]
  },
  {
    "objectID": "lessons/01_mosaic_violin.html#conclusion",
    "href": "lessons/01_mosaic_violin.html#conclusion",
    "title": "\n3  Mosaic & Box/Violin Plots\n",
    "section": "\n3.8 Conclusion",
    "text": "3.8 Conclusion\nThis lesson discusses three different plots for one-dimensional data: the Mosaic, Box, and Violin plots. Figure 3.1 is a basic mosaic plots that shows race by CTN project. In Figure 3.2, we added a third variable to the visualization: gender. The box plots, Figure 3.3 and Figure 3.4 we plotted age (continuous) by race and age by race and gender, respectively. Finally, Figure 3.5 shows a violin plot with an overlaid box plot for age by race in the CTN projects.\nMosaic plots are useful for proportionally visualizing the observations of two or more categorical variables. Box and violin plots can be used to visualize a continuous variable by one, or two in the case of box plots, categorical variables. Violin plots build on box plots in that they are able to provide quick information on the potential multimodal distribution(s) and skewness of a continuous variable across categories, as we saw in Figure 3.5.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Mosaic & Box/Violin Plots</span>"
    ]
  },
  {
    "objectID": "lessons/01_scatterplots.html",
    "href": "lessons/01_scatterplots.html",
    "title": "\n4  How to Create a Scatterplot\n",
    "section": "",
    "text": "4.1 Introduction to Scatterplots\nScatterplots display the relationship between two variables using dots to represent the values for each numeric variable. This presentation will examine the relationship between GDP per capita and Fertility over time using ggplot with facets.\nHypothesis: A negative relationship exists between GDP per capita and fertility i.e. as GDP per capita increases, fertility decreases.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>How to Create a Scatterplot</span>"
    ]
  },
  {
    "objectID": "lessons/01_scatterplots.html#gapminder-data-description",
    "href": "lessons/01_scatterplots.html#gapminder-data-description",
    "title": "\n4  How to Create a Scatterplot\n",
    "section": "\n4.2 Gapminder data description",
    "text": "4.2 Gapminder data description\nData was obtained from the dslabs package and comes from Gapminder a Swedish non-profit organization. The Gapmidner data set has health and income outcomes for 184 countries from 1960 to 2016. Gapminder aims to promote a fact-based worldview by providing accessible and understandable global development data. The dataset covers a wide range of variables, including economic, social, and health-related indicators like GDP, infant mortality, life expectancy, fertility, as well as population, making it a valuable resource for understanding global trends and patterns over time. Countries and territories with missing information were not excluded from the data set as the lack of information can also be looked into and shed light on why data was not collected or provided. To determine whether a country’s health and income outcomes are influenced by population sizes and GDP per capita, the data will be used to create a series of graphs to view different trends.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>How to Create a Scatterplot</span>"
    ]
  },
  {
    "objectID": "lessons/01_scatterplots.html#cleaning-the-data-to-create-a-model-data-frame",
    "href": "lessons/01_scatterplots.html#cleaning-the-data-to-create-a-model-data-frame",
    "title": "\n4  How to Create a Scatterplot\n",
    "section": "\n4.3 Cleaning the data to create a model data frame",
    "text": "4.3 Cleaning the data to create a model data frame\nA tibble was created from the gapminder dataset, and a new column was created to measure GDP per capita. Overall, using tibbles enhances the readability, usability, and compatibility of your code within the tidyverse ecosystem.\n\n# Creating gapminder dataset\\tibble\ngapminder_df &lt;-\n  as_tibble(gapminder) %&gt;%\n  mutate(gdp_per_capita = gdp / population)",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>How to Create a Scatterplot</span>"
    ]
  },
  {
    "objectID": "lessons/01_scatterplots.html#components-of-ggplot2",
    "href": "lessons/01_scatterplots.html#components-of-ggplot2",
    "title": "\n4  How to Create a Scatterplot\n",
    "section": "\n4.4 Components of ggplot2\n",
    "text": "4.4 Components of ggplot2\n\nggplot2 is a package used to create graphs and visualize data. The main three components of ggplot2 are the data, aesthetics and geom layers.\n\nThe data layer - states what data will be used to graph\nThe aesthetics layer - specifies the variables that are being mapped\nThe geom layer - specifies the type of graph to be produced",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>How to Create a Scatterplot</span>"
    ]
  },
  {
    "objectID": "lessons/01_scatterplots.html#code-to-run-interpretable-scatterplots-and-create-facets",
    "href": "lessons/01_scatterplots.html#code-to-run-interpretable-scatterplots-and-create-facets",
    "title": "\n4  How to Create a Scatterplot\n",
    "section": "\n4.5 Code to run interpretable scatterplots and create facets",
    "text": "4.5 Code to run interpretable scatterplots and create facets\nIn order to create a scatter-plot using ggplot, you must specify what data you will be using, state which variables will be mapped and how under aesthetics. What differentiates the scatter-plot from any other type of graph will be specified under the geom layer. For the scatter-plot, geom_point will be used.\nIn this example, we will analyze the relationship between fertility rates and gdp per capita for each country in 2011.\n\nfig_bubble_2011 &lt;-\n  ggplot(data = filter(gapminder_df, year == 2011)) +\n  aes(x = gdp_per_capita, y = fertility) +\n  geom_point()\n\nfig_bubble_2011 \n\n\n\n\n\n\nFigure 4.1: Association between fertility rates and gdp per capita for each country in 2011\n\n\n\n\nIn the example above, we have mapped out fertility as our y-axis and gdp per capita as our x-axis. However, at it’s very basic level, there is not enough information provided to accurately analyze the relationship between the two. For this reason, we can add additional layers that will provide more information to properly analyze the scatter-plot.\n\nfig_bubble_pretty_2011 &lt;-\n  ggplot(data = filter(gapminder_df, year == 2011)) +\n  aes(\n    x = gdp_per_capita,\n    y = fertility,\n    # will change the size of the point based on population size \n    size = population, \n    # will assign colors based on the continent the country is in \n    color = continent\n  ) +\n  # gives a range as to how big or small the points of population should be\n  scale_size(range = c(1, 20)) + \n  # removes N/A from the legend and titles it Continent \n  scale_colour_discrete(na.translate = F, name = \"Continent\") +\n  # removes population size from the legend \n  guides(size = \"none\") +\n  scale_x_continuous(\n    name = \"GDP per Capita\",\n    trans = \"log10\",\n    # transforms numbers from scientific notation to regular number \n    labels = scales::comma\n  ) +\n  labs(\n    title = \"Fertility rate descreases as GDP per capita increases in 2011\",\n    y = \"Fertility rates\",\n    caption = \"Source: Gapminder\"\n  ) +\n  # the ylim was set based on the fertility, lowest was near 1 & highest was above 7\n  ylim(1.2, 8.0) +\n  # alpha increases transparency of the points to ensure they can all be seen\n  geom_point(alpha = 0.5) \n\nfig_bubble_pretty_2011\n\n\n\n\n\n\nFigure 4.2: Association between fertility rates and gdp per capita for each country, grouped by continent, in 2011\n\n\n\n\nFigure 4.2 builds on the previous scatterplot of Fertility Rates (y axis) against GDP per capita (x axis) for 2011. The bubble size depicts respective country populations, and continents are coded by colors according to the key. This figure displays a negative relationship between GDP per capita and Fertility Rates. It supports the Hypothesis which states that as GDP per capita increases, Fertility Rates decreases. This trend can be confirmed for all continents, however, the degree to which fertility rates drop between continents varies. Most European country appear below a fertility rate of 2 babies per woman. The Americas appear to follow closely behind (under 4), followed by Oceania and Asia. A significant number of African countries still maintained higher fertility rates with lower GDP per capita for 2011.\nThis is an example of wanting to create four separate graphs to see the relationship between fertility rates and GDP per capita based on the years 1960, 1975, 1990 and 2005. In this example we omitted the facet argument.\n\nfig_bubble_multiple &lt;-\n  ggplot(data = filter(gapminder_df, year %in% c(1960, 1975, 1990, 2005))) +\n  aes(\n    x = gdp_per_capita,\n    y = fertility,\n    size = population,\n    color = continent\n  ) +\n  scale_x_continuous(\n    name = \"GDP per Capita\",\n    trans = \"log10\",\n    labels = scales::comma\n  ) +\n  scale_size(range = c(0, 20)) +\n  guides(size = \"none\") +\n  scale_colour_discrete(na.translate = FALSE, name = \"Continent\") +\n  labs(\n    title = \"Fertility continues to decrease as GDP per capita increases\",\n    caption = \"Source: Gapminder\",\n    y = \"Fertility rates\"\n  ) +\n  geom_point(alpha = 0.3) \n\nfig_bubble_multiple\n\n\n\n\n\n\nFigure 4.3: Association between fertility rates and GDP per capita based on the years 1960, 1975, 1990 and 2005\n\n\n\n\nWithout having used the facet argument, all points of all four years have been included into one graph. This graph does not provide us with the information we were looking for.\n\nfig_bubble_multiple_facet &lt;-\n  ggplot(data = filter(gapminder_df, year %in% c(1960, 1975, 1990, 2005))) +\n  aes(\n    x = gdp_per_capita,\n    y = fertility,\n    size = population,\n    color = continent\n  ) +\n  scale_x_continuous(\n    name = \"GDP per Capita\",\n    trans = \"log10\",\n    labels = scales::comma\n  ) +\n  scale_size(range = c(0, 20)) +\n  guides(size = \"none\") +\n  scale_colour_discrete(na.translate = FALSE , name = \"Continent\") +\n  labs(\n    title = \"Fertility continues to decrease as GDP per capita increases\",\n    caption = \"Source: Gapminder\",\n    y = \"Fertility rates\"\n  ) +\n  geom_point(alpha = 0.3) +\n  # specifiying we want the graphs split based on year\n  facet_wrap(~ year)\n\nfig_bubble_multiple_facet\n\n\n\n\n\n\nFigure 4.4: Association between fertility rates and GDP per capita based on the years 1960, 1975, 1990 and 2005, using facet\n\n\n\n\nNow that we’ve specified the facet argument, we now have four separate graphs that can be properly analysed. In Figure 4.4 we see an increasingly negative relationship between the two variables over time. This observation is congruent with the hypothesis that as GDP per capita increases, fertility decreases.\nThis global trend can be attributed to the increasing proportion of women in the workforce in the mid to late 20th century. As a result of World War II (1939-1945), women took on roles outside the home to compensate for men at war. Despite increased GDP per capita, this may have contributed to reduced fertility (babies per woman) over time. In 1960, a clear disparity among continents is seen. Most European countries’ fertility rates fell below 5, while their GDP per capita increased. Most African countries maintained high fertility rates above 5, but little change is seen in GDP per capita. The Asian continent shows the most variation among countries during that year. Some smaller Asian countries continued to maintain high fertility rates as GDP per capita increased in 1960. However, others displayed a drastic decrease in fertility rates by 1960. The Americas followed a steady decline over the years. By 2005, an overall negative relationship can be seen with most countries’ fertility rates below 5 babies per woman.\n\nfig_bubble_row_2011 &lt;-\n  ggplot(data = filter(gapminder_df, year == 2011)) +\n  aes(\n    x = gdp_per_capita,\n    y = fertility\n  ) +\n  geom_point(alpha = 0.5) +\n  facet_wrap(~ continent, nrow = 1)\n\nfig_bubble_row_2011 \n\n\n\n\n\n\nFigure 4.5: Association between fertility rates and GDP per capita based on continent\n\n\n\n\nIn the graph above, we see an example of separating the single graph into graphs based on continent. It has also been specified to have all graphs appear in one single row through the nrow argument. Very importantly however, this graph is unclear and cannot be used to compare the relationship between fertility and gdp per capita.\n\nfig_bubble_facet_2011 &lt;-\n  ggplot(data = filter(gapminder_df, year == 2011)) +\n  aes(\n    x = gdp_per_capita,\n    y = fertility\n  ) +\n  scale_x_continuous(\n    name = \"GDP per Capita\",\n    trans = \"log10\",\n    labels = scales::comma\n  )  +\n  geom_point(alpha = 0.5) +\n  facet_wrap(~ continent)\n\nfig_bubble_facet_2011 \n\n\n\n\n\n\nFigure 4.6: Association between fertility rates and GDP per capita based on continent not using nrow\n\n\n\n\nIn the next example above, we removed the nrow argument and the system automatically separated the graphs into three columns with two rows. Additionally, we changed the x-axis to a log scale to better interpret gdp per capita. There is a way to determine a relationship between fertility and gdp per capita by continent.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>How to Create a Scatterplot</span>"
    ]
  },
  {
    "objectID": "lessons/01_scatterplots.html#public-health-interpretation",
    "href": "lessons/01_scatterplots.html#public-health-interpretation",
    "title": "\n4  How to Create a Scatterplot\n",
    "section": "\n4.6 Public Health Interpretation",
    "text": "4.6 Public Health Interpretation\nA global negative trend is depicted between GDP per capita and fertility over time. Such changes were due to wars as well as social, cultural and economic changes that incentivize smaller families especially in Asian countries. Most European, American and Asian countries depicted significant decreases in fertility rates over time as GDP per capita increased. On the other hand, African countries remain in the top rank for fertility over the years. These differences are depicted in the population pyramid changes of developed vs developing countries. Public health policies can be tailored to incentivizing increased fertility in developed countries to ensure generation continuity, and effective family planning strategies in developing countries.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>How to Create a Scatterplot</span>"
    ]
  },
  {
    "objectID": "lessons/01_scatterplots.html#conclusion",
    "href": "lessons/01_scatterplots.html#conclusion",
    "title": "\n4  How to Create a Scatterplot\n",
    "section": "\n4.7 Conclusion",
    "text": "4.7 Conclusion\nIn this lesson, the basic functions of ggplot2 package were shown, which can create a scatterplot. There are three layers to the code to make a plot in R: data, aesthetic, and geometric. Within the aesthetic layer, functions can be added such as size and color to analyze more variables. Additionally, facets can split up graphs over a categorical variable, adding another potential variable to analyze in the plot.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>How to Create a Scatterplot</span>"
    ]
  },
  {
    "objectID": "lessons_original/01_skimr.html",
    "href": "lessons_original/01_skimr.html",
    "title": "\n5  Skimr Package\n",
    "section": "",
    "text": "5.1 Introduction\nSkimr is an R package designed to provide summary statistics about variables in data frames, tibbles, data tables and vectors. The function is modifiable where you can add additional variables, which are not a part of default summary function within R. Skimr allows us to quickly assess data quality by feature and type in a quick report. This is a critical step in Data Exploration, where Understanding our data helps us to generate a hypothesis and determine what data analysis are appropriate.\nThis presentation will cover the simplest and most effective ways to explore data in R.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Skimr Package</span>"
    ]
  },
  {
    "objectID": "lessons_original/01_skimr.html#introduction",
    "href": "lessons_original/01_skimr.html#introduction",
    "title": "\n5  Skimr Package\n",
    "section": "",
    "text": "5.1.1 Packages\nTo begin we will upload the packages necessary for the lesson, this includes the following:\n\n\nreadr() to import our data file\n\nknitr() that houses the kable() feature that allows us to construct and customize tables.\n\ntidyverse houses the dyplyrpackage that assists with data manipulation and visualization.\nTheskimrpackage provides a compact summary of the variables in a dataset.\n\n\n# install.packages(\"skimr\")\n# install.packages(\"knitr\")\n# install.packages(\"tidyverse\")\n\n# load all the packages we will need to analyze the data and use the skim\n#   function\nlibrary(skimr)\nlibrary(knitr)\nlibrary(readxl)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n5.1.2 Census Data\nFor this assignment we will be using the Census_2010 dataset. There is no code book associated with the data, making it difficult to provide an accurate description of the variables. The information recorded shows the United States population estimates from the years 2010-2015, as well as relevant variables like net population change, number of births, number of deaths, international and domestic migration. Within the dataframe, there are 3,193 observations and 100 variables.\nThe data can be imported into R from the following link: https://fiudit-my.sharepoint.com/:x:/g/personal/ssinc013_fiu_edu/ESK1A13PstVGtf7HUwNNt68Bnh1YPfH8L-hnvMUxjBuCVw?e=CCwQU9\n\n# import the data\n# census_2010 &lt;- read_csv(\"Data/census_2010.csv\")\ncensus_2010 &lt;- readxl::read_xlsx(\"../data/01_census_2010.xlsx\")\n\n# what are the variables\ncolnames(census_2010) %&gt;% \n  head(n = 10)\n\n [1] \"SUMLEV\"            \"REGION\"            \"DIVISION\"         \n [4] \"STATE\"             \"COUNTY\"            \"STNAME\"           \n [7] \"CTYNAME\"           \"CENSUS2010POP\"     \"ESTIMATESBASE2010\"\n[10] \"POPESTIMATE2010\"",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Skimr Package</span>"
    ]
  },
  {
    "objectID": "lessons_original/01_skimr.html#the-summary-function",
    "href": "lessons_original/01_skimr.html#the-summary-function",
    "title": "\n5  Skimr Package\n",
    "section": "\n5.2 The Summary() Function",
    "text": "5.2 The Summary() Function\nIn R, the most similar function is summary(). The summary() function in R can be used to quickly summarize the values in a data frame or vector.\nThis syntax shows examples of the summary function using both our data set, and a vector:\n\n#| label: Summary-syntax-with-data\n\n# Example using summary function with data\nsummary(census_2010$CENSUS2010POP)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n      82    11299    26424   193387    71404 37253956 \n\n# Example using summary function with vector\n# Define vector\nx &lt;- c(3, 4, 23, 5, 7, 8, 9, 12, 26, 15, 20, 21, NA)\n\n# Summarize values in vector\nsummary(x)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   3.00    6.50   10.50   12.75   20.25   26.00       1 \n\n\nThe summary() function automatically calculates: The minimum value, The value of the 1st quartile (25th percentile), The median value, The value of the 3rd quartile (75th percentile) and The maximum value. Any missing values (NA) in the vector, the summary() function will automatically exclude them when calculating the summary statistics.\nNow, let’s see how skim() compares.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Skimr Package</span>"
    ]
  },
  {
    "objectID": "lessons_original/01_skimr.html#skimr-package",
    "href": "lessons_original/01_skimr.html#skimr-package",
    "title": "\n5  Skimr Package\n",
    "section": "\n5.3 Skimr Package",
    "text": "5.3 Skimr Package\nThe skim() function will generate a summary of the variables in your dataset, including their data type, number of non-missing values, minimum and maximum values, median, mean, standard deviation, and more (Waring et al. 2022).\nThe following syntax ensures that the data is compatible with Skimr functions.\n\nCode# is the summary data a skimr dataframe\nskim(census_2010) %&gt;% \n  is_skim_df() # TRUE\n\n[1] TRUE\nattr(,\"message\")\ncharacter(0)\n\n\nWe can explore the data as a tibble:\n\nCode# use skim to get descriptive statistics of the data\nskim(census_2010) %&gt;% \n  head(n = 10)\n\n\nData summary\n\n\nName\ncensus_2010\n\n\nNumber of rows\n3193\n\n\nNumber of columns\n100\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n8\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\nSTNAME\n0\n1\n4\n20\n0\n51\n0\n\n\nCTYNAME\n0\n1\n4\n33\n0\n1927\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nSUMLEV\n0\n1\n49.84\n1.25\n40\n50\n50\n50\n50\n▁▁▁▁▇\n\n\nREGION\n0\n1\n2.67\n0.81\n1\n2\n3\n3\n4\n▁▆▁▇▂\n\n\nDIVISION\n0\n1\n5.19\n1.97\n1\n4\n5\n7\n9\n▂▇▅▆▃\n\n\nSTATE\n0\n1\n30.26\n15.15\n1\n18\n29\n45\n56\n▃▇▆▆▇\n\n\nCOUNTY\n0\n1\n101.92\n107.63\n0\n33\n77\n133\n840\n▇▁▁▁▁\n\n\nCENSUS2010POP\n0\n1\n193387.05\n1176201.45\n82\n11299\n26424\n71404\n37253956\n▇▁▁▁▁\n\n\nESTIMATESBASE2010\n0\n1\n193396.87\n1176244.25\n82\n11299\n26446\n71491\n37254503\n▇▁▁▁▁\n\n\nPOPESTIMATE2010\n0\n1\n193765.65\n1178710.28\n83\n11275\n26467\n71721\n37334079\n▇▁▁▁▁\n\n\n\n\n\nUsing skimr functions provides a cleaner and more detailed display of the results compared to the summary() function. In this example we are showing the first ten variables in our data set. The data summary tab shows the number of rows and columns, column type frequency and group variables. There is also additional descriptive information like missing values, unique characters.\nThis will be relevant for data cleaning as well as understanding the distribution. Both are critical to determine which statistical analysis would be most appropriate to use for a project.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Skimr Package</span>"
    ]
  },
  {
    "objectID": "lessons_original/01_skimr.html#other-skimr-features",
    "href": "lessons_original/01_skimr.html#other-skimr-features",
    "title": "\n5  Skimr Package\n",
    "section": "\n5.4 Other Skimr Features",
    "text": "5.4 Other Skimr Features\n\n5.4.1 Separate dataframes by type\nThe data frames produced by skim() are wide and sparse, filled with columns that are mostly NA. For that reason, it can be convenient to work with “by type” subsets of the original data frame. These smaller subsets have their NA columns removed.\nFeatures:\n\n\npartition() - Creates a list of smaller data frames. Each entry in the list is a data type from the original dataframe\n\nbind() - Takes the list and rebuilds the original dataframe.\n\nyank() - Extract a subtable from a dataframe with a particular type.\n\nThe following syntax is using partition() to separate the large census_df.\n\nCode# split the character and numeric data\nseparate_df &lt;- partition(skim(census_2010))\n# check only the character data\nseparate_df$character\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\nSTNAME\n0\n1\n4\n20\n0\n51\n0\n\n\nCTYNAME\n0\n1\n4\n33\n0\n1927\n0\n\n\n\n\nCode# create summary statistics for only numeric variables\nnumeric_separate_df &lt;- separate_df[2]\n# pull out the desired summary statistics in the nested list\nhead(numeric_separate_df$numeric[\"mean\"]) %&gt;% \n  kable(digits = 1) \n\n\n\nmean\n\n\n\n49.8\n\n\n2.7\n\n\n5.2\n\n\n30.3\n\n\n101.9\n\n\n193387.1\n\n\n\n\n\nThe following syntax is using bind() to combine the smaller character and numeric lists into the desired df.\n\nCode# combine the character and numeric data\nhead(bind(separate_df))\n\n\nData summary\n\n\nName\ncensus_2010\n\n\nNumber of rows\n3193\n\n\nNumber of columns\n100\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\nSTNAME\n0\n1\n4\n20\n0\n51\n0\n\n\nCTYNAME\n0\n1\n4\n33\n0\n1927\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nSUMLEV\n0\n1\n49.84\n1.25\n40\n50\n50\n50\n50\n▁▁▁▁▇\n\n\nREGION\n0\n1\n2.67\n0.81\n1\n2\n3\n3\n4\n▁▆▁▇▂\n\n\nDIVISION\n0\n1\n5.19\n1.97\n1\n4\n5\n7\n9\n▂▇▅▆▃\n\n\nSTATE\n0\n1\n30.26\n15.15\n1\n18\n29\n45\n56\n▃▇▆▆▇\n\n\n\n\nCode# confirm that the bound table is the same as the original skimmed table\nidentical(bind(separate_df), skim(census_2010)) \n\n[1] TRUE\n\n\nThe following syntax is using yank() to extract a specific table eg.character to examine.\n\nCode# Extract character data\nyank(skim(census_2010), \"character\")\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\nSTNAME\n0\n1\n4\n20\n0\n51\n0\n\n\nCTYNAME\n0\n1\n4\n33\n0\n1927\n0\n\n\n\n\n\n\n5.4.2 Skimr with Dplyr\nSkimr functions can be used in combination with Dplyr functions to examine specific variables within the census dataset.\nThe following example used skim() with filter() to display the variable CENSUS2010POP. The dataframe was further customized to display variable name and data type using select().\n\nCode# use dplyr functions on the statistics summary table\ncensus_filter &lt;- skim(census_2010) %&gt;% \n  filter(skim_variable == \"CENSUS2010POP\")\ncensus_filter\n\n\nData summary\n\n\nName\ncensus_2010\n\n\nNumber of rows\n3193\n\n\nNumber of columns\n100\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\nCENSUS2010POP\n0\n1\n193387\n1176201\n82\n11299\n26424\n71404\n37253956\n▇▁▁▁▁\n\n\n\nCodecensus_select &lt;- skim(census_2010) %&gt;% \n  select(skim_type, skim_variable)\nhead(census_select)\n\n# A tibble: 6 × 2\n  skim_type skim_variable\n  &lt;chr&gt;     &lt;chr&gt;        \n1 character STNAME       \n2 character CTYNAME      \n3 numeric   SUMLEV       \n4 numeric   REGION       \n5 numeric   DIVISION     \n6 numeric   STATE        \n\n\nYou can also customize the output of the skim() function by using various arguments. For example, you can use the numeric argument to specify which variables should be treated as numeric variables, or use the ranges argument to specify custom ranges for variables.\nUsing skim() in combination with mutate() we will compute a new variable to add to our skim dataframe.\n\nCode# create a new variable calculate the change in birth rate from 2010 to 2011\ncensus_2010 %&gt;% \n  # new variable\n  mutate(net_birth = BIRTHS2011 - BIRTHS2010) %&gt;% \n  # move the variable to the beginning of the dataset\n  relocate(net_birth, .after = CENSUS2010POP) %&gt;% \n  # summary statistics table\n  skim() %&gt;% \n  # only the first fifteen variables\n  head(n = 15) %&gt;% \n  # change the formatting \n  kable(digit = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_type\nskim_variable\nn_missing\ncomplete_rate\ncharacter.min\ncharacter.max\ncharacter.empty\ncharacter.n_unique\ncharacter.whitespace\nnumeric.mean\nnumeric.sd\nnumeric.p0\nnumeric.p25\nnumeric.p50\nnumeric.p75\nnumeric.p100\nnumeric.hist\n\n\n\ncharacter\nSTNAME\n0\n1\n4\n20\n0\n51\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\ncharacter\nCTYNAME\n0\n1\n4\n33\n0\n1927\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nnumeric\nSUMLEV\n0\n1\nNA\nNA\nNA\nNA\nNA\n49.84\n1.25\n40\n50\n50\n50\n50\n▁▁▁▁▇\n\n\nnumeric\nREGION\n0\n1\nNA\nNA\nNA\nNA\nNA\n2.67\n0.81\n1\n2\n3\n3\n4\n▁▆▁▇▂\n\n\nnumeric\nDIVISION\n0\n1\nNA\nNA\nNA\nNA\nNA\n5.19\n1.97\n1\n4\n5\n7\n9\n▂▇▅▆▃\n\n\nnumeric\nSTATE\n0\n1\nNA\nNA\nNA\nNA\nNA\n30.26\n15.15\n1\n18\n29\n45\n56\n▃▇▆▆▇\n\n\nnumeric\nCOUNTY\n0\n1\nNA\nNA\nNA\nNA\nNA\n101.92\n107.63\n0\n33\n77\n133\n840\n▇▁▁▁▁\n\n\nnumeric\nCENSUS2010POP\n0\n1\nNA\nNA\nNA\nNA\nNA\n193387.05\n1176201.45\n82\n11299\n26424\n71404\n37253956\n▇▁▁▁▁\n\n\nnumeric\nnet_birth\n0\n1\nNA\nNA\nNA\nNA\nNA\n1870.12\n11792.85\n-3\n96\n232\n639\n386443\n▇▁▁▁▁\n\n\nnumeric\nESTIMATESBASE2010\n0\n1\nNA\nNA\nNA\nNA\nNA\n193396.87\n1176244.25\n82\n11299\n26446\n71491\n37254503\n▇▁▁▁▁\n\n\nnumeric\nPOPESTIMATE2010\n0\n1\nNA\nNA\nNA\nNA\nNA\n193765.65\n1178710.28\n83\n11275\n26467\n71721\n37334079\n▇▁▁▁▁\n\n\nnumeric\nPOPESTIMATE2011\n0\n1\nNA\nNA\nNA\nNA\nNA\n195251.40\n1189647.76\n90\n11277\n26417\n72387\n37700034\n▇▁▁▁▁\n\n\nnumeric\nPOPESTIMATE2012\n0\n1\nNA\nNA\nNA\nNA\nNA\n196744.52\n1200508.37\n81\n11195\n26362\n72496\n38056055\n▇▁▁▁▁\n\n\nnumeric\nPOPESTIMATE2013\n0\n1\nNA\nNA\nNA\nNA\nNA\n198200.69\n1211123.45\n89\n11180\n26519\n72222\n38414128\n▇▁▁▁▁\n\n\nnumeric\nPOPESTIMATE2014\n0\n1\nNA\nNA\nNA\nNA\nNA\n199754.09\n1222669.36\n87\n11121\n26483\n72257\n38792291\n▇▁▁▁▁\n\n\n\n\n\n\n5.4.3 Adding Variables\n\nbase - An sfl that sets skimmers for all column types.\nappend - Whether the provided options should be in addition to the defaults already in skim. Default is TRUE.\n\nAs mentioned, skim() is designed to display default statistics, however you can use this function to change the summary statistics that it returns.\nskim_with() is type closure: a function that returns adds a new variable to the table. This lets you have several skimming functions in a single R session, but it also means that you need to assign the return of skim_with() before you can use it.\nYou assign values within skim_with() by using the sfl() helper (skimr function list). It identifies which skimming functions you want to remove, by setting them to NULL. Assign an sfl to each column type that you wish to modify.\nFor example, we will add the following variables to the dataframe: median, min, max, IQR, length.\n\nCodemy_skim &lt;- skim_with(\n  numeric = sfl(median, min, max, IQR),\n  character = sfl(length), \n  append = TRUE\n)\n\n# add new variables into the summary table\ncensus_2010 %&gt;% \n  my_skim() %&gt;% \n  head(n = 10)\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n3193\n\n\nNumber of columns\n100\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n8\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\nlength\n\n\n\nSTNAME\n0\n1\n4\n20\n0\n51\n0\n3193\n\n\nCTYNAME\n0\n1\n4\n33\n0\n1927\n0\n3193\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\nmedian\nmin\nmax\nIQR\n\n\n\nSUMLEV\n0\n1\n49.84\n1.25\n40\n50\n50\n50\n50\n▁▁▁▁▇\n50\n40\n50\n0\n\n\nREGION\n0\n1\n2.67\n0.81\n1\n2\n3\n3\n4\n▁▆▁▇▂\n3\n1\n4\n1\n\n\nDIVISION\n0\n1\n5.19\n1.97\n1\n4\n5\n7\n9\n▂▇▅▆▃\n5\n1\n9\n3\n\n\nSTATE\n0\n1\n30.26\n15.15\n1\n18\n29\n45\n56\n▃▇▆▆▇\n29\n1\n56\n27\n\n\nCOUNTY\n0\n1\n101.92\n107.63\n0\n33\n77\n133\n840\n▇▁▁▁▁\n77\n0\n840\n100\n\n\nCENSUS2010POP\n0\n1\n193387.05\n1176201.45\n82\n11299\n26424\n71404\n37253956\n▇▁▁▁▁\n26424\n82\n37253956\n60105\n\n\nESTIMATESBASE2010\n0\n1\n193396.87\n1176244.25\n82\n11299\n26446\n71491\n37254503\n▇▁▁▁▁\n26446\n82\n37254503\n60192\n\n\nPOPESTIMATE2010\n0\n1\n193765.65\n1178710.28\n83\n11275\n26467\n71721\n37334079\n▇▁▁▁▁\n26467\n83\n37334079\n60446",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Skimr Package</span>"
    ]
  },
  {
    "objectID": "lessons_original/01_skimr.html#conclusion",
    "href": "lessons_original/01_skimr.html#conclusion",
    "title": "\n5  Skimr Package\n",
    "section": "\n5.5 Conclusion",
    "text": "5.5 Conclusion\nOverall, Skimr is a useful package for quickly summarizing the variables in a dataset and gaining insights into its structure and content.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Skimr Package</span>"
    ]
  },
  {
    "objectID": "lessons_original/01_skimr.html#references",
    "href": "lessons_original/01_skimr.html#references",
    "title": "\n5  Skimr Package\n",
    "section": "\n5.6 References",
    "text": "5.6 References\n\n\n\n\nWaring, Elin, Michael Quinn, Amelia McNamara, Eduardo Arino de la Rubia, Hao Zhu, and Shannon Ellis. 2022. Skimr: Compact and Flexible Summaries of Data. https://docs.ropensci.org/skimr/.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Skimr Package</span>"
    ]
  },
  {
    "objectID": "lessons/01_table1.html",
    "href": "lessons/01_table1.html",
    "title": "\n6  Demographics Table With Table1\n",
    "section": "",
    "text": "6.1 Introduction\nIn most scientific research journals, the first included table is often referred to as Table1. It is a table that presents descriptive statistics of baseline characteristics of the study population stratified by exposure. This package makes it fairly straightforward to produce such a table using R. Table1 includes descriptive statistics for the total study sample, with the rows (explanatory variables) consisting of the key study variables that are often included in the final analysis1. Then within the columns (outcome of interest/response variable), you will find cells given as an (%) for categorical variables, whereas a mean, SD, or the median will be provided for continuous variables. Additionally, there will be a total column provided which can help in the assessment of the overall sample.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Demographics Table With Table1</span>"
    ]
  },
  {
    "objectID": "lessons/01_table1.html#necessary-packages",
    "href": "lessons/01_table1.html#necessary-packages",
    "title": "\n6  Demographics Table With Table1\n",
    "section": "\n6.2 Necessary Packages",
    "text": "6.2 Necessary Packages\nThe htmlTable package allows for the usage of the table1() function to create a table 1, while also making life easy when attempting to copy this table into a Word document.\nThe boot package was created to aid in performing bootstrapping analysis. With it comes numerous data sets, specifically clinical trial data sets to make this possible. However, there is no code book provided within the package when the data is downloaded as a csv file. This is a link on Github that explains and elaborates on every data within the package itself2.\n\n#install.packages(\"htmlTable\")\n#install.packages(\"boot\")\n\n# Load libraries\nlibrary(htmlTable)\nlibrary(table1)\nlibrary(boot)",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Demographics Table With Table1</span>"
    ]
  },
  {
    "objectID": "lessons/01_table1.html#data-source-and-description",
    "href": "lessons/01_table1.html#data-source-and-description",
    "title": "\n6  Demographics Table With Table1\n",
    "section": "\n6.3 Data source and description",
    "text": "6.3 Data source and description\nToday, we will be using the melanoma data set which consists of malignant melanoma measurements of patients. Each patient had their tumor surgically removed between the years of 1962 and 1977 at the Department of Plastic Surgery, University Hospital of Odense located in Denamrk. Each surgery consisted of the complete removal of the tumor with an additional removal of about 2.5cm of the surrounding skin. When this was completed, the thickness of the tumor was recorded along with the physical appearance of ulceration vs no ulceration, as it is an important prognostic indication of those with a thick/ulcerated tumor to have an increased chance of death as a consequence of melanoma.\n\ndata(melanoma, package = \"boot\")\nmelanoma_data &lt;- melanoma\n\n#Now that we loaded the raw data set, we will conduct a visual exploration before wrangling #the data and applying any functions, while also considering the requirements involved in #the construction of a table1.\n\nsummary(melanoma_data)\n\n      time          status          sex              age             year     \n Min.   :  10   Min.   :1.00   Min.   :0.0000   Min.   : 4.00   Min.   :1962  \n 1st Qu.:1525   1st Qu.:1.00   1st Qu.:0.0000   1st Qu.:42.00   1st Qu.:1968  \n Median :2005   Median :2.00   Median :0.0000   Median :54.00   Median :1970  \n Mean   :2153   Mean   :1.79   Mean   :0.3854   Mean   :52.46   Mean   :1970  \n 3rd Qu.:3042   3rd Qu.:2.00   3rd Qu.:1.0000   3rd Qu.:65.00   3rd Qu.:1972  \n Max.   :5565   Max.   :3.00   Max.   :1.0000   Max.   :95.00   Max.   :1977  \n   thickness         ulcer      \n Min.   : 0.10   Min.   :0.000  \n 1st Qu.: 0.97   1st Qu.:0.000  \n Median : 1.94   Median :0.000  \n Mean   : 2.92   Mean   :0.439  \n 3rd Qu.: 3.56   3rd Qu.:1.000  \n Max.   :17.42   Max.   :1.000",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Demographics Table With Table1</span>"
    ]
  },
  {
    "objectID": "lessons/01_table1.html#cleaning-the-data-to-create-a-model-data-frame",
    "href": "lessons/01_table1.html#cleaning-the-data-to-create-a-model-data-frame",
    "title": "\n6  Demographics Table With Table1\n",
    "section": "\n6.4 Cleaning the data to create a model data frame",
    "text": "6.4 Cleaning the data to create a model data frame\nLet us now explore the type of variables within the data set.\n\ntypeof(melanoma_data$status) \n\n[1] \"double\"\n\n\nWe will first provide a basic table1 to illustrate how the function works. Currently, all the variables are in numeric/double formats, however for the creation of a basic table1, it is of importance to convert the dependent/response variable of interest to reflect categories (factor).\nOur main variable of interest (dependent/response) is the status. According to the code book found in Github, status is coded into three levels that indicate the patients status at the end of the study. Level 1 indicates that they had died from melanoma, Level 2 indicates that they were still alive at the conclusion of the study, and Level 3 indicates that they had died from causes unrelated to their melanoma. As such, we will factor the “status” variable into three levels. With this in mind, let us go ahead and convert melanoma into a factor variable with three levels. For ease of analysis we will use 2 = “Alive” as the reference level. This can be done in two ways:\n\nAlthough more time consuming, it is highly recommended that beginners utilize the function as.factor() and then utilize the recode_factor() function to minimize the errors.\nWhen you become more skilled and are able to understand how the factor function works, it is possible to do everything in one step with the factor() function. In this function you can put levels and labels all in one function instead of having to break it up into more than one function.\n\nFor our example we will use as.factor then recode_factor() using 2 = “Alive” as our reference group.\n\nmelanoma_data$status &lt;-\n  as.factor(melanoma_data$status)\n\n# print the first six observations\nhead(melanoma_data$status)\n\n[1] 3 3 2 3 1 1\nLevels: 1 2 3\n\n# Recode\nmelanoma_data$status &lt;- recode_factor(\n  melanoma_data$status, \n  \"2\" = \"Alive\", # this is the reference group\n  \"1\" = \"Died from melanoma\",\n  \"3\" = \"Non-Melanoma death\"\n)\n\n# Print the first six observations\nhead(melanoma_data$status)\n\n[1] Non-Melanoma death Non-Melanoma death Alive              Non-Melanoma death\n[5] Died from melanoma Died from melanoma\nLevels: Alive Died from melanoma Non-Melanoma death\n\n\nAs you can see in the variable levels, “Alive” is the reference level. It is extremely important to pick a reference level to lay the foundation of the table along with highlighting the outcome of interest of your hypothesis. In summary, this lays the foundation of a well organized table.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Demographics Table With Table1</span>"
    ]
  },
  {
    "objectID": "lessons/01_table1.html#creation-of-basic-table-1",
    "href": "lessons/01_table1.html#creation-of-basic-table-1",
    "title": "\n6  Demographics Table With Table1\n",
    "section": "\n6.5 Creation of basic table 1",
    "text": "6.5 Creation of basic table 1\nNow that our main variable of interest is a factor with three levels, we will run a basic table1 with the independent/explanatory variables of interest: sex, age, ulcer, and thickness.\nRecall that the explanatory variables of interest are still in “double” formats. Conveniently, to analyze data before the independent variables are converted to factors and labeled, the table1 provides the ability to highlight level results. This only applies for independent variables that are in numeric/double formats in which each number represents a group. For instance 0 although is a number format we know it has a group meaning such as male.\nFor the independent variables, if they have factors in the front, it provides the number of cases (aka observations). If they are a continuous variable, we will get the mean, the SD, the minimum and the maximum amounts.\n\nbasic_table1 &lt;- table1( \n  ~ factor(sex) + age + factor(ulcer) + thickness | status, \n  data = melanoma_data\n)\n\nbasic_table1\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlive(N=134)\nDied from melanoma(N=57)\nNon-Melanoma death(N=14)\nOverall(N=205)\n\n\n\nfactor(sex)\n\n\n\n\n\n\n0\n91 (67.9%)\n28 (49.1%)\n7 (50.0%)\n126 (61.5%)\n\n\n1\n43 (32.1%)\n29 (50.9%)\n7 (50.0%)\n79 (38.5%)\n\n\nage\n\n\n\n\n\n\nMean (SD)\n50.0 (15.9)\n55.1 (17.9)\n65.3 (10.9)\n52.5 (16.7)\n\n\nMedian [Min, Max]\n52.0 [4.00, 84.0]\n56.0 [14.0, 95.0]\n65.0 [49.0, 86.0]\n54.0 [4.00, 95.0]\n\n\nfactor(ulcer)\n\n\n\n\n\n\n0\n92 (68.7%)\n16 (28.1%)\n7 (50.0%)\n115 (56.1%)\n\n\n1\n42 (31.3%)\n41 (71.9%)\n7 (50.0%)\n90 (43.9%)\n\n\nthickness\n\n\n\n\n\n\nMean (SD)\n2.24 (2.33)\n4.31 (3.57)\n3.72 (3.63)\n2.92 (2.96)\n\n\nMedian [Min, Max]\n1.36 [0.100, 12.9]\n3.54 [0.320, 17.4]\n2.26 [0.160, 12.6]\n1.94 [0.100, 17.4]\n\n\n\n\n\n\n\nNote that the table1 package uses a familiar formula interface, where the variables to include in the table are separated by ‘+’ symbols, the “stratification” variable (which creates the columns) appears to the right of a “conditioning” symbol ‘|’, and the data argument specifies a data.frame that contains the variables in the formula.\nIf we do not put factor for a grouped variable then the following will happen:\n\nwrong_table1 &lt;- table1(\n  ~ sex + age + ulcer + thickness | status, \n  data = melanoma_data\n)\n\nwrong_table1\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlive(N=134)\nDied from melanoma(N=57)\nNon-Melanoma death(N=14)\nOverall(N=205)\n\n\n\nsex\n\n\n\n\n\n\nMean (SD)\n0.321 (0.469)\n0.509 (0.504)\n0.500 (0.519)\n0.385 (0.488)\n\n\nMedian [Min, Max]\n0 [0, 1.00]\n1.00 [0, 1.00]\n0.500 [0, 1.00]\n0 [0, 1.00]\n\n\nage\n\n\n\n\n\n\nMean (SD)\n50.0 (15.9)\n55.1 (17.9)\n65.3 (10.9)\n52.5 (16.7)\n\n\nMedian [Min, Max]\n52.0 [4.00, 84.0]\n56.0 [14.0, 95.0]\n65.0 [49.0, 86.0]\n54.0 [4.00, 95.0]\n\n\nulcer\n\n\n\n\n\n\nMean (SD)\n0.313 (0.466)\n0.719 (0.453)\n0.500 (0.519)\n0.439 (0.497)\n\n\nMedian [Min, Max]\n0 [0, 1.00]\n1.00 [0, 1.00]\n0.500 [0, 1.00]\n0 [0, 1.00]\n\n\nthickness\n\n\n\n\n\n\nMean (SD)\n2.24 (2.33)\n4.31 (3.57)\n3.72 (3.63)\n2.92 (2.96)\n\n\nMedian [Min, Max]\n1.36 [0.100, 12.9]\n3.54 [0.320, 17.4]\n2.26 [0.160, 12.6]\n1.94 [0.100, 17.4]\n\n\n\n\n\n\n\nAs you can see above, we have the incorrect values provided of the explanatory variables. For example, in the variable of sex, we expect to see the number of individuals who identify as male or female, but instead we observe the mean, which is not a proper descriptive statistic as sex is a categorical variable.\nTo avoid this issue as well as problems in other procedures (like logistic regressions), it is crucial that we remember to factor the variables before we run any function. But because we don’t have nice labels for the variables and categories, it doesn’t look great. To improve things, we can create factors with descriptive labels for the categorical variables (sex and ulcer), label each variable the way we want, and specify units for the continuous variables (age and thickness). According to the code book, the patient’s sex: 1 = male, 0 = female, and ulcer is an indicator of ulceration : 1 = present, 0 = absent. We also specify that the overall column to be labeled “Total” and be positioned on the left, and add a caption and footnote:\n\nmelanoma_data$sex &lt;- as.factor(melanoma_data$sex)\n\n# print the first six observations\nhead(melanoma_data$sex)\n\n[1] 1 1 1 0 1 1\nLevels: 0 1\n\n# Recode\nmelanoma_data$sex &lt;- recode_factor(\n  melanoma_data$sex, \n  \"0\" = \"Female\",\n  \"1\" = \"Male\"\n)\n\n# Print the first six observations\nhead(melanoma_data$sex)\n\n[1] Male   Male   Male   Female Male   Male  \nLevels: Female Male\n\n\n\ntypeof(melanoma_data$ulcer)\n\n[1] \"double\"\n\nmelanoma_data$ulcer &lt;- as.factor(melanoma_data$ulcer)\n\n# print the first six observations\nhead(melanoma_data$ulcer)\n\n[1] 1 0 0 0 1 1\nLevels: 0 1\n\n# Recode\nmelanoma_data$ulcer &lt;- recode_factor(\n  melanoma_data$ulcer, \n  \"0\" = \"Absent\",\n  \"1\" = \"Present\"\n)\n\n# Print the first six observations\nhead(melanoma_data$ulcer)\n\n[1] Present Absent  Absent  Absent  Present Present\nLevels: Absent Present\n\n\nIn addition, we need to add units to the two continuous variables age and thickness. According to the code book, age is the patient’s age measured in years and thickness corresponds to the tumor’s thickness in millimeters (mm). The package table1 provides an easy way to demonstrate measurement information:\n\nunits(melanoma_data$age) &lt;- \"years\"\nunits(melanoma_data$thickness) &lt;- \"mm\"\n\nAdditionally, for visual and descriptive purposes, the function table1 is able to easily provide labels for the variables that will be shown in the final table using the label() function. Also, (caption \\&lt;-) provides a title for the table and (footnote \\&lt;-) provides any footnote information.\n\nlabel(melanoma_data$sex) &lt;- \"Sex\"\nlabel(melanoma_data$age) &lt;- \"Age\"\nlabel(melanoma_data$ulcer) &lt;- \"Ulceration\"\nlabel(melanoma_data$thickness) &lt;-\"Thickness*\"\n\ncaption_char &lt;- \"Table 1. Melanoma Dataset Descriptive Statistics\"\nfootnote_char &lt;- \"*Also known as Breslow thickness\"\n\nBelow, we can demonstrate the final table1 layout. As you can see, you no longer use factor() in front of the variable as we already factorized it in the previous steps.\n\ntable1(\n  ~ sex + age + ulcer + thickness | status, \n  data = melanoma_data,\n  overall = c(left = \"Total\"), \n  caption = caption_char, \n  footnote = footnote_char\n)\n\n\nTable 1. Melanoma Dataset Descriptive Statistics\n\n\n\nTotal(N=205)\nAlive(N=134)\nDied from melanoma(N=57)\nNon-Melanoma death(N=14)\n\n\n*Also known as Breslow thickness\n\n\n\nSex\n\n\n\n\n\n\nFemale\n126 (61.5%)\n91 (67.9%)\n28 (49.1%)\n7 (50.0%)\n\n\nMale\n79 (38.5%)\n43 (32.1%)\n29 (50.9%)\n7 (50.0%)\n\n\nAge (years)\n\n\n\n\n\n\nMean (SD)\n52.5 (16.7)\n50.0 (15.9)\n55.1 (17.9)\n65.3 (10.9)\n\n\nMedian [Min, Max]\n54.0 [4.00, 95.0]\n52.0 [4.00, 84.0]\n56.0 [14.0, 95.0]\n65.0 [49.0, 86.0]\n\n\nUlceration\n\n\n\n\n\n\nAbsent\n115 (56.1%)\n92 (68.7%)\n16 (28.1%)\n7 (50.0%)\n\n\nPresent\n90 (43.9%)\n42 (31.3%)\n41 (71.9%)\n7 (50.0%)\n\n\nThickness* (mm)\n\n\n\n\n\n\nMean (SD)\n2.92 (2.96)\n2.24 (2.33)\n4.31 (3.57)\n3.72 (3.63)\n\n\nMedian [Min, Max]\n1.94 [0.100, 17.4]\n1.36 [0.100, 12.9]\n3.54 [0.320, 17.4]\n2.26 [0.160, 12.6]",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Demographics Table With Table1</span>"
    ]
  },
  {
    "objectID": "lessons/01_table1.html#changing-the-tables-appearance",
    "href": "lessons/01_table1.html#changing-the-tables-appearance",
    "title": "\n6  Demographics Table With Table1\n",
    "section": "\n6.6 Changing the table’s appearance",
    "text": "6.6 Changing the table’s appearance\nThe default style of table1 uses an Arial font, and resembles the booktabs style commonly used in LaTeX. While this default style is not ugly, inevitably there will be a desire to customize the visual appearance of the table (fonts, colors, gridlines, etc). The package provides a limited number of built-in options for changing the style, while further customization can be achieved in R Markdown documents using CSS.3\n\n6.6.1 Using built-in styles\nThe package includes a limited number of built-in styles including:\n\nzebra: alternating shaded and unshaded rows (zebra stripes)\ngrid: show all grid lines\nshade: shade the header row(s) in gray\ntimes: use a serif font\n\nThese styles can be selected using the topclass argument of table1. Some examples follow:\n\ntable1(~ sex + age + ulcer + thickness | status, \n       data = melanoma_data,\n       overall = c(left = \"Total\"), \n       caption = caption_char, \n       footnote = footnote_char, \n       topclass=\"Rtable1-zebra\"\n)\n\n\nTable 1. Melanoma Dataset Descriptive Statistics\n\n\n\nTotal(N=205)\nAlive(N=134)\nDied from melanoma(N=57)\nNon-Melanoma death(N=14)\n\n\n*Also known as Breslow thickness\n\n\n\nSex\n\n\n\n\n\n\nFemale\n126 (61.5%)\n91 (67.9%)\n28 (49.1%)\n7 (50.0%)\n\n\nMale\n79 (38.5%)\n43 (32.1%)\n29 (50.9%)\n7 (50.0%)\n\n\nAge (years)\n\n\n\n\n\n\nMean (SD)\n52.5 (16.7)\n50.0 (15.9)\n55.1 (17.9)\n65.3 (10.9)\n\n\nMedian [Min, Max]\n54.0 [4.00, 95.0]\n52.0 [4.00, 84.0]\n56.0 [14.0, 95.0]\n65.0 [49.0, 86.0]\n\n\nUlceration\n\n\n\n\n\n\nAbsent\n115 (56.1%)\n92 (68.7%)\n16 (28.1%)\n7 (50.0%)\n\n\nPresent\n90 (43.9%)\n42 (31.3%)\n41 (71.9%)\n7 (50.0%)\n\n\nThickness* (mm)\n\n\n\n\n\n\nMean (SD)\n2.92 (2.96)\n2.24 (2.33)\n4.31 (3.57)\n3.72 (3.63)\n\n\nMedian [Min, Max]\n1.94 [0.100, 17.4]\n1.36 [0.100, 12.9]\n3.54 [0.320, 17.4]\n2.26 [0.160, 12.6]\n\n\n\n\n\n\ntable1(~ sex + age + ulcer + thickness | status, \n       data = melanoma_data,\n       overall = c(left = \"Total\"), \n       caption = caption_char, \n       footnote = footnote_char, \n       topclass=\"Rtable1-grid\"\n)\n\n\nTable 1. Melanoma Dataset Descriptive Statistics\n\n\n\nTotal(N=205)\nAlive(N=134)\nDied from melanoma(N=57)\nNon-Melanoma death(N=14)\n\n\n*Also known as Breslow thickness\n\n\n\nSex\n\n\n\n\n\n\nFemale\n126 (61.5%)\n91 (67.9%)\n28 (49.1%)\n7 (50.0%)\n\n\nMale\n79 (38.5%)\n43 (32.1%)\n29 (50.9%)\n7 (50.0%)\n\n\nAge (years)\n\n\n\n\n\n\nMean (SD)\n52.5 (16.7)\n50.0 (15.9)\n55.1 (17.9)\n65.3 (10.9)\n\n\nMedian [Min, Max]\n54.0 [4.00, 95.0]\n52.0 [4.00, 84.0]\n56.0 [14.0, 95.0]\n65.0 [49.0, 86.0]\n\n\nUlceration\n\n\n\n\n\n\nAbsent\n115 (56.1%)\n92 (68.7%)\n16 (28.1%)\n7 (50.0%)\n\n\nPresent\n90 (43.9%)\n42 (31.3%)\n41 (71.9%)\n7 (50.0%)\n\n\nThickness* (mm)\n\n\n\n\n\n\nMean (SD)\n2.92 (2.96)\n2.24 (2.33)\n4.31 (3.57)\n3.72 (3.63)\n\n\nMedian [Min, Max]\n1.94 [0.100, 17.4]\n1.36 [0.100, 12.9]\n3.54 [0.320, 17.4]\n2.26 [0.160, 12.6]\n\n\n\n\n\n\ntable1(~ sex + age + ulcer + thickness | status, \n       data = melanoma_data,\n       overall = c(left = \"Total\"), \n       caption = caption_char, \n       footnote = footnote_char, \n       topclass=\"Rtable1-grid Rtable1-shade Rtable1-times\"\n)\n\n\nTable 1. Melanoma Dataset Descriptive Statistics\n\n\n\nTotal(N=205)\nAlive(N=134)\nDied from melanoma(N=57)\nNon-Melanoma death(N=14)\n\n\n*Also known as Breslow thickness\n\n\n\nSex\n\n\n\n\n\n\nFemale\n126 (61.5%)\n91 (67.9%)\n28 (49.1%)\n7 (50.0%)\n\n\nMale\n79 (38.5%)\n43 (32.1%)\n29 (50.9%)\n7 (50.0%)\n\n\nAge (years)\n\n\n\n\n\n\nMean (SD)\n52.5 (16.7)\n50.0 (15.9)\n55.1 (17.9)\n65.3 (10.9)\n\n\nMedian [Min, Max]\n54.0 [4.00, 95.0]\n52.0 [4.00, 84.0]\n56.0 [14.0, 95.0]\n65.0 [49.0, 86.0]\n\n\nUlceration\n\n\n\n\n\n\nAbsent\n115 (56.1%)\n92 (68.7%)\n16 (28.1%)\n7 (50.0%)\n\n\nPresent\n90 (43.9%)\n42 (31.3%)\n41 (71.9%)\n7 (50.0%)\n\n\nThickness* (mm)\n\n\n\n\n\n\nMean (SD)\n2.92 (2.96)\n2.24 (2.33)\n4.31 (3.57)\n3.72 (3.63)\n\n\nMedian [Min, Max]\n1.94 [0.100, 17.4]\n1.36 [0.100, 12.9]\n3.54 [0.320, 17.4]\n2.26 [0.160, 12.6]\n\n\n\n\n\nNote that the style name needs to be preceded by the prefix Rtable1-. Multiple styles can be applied in combination by separating them with a space.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Demographics Table With Table1</span>"
    ]
  },
  {
    "objectID": "lessons/01_table1.html#conclusion",
    "href": "lessons/01_table1.html#conclusion",
    "title": "\n6  Demographics Table With Table1\n",
    "section": "\n6.7 Conclusion",
    "text": "6.7 Conclusion\nIn conclusion, table1 is one of the most utilized tools in the scientific research field. Understanding how to use the table1 package in R can be of benefit to many. It is important to note that this presentation is just a brief summary with what is possible with this package. For example, you can add extra columns to the table, other than descriptive statistics. This can be accomplished using the extra.col option. In addition, you can also stratify the response variable to highlight two of the responses, like dead or alive in our example.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Demographics Table With Table1</span>"
    ]
  },
  {
    "objectID": "lessons/01_table1.html#references",
    "href": "lessons/01_table1.html#references",
    "title": "\n6  Demographics Table With Table1\n",
    "section": "References",
    "text": "References\n\n\n\n\n1. \nHayes-Larson E, Kezios KL, Mooney SJ, Lovasi G. Who is in this study, anyway? Guidelines for a useful Table 1. Journal of Clinical Epidemiology [Internet] 2019;114:125–32. Available from: http://dx.doi.org/10.1016/j.jclinepi.2019.06.011\n\n\n\n2. \nA. C. Davison, D. V. Hinkley. Bootstrap methods and their applications [Internet]. Cambridge: Cambridge University Press; 1997. Available from: doi:10.1017/CBO9780511802843\n\n\n\n3. \nCohen J. Statistical power analysis for the behavioral sciences. 2nd ed. Hillsdale, N.J: L. Erlbaum Associates; 1988.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Demographics Table With Table1</span>"
    ]
  },
  {
    "objectID": "lessons/01_gtsummary.html",
    "href": "lessons/01_gtsummary.html",
    "title": "\n7  Table by gtsummary\n",
    "section": "",
    "text": "7.1 Packages for this Lesson\n# Installing Required Packages\n# install.packages(\"public.ctn0094data\")\n# install.packages(\"tidyverse\")\n# install.packages(\"gtsummary\")\n\n# Loading Required Packages\nlibrary(public.ctn0094data)\nlibrary(tidyverse)\nlibrary(gtsummary)\nlibrary(dplyr) # for re-coding",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Table by gtsummary</span>"
    ]
  },
  {
    "objectID": "lessons/01_gtsummary.html#introduction-to-gtsummary",
    "href": "lessons/01_gtsummary.html#introduction-to-gtsummary",
    "title": "\n7  Table by gtsummary\n",
    "section": "\n7.2 Introduction to ‘gtsummary’",
    "text": "7.2 Introduction to ‘gtsummary’\nThe gtsummary package is useful mainly for creating publication-ready tables (i.e.demographic table, simple summary table, contingency-table, regression table, etc.). The best feature of this package is it can automatically detect if the data is continuous, dichotomous or categorical, and which descriptive statistics needs to apply.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Table by gtsummary</span>"
    ]
  },
  {
    "objectID": "lessons/01_gtsummary.html#data-source-and-description",
    "href": "lessons/01_gtsummary.html#data-source-and-description",
    "title": "\n7  Table by gtsummary\n",
    "section": "\n7.3 Data Source and Description",
    "text": "7.3 Data Source and Description\nThe public.ctn0094data package provides harmonized and normalized data sets from the CTN-0094 clinical trial. These data sets describe the experiences of care-seeking individuals suffering from opioid use disorder (OUD). The trial is part of the Clinical Trials Network (CTN) protocol number 0094, funded by the US National Institute of Drug Abuse (NIDA). It is used by the NIDA to develop, validate, refine, and deliver new treatment options to patients.\nIn this lesson, I used the demographics, and fagerstrom data sets from the public.ctn0094data package to demonstrate the gtsummary function. The demographics part contains the demographic variables such as age, sex, race, marital status etc. The fagerstrom part contains data on smoking habit (smoker/non-smoker, Fagerstrom Test for Nicotine Dependence Score (ranging from 0 to 10) ~ FTND, Number of cigarettes smoked per day.). The FTND is a questionnaire that assesses the physical dependence of adults on nicotine. The test uses yes/no questions scored from 0 to 1 and multiple-choice questions scored from 0 to 3, and the total score ranges from 0 to 10. The higher the score, the more intense the patient’s nicotine dependence is. The score categories are: 8+: High dependence, 7–5: Moderate dependence, 4–3: Low to moderate dependence and 0–2: Low dependence.\n\n# Searching suitable data sets: You can skip \ndata(package = \"public.ctn0094data\")\n#data(demographics, package = \"public.ctn0094data\")\n#names(demographics)\n#data(fagerstrom, package = \"public.ctn0094data\")\n#names(fagerstrom)\n#table(fagerstrom$ftnd)",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Table by gtsummary</span>"
    ]
  },
  {
    "objectID": "lessons/01_gtsummary.html#creating-model-data-frames",
    "href": "lessons/01_gtsummary.html#creating-model-data-frames",
    "title": "\n7  Table by gtsummary\n",
    "section": "\n7.4 Creating Model Data Frames",
    "text": "7.4 Creating Model Data Frames\nThe demographics and fagerstrom data sets within the public.ctn0094data package were joined by ID (who variable) and a new dta frame smoking_df is created.\n\n# Joining data sets: \nsmoking_df &lt;- demographics %&gt;% \n  left_join(fagerstrom, by = \"who\")",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Table by gtsummary</span>"
    ]
  },
  {
    "objectID": "lessons/01_gtsummary.html#demographic-table-with-tbl_summary-function",
    "href": "lessons/01_gtsummary.html#demographic-table-with-tbl_summary-function",
    "title": "\n7  Table by gtsummary\n",
    "section": "\n7.5 Demographic Table with tbl_summary Function",
    "text": "7.5 Demographic Table with tbl_summary Function\n\n7.5.1 Creating Table 1: Demographic Characteristic\nIn order to create a basic demographic table, I will now select which variables I want to show in the table and then use the tbl_summary function to create the table. I am also adding the description of the variables I included in my table.\n\n\nage: an integer variable that indicates the Age of the patient.\n\nrace: a factor variable with levels ‘Black’, ‘Other Refused/missing’, and ‘White’, which represents the Self-reported race of the patient.\n\neducation: a factor variable denotes the Education level at intake, with levels ‘HS/GED’ for high school graduate or equivalent, ‘Less than HS’ for less than high school education, ‘More than HS’ for some education beyond high school, and ‘Missing’ if the information is not provided.\n\nis_male: a factor variable with levels ‘No’ and ‘Yes’, describing the Sex (not gender) of the patient, where ‘Yes’ indicates male.\n\nmarital: a factor variable indicating the Marital status at intake, with levels ‘Married or Partnered’, ‘Never married’, ‘Separated/Divorced/Widowed’, and ‘Not answered’ if the question was not asked during intake.\n\nis_smoker: a factor indicating whether the patient is a smoker or not. Levels include “No” (not a smoker) and “Yes” (a smoker).\n\n\n# Selecting variables in a new data frame `table_1df` for table 1\ntable_1df &lt;- smoking_df %&gt;% \n  select(age, race, education, is_male, marital, is_smoker)\n\n# Table 1\ntable_1 &lt;- table_1df  %&gt;% tbl_summary()\n\ntable_1\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nN = 3,5601\n\n\n\n\nage\n34 (27, 45)\n\n\n    Unknown\n208\n\n\nrace\n\n\n\n    Black\n365 (10%)\n\n\n    Other\n506 (14%)\n\n\n    Refused/missing\n58 (1.6%)\n\n\n    White\n2,631 (74%)\n\n\neducation\n\n\n\n    HS/GED\n691 (39%)\n\n\n    Less than HS\n352 (20%)\n\n\n    More than HS\n724 (41%)\n\n\n    Unknown\n1,793\n\n\nis_male\n2,351 (66%)\n\n\n    Unknown\n4\n\n\nmarital\n\n\n\n    Married or Partnered\n329 (19%)\n\n\n    Never married\n1,028 (59%)\n\n\n    Separated/Divorced/Widowed\n394 (23%)\n\n\n    Unknown\n1,809\n\n\nis_smoker\n2,631 (85%)\n\n\n    Unknown\n460\n\n\n\n\n1 Median (IQR); n (%)\n\n\n\n\n\n\n\n7.5.2 Customizing Table 1: Changing the Label\nI am using label function to change the label of all variables. Other customization will be shown in the next contingency table.\n\n# Changing the Label\n\ntable_1 &lt;-\n  table_1df %&gt;% \n  tbl_summary(\n    label = list(\n      age = \"Age\",\n      race = \"Race\",\n      education = \"Education level\",\n      is_male = \"Male\",\n      marital = \"Marital status\",\n      is_smoker = \"Smoker\"\n    )\n  )\n\ntable_1\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nN = 3,5601\n\n\n\n\nAge\n34 (27, 45)\n\n\n    Unknown\n208\n\n\nRace\n\n\n\n    Black\n365 (10%)\n\n\n    Other\n506 (14%)\n\n\n    Refused/missing\n58 (1.6%)\n\n\n    White\n2,631 (74%)\n\n\nEducation level\n\n\n\n    HS/GED\n691 (39%)\n\n\n    Less than HS\n352 (20%)\n\n\n    More than HS\n724 (41%)\n\n\n    Unknown\n1,793\n\n\nMale\n2,351 (66%)\n\n\n    Unknown\n4\n\n\nMarital status\n\n\n\n    Married or Partnered\n329 (19%)\n\n\n    Never married\n1,028 (59%)\n\n\n    Separated/Divorced/Widowed\n394 (23%)\n\n\n    Unknown\n1,809\n\n\nSmoker\n2,631 (85%)\n\n\n    Unknown\n460\n\n\n\n\n1 Median (IQR); n (%)",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Table by gtsummary</span>"
    ]
  },
  {
    "objectID": "lessons/01_gtsummary.html#contingency-table-with-tbl_summary-function",
    "href": "lessons/01_gtsummary.html#contingency-table-with-tbl_summary-function",
    "title": "\n7  Table by gtsummary\n",
    "section": "\n7.6 Contingency Table with tbl_summary Function",
    "text": "7.6 Contingency Table with tbl_summary Function\n\n7.6.1 Creating Table 2: Demographic Variables by Smoking Status\nI will now show the table 1 demographic variables by smoking habit status (is_smoker, Yes = smoker and No = non-smokers)\n\n# Contingency table \ntable_2 &lt;- table_1df %&gt;% tbl_summary(by = is_smoker) \n\ntable_2\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nNo, N = 4691\n\n\nYes, N = 2,6311\n\n\n\n\nage\n36 (28, 47)\n33 (26, 44)\n\n\n    Unknown\n7\n79\n\n\nrace\n\n\n\n\n    Black\n46 (9.8%)\n259 (9.8%)\n\n\n    Other\n68 (14%)\n376 (14%)\n\n\n    Refused/missing\n1 (0.2%)\n18 (0.7%)\n\n\n    White\n354 (75%)\n1,978 (75%)\n\n\neducation\n\n\n\n\n    HS/GED\n104 (34%)\n531 (41%)\n\n\n    Less than HS\n23 (7.5%)\n290 (22%)\n\n\n    More than HS\n179 (58%)\n488 (37%)\n\n\n    Unknown\n163\n1,322\n\n\nis_male\n336 (72%)\n1,724 (66%)\n\n\nmarital\n\n\n\n\n    Married or Partnered\n75 (25%)\n236 (18%)\n\n\n    Never married\n152 (50%)\n775 (59%)\n\n\n    Separated/Divorced/Widowed\n77 (25%)\n293 (22%)\n\n\n    Unknown\n165\n1,327\n\n\n\n\n1 Median (IQR); n (%)\n\n\n\n\n\n\n\n7.6.2 Removing Missing Data\nIf I do not want to show the missing data in my table, I will use missing = \"no\".\n\n# Removing Missing Data\ntable_2nm &lt;- table_1df %&gt;% tbl_summary(by = is_smoker,\n                                   missing = \"no\") \ntable_2nm\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nNo, N = 4691\n\n\nYes, N = 2,6311\n\n\n\n\nage\n36 (28, 47)\n33 (26, 44)\n\n\nrace\n\n\n\n\n    Black\n46 (9.8%)\n259 (9.8%)\n\n\n    Other\n68 (14%)\n376 (14%)\n\n\n    Refused/missing\n1 (0.2%)\n18 (0.7%)\n\n\n    White\n354 (75%)\n1,978 (75%)\n\n\neducation\n\n\n\n\n    HS/GED\n104 (34%)\n531 (41%)\n\n\n    Less than HS\n23 (7.5%)\n290 (22%)\n\n\n    More than HS\n179 (58%)\n488 (37%)\n\n\nis_male\n336 (72%)\n1,724 (66%)\n\n\nmarital\n\n\n\n\n    Married or Partnered\n75 (25%)\n236 (18%)\n\n\n    Never married\n152 (50%)\n775 (59%)\n\n\n    Separated/Divorced/Widowed\n77 (25%)\n293 (22%)\n\n\n\n\n1 Median (IQR); n (%)\n\n\n\n\n\n\n\n7.6.3 Applying Statistical Tests\nI will use add_p function to show the statistical analysis. This will automatically detect if data in each variable is continuous, dichotomous or categorical, and apply the appropriate descriptive statistics accordingly.\n\n# Adding p-value\ntable_2 &lt;- table_1df %&gt;% tbl_summary(by = is_smoker,\n                                   missing = \"no\") %&gt;% \n  add_p()\n\ntable_2\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nNo, N = 4691\n\n\nYes, N = 2,6311\n\n\np-value2\n\n\n\n\nage\n36 (28, 47)\n33 (26, 44)\n&lt;0.001\n\n\nrace\n\n\n0.8\n\n\n    Black\n46 (9.8%)\n259 (9.8%)\n\n\n\n    Other\n68 (14%)\n376 (14%)\n\n\n\n    Refused/missing\n1 (0.2%)\n18 (0.7%)\n\n\n\n    White\n354 (75%)\n1,978 (75%)\n\n\n\neducation\n\n\n&lt;0.001\n\n\n    HS/GED\n104 (34%)\n531 (41%)\n\n\n\n    Less than HS\n23 (7.5%)\n290 (22%)\n\n\n\n    More than HS\n179 (58%)\n488 (37%)\n\n\n\nis_male\n336 (72%)\n1,724 (66%)\n0.010\n\n\nmarital\n\n\n0.006\n\n\n    Married or Partnered\n75 (25%)\n236 (18%)\n\n\n\n    Never married\n152 (50%)\n775 (59%)\n\n\n\n    Separated/Divorced/Widowed\n77 (25%)\n293 (22%)\n\n\n\n\n\n\n1 Median (IQR); n (%)\n\n\n\n2 Wilcoxon rank sum test; Fisher’s exact test; Pearson’s Chi-squared test\n\n\n\n\n\n\n\nNote: The footnote 2 shows all the statistical tests applied to this table. It can be understandable from the table that for categorical variable it applied Pearson’s Chi-squared test, for continuous non-normal distributed variable it applied Wilcoxon rank sum test; and for small sample data, it applied Fisher’s exact test. It would be great to see different footnotes for each of the test next to each p-value, however, I did not find a way to do that.\n\n7.6.4 Customizing Table 2(a)\nI will now customize the table 2 to show total number and overall number and show missing values by using the following functions:\n\n# Adding total and overall number \ntable_2 &lt;- table_1df %&gt;% tbl_summary(by = is_smoker,\n                                   label = list(\n                                     age = \"Age\",\n                                     race = \"Race\",\n                                     education = \"Education level\",\n                                     is_male = \"Male\",\n                                     marital = \"Marital status\"\n                                   ),\n                                   missing_text = \"(Missing)\"\n                                  ) %&gt;% \n  add_p() %&gt;%\n  add_n() %&gt;%\n  add_overall() \n\ntable_2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nN\n\nOverall, N = 3,1001\n\n\nNo, N = 4691\n\n\nYes, N = 2,6311\n\n\np-value2\n\n\n\n\nAge\n3,014\n34 (27, 45)\n36 (28, 47)\n33 (26, 44)\n&lt;0.001\n\n\n    (Missing)\n\n86\n7\n79\n\n\n\nRace\n3,100\n\n\n\n0.8\n\n\n    Black\n\n305 (9.8%)\n46 (9.8%)\n259 (9.8%)\n\n\n\n    Other\n\n444 (14%)\n68 (14%)\n376 (14%)\n\n\n\n    Refused/missing\n\n19 (0.6%)\n1 (0.2%)\n18 (0.7%)\n\n\n\n    White\n\n2,332 (75%)\n354 (75%)\n1,978 (75%)\n\n\n\nEducation level\n1,615\n\n\n\n&lt;0.001\n\n\n    HS/GED\n\n635 (39%)\n104 (34%)\n531 (41%)\n\n\n\n    Less than HS\n\n313 (19%)\n23 (7.5%)\n290 (22%)\n\n\n\n    More than HS\n\n667 (41%)\n179 (58%)\n488 (37%)\n\n\n\n    (Missing)\n\n1,485\n163\n1,322\n\n\n\nMale\n3,100\n2,060 (66%)\n336 (72%)\n1,724 (66%)\n0.010\n\n\nMarital status\n1,608\n\n\n\n0.006\n\n\n    Married or Partnered\n\n311 (19%)\n75 (25%)\n236 (18%)\n\n\n\n    Never married\n\n927 (58%)\n152 (50%)\n775 (59%)\n\n\n\n    Separated/Divorced/Widowed\n\n370 (23%)\n77 (25%)\n293 (22%)\n\n\n\n    (Missing)\n\n1,492\n165\n1,327\n\n\n\n\n\n\n1 Median (IQR); n (%)\n\n\n\n2 Wilcoxon rank sum test; Fisher’s exact test; Pearson’s Chi-squared test\n\n\n\n\n\n\n\n\n7.6.5 Customizing Table 2(b)\nI will now customize the title, caption and header and made the variable names bold of table 2 by using the following functions:\n\n# Adding title, caption and header \ntable_2 &lt;- table_1df %&gt;% tbl_summary(by = is_smoker,\n                                   label = list(\n                                     age = \"Age\",\n                                     race = \"Race\",\n                                     education = \"Education level\",\n                                     is_male = \"Male\",\n                                     marital = \"Marital status\"\n                                   ),\n                                   missing_text = \"(Missing)\"\n                                  ) %&gt;% \n  add_p() %&gt;%\n  add_n() %&gt;%\n  add_overall() %&gt;%\n  bold_labels() %&gt;%\n  modify_caption(\"Table 2. Demographic characteristics according to smoking status\") %&gt;%\n  modify_header(label ~ \"**Demographic characteristics**\") %&gt;%\n  modify_spanning_header(c(\"stat_1\", \"stat_2\") ~ \"**Smoking status**\") \n  \ntable_2\n\n\n\n\n\nTable 2. Demographic characteristics according to smoking status\n\n\n\n\n\n\n\n\n\n\nDemographic characteristics\nN\n\nOverall, N = 3,1001\n\nSmoking status\n\np-value2\n\n\n\n\nNo, N = 4691\n\n\nYes, N = 2,6311\n\n\n\n\n\nAge\n3,014\n34 (27, 45)\n36 (28, 47)\n33 (26, 44)\n&lt;0.001\n\n\n    (Missing)\n\n86\n7\n79\n\n\n\nRace\n3,100\n\n\n\n0.8\n\n\n    Black\n\n305 (9.8%)\n46 (9.8%)\n259 (9.8%)\n\n\n\n    Other\n\n444 (14%)\n68 (14%)\n376 (14%)\n\n\n\n    Refused/missing\n\n19 (0.6%)\n1 (0.2%)\n18 (0.7%)\n\n\n\n    White\n\n2,332 (75%)\n354 (75%)\n1,978 (75%)\n\n\n\nEducation level\n1,615\n\n\n\n&lt;0.001\n\n\n    HS/GED\n\n635 (39%)\n104 (34%)\n531 (41%)\n\n\n\n    Less than HS\n\n313 (19%)\n23 (7.5%)\n290 (22%)\n\n\n\n    More than HS\n\n667 (41%)\n179 (58%)\n488 (37%)\n\n\n\n    (Missing)\n\n1,485\n163\n1,322\n\n\n\nMale\n3,100\n2,060 (66%)\n336 (72%)\n1,724 (66%)\n0.010\n\n\nMarital status\n1,608\n\n\n\n0.006\n\n\n    Married or Partnered\n\n311 (19%)\n75 (25%)\n236 (18%)\n\n\n\n    Never married\n\n927 (58%)\n152 (50%)\n775 (59%)\n\n\n\n    Separated/Divorced/Widowed\n\n370 (23%)\n77 (25%)\n293 (22%)\n\n\n\n    (Missing)\n\n1,492\n165\n1,327\n\n\n\n\n\n\n1 Median (IQR); n (%)\n\n\n\n2 Wilcoxon rank sum test; Fisher’s exact test; Pearson’s Chi-squared test\n\n\n\n\n\n\n\n\n7.6.6 Customizing Table 2(c)\nHere, I am keeping only those customization that I prefer to have in my final table 2.\n\n# Final table\ntable_2 &lt;- table_1df %&gt;% tbl_summary(by = is_smoker,\n                                   label = list(\n                                     age = \"Age\",\n                                     race = \"Race\",\n                                     education = \"Education level\",\n                                     is_male = \"Male\",\n                                     marital = \"Marital status\"\n                                   ),\n                                   missing = \"no\"\n                                  ) %&gt;% \n  add_p() %&gt;%\n  #add_n() %&gt;%\n  #add_overall() %&gt;%\n  bold_labels() %&gt;%\n  modify_caption(\"Table 2. Demographic characteristics according to smoking status\") %&gt;%\n  modify_header(label ~ \"**Characteristics**\") %&gt;%\n  modify_spanning_header(c(\"stat_1\", \"stat_2\") ~ \"**Smoking Status**\") %&gt;%\n  modify_footnote(all_stat_cols() ~ \"Median (IQR) for Age; n (%) for all other variables\") \n  \ntable_2\n\n\n\n\n\nTable 2. Demographic characteristics according to smoking status\n\n\n\n\n\n\n\n\nCharacteristics\nSmoking Status\n\np-value2\n\n\n\n\nNo, N = 4691\n\n\nYes, N = 2,6311\n\n\n\n\n\nAge\n36 (28, 47)\n33 (26, 44)\n&lt;0.001\n\n\nRace\n\n\n0.8\n\n\n    Black\n46 (9.8%)\n259 (9.8%)\n\n\n\n    Other\n68 (14%)\n376 (14%)\n\n\n\n    Refused/missing\n1 (0.2%)\n18 (0.7%)\n\n\n\n    White\n354 (75%)\n1,978 (75%)\n\n\n\nEducation level\n\n\n&lt;0.001\n\n\n    HS/GED\n104 (34%)\n531 (41%)\n\n\n\n    Less than HS\n23 (7.5%)\n290 (22%)\n\n\n\n    More than HS\n179 (58%)\n488 (37%)\n\n\n\nMale\n336 (72%)\n1,724 (66%)\n0.010\n\n\nMarital status\n\n\n0.006\n\n\n    Married or Partnered\n75 (25%)\n236 (18%)\n\n\n\n    Never married\n152 (50%)\n775 (59%)\n\n\n\n    Separated/Divorced/Widowed\n77 (25%)\n293 (22%)\n\n\n\n\n\n\n1 Median (IQR) for Age; n (%) for all other variables\n\n\n\n2 Wilcoxon rank sum test; Fisher’s exact test; Pearson’s Chi-squared test\n\n\n\n\n\n\n\n\n7.6.7 Interpretation of Table 2\nInterpreting the variable Education level:\nNull Hypothesis (H₀): There is no association between education level and smoking status.\nAlternative Hypothesis (H₁): There is an association between education level and smoking status.\nSince the p-value is less than 0.001, we reject the null hypothesis. This indicates that there is a statistically significant association between education level and smoking status. However, to understand the nature of this association (whether education level affects smoking status or vice versa), further analysis would be needed.\n\n7.6.8 Missing value distribution in Table 2\nWe often want to see the missing value distribution among the the demographic variables. For example, we want to see the missing value distribution for the smoking status variable. First, we need to re-code the NA into a new category for is_smoker variable and recreate the table.\n\n7.6.8.1 Missing value data creation\n\n# Recoding `is_smoker` variable into `is_smoker_new`\ntable_1df &lt;- table_1df %&gt;% \n  mutate(is_smoker_new = ifelse(is.na(is_smoker), 99, is_smoker))  # converting all NA to 99\n\n# Convert into factor\ntable_1df$is_smoker_new &lt;- factor(table_1df$is_smoker_new,\n                                  levels = c(1, 2, 99),\n                                  labels = c(\"No\", \"Yes\", \"Missing\"))\n\n# New data frame \ntable_1df_new &lt;- table_1df %&gt;% \n  select(age, race, education, is_male, marital, is_smoker_new)\n\n\n7.6.8.2 Missing value table creation\n\n# Final table\ntable_2miss &lt;- table_1df_new %&gt;% tbl_summary(by = is_smoker_new,\n                                   label = list(\n                                     age = \"Age\",\n                                     race = \"Race\",\n                                     education = \"Education level\",\n                                     is_male = \"Male\",\n                                     marital = \"Marital status\"\n                                   ),\n                                   missing = \"no\"\n                                  ) %&gt;% \n  add_p() %&gt;%\n  #add_n() %&gt;%\n  #add_overall() %&gt;%\n  bold_labels() %&gt;%\n  modify_caption(\"Table 2. Demographic characteristics according to smoking status\") %&gt;%\n  modify_header(label ~ \"**Characteristics**\") %&gt;%\n  modify_spanning_header(c(\"stat_1\", \"stat_2\", \"stat_3\") ~ \"**Smoking Status**\") %&gt;%\n  modify_footnote(all_stat_cols() ~ \"Median (IQR) for Age; n (%) for all other variables\") \n  \ntable_2miss\n\n\n\n\n\nTable 2. Demographic characteristics according to smoking status\n\n\n\n\n\n\n\n\n\nCharacteristics\nSmoking Status\n\np-value2\n\n\n\n\nNo, N = 4691\n\n\nYes, N = 2,6311\n\n\nMissing, N = 4601\n\n\n\n\n\nAge\n36 (28, 47)\n33 (26, 44)\n39 (29, 47)\n&lt;0.001\n\n\nRace\n\n\n\n&lt;0.001\n\n\n    Black\n46 (9.8%)\n259 (9.8%)\n60 (13%)\n\n\n\n    Other\n68 (14%)\n376 (14%)\n62 (13%)\n\n\n\n    Refused/missing\n1 (0.2%)\n18 (0.7%)\n39 (8.5%)\n\n\n\n    White\n354 (75%)\n1,978 (75%)\n299 (65%)\n\n\n\nEducation level\n\n\n\n&lt;0.001\n\n\n    HS/GED\n104 (34%)\n531 (41%)\n56 (37%)\n\n\n\n    Less than HS\n23 (7.5%)\n290 (22%)\n39 (26%)\n\n\n\n    More than HS\n179 (58%)\n488 (37%)\n57 (38%)\n\n\n\nMale\n336 (72%)\n1,724 (66%)\n291 (64%)\n0.019\n\n\nMarital status\n\n\n\n&lt;0.001\n\n\n    Married or Partnered\n75 (25%)\n236 (18%)\n18 (13%)\n\n\n\n    Never married\n152 (50%)\n775 (59%)\n101 (71%)\n\n\n\n    Separated/Divorced/Widowed\n77 (25%)\n293 (22%)\n24 (17%)\n\n\n\n\n\n\n1 Median (IQR) for Age; n (%) for all other variables\n\n\n\n2 Kruskal-Wallis rank sum test; Pearson’s Chi-squared test",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Table by gtsummary</span>"
    ]
  },
  {
    "objectID": "lessons/01_gtsummary.html#regression-table-with-tbl_regression-function",
    "href": "lessons/01_gtsummary.html#regression-table-with-tbl_regression-function",
    "title": "\n7  Table by gtsummary\n",
    "section": "\n7.7 Regression Table with tbl_regression() Function",
    "text": "7.7 Regression Table with tbl_regression() Function\n\n7.7.1 Creating Regression Model\nHere, we are creating a logistic regression model where smoking status is the response variable, education is exploratory variable and age, race and sex are considered as confounders.\n\n# Building the Multivariable logistic model\nm1 &lt;- glm(is_smoker ~  education + age + race + is_male, \n          table_1df, \n          family = binomial)\n\n# View raw model results\nsummary(m1)$coefficients\n\n                         Estimate  Std. Error     z value     Pr(&gt;|z|)\n(Intercept)            3.50434562 0.389242905  9.00297879 2.196748e-19\neducationLess than HS  1.01143171 0.252724965  4.00210447 6.278157e-05\neducationMore than HS -0.60886151 0.144410039 -4.21619932 2.484542e-05\nage                   -0.04564764 0.006417912 -7.11253757 1.139285e-12\nraceOther             -0.24842217 0.315210858 -0.78811425 4.306299e-01\nraceRefused/missing    0.39602359 1.124629178  0.35213704 7.247355e-01\nraceWhite             -0.01922531 0.251971208 -0.07629961 9.391807e-01\nis_maleYes            -0.39712021 0.147363550 -2.69483338 7.042384e-03\n\n\n\n7.7.2 Creating Table 3: Regression Table\nHere, I am using tbl_regression function to see the regression results in the table. The exponentiate = TRUE shows the data as Odds Ratio after exponentiation of the beta values.\n\n# Creating Regression Table \ntable_3 &lt;- tbl_regression(m1, exponentiate = TRUE)\n\ntable_3\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nOR1\n\n\n95% CI1\n\np-value\n\n\n\neducation\n\n\n\n\n\n    HS/GED\n—\n—\n\n\n\n    Less than HS\n2.75\n1.71, 4.61\n&lt;0.001\n\n\n    More than HS\n0.54\n0.41, 0.72\n&lt;0.001\n\n\nage\n0.96\n0.94, 0.97\n&lt;0.001\n\n\nrace\n\n\n\n\n\n    Black\n—\n—\n\n\n\n    Other\n0.78\n0.42, 1.44\n0.4\n\n\n    Refused/missing\n1.49\n0.23, 29.4\n0.7\n\n\n    White\n0.98\n0.59, 1.59\n&gt;0.9\n\n\nis_male\n\n\n\n\n\n    No\n—\n—\n\n\n\n    Yes\n0.67\n0.50, 0.89\n0.007\n\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n7.7.3 Customizing Table 3\nHere, I have customized the table 3 by using functions I applied in table 1.\n\n# Customizing Regression Table \ntable_3 &lt;- tbl_regression(m1, exponentiate = TRUE,\n                           label = list(\n                             age = \"Age\",\n                             race = \"Race\",\n                             education = \"Education level\",\n                             is_male = \"Male\"\n                             ),\n                          missing = \"no\"\n                          ) %&gt;% \n  bold_labels() %&gt;%\n  bold_p(t = 0.10) %&gt;%  \n  italicize_levels() %&gt;%\n  modify_caption(\"Table 3. Logistic Regression for smoking status as response varialbe (n=3014)\")\n\ntable_3\n\n\n\n\n\nTable 3. Logistic Regression for smoking status as response varialbe (n=3014)\n\n\n\n\n\n\n\nCharacteristic\n\nOR1\n\n\n95% CI1\n\np-value\n\n\n\nEducation level\n\n\n\n\n\n    HS/GED\n—\n—\n\n\n\n    Less than HS\n2.75\n1.71, 4.61\n&lt;0.001\n\n\n    More than HS\n0.54\n0.41, 0.72\n&lt;0.001\n\n\nAge\n0.96\n0.94, 0.97\n&lt;0.001\n\n\nRace\n\n\n\n\n\n    Black\n—\n—\n\n\n\n    Other\n0.78\n0.42, 1.44\n0.4\n\n\n    Refused/missing\n1.49\n0.23, 29.4\n0.7\n\n\n    White\n0.98\n0.59, 1.59\n&gt;0.9\n\n\nMale\n\n\n\n\n\n    No\n—\n—\n\n\n\n    Yes\n0.67\n0.50, 0.89\n0.007\n\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n7.7.4 Interpreting Table 3\nInterpreting the variable Education level:\nFor individuals with less than high school education, the odds of being a smoker are 2.75 times higher compared to those with HS/GED, after adjusting for age, race, and sex.\nConversely, for individuals with more than high school education, the odds of being a smoker are 0.54 times lower compared to those with HS/GED, after adjusting for age, race, and sex.\nInterpreting the variable Age:\nFor each unit increase in age, the odds of being a smoker decrease by a factor of 0.96 (or 4%), after adjusting for education, race, and sex.\nIn R, for interpreting categorical variables, reference level is selected by alphabetic order, therefore, the HS/GED is selected as reference level (H), next one is Less than HS (L) and then More than HS (M).\n\n7.7.5 Changing the Reference Level in Table 3\nOften, we need to change the reference level as per our analysis need or aim of the study. We can select the specific reference level and run the table 3. First step is to check if the variable is in factor format. If it is not in factor format, we need to convert it into factor. Next, we can use the following codes to refer and use in table 3.\n\n7.7.5.1 New Model with New Reference Level\nHere I am creating model 2 (m2) wit the new reference as Less than HS for the education variable.\n\n# Check factor format\nstr(table_1df$education) # It shows that it is in factor format.\n\n Factor w/ 3 levels \"HS/GED\",\"Less than HS\",..: 3 3 3 3 NA 1 3 NA 1 3 ...\n\n# Building the glm model with specific reference level for education  = \"Less than HS\".\nm2 &lt;- glm(is_smoker ~  relevel(factor(education), ref = \"Less than HS\")  + age + race + is_male, \n          table_1df, \n          family = binomial)\n\n# View raw model results\nsummary(m2)$coefficients\n\n                                                                Estimate\n(Intercept)                                                   4.51577733\nrelevel(factor(education), ref = \"Less than HS\")HS/GED       -1.01143171\nrelevel(factor(education), ref = \"Less than HS\")More than HS -1.62029322\nage                                                          -0.04564764\nraceOther                                                    -0.24842217\nraceRefused/missing                                           0.39602359\nraceWhite                                                    -0.01922531\nis_maleYes                                                   -0.39712021\n                                                              Std. Error\n(Intercept)                                                  0.436459823\nrelevel(factor(education), ref = \"Less than HS\")HS/GED       0.252724965\nrelevel(factor(education), ref = \"Less than HS\")More than HS 0.244106320\nage                                                          0.006417912\nraceOther                                                    0.315210858\nraceRefused/missing                                          1.124629178\nraceWhite                                                    0.251971208\nis_maleYes                                                   0.147363550\n                                                                 z value\n(Intercept)                                                  10.34637575\nrelevel(factor(education), ref = \"Less than HS\")HS/GED       -4.00210447\nrelevel(factor(education), ref = \"Less than HS\")More than HS -6.63765370\nage                                                          -7.11253757\nraceOther                                                    -0.78811425\nraceRefused/missing                                           0.35213704\nraceWhite                                                    -0.07629961\nis_maleYes                                                   -2.69483338\n                                                                 Pr(&gt;|z|)\n(Intercept)                                                  4.346284e-25\nrelevel(factor(education), ref = \"Less than HS\")HS/GED       6.278157e-05\nrelevel(factor(education), ref = \"Less than HS\")More than HS 3.187156e-11\nage                                                          1.139285e-12\nraceOther                                                    4.306299e-01\nraceRefused/missing                                          7.247355e-01\nraceWhite                                                    9.391807e-01\nis_maleYes                                                   7.042384e-03\n\n\n\n7.7.5.2 Creating and Customizing New Table 3 with New Reference Level\nHere, I have created the new table 3 for m2 model and customized it accordingly.\n\n# Customizing Regression Table \ntable_3n &lt;- tbl_regression(m2, exponentiate = TRUE,  # Creating the table\n                           label = list(\n                             age = \"Age\",\n                             race = \"Race\",\n                             education = \"Education level\",\n                             is_male = \"Male\"\n                             ),\n                          missing = \"no\"\n                          ) %&gt;% \n  bold_labels() %&gt;%\n  bold_p(t = 0.10) %&gt;%  \n  italicize_levels() %&gt;%\n  modify_caption(\"Table 3. Logistic Regression for smoking status as response varialbe (n=3014)\")\n\ntable_3n\n\n\n\n\n\nTable 3. Logistic Regression for smoking status as response varialbe (n=3014)\n\n\n\n\n\n\n\nCharacteristic\n\nOR1\n\n\n95% CI1\n\np-value\n\n\n\nrelevel(factor(education), ref = \"Less than HS\")\n\n\n\n\n\n    Less than HS\n—\n—\n\n\n\n    HS/GED\n0.36\n0.22, 0.59\n&lt;0.001\n\n\n    More than HS\n0.20\n0.12, 0.31\n&lt;0.001\n\n\nAge\n0.96\n0.94, 0.97\n&lt;0.001\n\n\nRace\n\n\n\n\n\n    Black\n—\n—\n\n\n\n    Other\n0.78\n0.42, 1.44\n0.4\n\n\n    Refused/missing\n1.49\n0.23, 29.4\n0.7\n\n\n    White\n0.98\n0.59, 1.59\n&gt;0.9\n\n\nMale\n\n\n\n\n\n    No\n—\n—\n\n\n\n    Yes\n0.67\n0.50, 0.89\n0.007\n\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Table by gtsummary</span>"
    ]
  },
  {
    "objectID": "lessons/01_gtsummary.html#conclusion-take-home-message",
    "href": "lessons/01_gtsummary.html#conclusion-take-home-message",
    "title": "\n7  Table by gtsummary\n",
    "section": "\n7.8 Conclusion (Take Home Message)",
    "text": "7.8 Conclusion (Take Home Message)\n\nWe can use gtsummary package for creating publication-ready tables.\nThe tbl_summary() and the tbl_regression() are the frequently used functions in this package.\nMultiple other functions can be used to customize the table and can address the journal requirements.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Table by gtsummary</span>"
    ]
  },
  {
    "objectID": "02_header_one-sample.html",
    "href": "02_header_one-sample.html",
    "title": "One-Sample Tests",
    "section": "",
    "text": "Text Outline\nThis text covers the basics of:",
    "crumbs": [
      "One-Sample Tests"
    ]
  },
  {
    "objectID": "02_header_one-sample.html#text-outline",
    "href": "02_header_one-sample.html#text-outline",
    "title": "One-Sample Tests",
    "section": "",
    "text": "Exploring data\nOne-sample tests\nTwo-sample tests\nLinear regression and ANOVA\nGeneralized linear models\nSome special topics\nMaybe a bit about power calculations…",
    "crumbs": [
      "One-Sample Tests"
    ]
  },
  {
    "objectID": "02_header_one-sample.html#part-outline",
    "href": "02_header_one-sample.html#part-outline",
    "title": "One-Sample Tests",
    "section": "Part Outline",
    "text": "Part Outline\nThis part of the text contains the chapters on various one-sample statistical tests:\n\n\\(Z\\)-test\nPaired \\(t\\)-test\nPaired Wilcoxon test\nTransformations to Normality\nMcNemar’s Test\nFisher’s Exact Test\nChi-Square Goodness of Fit\nBootstrapped Confidence Intervals",
    "crumbs": [
      "One-Sample Tests"
    ]
  },
  {
    "objectID": "lessons/02_z-test_one_prop.html",
    "href": "lessons/02_z-test_one_prop.html",
    "title": "\n8  Z-Test for One Proportion\n",
    "section": "",
    "text": "8.1 Introduction to One-Sample \\(Z\\)-Tests\nThe one-sample \\(Z\\)-test is used to compare a sample proportion to a population proportion.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Z-Test for One Proportion</span>"
    ]
  },
  {
    "objectID": "lessons/02_z-test_one_prop.html#mathematical-definition-of-the-one-sample-z-test",
    "href": "lessons/02_z-test_one_prop.html#mathematical-definition-of-the-one-sample-z-test",
    "title": "\n8  Z-Test for One Proportion\n",
    "section": "\n8.2 Mathematical definition of the One-Sample \\(Z\\)-Test",
    "text": "8.2 Mathematical definition of the One-Sample \\(Z\\)-Test\nConsider a sample of size \\(n\\) with binary values (such as “true” or “false”). Let \\(p_{s}\\) and \\(p_{E}\\) be the observed sample and expected (population) proportions, respectively. The formula to calculate the \\(z\\) statistic is\n\\[\nz \\equiv \\frac{\n  p_s - p_E\n}{\n  \\sqrt{\n    \\frac{1}{n}p_s(1 - p_s)\n  }\n}.\n\\]",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Z-Test for One Proportion</span>"
    ]
  },
  {
    "objectID": "lessons/02_z-test_one_prop.html#data-source-and-description",
    "href": "lessons/02_z-test_one_prop.html#data-source-and-description",
    "title": "\n8  Z-Test for One Proportion\n",
    "section": "\n8.3 Data source and description",
    "text": "8.3 Data source and description\nWe will use the CTN-0094 data set, a data set of harmonized clinical trials for opioid use disorder. The full database is in public.ctn0094data::, engineered features are in public.ctn0094extra::, and clinical trial outcomes (wrangled dependent variables) are in CTNote::. We will install all three packages, but only use CTNote:: for now.\n\n# install.packages(\"public.ctn0094data\")\n# install.packages(\"public.ctn0094extra\")\n# install.packages(\"CTNote\")\n\nlibrary(CTNote)\nlibrary(tidyverse)",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Z-Test for One Proportion</span>"
    ]
  },
  {
    "objectID": "lessons/02_z-test_one_prop.html#cleaning-the-data-to-create-a-model-data-frame",
    "href": "lessons/02_z-test_one_prop.html#cleaning-the-data-to-create-a-model-data-frame",
    "title": "\n8  Z-Test for One Proportion\n",
    "section": "\n8.4 Cleaning the data to create a model data frame",
    "text": "8.4 Cleaning the data to create a model data frame\nBecause our method requires only one sample, we have very little work to do. We will use the Kosten et al. (1993) definition of opioid abstinence, provided in the data set outcomesCTN0094 as the column kosten1993_isAbs.\n\n# What do the values look like?\nsummary(outcomesCTN0094$kosten1993_isAbs)\n\n   Mode   FALSE    TRUE \nlogical    2158    1402 \n\n# How many samples are there?\nnrow(outcomesCTN0094)\n\n[1] 3560\n\n\nThere are 3560 logical values, and TRUE indicates that the trial participant achieved abstinence according to the definition used in Kosten et al. (1993).",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Z-Test for One Proportion</span>"
    ]
  },
  {
    "objectID": "lessons/02_z-test_one_prop.html#assumptions-of-the-one-sample-z-test",
    "href": "lessons/02_z-test_one_prop.html#assumptions-of-the-one-sample-z-test",
    "title": "\n8  Z-Test for One Proportion\n",
    "section": "\n8.5 Assumptions of the One-Sample \\(Z\\)-Test",
    "text": "8.5 Assumptions of the One-Sample \\(Z\\)-Test\nTo use a one-sample \\(Z\\)-test, we make the following assumptions:\n\nThe data are from a random sample\nEach observation in the data are independent\nNeither the sample proportion nor population proportions are “extreme”; usually we apply this method if these proportions are between 5% and 95%.\nThe data can be described as “successes” and “failures”, and there are at least 10 samples in each category.\n\nIf these assumptions hold, then \\[\nz \\sim N(0, 1).\n\\]",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Z-Test for One Proportion</span>"
    ]
  },
  {
    "objectID": "lessons/02_z-test_one_prop.html#checking-the-assumptions-with-plots",
    "href": "lessons/02_z-test_one_prop.html#checking-the-assumptions-with-plots",
    "title": "\n8  Z-Test for One Proportion\n",
    "section": "\n8.6 Checking the assumptions with plots",
    "text": "8.6 Checking the assumptions with plots\n\n8.6.1 Independence and Randomness\nBecause the samples were collected at random via an FDA approved clinical trial protocol, we assume that all the participants were randomly selected and are independent of each other.\n\n8.6.2 “Extreme” Proportions\nAccording to Ling et al. (2020), the 12-month abstinence proportion of all 533 participants in their study was 40.5 percent. As we can see here, our abstinence rates are 39.4. Neither these proportions are smaller than 5% or greater than 95%.\n\n(pExpected &lt;- 0.508 * (425/533))\n\n[1] 0.4050657\n\n# Count the number of TRUE values\n(nAbstinent &lt;- sum(outcomesCTN0094$kosten1993_isAbs))\n\n[1] 1402\n\n\n\n8.6.3 Type and Counts of Data\nWe observe binary data, and we see at least 10 successes and at least 10 failures.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Z-Test for One Proportion</span>"
    ]
  },
  {
    "objectID": "lessons/02_z-test_one_prop.html#code-to-run-a-one-sample-z-test",
    "href": "lessons/02_z-test_one_prop.html#code-to-run-a-one-sample-z-test",
    "title": "\n8  Z-Test for One Proportion\n",
    "section": "\n8.7 Code to run a One-Sample \\(Z\\)-Test",
    "text": "8.7 Code to run a One-Sample \\(Z\\)-Test\nNow that we have checked our assumptions, we can perform the one-sample \\(Z\\)-test for proportions.\n\nprop.test(\n  x = nAbstinent,\n  n = nrow(outcomesCTN0094),\n  p = pExpected\n)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  nAbstinent out of nrow(outcomesCTN0094), null probability pExpected\nX-squared = 1.8218, df = 1, p-value = 0.1771\nalternative hypothesis: true p is not equal to 0.4050657\n95 percent confidence interval:\n 0.3777537 0.4101176\nsample estimates:\n        p \n0.3938202",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Z-Test for One Proportion</span>"
    ]
  },
  {
    "objectID": "lessons/02_z-test_one_prop.html#brief-interpretation-of-the-output",
    "href": "lessons/02_z-test_one_prop.html#brief-interpretation-of-the-output",
    "title": "\n8  Z-Test for One Proportion\n",
    "section": "\n8.8 Brief interpretation of the output",
    "text": "8.8 Brief interpretation of the output\nThe 95% confidence interval contains the population proportion, so we fail to reject the hypothesis that the patients from these clinical trials achieve different abstinence rates than the general population.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Z-Test for One Proportion</span>"
    ]
  },
  {
    "objectID": "lessons/02_wilcoxon_one_samp.html",
    "href": "lessons/02_wilcoxon_one_samp.html",
    "title": "\n9  Wilcoxon Signed Rank Test for One Sample\n",
    "section": "",
    "text": "9.1 Introduction to Wilcoxson Signed Rank Test\nThe one-sample Wilcoxson Signed Rank Test is used to compare a sample proportion to a population proportion.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Wilcoxon Signed Rank Test for One Sample</span>"
    ]
  },
  {
    "objectID": "lessons/02_wilcoxon_one_samp.html#mathematical-definition-of-the-wilcoxson-signed-rank-test",
    "href": "lessons/02_wilcoxon_one_samp.html#mathematical-definition-of-the-wilcoxson-signed-rank-test",
    "title": "\n9  Wilcoxon Signed Rank Test for One Sample\n",
    "section": "\n9.2 Mathematical definition of the Wilcoxson Signed Rank Test",
    "text": "9.2 Mathematical definition of the Wilcoxson Signed Rank Test\nLet’s assume that we have one sample of size \\(n\\), \\(x_1, x_2, \\ldots, x_n\\), which cannot be approximated by a normal distribution. Because of this, we are no longer comparing \\(\\bar{x}\\) to \\(\\mu\\), but we are instead asking if the sample median is equal to a population median, \\(M\\). For more detail, see the maths here: https://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test.\nHere are the steps to calculate this test statistic manually:\n\nSubtract the population median from each sample: \\(x^*_i := x_1 - M\\)\n\nTake the absolute value of the shifted samples, \\(|x^*_i|\\).\nRank these absolute values.\nMultiply the signs of the shifted samples by the ranks of the absolute values.\nSum these products and compare them to a normal distribution with mean 0 and \\(\\sigma^2 = \\frac{1}{6}(2n+1)(n+1)n\\). (We will not explain the maths here to show why this can be approximately normal, or why this is the estimated variance.)\n\nConsider a simple example: we want to ask if the number of people visiting a local clinic per hour is different from the county median of 2.9 visits per hour. Here is a small sample of simulated (non-normal) data:\n\nset.seed(123)\n\nN &lt;- 15\nnClinicVisits &lt;- rpois(n = N, lambda = 4)\n\n# Plot the data and visually compare to the county median.\nhist(nClinicVisits)\nabline(v = 2.9, lwd = 2, col = \"red\")\n\n\n\n\n\n\n\nNow let’s go through our steps:\n\nsteps_df &lt;- tibble::tibble(x = nClinicVisits)\n\n# 1. shift the sample by the population median\nsteps_df$xStar &lt;- steps_df$x - 2.9\n\n# 2. absolute value\nsteps_df$absXStar &lt;- abs(steps_df$xStar)\n\n# 3. ranks\nsteps_df$xRank &lt;- rank(steps_df$absXStar)\n\n# 4. signs x ranks\nsteps_df$signRank &lt;- sign(steps_df$xStar) * steps_df$xRank\n\n# Inspect our steps\nsteps_df\n\n# A tibble: 15 × 5\n       x  xStar absXStar xRank signRank\n   &lt;int&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1     3  0.100    0.100   1.5      1.5\n 2     6  3.1      3.1    11.5     11.5\n 3     3  0.100    0.100   1.5      1.5\n 4     6  3.1      3.1    11.5     11.5\n 5     7  4.1      4.1    13.5     13.5\n 6     1 -1.9      1.9     9       -9  \n 7     4  1.1      1.1     6        6  \n 8     7  4.1      4.1    13.5     13.5\n 9     4  1.1      1.1     6        6  \n10     4  1.1      1.1     6        6  \n11     8  5.1      5.1    15       15  \n12     4  1.1      1.1     6        6  \n13     5  2.1      2.1    10       10  \n14     4  1.1      1.1     6        6  \n15     2 -0.9      0.9     3       -3  \n\n\nNow we can calculate the Wilcoxon Signed Rank test statisic and compare it to its asymptotic \\(p\\)-value.\n\n# 5. Compare sum to normal distribution and calculate the p-value\noneTailP &lt;- pnorm(\n  q = sum(steps_df$signRank),\n  mean = 0,\n  sd = sqrt((2 * N + 1) * (N + 1) * N / 6)\n)\n(1 - oneTailP) / 2\n\n[1] 0.001601623\n\n\nHow does this compare to the exact distribution \\(p\\)-value?\n\nwilcox.test(x = nClinicVisits, mu = 2.9)\n\nWarning in wilcox.test.default(x = nClinicVisits, mu = 2.9): cannot compute\nexact p-value with ties\n\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  nClinicVisits\nV = 108, p-value = 0.00672\nalternative hypothesis: true location is not equal to 2.9",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Wilcoxon Signed Rank Test for One Sample</span>"
    ]
  },
  {
    "objectID": "lessons/02_wilcoxon_one_samp.html#data-source-and-description",
    "href": "lessons/02_wilcoxon_one_samp.html#data-source-and-description",
    "title": "\n9  Wilcoxon Signed Rank Test for One Sample\n",
    "section": "\n9.3 Data source and description",
    "text": "9.3 Data source and description\nNow that we have seen how the test works, we will apply it to a real data scenario. We will use gene-level \\(p\\)-values from the Golub and Van Loan (1999) data set from the R package multtest:: (https://rdrr.io/bioc/multtest/man/golub.html); the original is a data set of data set of gene expression values for leukemia, but we have gene-specific \\(p\\)-values from a gene-level hypothesis test. We created these \\(p\\)-values in the script R/create_golub_data_20240523.R, but they do not represent any real analysis results.\n\nlibrary(tidyverse)",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Wilcoxon Signed Rank Test for One Sample</span>"
    ]
  },
  {
    "objectID": "lessons/02_wilcoxon_one_samp.html#cleaning-the-data-to-create-a-model-data-frame",
    "href": "lessons/02_wilcoxon_one_samp.html#cleaning-the-data-to-create-a-model-data-frame",
    "title": "\n9  Wilcoxon Signed Rank Test for One Sample\n",
    "section": "\n9.4 Cleaning the data to create a model data frame",
    "text": "9.4 Cleaning the data to create a model data frame\nBecause our method requires only one sample, we have very little work to do. We import the data set of \\(p\\)-values.\n\ngolub_pVals_num &lt;- readRDS(file = \"../data/02_golub_pVals_20240523.rds\")\n\nThere are 3051 \\(p\\)-values. The null hypothesis would be that there is no statistically significant effects in the data, so the distribution of these \\(p\\)-values should be a Uniform distribution. Our hypothesis is that the population mean is then 0.5 (the average value of a Uniform distribution).",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Wilcoxon Signed Rank Test for One Sample</span>"
    ]
  },
  {
    "objectID": "lessons/02_wilcoxon_one_samp.html#assumptions-of-the-wilcoxson-signed-rank-test",
    "href": "lessons/02_wilcoxon_one_samp.html#assumptions-of-the-wilcoxson-signed-rank-test",
    "title": "\n9  Wilcoxon Signed Rank Test for One Sample\n",
    "section": "\n9.5 Assumptions of the Wilcoxson Signed Rank Test",
    "text": "9.5 Assumptions of the Wilcoxson Signed Rank Test\nTo use a one-sample Wilcoxson Signed Rank Test, we make the following assumptions:\n\nThe data are from a random sample\nEach observation in the data are independent\nThe values can be “ranked” (this assumption gets fuzzy when you have discrete data, because it’s possible to get ties or values that are exactly 0 in those cases)\n\nIf these assumptions hold, then the test statistic is asymptotically normal. If your data has lots of zeros or equal values (which would result in tied ranks), then use this method with caution.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Wilcoxon Signed Rank Test for One Sample</span>"
    ]
  },
  {
    "objectID": "lessons/02_wilcoxon_one_samp.html#checking-the-assumptions",
    "href": "lessons/02_wilcoxon_one_samp.html#checking-the-assumptions",
    "title": "\n9  Wilcoxon Signed Rank Test for One Sample\n",
    "section": "\n9.6 Checking the assumptions",
    "text": "9.6 Checking the assumptions\n\n9.6.1 Independence and Randomness\nThese are gene-level \\(p\\)-values, so we do not have “independence”. However, because this is a pedagogical example, we will take a random sample of these genes to test (and this random sample should be independent enough, but we have no guarantee of this).\n\n# Create random sample of genes to test\nset.seed(20150516)\ngene_sample &lt;- sample(\n  x = golub_pVals_num,\n  size = 200,\n  replace = FALSE\n)\n\nWhat does the data distribution look like?\n\nhist(gene_sample)\n\n\n\n\n\n\n\nRemember, this is a “fake” analysis (all 38 samples in this data are leukemia cases, and I tested one half against the other—there should absolutely NOT be any real biological signal in this data).\n\n9.6.2 Type of Data\nThese values are \\(p\\)-values, so they can be ranked.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Wilcoxon Signed Rank Test for One Sample</span>"
    ]
  },
  {
    "objectID": "lessons/02_wilcoxon_one_samp.html#code-to-run-a-wilcoxson-signed-rank-test",
    "href": "lessons/02_wilcoxon_one_samp.html#code-to-run-a-wilcoxson-signed-rank-test",
    "title": "\n9  Wilcoxon Signed Rank Test for One Sample\n",
    "section": "\n9.7 Code to run a Wilcoxson Signed Rank Test",
    "text": "9.7 Code to run a Wilcoxson Signed Rank Test\nNow that we have checked our assumptions, we can perform the Wilcoxson Signed Rank Test on random samples of the genes to test if they have an average value of 0.5.\n\nwilcox.test(\n  x = gene_sample,\n  mu = 0.5, # average from all theoretical p-values under H0\n  alternative = \"less\" # H1: random p-values &lt; 0.5\n)\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  gene_sample\nV = 5859, p-value = 1.584e-07\nalternative hypothesis: true location is less than 0.5",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Wilcoxon Signed Rank Test for One Sample</span>"
    ]
  },
  {
    "objectID": "lessons/02_wilcoxon_one_samp.html#brief-interpretation-of-the-output",
    "href": "lessons/02_wilcoxon_one_samp.html#brief-interpretation-of-the-output",
    "title": "\n9  Wilcoxon Signed Rank Test for One Sample\n",
    "section": "\n9.8 Brief interpretation of the output",
    "text": "9.8 Brief interpretation of the output\nThe \\(p\\)-value for this test is less than 0.05, so we reject the hypothesis that the average gene-specific \\(p\\)-value for this set of results is greater than or equal to 0.5 (the theoretical average of \\(p\\)-values under the null hypothesis).",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Wilcoxon Signed Rank Test for One Sample</span>"
    ]
  },
  {
    "objectID": "lessons/02_mcnemar_paired_samp.html",
    "href": "lessons/02_mcnemar_paired_samp.html",
    "title": "\n10  McNemar’s Test\n",
    "section": "",
    "text": "10.1 Introduction to McNemar’s Test\nMcNemar’s Test is a non-parametric test used to analyze dichotomous data (in a 2 x 2) for paired samples. It is similar to a paired \\(t\\)-test, but for dichotomous rather than continuous variables. It is also akin to a Fisher’s exact test, but for paired data rather than un-paired data. The test requires one nominal dependent variable with 2 categories, and one independent variable with 2 dependent, mutually exclusive, groups. It is important to note that a “pair” can also represent a single individual’s pre- and post-test/intervention results.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>McNemar's Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_mcnemar_paired_samp.html#mathematical-definition-of-mcnemars-test",
    "href": "lessons/02_mcnemar_paired_samp.html#mathematical-definition-of-mcnemars-test",
    "title": "\n10  McNemar’s Test\n",
    "section": "\n10.2 Mathematical definition of McNemar’s Test",
    "text": "10.2 Mathematical definition of McNemar’s Test\nConsider binary repeated measured data for \\(n\\) observations; the observations come from one group under two conditions (e.g. “time period 1” vs “time period 2”, 0 vs. 1), the samples are treated with some treatment within equally across each condition, and the outcome of interest is dichotomous (such as “success” or “failure”, 1 vs. 0). If these restrictive assumptions are met, then the data can be compactly represented as a contingency table that looks like this:\n\n\n\nCondition 2 + (1)\nCondition 2 - (0)\n\n\n\nCondition 1 + (1)\n(a)\n(b)\n\n\nCondition 1 - (0)\n(c)\n(d)\n\n\n\nWhere:\n\n\na: is the number of pairs where both conditions are positive. E.g., the count of participants for whom the treatment was effective at time points 1 and 2.\n\nb: is the number of pairs where the first condition is positive and the second condition is negative. E.g., the count of participants for whom the treatment was effective at time 1 but not effective at time 2.\n\nc: is the number of pairs where the first condition is negative and the second condition is positive. E.g., the count of participants for whom the treatment was not effective at time 1 but was effective at time 2.\n\nd: is the number of pairs where both conditions are negative. E.g., the count of participants for whom the treatment was neither effective at time 1 nor at time 2.\n\nThe test focuses on the discordant pairs, b and c, which are pairs that change from one condition to the other. With a sufficiently large number of discordant pairs, McNemar’s Test follows a Chi-squared distribution with 1 degree of freedom. The formula for the test statistic, \\(\\chi^{2}_{\\text{Obs}}\\), is as follows:\n\\(\\chi^{2}_{\\text{Obs}} := \\frac{(b - c)^2}{b + c};\\ \\chi^{2}_{\\text{Obs}} \\sim \\chi^{2}_{\\nu = 1}.\\)",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>McNemar's Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_mcnemar_paired_samp.html#data-description",
    "href": "lessons/02_mcnemar_paired_samp.html#data-description",
    "title": "\n10  McNemar’s Test\n",
    "section": "\n10.3 Data Description",
    "text": "10.3 Data Description\nFor this lesson, we will look at patients in treatment for opioid use disorder, We will measure abstinence at the start and end of treatment via weekly Urine Drug Screen (UDS). The abstinence measurements are in the public.ctn0094extra::derived_weeklyOpioidPattern data set. Our definition of abstinence will be “3 consecutive negative urine screens” from weeks 2-4 (start of treatment) and weeks 10-12 (end of treatment).\n\n10.3.1 Cleaning UDS Data\n\ntxSuccess_df &lt;- \n  public.ctn0094extra::derived_weeklyOpioidPattern %&gt;% \n  # Combine UDS across treatment phases\n  mutate(udsPattern = paste0(Phase_1, Phase_2)) %&gt;% \n  # Extract the UDS patterns for the start and end of treatment\n  mutate(\n    startTxPattern = str_sub(udsPattern, start = 2, end = 4),\n    endTxPattern = str_sub(udsPattern, start = 10, end = 12)\n  ) %&gt;% \n  # Check for abstinence during the start and end of treatment\n  mutate(\n    startAbs = startTxPattern == \"---\",\n    endAbs = endTxPattern == \"---\"\n  ) %&gt;% \n  select(who, udsPattern, startAbs, endAbs) \n\ntxSuccess_df\n\n# A tibble: 3,560 × 4\n     who udsPattern                startAbs endAbs\n   &lt;int&gt; &lt;chr&gt;                     &lt;lgl&gt;    &lt;lgl&gt; \n 1     1 ooooooooooooooo           FALSE    FALSE \n 2     2 ----oo-o-o-o+o            TRUE     FALSE \n 3     3 o-ooo-ooooooooooooooooo   FALSE    FALSE \n 4     4 --------------------o-oo  TRUE     TRUE  \n 5     5 ooooooooooooooo           FALSE    FALSE \n 6     6 -ooooooooooooo            FALSE    FALSE \n 7     7 ----oooooooooooooooooooo  TRUE     FALSE \n 8     8 ooooooooooooooooooooooooo FALSE    FALSE \n 9     9 oooooooooooooooooooooo    FALSE    FALSE \n10    10 --o--*++o-++++++++o+-o    FALSE    FALSE \n# ℹ 3,550 more rows\n\n\n\n10.3.2 Creating Comparison Table from this Dataset\nNow that we have a binary measure of success at two time points (start and end of treatment), we can create a 2 x 2 contingency table:\n\ntxAbs_tbl &lt;- table(\n  # Rows of the table\n  txSuccess_df$startAbs,\n  # Columns of the table\n  txSuccess_df$endAbs\n)\n\ntxAbs_tbl\n\n       \n        FALSE TRUE\n  FALSE  2676  268\n  TRUE    395  221\n\n\nHere are the categories for each of the states of the patients:\n\n\na (FALSE & FALSE) means that the subject was abstinent from opioids neither at the start nor end of the trial,\n\nb (FALSE & TRUE) means that the subject was not abstinent from opioids at the start of the trial but abstinent at the end,\n\nc (TRUE & FALSE) means that the subject was abstinent from opioids at the start of the trial but not abstinent at the end, and\n\nd (TRUE & TRUE) means that the subject was abstinent from opioids both at the start and end of the trial.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>McNemar's Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_mcnemar_paired_samp.html#assumptions-of-mcnemars-test",
    "href": "lessons/02_mcnemar_paired_samp.html#assumptions-of-mcnemars-test",
    "title": "\n10  McNemar’s Test\n",
    "section": "\n10.4 Assumptions of McNemar’s Test",
    "text": "10.4 Assumptions of McNemar’s Test\nThe assumptions of McNemar’s Test are as follows:\n\n\nAssumption 1: You have one categorical dependent variable with two categories (i.e., a dichotomous variable) and one categorical independent variable with two related groups.\n\nAssumption 2: The two groups of the dependent variable are mutually exclusive, which means that the groups do not overlap—a participant can only be in one of the two groups.\n\nAssumption 3: The cases are a random sample from the population of interest.\n\nAssumption 4: At least 25 discordant pairs (\\(c + b \\geq 25\\))",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>McNemar's Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_mcnemar_paired_samp.html#checking-the-assumptions-of-mcnemars-test",
    "href": "lessons/02_mcnemar_paired_samp.html#checking-the-assumptions-of-mcnemars-test",
    "title": "\n10  McNemar’s Test\n",
    "section": "\n10.5 Checking the Assumptions of McNemar’s Test",
    "text": "10.5 Checking the Assumptions of McNemar’s Test\nTo check our assumptions, we will review and inspect the contingency table with the two variables of interest.\n\n# Printing the contigency table with margin totals and overal totals\naddmargins(txAbs_tbl)\n\n       \n        FALSE TRUE  Sum\n  FALSE  2676  268 2944\n  TRUE    395  221  616\n  Sum    3071  489 3560\n\n\nThe categorical dependent variable is abstinence of opioids in urine samples (e.g positive or negative for the substance); we see that the dependent variable at the start of treatment is related to the dependent variable at the end of treatment because we are detecting opioids within the same person. The categorical independent variable is the two time periods (start and end of treatment). The two groups are mutually exclusive, as urine cannot be simultaneously positive and negative and the participant cannot simultaneously be at the start and end of treatment. Finally, we note that the sum of the off-diagonal cells is at least 25.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>McNemar's Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_mcnemar_paired_samp.html#code-to-run-mcnemars-test",
    "href": "lessons/02_mcnemar_paired_samp.html#code-to-run-mcnemars-test",
    "title": "\n10  McNemar’s Test\n",
    "section": "\n10.6 Code to run McNemar’s Test",
    "text": "10.6 Code to run McNemar’s Test\nRecall that we created a 2x2 contingency table with the table() function above. This table object is one of the the data structures which can be supplied to the function mcnemar.test(). The other is the two columns of binary values (which can be coercible to binary factors).\n\n# Table Input Syntax\nmcnemar.test(txAbs_tbl)\n\n\n    McNemar's Chi-squared test with continuity correction\n\ndata:  txAbs_tbl\nMcNemar's chi-squared = 23.946, df = 1, p-value = 9.909e-07\n\n# Factor Vector Input Syntax\nmcnemar.test(\n  x = txSuccess_df$startAbs,\n  y = txSuccess_df$endAbs\n)\n\n\n    McNemar's Chi-squared test with continuity correction\n\ndata:  txSuccess_df$startAbs and txSuccess_df$endAbs\nMcNemar's chi-squared = 23.946, df = 1, p-value = 9.909e-07\n\n\nThe output from the McNemar’s test will show the Chi-Squared value, degrees of freedom (expected to be 1 as both categories only have 2 possible values), and the \\(p\\)-value.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>McNemar's Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_mcnemar_paired_samp.html#brief-interpretation-of-the-output",
    "href": "lessons/02_mcnemar_paired_samp.html#brief-interpretation-of-the-output",
    "title": "\n10  McNemar’s Test\n",
    "section": "\n10.7 Brief Interpretation of the Output",
    "text": "10.7 Brief Interpretation of the Output\nThe resulting \\(p\\)-value in this case is below 0.05, which indicates that the marginal probabilities between startAbs and endAbs are different. What is curious is that the count for start of treatment abstinence is higher than the count for end of treatment abstinence.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>McNemar's Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_mcnemar_paired_samp.html#conclusion",
    "href": "lessons/02_mcnemar_paired_samp.html#conclusion",
    "title": "\n10  McNemar’s Test\n",
    "section": "\n10.8 Conclusion",
    "text": "10.8 Conclusion\nIf you have paired samples, and the variable of interest is binary, then use McNemar’s test. The pairing could be within subject but across time or space, or it could represent different measures of success or failure on the same subject (of use in psychometrics). Regardless, it’s often that you’d like to include some covariate or additional factor, which this technique does not allow. In those cases, you may want Survival analysis (an event may occur or not within a time interval) or some other technique.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>McNemar's Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_cochrans_q_paired_samp.html",
    "href": "lessons/02_cochrans_q_paired_samp.html",
    "title": "\n11  Cochran’s Q Test\n",
    "section": "",
    "text": "11.1 Introduction to the Cochran’s \\(Q\\) Test\nAn extension of McNemar’s Test (a non-parametric test used to analyze dichotomous data (in a 2 x 2) for paired samples) is Cochran’s \\(Q\\) Test, which is a non-parametric test to analyze dichotomous data in a repeated measures design. Whereas McNemar’s Test would look at a binary outcome for the same individuals measured at only two time points, Cochran’s \\(Q\\) Test deals with binary outcomes for the same individuals at three or more time points. The test requires one nominal dependent variable with 2 categories, and one independent variable with 3 or more dependent, mutually exclusive, groups. Common data examples include presence/absence of a condition for a single individual at various time points in treatment, or positive/negative results for a single individual from various potentially-competing assessment instruments. It can also be used to evaluate “inter-observer variability” (where different judges of the same event/phenomenon measure success or failure) or “inter-instrument/concurrent validity” (where multiple instruments applied to the same subject measure success or failure).",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Cochran's Q Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_cochrans_q_paired_samp.html#mathematical-definition-of-cochrans-q-test",
    "href": "lessons/02_cochrans_q_paired_samp.html#mathematical-definition-of-cochrans-q-test",
    "title": "\n11  Cochran’s Q Test\n",
    "section": "\n11.2 Mathematical definition of Cochran’s \\(Q\\) Test",
    "text": "11.2 Mathematical definition of Cochran’s \\(Q\\) Test\nConsider binary repeated measured data for \\(n\\) observations; the observations come from one group under three or more conditions (e.g. “time period 1”, “time period 2”, “time period 3”, or “instrument 1” vs. “instrument 2” vs. “instrument 3”), the samples are treated with some treatment within equally across each condition, and the outcome of interest is dichotomous (such as “success” or “failure”, 1 vs. 0). The data then can be “wrangled” into a form that looks like this:\n\neg_df &lt;- tibble(\n  SubjectID = c(\"Bob\", \"Larry\", \"Junior\", \"Archibald\", \"Lunt\"),\n  test1 = c(0L, 1L, 1L, 0L, 0L),\n  test2 = c(0L, 1L, 1L, 0L, 1L),\n  test3 = c(1L, 1L, 1L, 0L, 1L)\n)\n\nknitr::kable(eg_df)\n\n\n\nSubjectID\ntest1\ntest2\ntest3\n\n\n\nBob\n0\n0\n1\n\n\nLarry\n1\n1\n1\n\n\nJunior\n1\n1\n1\n\n\nArchibald\n0\n0\n0\n\n\nLunt\n0\n1\n1\n\n\n\n\n\nThis is a simple example, but notice the structure of the data:\n\nThere are \\(B = 5\\) “blocks” (subjects, organisms, experimental units, etc.)\nThere are \\(K = 3\\) “experiments” (treatments, assessments, time points, exams, interviews, instruments, etc.)\nThe outcome of each “experiment” for each “block” is only ever binary (True/False, success/failure, presence/absence, etc.)\nThis data is in a \\(B \\times K\\) matrix.\n\nBefore we define the test statistic, we need some mathematical notation. Let \\(S(X_b)\\) be the row sum for row \\(b \\in 1, \\ldots, B\\) of the binary data matrix \\(X\\). Similarly let \\(S(X_k)\\) be the column sum for column \\(k \\in 1, \\ldots, K\\) of the binary data matrix \\(X\\). Finally, we define \\(N := S(X)\\) be the sum of all rows and columns. Using this notation, the formula for the test statistic is as follows:\n\\[\n\\chi^{2}_{\\text{Obs}} := k(k - 1) \\times\n\\frac{\n  \\sum\\limits_{k = 1}^K \\left[ S(X_k) -N/k \\right]^2\n}{\n  \\sum\\limits_{b = 1}^B S(X_b) \\left( k - S(X_b) \\right)\n},\n\\]\nwhere If the assumptions are met, then Cochran’s \\(Q\\) Test follows a Chi-squared distribution with \\(k - 1\\) degrees of freedom; that is, \\(\\chi^{2}_{\\text{Obs}} \\sim \\chi^{2}_{\\nu = k - 1}\\).",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Cochran's Q Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_cochrans_q_paired_samp.html#data-description",
    "href": "lessons/02_cochrans_q_paired_samp.html#data-description",
    "title": "\n11  Cochran’s Q Test\n",
    "section": "\n11.3 Data Description",
    "text": "11.3 Data Description\nFor this lesson, we will look at patients in treatment for opioid use disorder, We will measure abstinence within each month (4-week period) via weekly Urine Drug Screen (UDS). The abstinence measurements are in the public.ctn0094extra::derived_weeklyOpioidPattern data set. Our definition of abstinence will be “75% negative urine screens” from weeks 1-4 (start of treatment), weeks 5-8 (mid-treatment), and weeks 9-12 (end of treatment).\n\n11.3.1 Cleaning UDS Data\n\ntxSuccess_df &lt;- \n  public.ctn0094extra::derived_weeklyOpioidPattern %&gt;% \n  # Combine UDS across treatment phases\n  mutate(udsPattern = paste0(Phase_1, Phase_2)) %&gt;% \n  # The CTNote outcome builder functions are not vectorized, so we need to\n  #   use rowwise() to call them\n  rowwise() %&gt;% \n  # Count negative UDS in each phase of treatment\n  mutate(\n    nStartTxNeg = CTNote::count_matches(\n      udsPattern, match_is = \"-\", start = 1, end = 4\n    ),\n    nMidTxNeg = CTNote::count_matches(\n      udsPattern, match_is = \"-\", start = 5, end = 8\n    ),\n    nEndTxNeg = CTNote::count_matches(\n      udsPattern, match_is = \"-\", start = 9, end = 12\n    )\n  ) %&gt;% \n  # Check for abstinence during the start and end of treatment\n  mutate(\n    startAbs = nStartTxNeg &gt;= 3,\n    midAbs = nMidTxNeg &gt;= 3,\n    endAbs = nEndTxNeg &gt;= 3\n  ) %&gt;% \n  select(who, udsPattern, startAbs, midAbs, endAbs) %&gt;% \n  ungroup()\n\ntxSuccess_df\n\n# A tibble: 3,560 × 5\n     who udsPattern                startAbs midAbs endAbs\n   &lt;int&gt; &lt;chr&gt;                     &lt;lgl&gt;    &lt;lgl&gt;  &lt;lgl&gt; \n 1     1 ooooooooooooooo           FALSE    FALSE  FALSE \n 2     2 ----oo-o-o-o+o            TRUE     FALSE  FALSE \n 3     3 o-ooo-ooooooooooooooooo   FALSE    FALSE  FALSE \n 4     4 --------------------o-oo  TRUE     TRUE   TRUE  \n 5     5 ooooooooooooooo           FALSE    FALSE  FALSE \n 6     6 -ooooooooooooo            FALSE    FALSE  FALSE \n 7     7 ----oooooooooooooooooooo  TRUE     FALSE  FALSE \n 8     8 ooooooooooooooooooooooooo FALSE    FALSE  FALSE \n 9     9 oooooooooooooooooooooo    FALSE    FALSE  FALSE \n10    10 --o--*++o-++++++++o+-o    TRUE     FALSE  FALSE \n# ℹ 3,550 more rows\n\n\n\n11.3.2 Creating Comparison Table from this Dataset\nWe now have a binary measure of success at three time points (start, middle, and end of treatment). Our first step is to pivot the data so that we have 3 columns: who, when (start, middle, end), and value (“treatment success” measured as abstinence or not). Note that we also need to save the when column as an ordered factor so that the levels aren’t sorted alphabetically.\n\ntxSuccessLong_df &lt;- \n  txSuccess_df %&gt;% \n  select(-udsPattern) %&gt;% \n  pivot_longer(\n    cols = startAbs:endAbs,\n    names_to = \"when\",\n    values_to = \"abstinent\"\n  ) %&gt;% \n  mutate(\n    when = str_remove(when, pattern = \"Abs\")\n  ) %&gt;% \n  mutate(\n    when = factor(when, levels = c(\"start\", \"mid\", \"end\"), ordered = TRUE)\n  )\n\ntxSuccessLong_df\n\n# A tibble: 10,680 × 3\n     who when  abstinent\n   &lt;int&gt; &lt;ord&gt; &lt;lgl&gt;    \n 1     1 start FALSE    \n 2     1 mid   FALSE    \n 3     1 end   FALSE    \n 4     2 start TRUE     \n 5     2 mid   FALSE    \n 6     2 end   FALSE    \n 7     3 start FALSE    \n 8     3 mid   FALSE    \n 9     3 end   FALSE    \n10     4 start TRUE     \n# ℹ 10,670 more rows\n\n\nNow, we can create a 2 x 3 contingency table:\n\ntxAbs_tbl &lt;- stats::xtabs(\n  ~abstinent + when, data = txSuccessLong_df\n)\n\ntxAbs_tbl\n\n         when\nabstinent start  mid  end\n    FALSE  2599 2825 2765\n    TRUE    961  735  795",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Cochran's Q Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_cochrans_q_paired_samp.html#assumptions-of-cochrans-q-test",
    "href": "lessons/02_cochrans_q_paired_samp.html#assumptions-of-cochrans-q-test",
    "title": "\n11  Cochran’s Q Test\n",
    "section": "\n11.4 Assumptions of Cochran’s \\(Q\\) Test",
    "text": "11.4 Assumptions of Cochran’s \\(Q\\) Test\nThe assumptions of Cochran’s \\(Q\\) Test are as follows:\n\n\nAssumption 1: You have one categorical dependent variable with two categories (i.e., a dichotomous variable) and one categorical independent variable with \\(K \\ge 3\\) related groups.\n\nAssumption 2: The two groups of the dependent variable are mutually exclusive, which means that the groups do not overlap—a participant can only be in one of the two groups.\n\nAssumption 3: The cases are a random sample from the population of interest.\n\nAssumption 4: There are a sufficiently large number of independent subjects, \\(B\\) (also called “blocks”); note that there is no definition of exactly how many samples are needed to be considered “large”.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Cochran's Q Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_cochrans_q_paired_samp.html#checking-the-assumptions-of-cochrans-q-test",
    "href": "lessons/02_cochrans_q_paired_samp.html#checking-the-assumptions-of-cochrans-q-test",
    "title": "\n11  Cochran’s Q Test\n",
    "section": "\n11.5 Checking the Assumptions of Cochran’s \\(Q\\) Test",
    "text": "11.5 Checking the Assumptions of Cochran’s \\(Q\\) Test\nTo check our assumptions, we will review and inspect the contingency table with the two variables of interest. The categorical dependent variable is abstinence of opioids in urine samples (e.g positive or negative for the substance); we see that the dependent variable at the start of treatment is related to the dependent variable in the middle and at the end of treatment because we are detecting opioids within the same person. The categorical independent variable is the three time periods (start, middle, and end of treatment). The two groups are mutually exclusive, as urine cannot be simultaneously positive and negative and the participant cannot simultaneously be at the start and end of treatment. Finally, we note that there are 3560 samples, which counts as \\(B\\) being “large”.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Cochran's Q Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_cochrans_q_paired_samp.html#code-to-run-cochrans-q-test",
    "href": "lessons/02_cochrans_q_paired_samp.html#code-to-run-cochrans-q-test",
    "title": "\n11  Cochran’s Q Test\n",
    "section": "\n11.6 Code to run Cochran’s \\(Q\\) Test",
    "text": "11.6 Code to run Cochran’s \\(Q\\) Test\nRecall that we created a 2x3 contingency table with the xtab() function (from the stats:: package) above. This table object is helpful for us to check assumptions, but it is not a required data structure for the cochran_qtest() function (from the rstatix:: package). We can supply our pivoted “long” data to this function, but we have to use a creative formula object to define the experimental design. From the help documentation, this formula looks like a ~ b | c, where a is the outcome variable name (abstinent for us); b is the within-subjects factor variables (when); and c is the column name containing individuals/subjects identifier (who).\n\nrstatix::cochran_qtest(\n  data = txSuccessLong_df,\n  formula = abstinent ~ when | who\n)\n\n# A tibble: 1 × 6\n  .y.           n statistic    df        p method          \n* &lt;chr&gt;     &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;           \n1 abstinent  3560      84.4     2 4.64e-19 Cochran's Q test\n\n\nThe output from the Cochran’s \\(Q\\) Test will show the Chi-Squared test statistic, degrees of freedom (expected to be \\(3 - 1 = 2\\) as there are only 3 time points), and the \\(p\\)-value.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Cochran's Q Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_cochrans_q_paired_samp.html#brief-interpretation-of-the-output",
    "href": "lessons/02_cochrans_q_paired_samp.html#brief-interpretation-of-the-output",
    "title": "\n11  Cochran’s Q Test\n",
    "section": "\n11.7 Brief Interpretation of the Output",
    "text": "11.7 Brief Interpretation of the Output\nThe resulting \\(p\\)-value in this case is below 0.05, which indicates that the marginal probabilities between start, mid, and end measures of abstinent are different. Going back to review txAbs_tbl, we see that the count for start of treatment abstinence is higher than the counts for middle and end of treatment abstinence. Note that there is no “post-hoc” test for Cochran’s \\(Q\\) test, so if you really need to know which groups are different, then you have to use \\(k(k - 1)\\) pairwise McNemar tests (which is not a great idea).",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Cochran's Q Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_cochrans_q_paired_samp.html#conclusion",
    "href": "lessons/02_cochrans_q_paired_samp.html#conclusion",
    "title": "\n11  Cochran’s Q Test\n",
    "section": "\n11.8 Conclusion",
    "text": "11.8 Conclusion\nIf you have repeated paired samples, and the variable of interest is binary, then use Cochran’s \\(Q\\) Test. The pairing could be within subject but across time or space, or it could represent different measures of success or failure on the same subject (of use in psychometrics). Regardless, it’s often that you’d like to include some covariate or additional factor, which this technique does not allow. Also, you probably want to know which time points or which interventions are different. In those cases, you may want Survival analysis (an event may occur or not within a time interval) or some other more sophisticated technique.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Cochran's Q Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_chiSq_GoF.html",
    "href": "lessons/02_chiSq_GoF.html",
    "title": "\n12  The Chi Squared Goodness-of-Fit Test\n",
    "section": "",
    "text": "12.1 Introduction to the \\(\\chi^2\\) Goodness of Fit (GoF) Test\nNon-parametric tests and robust tests often have lower statistical power than their traditional counterparts. These more traditional hypothesis tests make distributional assumptions. For instance, a test may assume that the original observations are approximately normal or that counts follow a Poisson distribution. While we most commonly assess distributional assumptions visually, for instance with a Q-Q plot or histogram, there are some instances where we need to test adherence of data to a specified distribution with a proper statistical hypothesis test. One such test is the \\(\\chi^2\\) Goodness of Fit (GoF) test, published in the textbook Statistical Methods (Snedecor and Cochran, 1989). We draw some of our formulation and examples from this US NIST handbook: https://www.itl.nist.gov/div898/handbook/eda/section3/eda35f.htm.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>The Chi Squared Goodness-of-Fit Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_chiSq_GoF.html#mathematical-definition-of-the-chi2-goodness-of-fit-gof-test",
    "href": "lessons/02_chiSq_GoF.html#mathematical-definition-of-the-chi2-goodness-of-fit-gof-test",
    "title": "\n12  The Chi Squared Goodness-of-Fit Test\n",
    "section": "\n12.2 Mathematical definition of the \\(\\chi^2\\) Goodness of Fit (GoF) Test",
    "text": "12.2 Mathematical definition of the \\(\\chi^2\\) Goodness of Fit (GoF) Test\nThe idea of this test is to “bin” all the observations (think histogram) and then test the counts of the observations in each bin (observed) against the number of counts in the same bins from the target distribution (expected).\nConsider a sample of \\(n\\) observations, \\(x_1, x_2, \\ldots, x_n\\), from an unknown distribution with cumulative distribution function (CDF) \\(\\mathcal{F}_0\\) with support \\((-\\infty, \\infty)\\). Let \\(\\mathcal{F}_A\\) denote the CDF distribution to compare against (such as a Normal distribution, Gamma distribution, Negative Binomial distribution, or whatever distribution you think best describes the population data). Partition the \\(n\\) observations into \\(k+1\\) bins using \\(k\\) cut points \\(T_1, T_2, \\ldots, T_k\\), creating the non-intersecting intervals \\(\\{ {(-\\infty, T_1]},\\ (T_1, T_2],\\ \\ldots,\\ (T_k, \\infty)\\}\\) which span the support of \\(\\mathcal{F}_0\\). For the sake of notation, let \\(T_0 = -\\infty\\) and \\(T_{k+1} = \\infty\\).\nOnce we have the data partitions, we then tally the observed count in each bin, \\(O_1, O_2, \\ldots, O_{k+1}\\). We also find the probabilities of being in each bin \\((T_{i-1}, T_i],\\ i \\in 1, 2, \\ldots, k+1\\) from the target distribution \\(\\mathcal{F}_A\\). That is, calculate \\(p_i \\equiv \\mathcal{F}_A(T_i) - \\mathcal{F}_A(T_{i-1})\\), noting that \\(\\sum_{i = 1}^{k+1} p_i = 1\\). Multiply the probabilities for each bin, \\(p_i\\), by the total sample size, \\(n\\), to generate the expected count for each bin, \\(E_1, E_2, \\ldots, E_{k+1}\\).\nNow that we have the observed counts, \\(O_i\\), and the expected counts, \\(E_i\\), we calculate the \\(\\chi^2\\) Goodness of Fit Test statistic as \\[\n\\chi^2_{\\text{Obs}} \\equiv \\sum\\limits_{i = 1}^{k+1} \\frac{(O_i - E_i)^2}{E_i}.\n\\]\nThe distribution of this test statistic is approximately \\(\\chi^2\\) with \\(k^* + (p + 1)\\) degrees of freedom, where \\(k^*\\) is the number of non-empty bins and \\(p\\) is the number of parameters of \\(\\mathcal{F}_A\\). For example, let’s assume that we are comparing the data against a Beta distribution with two parameters \\(\\{\\alpha,\\beta\\}\\). Further, assume we choose to bin the observed data into deciles so that we have at least one observation in each bin. Thus, we would have \\(k^* = 10\\) bins and \\(p = 2\\) parameters. So, we would compare the test statistic \\(\\chi^2_{\\text{Obs}}\\) against a \\(\\chi^2\\) distribution with \\(k^* + (p + 1) = (10) + (2 + 1) = 13\\) degrees of freedom.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>The Chi Squared Goodness-of-Fit Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_chiSq_GoF.html#data-source-and-description",
    "href": "lessons/02_chiSq_GoF.html#data-source-and-description",
    "title": "\n12  The Chi Squared Goodness-of-Fit Test\n",
    "section": "\n12.3 Data source and description",
    "text": "12.3 Data source and description\nWhen we treat people who are addicted to licit and/or illicit substances, one potential predictor of recovery is baseline “risky health behaviours”; the Addictions, Drug, and Alcohol Institute hosts a copy of the Risk Behavior Survey questionnaire: https://adai.uw.edu/instruments/pdf/Risk_Behavior_Survey_209.pdf. While exploring the results of this questionnaire among patients in treatment, we might want to know if the total number of sexual partners (column txx_frq in the data set sex) follow a Poisson distribution.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>The Chi Squared Goodness-of-Fit Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_chiSq_GoF.html#cleaning-the-data-to-create-a-model-data-frame",
    "href": "lessons/02_chiSq_GoF.html#cleaning-the-data-to-create-a-model-data-frame",
    "title": "\n12  The Chi Squared Goodness-of-Fit Test\n",
    "section": "\n12.4 Cleaning the data to create a model data frame",
    "text": "12.4 Cleaning the data to create a model data frame\nThis data is already relatively clean, other than the missing values. Let’s remove them.\n\nsummary(public.ctn0094data::sex$txx_frq)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n    1.0     3.0     6.0    10.4    16.0    44.0    1327 \n\nnPartners_int &lt;- \n  public.ctn0094data::sex %&gt;% \n  drop_na(txx_frq) %&gt;% \n  pull(txx_frq) %&gt;% \n  as.integer()\n\nHere is a histogram of the total number of sexual partners for participants in the CTN-0094 data warehouse:\n\nhist(\n  nPartners_int,\n  main = \"Number of Sexual Partners per Patient\",\n  xlab = \"Count, using 'Sturges' Bins\"\n)\n\n\n\n\n\n\nFigure 12.1\n\n\n\n\nIt does not appear that this data follows a Poisson distribution. That said, a journal reviewer may still ask for a \\(p\\)-value as evidence.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>The Chi Squared Goodness-of-Fit Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_chiSq_GoF.html#assumptions-of-the-chi2-goodness-of-fit-gof-test",
    "href": "lessons/02_chiSq_GoF.html#assumptions-of-the-chi2-goodness-of-fit-gof-test",
    "title": "\n12  The Chi Squared Goodness-of-Fit Test\n",
    "section": "\n12.5 Assumptions of the \\(\\chi^2\\) Goodness of Fit (GoF) Test",
    "text": "12.5 Assumptions of the \\(\\chi^2\\) Goodness of Fit (GoF) Test\nRecall that the \\(k^*\\) parameter counts only the number of non-empty bins, so we should choose a small enough number of bins to ensure that all bins have at least a couple data points. I don’t actually know for sure what the minimum should be, but I’d say at least 5 per bin, but the traditional \\(\\chi^2\\) Test of Independence assumes at least 10 values per bin. The more bins you use (up to a point), the better the empirical CDF can approximate the shape of the distributional CDF. However, two things work against you: 1) more bins require more samples, and 2) more bins mean that you’re more likely to reject the null hypothesis even when you shouldn’t.\nSummary of Assumptions:\n\nYour samples are independent\nYour sample was taken at random\nYour samples are (or can be represented as) count data\nYou have enough samples to have a few per bin (at least 5-10)",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>The Chi Squared Goodness-of-Fit Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_chiSq_GoF.html#choosing-the-correct-number-of-bins",
    "href": "lessons/02_chiSq_GoF.html#choosing-the-correct-number-of-bins",
    "title": "\n12  The Chi Squared Goodness-of-Fit Test\n",
    "section": "\n12.6 Choosing the Correct Number of Bins",
    "text": "12.6 Choosing the Correct Number of Bins\nFor a test that depends on the number of bins, we would expect there to be some “correct” number. This is a reasonably important question. Unfortunately, as best as these authors are aware, there isn’t one best rule for selecting the correct number of bins. Thankfully, there has been some research done on the optimal number of bins for histograms (which is very similar). We can apply these various “rules” to the \\(\\chi^2\\) GoF test as well.\nThe standard method to accomplish this is to set breaks = k+1 in the hist() call. However, this number of bins doesn’t always change when you want it to: the help file says that if you give a number of bins for the breaks argument, that “the number is a suggestion only”. In order to “force” the number of bins to be what you want, you have to create a sequence of cut points yourself, using the seq() function. But note that the cut points include the minimum and the maximum of the data (\\(T_0\\) and \\(T_{k+1}\\)); so if you want 10 bins, you’ll need a vector of “cuts” with length 11. An example of this code is:\n\nnBins &lt;- 10\nseq(\n  from = min(nPartners_int),\n  to = max(nPartners_int),\n  length.out = nBins + 1\n)\n\n [1]  1.0  5.3  9.6 13.9 18.2 22.5 26.8 31.1 35.4 39.7 44.0\n\n\n\n12.6.1 Bins by Decile\nWe could divide the observations by into bins at every 10th percentile of the data; this would yield 197.2 participants per bin, on average. It would work pretty well for this data set, but that’s purely a coincidence (but see the odd trivia below the Sturge’s Rule section). This would yield the same default histogram as above.\n\nhist(\n  nPartners_int,\n  breaks = seq(\n    from = min(nPartners_int),\n    to = max(nPartners_int),\n    length.out = 10 + 1\n  ),\n  main = \"Number of Sexual Partners per Patient\",\n  xlab = \"Count, using Decile Bins\"\n)\n\n\n\n\n\n\nFigure 12.2\n\n\n\n\n\n12.6.2 A Bin for Each Unique Count\nA simple rule for discrete data would be a bin for each unique value. In our example, the most number of bins which make sense are all the observed counts (in our case, 44 bins). Let’s rebuild the above histogram with a bin for each unique count value.\n\nhist(\n  nPartners_int,\n  breaks = seq(\n    from = min(nPartners_int),\n    to = max(nPartners_int),\n    length.out = length(unique(nPartners_int)) + 1\n  ),\n  main = \"Number of Sexual Partners per Patient\",\n  xlab = \"Count, using Unique Value Bins\"\n)\n\n\n\n\n\n\nFigure 12.3\n\n\n\n\nThis retains some of the shape of the data we saw with 10 bins, but it’s very “noisy”. This is probably not a good option to use.\n\n12.6.3 Sturges Breaks\nWe should recall that in the first histogram, we changed the number of breaks from the default. The default number of histogram breaks in R is an option called \"Sturges\". Sturge’s Rule states that an optimal number of bins for a histogram (and subsequently, a \\(\\chi^2\\) GoF test) is \\(\\log_2(n) + 1\\), rounded up. For our example, this would be 12 breaks:\n\n# Sample size\nn_int &lt;- length(nPartners_int)\n\n# Sturge's rule: number of breaks (round up)\nlog2(n_int) + 1\n\n[1] 11.94544\n\n# Histogram\nhist(\n  nPartners_int,\n  breaks = seq(\n    from = min(nPartners_int),\n    to = max(nPartners_int),\n    length.out = 12 + 1\n  ),\n  main = \"Number of Sexual Partners per Patient\",\n  xlab = \"Count, using Sturge's Bins\"\n)\n\n\n\n\n\n\nFigure 12.4\n\n\n\n\n\nAside: why does using decile bins work here? According to Sturge’s rule, for any data set with number of samples between 70 and 2000, 10 bins is reasonably close to the optimal number calculated by Sturge’s rule (for 70 samples, the optimal Sturge’s bins is 8; for 2000, it’s 12). So while it’s purely coincidence for our data set, using decile-based bins will work for many of the data sets that students will encounter in their statistics and methods classes.\n\n\n12.6.4 Other Rules to Calculate the Breaks\nThere are a few other rules to calculate the number of bins, all rounded up:\n\nThe Square Root rule: \\(\\sqrt{n}\\)\n\nThe Rice rule: \\(\\sqrt[3]{2n}\\)\n\nScott’s rule: \\(R / \\frac{3.49s}{\\sqrt[3]{n}}\\), where \\(R\\) is the range of the data and \\(s\\) is the sample standard deviation (this rule assumes the data can be approximated by a Normal distribution)\nFreedman-Diaconis’s rule: \\(R / \\frac{3.49I}{\\sqrt[3]{n}}\\), where \\(R\\) is the range of the data and \\(I\\) is the sample interquartile range (this rule is an extension of Scott’s rule, and therefore also assumes the data can be approximated by a Normal distribution)\n\n\nNote: from what I’ve found online, there are a few different formulations to these rules, but I’m using those in the link above, as corroborated by page 26 of these notes from Prof. Fawcett at the University of Newcastle: http://www.mas.ncl.ac.uk/~nlf8/teaching/mas1343/notes/chap4-5.pdf\n\n\n# Range (for Scott's and F-D rules)\nrange_int &lt;- max(nPartners_int) - min(nPartners_int)\n\n# Square Root rule\nsqrt(n_int)\n\n[1] 44.40721\n\n# The Rice rule\n(2 * n_int) ^ (1/3)\n\n[1] 15.79958\n\n# Scott's rule\nrange_int / ( 3.49 * sd(nPartners_int) * n_int ^ (-1/3) )\n\n[1] 18.55387\n\n# Freedman-Diaconis's rule (two versions)\nrange_int / ( 2 * IQR(nPartners_int) / (n_int ^ (1/3)) )\n\n[1] 20.73946\n\nrange_int / ( 3.49 * IQR(nPartners_int) / (n_int ^ (1/3)) )\n\n[1] 11.88508\n\n\nSo here are the histograms using these rules:\n\npar(mfrow = c(2, 2))\n\nhist(\n  nPartners_int,\n  breaks = seq(\n    from = min(nPartners_int),\n    to = max(nPartners_int),\n    length.out = 45 + 1\n  ),\n  main = \"Number of Sexual Partners per Patient\",\n  xlab = \"Count, using Square Root rule for Bins\"\n)\n\nhist(\n  nPartners_int,\n  breaks = seq(\n    from = min(nPartners_int),\n    to = max(nPartners_int),\n    length.out = 16 + 1\n  ),\n  main = \"Number of Sexual Partners per Patient\",\n  xlab = \"Count, using The Rice rule for Bins\"\n)\n\nhist(\n  nPartners_int,\n  breaks = seq(\n    from = min(nPartners_int),\n    to = max(nPartners_int),\n    length.out = 21 + 1\n  ),\n  main = \"Number of Sexual Partners per Patient\",\n  xlab = \"Count, using Scott's rule for Bins\"\n)\n\nhist(\n  nPartners_int,\n  breaks = seq(\n    from = min(nPartners_int),\n    to = max(nPartners_int),\n    length.out = 12 + 1\n  ),\n  main = \"Number of Sexual Partners per Patient\",\n  xlab = \"Count, using Freedman-Diaconis's rule for Bins\"\n)\n\n\npar(mfrow = c(1, 1))\n\n\n\n\n\n\nFigure 12.5",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>The Chi Squared Goodness-of-Fit Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_chiSq_GoF.html#code-to-run-the-chi2-goodness-of-fit-gof-test",
    "href": "lessons/02_chiSq_GoF.html#code-to-run-the-chi2-goodness-of-fit-gof-test",
    "title": "\n12  The Chi Squared Goodness-of-Fit Test\n",
    "section": "\n12.7 Code to run the \\(\\chi^2\\) Goodness of Fit (GoF) Test",
    "text": "12.7 Code to run the \\(\\chi^2\\) Goodness of Fit (GoF) Test\nThe Sturge and Freedman-Diaconis rules both yielded 12 bins, so we will use this number. Here is a list of the information we need:\n\nthe original observed tally of sexual partners per patient\na choice for the number of bins (12, in this case), and\na target distribution (Poisson).\n\nRecall that from our summary statement, the minimum and maximum number of sexual partners were 1 and 44, respectively. The cut() function will return the number of intervals requested which divide these observed values. These would correspond to \\(x\\)-axis values of the each left and right column of our histogram. Here are our steps to assign each observed tally to its appropriate bin:\n\nbins_df &lt;- \n  tibble(original = nPartners_int) %&gt;% \n  # Create 12 bins\n  mutate(partition = cut(original, breaks = 12)) %&gt;% \n  # Extract lower and upper limit of the bins; retain the original partition\n  #   column to compare our work\n  separate(partition, into = c(\"lower\", \"upper\"), sep = \",\", remove = FALSE) %&gt;%\n  # Remove the leading \"(\" and trailing \"]\", then transform to numeric\n  mutate(\n    lower = as.numeric(str_sub(lower, start = 2L)),\n    upper = as.numeric(str_sub(upper, end = -2L))\n  ) %&gt;% \n  # Replace the smallest and largest limits with the support of the Poisson\n  #   distribution. Note that this will depend on the target distribution you \n  #   choose.\n  mutate(\n    lower = case_when(\n      lower == min(lower) ~ 0,\n      lower &gt; min(lower) ~ lower\n    ),\n    upper = case_when(\n      upper == max(upper) ~ Inf,\n      upper &lt; max(upper) ~ upper\n    )\n  )\n\nbins_df\n\n# A tibble: 1,972 × 4\n   original partition    lower upper\n      &lt;int&gt; &lt;fct&gt;        &lt;dbl&gt; &lt;dbl&gt;\n 1       16 (15.3,18.9]  15.3  18.9 \n 2        6 (4.58,8.17]   4.58  8.17\n 3       16 (15.3,18.9]  15.3  18.9 \n 4        9 (8.17,11.8]   8.17 11.8 \n 5       16 (15.3,18.9]  15.3  18.9 \n 6       20 (18.9,22.5]  18.9  22.5 \n 7        5 (4.58,8.17]   4.58  8.17\n 8        3 (0.957,4.58]  0     4.58\n 9        2 (0.957,4.58]  0     4.58\n10        5 (4.58,8.17]   4.58  8.17\n# ℹ 1,962 more rows\n\n\nNow, we can calculate the observed counts, the bin expected probabilities according to the specified Poisson distribution, and the expected counts\n\n# Poisson parameter\nestLambda_num &lt;- mean(nPartners_int)\n\nsteps_df &lt;- \n  bins_df %&gt;% \n  group_by(partition) %&gt;% \n  summarise(\n    lower = unique(lower),\n    upper = unique(upper),\n    observed = n()\n  ) %&gt;% \n  # Now calculate the probabilities of a random value from the target Poisson\n  #   distribution falling into these bins\n  mutate(\n    p = ppois(q = upper, lambda = estLambda_num) - \n      ppois(q = lower, lambda = estLambda_num)\n  ) %&gt;% \n  # And now the expected counts (I'm rounding here just for readability, so I \n  #   also include the non-rounded version for computation)\n  mutate(expected = p * n_int) %&gt;% \n  mutate(expected_rd = round(p * n_int, 1)) %&gt;% \n  # (O - E)^2 / E\n  mutate(summand = (observed - expected)^2 / expected)\n\n# Do our probabilities sum to 1?\nsum(steps_df$p)\n\n[1] 0.9999697\n\nsteps_df\n\n# A tibble: 12 × 8\n   partition    lower  upper observed        p expected expected_rd      summand\n   &lt;fct&gt;        &lt;dbl&gt;  &lt;dbl&gt;    &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;\n 1 (0.957,4.58]  0      4.58      610 2.24e- 2  4.42e+1        44.2      7.24e 3\n 2 (4.58,8.17]   4.58   8.17      440 2.67e- 1  5.26e+2       526.       1.40e 1\n 3 (8.17,11.8]   8.17  11.8        84 3.61e- 1  7.12e+2       712.       5.53e 2\n 4 (11.8,15.3]  11.8   15.3        31 2.86e- 1  5.64e+2       564.       5.03e 2\n 5 (15.3,18.9]  15.3   18.9       556 5.37e- 2  1.06e+2       106.       1.91e 3\n 6 (18.9,22.5]  18.9   22.5        47 1.01e- 2  1.98e+1        19.8      3.73e 1\n 7 (22.5,26.1]  22.5   26.1       130 4.92e- 4  9.70e-1         1        1.72e 4\n 8 (26.1,29.7]  26.1   29.7        32 1.22e- 5  2.41e-2         0        4.24e 4\n 9 (29.7,33.2]  29.7   33.2         5 5.55e- 7  1.10e-3         0        2.28e 4\n10 (33.2,36.8]  33.2   36.8        27 5.47e- 9  1.08e-5         0        6.76e 7\n11 (36.8,40.4]  36.8   40.4         4 1.30e-10  2.57e-7         0        6.22e 7\n12 (40.4,44]    40.4  Inf           6 6.11e-13  1.20e-9         0        2.99e10\n\n\nWe can now calculate the test statistic and \\(p\\) value.\n\n# Test statistic\n(chiSq_ts &lt;- sum(steps_df$summand))\n\n[1] 30026566644\n\n# critical value (k* = 12, p = 1)\n(chiSq_cv &lt;- qchisq(p = 1 - 0.025, df = 12 + 1 + 1))\n\n[1] 26.11895\n\n# The test statistic &gt; the critical value, so reject F_A as the distribution\n\n# p-value\n1 - pchisq(q = chiSq_ts, df = 12 + 1 + 1)\n\n[1] 0",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>The Chi Squared Goodness-of-Fit Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_chiSq_GoF.html#brief-interpretation-of-the-output",
    "href": "lessons/02_chiSq_GoF.html#brief-interpretation-of-the-output",
    "title": "\n12  The Chi Squared Goodness-of-Fit Test\n",
    "section": "\n12.8 Brief interpretation of the output",
    "text": "12.8 Brief interpretation of the output\nShockingly (this is sarcasm), the data that had two distinct peaks and looked nothing like a Poisson distribution can not actually be approximated by a Poisson distribution. Go figure. Regardless, this process can be replicated for your own data as well. We are showing off the \\(\\chi^2\\) GoF version because it works for ANY distribution you can think of.\n\n12.8.1 Brief Aside: Simple Normality Tests\nHowever, this all gets much simpler if you only care about testing for normality (which is usually a dumb thing to do anyway, but I digress). If your data are continuous and “normal-ish”, just use the Kolmogorov-Smirnov test, Shapiro-Wilk test, or the Anderson-Darling test instead. Our data are NOT “normal-ish”, so these results are invalid (but I’m showing you the code anyway).\n\n# Kolmogorov-Smirnov test\nks.test(nPartners_int, \"pnorm\")\n\nWarning in ks.test.default(nPartners_int, \"pnorm\"): ties should not be present\nfor the one-sample Kolmogorov-Smirnov test\n\n\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  nPartners_int\nD = 0.9311, p-value &lt; 2.2e-16\nalternative hypothesis: two-sided\n\n# Shapiro-Wilk test\nshapiro.test(nPartners_int)\n\n\n    Shapiro-Wilk normality test\n\ndata:  nPartners_int\nW = 0.86427, p-value &lt; 2.2e-16\n\n# Anderson-Darling test (requires the nortest:: package)\nnortest::ad.test(nPartners_int)\n\n\n    Anderson-Darling normality test\n\ndata:  nPartners_int\nA = 102.73, p-value &lt; 2.2e-16\n\n\nLook at those magical tiny \\(p\\)-values! So we’ve learned that non-normal data isn’t normal. How surprising (again, sarcasm).",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>The Chi Squared Goodness-of-Fit Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_chiSq_GoF.html#wrapping-up",
    "href": "lessons/02_chiSq_GoF.html#wrapping-up",
    "title": "\n12  The Chi Squared Goodness-of-Fit Test\n",
    "section": "\n12.9 Wrapping Up",
    "text": "12.9 Wrapping Up\nIf you need to check if your data are approximately normal, or approximately any other distribution, just plot the data. Use a density, Q-Q plot, or even a histogram. The material in this lesson is to help you when you run into a pesky reviewer / boss / collaborator (who just happens to be addicted to \\(p\\)-values) and they want to perform a test for distributional assumptions.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>The Chi Squared Goodness-of-Fit Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_transformations.html",
    "href": "lessons/02_transformations.html",
    "title": "\n13  Transformations to Normality\n",
    "section": "",
    "text": "13.1 Introduction\nThe pattern of values obtained when a variable is measured in a large number of individuals is called a distribution. Distributions can be broadly classified as normal and non-normal. The normal distribution is also called ‘Gaussian distribution’ as it was first described by K.F. Gauss. This chapter outlines the process of transforming data to achieve a normal distribution in R. Parametric methods, such as t-tests and ANOVA, require that the dependent (outcome) variable is approximately normally distributed within each group being compared. When the normality assumption is not satisfied, transforming the data can correct the non-normal distributions. For t-tests and ANOVA, it is sufficient to transform the dependent variable. However, for linear regression, transformations may be applied to the independent variable, the dependent variable, or both to achieve a linear relationship between variables and ensure homoscedasticity.\nHere are the libraries we will use for this material:\nlibrary(ggplot2)\nlibrary(gridExtra)\nlibrary(GGally)\nlibrary(moments)\nlibrary(knitr)\nlibrary(MASS)",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Transformations to Normality</span>"
    ]
  },
  {
    "objectID": "lessons/02_transformations.html#when-to-apply-transformations-to-normality",
    "href": "lessons/02_transformations.html#when-to-apply-transformations-to-normality",
    "title": "\n13  Transformations to Normality\n",
    "section": "\n13.2 When to Apply Transformations to Normality",
    "text": "13.2 When to Apply Transformations to Normality\nOne of the critical assumptions of statistical hypothesis testing is that the data are samples from a normal distribution. Therefore, it is essential to identify whether distributions are skewed or normal. There are several straightforward methods to detect skewness. Firstly, if the mean is less than twice the standard deviation, the distribution is likely skewed. Additionally, in a normally distributed population, the mean and standard deviation of the samples are independent. This characteristic can be used to detect skewness; if the standard deviation increases as the mean increases across groups from a population, the distribution is skewed. Beyond these simple methods, normality can be verified using statistical tests such as the Shapiro-Wilk test, the Kolmogorov-Smirnov test, and the Anderson-Darling test. Additionally, the moments package in R can be used to calculate skewness quantitatively. The skewness is determined using the third standardized moment, providing a measure of the asymmetry of the data distribution. If skewness is identified, efforts should be made to transform the data to achieve a normal distribution. This transformation is crucial for applying robust parametric tests in the analysis.\nTransformations can also be employed to facilitate comparison and interpretation. A classical example of a variable commonly reported after logarithmic transformation is the hydrogen ion concentration (pH). Another instance where transformation aids in data comparison is the logarithmic transformation of a dose-response curve. When plotted, the dose-response relationship is curvilinear; however, plotting the response against the logarithm of the dose (log dose-response plot) results in an elongated S-shaped curve. The middle portion of this curve forms a straight line, making it easier to compare two straight lines by measuring their slopes than to compare two curves. Thus, transformation can significantly enhance data comparison.\nIn summary, transformations can be applied to normalize data distribution or to simplify interpretation and comparison.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Transformations to Normality</span>"
    ]
  },
  {
    "objectID": "lessons/02_transformations.html#types-of-transformations-to-normality",
    "href": "lessons/02_transformations.html#types-of-transformations-to-normality",
    "title": "\n13  Transformations to Normality\n",
    "section": "\n13.3 Types of Transformations to Normality",
    "text": "13.3 Types of Transformations to Normality\nOften, the transformation that normalizes the distribution also equalizes the variance. While there are several types of transformations available, such as logarithmic, square root, reciprocal, cube root, and Box-Cox, the first three are the most commonly used. Among the transformations discussed in this section, the logarithmic transformation is the most often used. The following guidelines can help in selecting the appropriate method of transformation:\n\n13.3.1 Logarithmic Transformation\nIf the standard deviation is proportional to the mean, the distribution is positively skewed, making logarithmic transformation ideal. Note that when using a log transformation, a constant should be added to all values to ensure they are positive before transformation. The log tranformation is \\[\ny' = \\log(y + c),\n\\] where \\(y\\) is the original value and \\(c\\) is a constant added to ensure all values are positive.\n\n\n\n\n\n\n\n\n\n\n\n13.3.2 Square Root Transformation\nWhen the variance is proportional to the mean, square root transformation is preferred. This is particularly applicable to variables measured as counts, such as the number of malignant cells in a microscopic field or the number of deaths from swine flu. The square root transformation is: \\[\ny' = \\sqrt{y}.\n\\]\n\n\n\n\n\n\n\n\n\n\n\n13.3.3 Arithmetic Reciprocal Transformation\nIf the observations are truncated on the right (such as often the case for academic grade distributions), then one preliminary transformation is to “reverse” the values by subtracting each value from the maximum of all observed values (or from the maximum possible value for observations on a defined scale). This operation “flips” the data distribution from having a heavy left tail to having a heavy right tail, which allows us to perform a secondary transformation (such as a log or square root). This transformation is: \\[\ny' = \\max(y) - y.\n\\]\n\n\n\n\n\n\n\n\nNOTE: now that the data are right-skewed, other transformations can be applied as usual.\n\n13.3.4 Geometric Reciprocal Transformation\nIf the standard deviation is proportional to the mean squared, a reciprocal transformation is appropriate. This is typically used for highly variable quantities, such as serum creatinine levels. Note that this transformation requires all values to be positive or all values to be negative before applying it. \\[\ny' = \\frac{1}{y \\pm c},\n\\] where \\(y\\) is the original value and \\(c\\) is a constant added (subtracted) to ensure all values are positive (negative).\n\n\n\n\n\n\n\n\n\n\n\n13.3.5 Box-Cox Transformation\nThe Box-Cox transformation is a family of power transformations that can be used to stabilize variance and make the data more closely conform to a normal distribution, especially when the best power transformation (e.g., square root, logarithmic) is uncertain. By estimating an optimal parameter \\(\\lambda\\) from the data, the Box-Cox transformation tailors the transformation to the specific dataset’s needs. The transformation is defined as:\n\\[\ny(\\lambda) = \\begin{cases}\n  \\frac{y^\\lambda - 1}{\\lambda} & \\text{if } \\lambda \\neq 0 \\\\\n  \\log(y) & \\text{if } \\lambda = 0\n\\end{cases}\n\\] Here, \\(\\lambda\\) is a parameter that is estimated from the data. The Box-Cox transformation is particularly useful because it includes many of the other transformations (such as the logarithmic and square root transformations) as special cases.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Transformations to Normality</span>"
    ]
  },
  {
    "objectID": "lessons/02_transformations.html#examples-of-transformations-to-normality",
    "href": "lessons/02_transformations.html#examples-of-transformations-to-normality",
    "title": "\n13  Transformations to Normality\n",
    "section": "\n13.4 Examples of Transformations to Normality",
    "text": "13.4 Examples of Transformations to Normality\n\n13.4.1 Data Source and Description\nThe USJudgeRatings dataset is a built-in dataset in R that contains ratings of 43 judges in the US Superior Court. The ratings are based on the evaluations from lawyers who have had cases before these judges. The dataset includes multiple variables that represent different aspects of the judges’ performance.\n\n13.4.1.1 Variables in the Dataset\n\n\nCONT: Judicial “controlling” or authoritative nature.\n\nINTG: Judicial integrity.\n\nDMNR: Judicial demeanor.\n\nDILG: Judicial diligence.\n\nCFMG: Case flow management.\n\nDECI: Judicial decision-making.\n\nPREP: Judicial preparation.\n\nFAMI: Familiarity with the law.\n\nORAL: Oral skills.\n\nWRIT: Written skills.\n\nPHYS: Physical ability.\n\nRTEN: Willingness to follow trends.\n\nThis dataset is useful for analyzing various performance metrics of judges and can be used to explore relationships between different aspects of judicial performance. In the following examples, we’ll consider two variables:\n\n\nCONT: Number of contacts of lawyer with judge. Positively skewed.\n\nPHYS: Physical ability. Negatively skewed\n\n13.4.2 Loading the Data\n\n# Load the USJudgeRatings dataset\ndata(\"USJudgeRatings\")\ndf &lt;- USJudgeRatings\n\n# Display the first few rows of the dataset\nhead(df)\n\n               CONT INTG DMNR DILG CFMG DECI PREP FAMI ORAL WRIT PHYS RTEN\nAARONSON,L.H.   5.7  7.9  7.7  7.3  7.1  7.4  7.1  7.1  7.1  7.0  8.3  7.8\nALEXANDER,J.M.  6.8  8.9  8.8  8.5  7.8  8.1  8.0  8.0  7.8  7.9  8.5  8.7\nARMENTANO,A.J.  7.2  8.1  7.8  7.8  7.5  7.6  7.5  7.5  7.3  7.4  7.9  7.8\nBERDON,R.I.     6.8  8.8  8.5  8.8  8.3  8.5  8.7  8.7  8.4  8.5  8.8  8.7\nBRACKEN,J.J.    7.3  6.4  4.3  6.5  6.0  6.2  5.7  5.7  5.1  5.3  5.5  4.8\nBURNS,E.B.      6.2  8.8  8.7  8.5  7.9  8.0  8.1  8.0  8.0  8.0  8.6  8.6\n\n\n\n13.4.3 Visualizations of CONT and PHYS Variables\n\nggplot(df) + \n  aes(x = CONT) + \n  scale_x_continuous(limits = c(3, 12))+\n  labs(title = \"Density Plot of CONT\", x = \"CONT\", y = \"Density\") +\n  geom_density(fill = \"lightgray\") +\n  stat_function(\n    fun = dnorm,\n    args = list(mean = mean(df$CONT), sd = sd(df$CONT)),\n    color = \"red\",\n    linetype = \"dashed\"\n  ) \n\n\n\n\n\n\nFigure 13.1: Distribution of CONT Variable\n\n\n\n\n\nggplot(df) +\n  aes(x = PHYS) + \n  scale_x_continuous(limits = c(3, 12)) +\n  labs(title = \"Density Plot of PHYS\", x = \"PHYS\", y = \"Density\") +\n  geom_density(fill = \"lightgray\") +\n  stat_function(\n    fun = dnorm,\n    args = list(mean = mean(df$PHYS), sd = sd(df$PHYS)),\n    color = \"red\",\n    linetype = \"dashed\"\n  )\n\n\n\n\n\n\nFigure 13.2: Distribution of PHYS Variable\n\n\n\n\n\n13.4.4 Summary Statistics for CONT and PHYS Variables\n\n# Get the summary statistics for CONT and PHYS variables; note that the\n#   summary() function returns a named numeric vector, so to preserve the names\n#   we transform this vector to a matrix first (before creating the data frame).\nsummary_df &lt;- data.frame(\n  CONT = as.matrix(summary(df$CONT)),\n  PHYS = as.matrix(summary(df$PHYS))\n)\n\n# Display summary statistics as a table\nkable(summary_df)\n\n\nTable 13.1: Summary Statistics for CONT and PHYS Variables\n\n\n\n\n\nCONT\nPHYS\n\n\n\nMin.\n5.700000\n4.700000\n\n\n1st Qu.\n6.850000\n7.700000\n\n\nMedian\n7.300000\n8.100000\n\n\nMean\n7.437209\n7.934884\n\n\n3rd Qu.\n7.900000\n8.500000\n\n\nMax.\n10.600000\n9.100000\n\n\n\n\n\n\n\n\n\n13.4.5 Skewness and Kurtosis for CONT and PHYS Variables\n\n# Calculate skewness and kurtosis for CONT and PHYS variables with moments:: \nskewness_vals &lt;- sapply(df[, c(\"CONT\", \"PHYS\")], moments::skewness)\nkurtosis_vals &lt;- sapply(df[, c(\"CONT\", \"PHYS\")], moments::kurtosis)\n\n# Create a data frame to display skewness and kurtosis\nskew_kurt_df &lt;- data.frame(\n  Skewness = skewness_vals,\n  Kurtosis = kurtosis_vals\n)\n\n# Display the data frame as a table\nkable(skew_kurt_df)\n\n\nTable 13.2: Skewness and Kurtosis for CONT and PHYS Variables\n\n\n\n\n\nSkewness\nKurtosis\n\n\n\nCONT\n1.085973\n4.729637\n\n\nPHYS\n-1.558215\n5.408086\n\n\n\n\n\n\n\n\n\n13.4.6 Visualizations of Transformed CONT and PHYS Variables\nWe will first apply a natural log transformation to the “controlling/authoritarian” variable.\n\n# Apply log transformation to CONT variable\ndf$LOG_CONT &lt;- log(df$CONT)\n\n# Plot density of log-transformed CONT variable\nggplot(df) + \n  aes(x = LOG_CONT) +\n  geom_density(fill = \"lightgray\") +\n  labs(\n    title = \"Density Plot of Log-Transformed CONT\",\n    x = \"Log-Transformed CONT\",\n    y = \"Density\"\n  ) +\n  stat_function(\n    fun = dnorm,\n    args = list(mean = mean(df$LOG_CONT), sd = sd(df$LOG_CONT)),\n    color = \"red\",\n    linetype = \"dashed\"\n  )\n\n\n\n\n\n\nFigure 13.3: Distribution of log transformed CONT Variable\n\n\n\n\nNow we will apply a Box-Cox transformation to the “physical ability” variable.\n\n# Apply Box-Cox transformation to PHYS using MASS:: package\nbc &lt;- MASS::boxcox(df$PHYS ~ 1, lambda = seq(-5, 5, 0.1), plotit = TRUE)\nlambda &lt;- bc$x[which.max(bc$y)]\ndf$BOX_COX_PHYS &lt;- (df$PHYS^lambda - 1) / lambda\n\n\n\n\n\n\nFigure 13.4: Distribution of optimal lambda determined by the boxcox function\n\n\n\n\n\n# Plot density of Box-Cox transformed PHYS variable\nggplot(df) +\n  aes(x = BOX_COX_PHYS) +\n  labs(\n    title = \"Density Plot of Box-Cox Transformed PHYS\",\n    x = \"Box-Cox Transformed PHYS\",\n    y = \"Density\"\n  ) + \n  geom_density(fill = \"lightgray\") +\n  stat_function(\n    fun = dnorm,\n    args = list(mean = mean(df$BOX_COX_PHYS), sd = sd(df$BOX_COX_PHYS)),\n    color = \"red\",\n    linetype = \"dashed\"\n  )\n\n\n\n\n\n\nFigure 13.5: Distribution of Box-Cox transformed PHYS Variable\n\n\n\n\n\n13.4.7 Skewness and Kurtosis for Transformed CONT and PHYS Variables\n\n# Calculate skewness and kurtosis for transformed variables\nskewness_vals_trans &lt;- sapply(df[, c(\"LOG_CONT\", \"BOX_COX_PHYS\")], skewness)\nkurtosis_vals_trans &lt;- sapply(df[, c(\"LOG_CONT\", \"BOX_COX_PHYS\")], kurtosis)\n\n# Create a data frame to display skewness and kurtosis for transformed variables\nskew_kurt_df_trans &lt;- data.frame(\n  Skewness = skewness_vals_trans,\n  Kurtosis = kurtosis_vals_trans\n)\n\n# Display the data frame as a table\nkable(skew_kurt_df_trans)\n\n\nTable 13.3: Skewness and Kurtosis for Transformed Variables (LOG_CONT and BOX_COX_PHYS)\n\n\n\n\n\nSkewness\nKurtosis\n\n\n\nLOG_CONT\n0.6555572\n3.758254\n\n\nBOX_COX_PHYS\n-0.3813574\n2.501916",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Transformations to Normality</span>"
    ]
  },
  {
    "objectID": "lessons/02_transformations.html#results",
    "href": "lessons/02_transformations.html#results",
    "title": "\n13  Transformations to Normality\n",
    "section": "\n13.5 Results",
    "text": "13.5 Results\nThe application of log and Box-Cox transformations has effectively improved the normality of the CONT and PHYS variables, respectively. For the CONT variable, the original distribution exhibited a positive skewness of 1.086 and a kurtosis of 4.730, indicating a right-skewed distribution with heavy tails and a sharp peak. The log transformation reduced the skewness to 0.656 and the kurtosis to 3.758, demonstrating a significant move towards normality, though the distribution still retains some right-skewness and heavier tails compared to a normal distribution. The PHYS variable originally had a negative skewness of -1.558 and a kurtosis of 5.408, reflecting a left-skewed distribution with heavy tails and a pronounced peak. Following the Box-Cox transformation, the skewness was reduced to -0.381 and the kurtosis to 2.502. These results indicate that the transformed PHYS distribution is much closer to normality, with reduced skewness and lighter tails, achieving a more symmetric distribution. In summary, the transformations have substantially mitigated the skewness and kurtosis of both variables, enhancing their suitability for statistical analyses that assume normality. This adjustment ensures more reliable and valid results in subsequent analyses.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Transformations to Normality</span>"
    ]
  },
  {
    "objectID": "lessons/02_transformations.html#conclusion-and-discussion",
    "href": "lessons/02_transformations.html#conclusion-and-discussion",
    "title": "\n13  Transformations to Normality\n",
    "section": "\n13.6 Conclusion and Discussion",
    "text": "13.6 Conclusion and Discussion\nThe transformations applied to the CONT and PHYS variables demonstrate the effectiveness of data transformation techniques in improving the normality of distributions. By addressing skewness and kurtosis, transformations like the log and Box-Cox methods help in stabilizing variance and making data more symmetric. This enhancement is crucial for statistical analyses that rely on the assumption of normality, ensuring more accurate and reliable results. Overall, the use of appropriate transformations is a vital step in data preprocessing, significantly enhancing the suitability of data for various analytical procedures and improving the robustness of statistical inferences. However, caution is warranted in the interpretation of results after transformation. Transformed data can sometimes complicate the understanding of results and their real-world implications, as the transformed scale may not directly relate to the original measurements. It is essential to back-transform results when interpreting findings to ensure they are meaningful and relevant to the original context.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Transformations to Normality</span>"
    ]
  },
  {
    "objectID": "lessons/02_transformations.html#references",
    "href": "lessons/02_transformations.html#references",
    "title": "\n13  Transformations to Normality\n",
    "section": "\n13.7 References",
    "text": "13.7 References\n\nManikandan S. (2010). Data transformation. Journal of pharmacology & pharmacotherapeutics, 1(2), 126–127. https://doi.org/10.4103/0976-500X.72373\nWest R. M. (2022). Best practice in statistics: The use of log transformation. Annals of clinical biochemistry, 59(3), 162–165. https://doi.org/10.1177/00045632211050531\nLee D. K. (2020). Data transformation: a focus on the interpretation. Korean journal of anesthesiology, 73(6), 503–508. https://doi.org/10.4097/kja.20137",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Transformations to Normality</span>"
    ]
  },
  {
    "objectID": "lessons/02_fisher_exact_test.html",
    "href": "lessons/02_fisher_exact_test.html",
    "title": "\n14  Fisher’s Exact Test\n",
    "section": "",
    "text": "14.1 Introduction to Fisher’s Exact test",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Fisher's Exact Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_fisher_exact_test.html#introduction-to-fishers-exact-test",
    "href": "lessons/02_fisher_exact_test.html#introduction-to-fishers-exact-test",
    "title": "\n14  Fisher’s Exact Test\n",
    "section": "",
    "text": "Fisher’s exact test is a non-parametric statistical test used to test an association between categorical variables.\nIt is analogous to Chi-square test, but Fisher’s exact test is conducted when rule of Chi-square test cannot be applied, such as when the sample size in small and more than 20% of cells have expected frequency count of &lt;5 in a contingency table (Bower 2003).\nUsed to assess whether the proportions of categories in two group variables significantly differ from each other.\nUses (hypergeometric) marginal distribution to compute exact p-values which are not approximated, which are also somewhat conservative.\nThis particular test is used to obtain the probability of observing the combination of frequencies that we can actually see.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Fisher's Exact Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_fisher_exact_test.html#assumptions",
    "href": "lessons/02_fisher_exact_test.html#assumptions",
    "title": "\n14  Fisher’s Exact Test\n",
    "section": "\n14.2 Assumptions",
    "text": "14.2 Assumptions\n\nAssumes that the individual observations are independent - variable are not paired or related.\nAssumes that the row and column totals are fixed or conditioned.\nThe variables are categorical and randomly sampled.\nObservations are count data.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Fisher's Exact Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_fisher_exact_test.html#hypotheses",
    "href": "lessons/02_fisher_exact_test.html#hypotheses",
    "title": "\n14  Fisher’s Exact Test\n",
    "section": "\n14.3 Hypotheses",
    "text": "14.3 Hypotheses\nThe hypotheses of Fisher’s exact test are similar to Chi-square test:\n\nNull hypothesis:\\((H_0)\\) There is no significant relationship between the categorical variables (variables are independent).\nAlternative hypothesis: \\((H_1)\\) There is a significant relationship between the categorical variables (variables are dependent).",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Fisher's Exact Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_fisher_exact_test.html#mathematical-definition-of-fishers-exact-test",
    "href": "lessons/02_fisher_exact_test.html#mathematical-definition-of-fishers-exact-test",
    "title": "\n14  Fisher’s Exact Test\n",
    "section": "\n14.4 Mathematical definition of Fisher’s Exact test",
    "text": "14.4 Mathematical definition of Fisher’s Exact test\nThis test is usually used as a one-tailed test. It can also be used as a two tailed test. Fisher’s exact test for a one-tailed \\(p\\)-value is calculated using the following formula:\n\\[\np = {(a+b)!(c+d)!(a+c)!(b+d)! \\over a! b! c! d! n!},\n\\] where \\(a\\),\\(b\\),\\(c\\), and \\(d\\) are the individual frequencies on the 2x2 contingency table and \\(n\\) is the population size (total frequency).",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Fisher's Exact Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_fisher_exact_test.html#intalling-required-packages",
    "href": "lessons/02_fisher_exact_test.html#intalling-required-packages",
    "title": "\n14  Fisher’s Exact Test\n",
    "section": "\n14.5 Intalling required packages",
    "text": "14.5 Intalling required packages\nFirst, we installed and loaded the packages needed for this presentation.\n\n#Installing Required Packages\n#install.packages(\"public.ctn0094data\")\n#install.packages(\"gtsummary\")\n#Install ggstatsplot package\n#install.packages(\"ggstatsplot\")\n#install.packages(\"ggmosaic\")\n#install.packages(\"tidyverse\")\n\n# Loading Required Packages\nlibrary(public.ctn0094data)\nlibrary(gtsummary)\nlibrary(ggstatsplot)\nlibrary(ggmosaic) \nlibrary(tidyverse)",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Fisher's Exact Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_fisher_exact_test.html#data-source-and-description",
    "href": "lessons/02_fisher_exact_test.html#data-source-and-description",
    "title": "\n14  Fisher’s Exact Test\n",
    "section": "\n14.6 Data source and description",
    "text": "14.6 Data source and description\nFor this demonstration of the Fisher’s Exact test , we utilized the demographics, and psychiatric data sets from the public.ctn0094data package the public.ctn0094data package. The public.ctn0094data package contains de-identified and harmonized datasets from the Clinical Trials Network (CTN) protocol number 0094. This project, funded by the US National Institute on Drug Abuse (NIDA), focuses on opioid use disorder (OUD) and includes data from three clinical trials: CTN-0027, CTN-0030, and CTN-0051.The data describe the experiences of patients seeking care for opoid use disorder (OUD).\nThe demographics dataset contains the demographic variables such as age, sex, race, living condition, marital status etc. The psychiatric dataset contains data on different mental health issues and susbstance use, including bipolar, depression, schizophrenia, cocaine use etc.\n\n# # Search for suitable data sets; this lists of all datasets in package\n# data(package = \"public.ctn0094data\") \n\ndata(demographics, package = \"public.ctn0094data\")\ncolnames(demographics)\n\n[1] \"who\"              \"age\"              \"is_hispanic\"      \"race\"            \n[5] \"job\"              \"is_living_stable\" \"education\"        \"marital\"         \n[9] \"is_male\"         \n\ndata(psychiatric, package = \"public.ctn0094data\")\ncolnames(psychiatric) \n\n [1] \"who\"                 \"has_schizophrenia\"   \"has_major_dep\"      \n [4] \"has_bipolar\"         \"has_anx_pan\"         \"has_brain_damage\"   \n [7] \"has_epilepsy\"        \"depression\"          \"anxiety\"            \n[10] \"schizophrenia\"       \"has_opiates_dx\"      \"has_alcol_dx\"       \n[13] \"has_amphetamines_dx\" \"has_cannabis_dx\"     \"has_cocaine_dx\"     \n[16] \"has_sedatives_dx\"   \n\n\n\n14.6.1 Create a model data frame\nWe joined the demographics and psychiatric data sets within the public.ctn0094data package by participants ID (who variable) to create new data frame.\n\n# Joining data sets: \nmodel_df &lt;- \n  demographics %&gt;% \n  left_join(psychiatric, by = \"who\") %&gt;% \n  # Selecting variables of interest for our analysis\n  select(\n    age, race, education, is_male, marital, is_living_stable, has_schizophrenia\n  )\n\n\n14.6.2 Participants characteristics Summary table\nHere, we want to view the frequency of the variables in our dataset using the table summary function tbl_summary ().\n\n# Create Table 1, change the Label using the label function and also view the missing values\nmodel_df %&gt;% \n  tbl_summary(\n    label = list(\n      age = \"Age\",\n      race = \"Race\",\n      education = \"Education_Level\",\n      is_male = \"Male\",\n      marital = \"Marital_Status\",\n      is_living_stable = \"Living_Condition\",\n      has_schizophrenia = \"Schizophrenia\"\n    ),\n    missing_text = \"(Missing)\"\n  )\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nN = 3,5601\n\n\n\n\nAge\n34 (27, 45)\n\n\n    (Missing)\n208\n\n\nRace\n\n\n\n    Black\n365 (10%)\n\n\n    Other\n506 (14%)\n\n\n    Refused/missing\n58 (1.6%)\n\n\n    White\n2,631 (74%)\n\n\nEducation_Level\n\n\n\n    HS/GED\n691 (39%)\n\n\n    Less than HS\n352 (20%)\n\n\n    More than HS\n724 (41%)\n\n\n    (Missing)\n1,793\n\n\nMale\n2,351 (66%)\n\n\n    (Missing)\n4\n\n\nMarital_Status\n\n\n\n    Married or Partnered\n329 (19%)\n\n\n    Never married\n1,028 (59%)\n\n\n    Separated/Divorced/Widowed\n394 (23%)\n\n\n    (Missing)\n1,809\n\n\nLiving_Condition\n1,535 (96%)\n\n\n    (Missing)\n1,962\n\n\nSchizophrenia\n73 (2.4%)\n\n\n    (Missing)\n469\n\n\n\n\n1 Median (IQR); n (%)\n\n\n\n\n\n\n\n14.6.3 Recode to indicate variable factor levels\n\n# Recoding `is_living_stable` and has_schizophrenia`converting all NA to 99\nmodel_df &lt;- \n  model_df %&gt;% \n  mutate(\n    Living_stable = ifelse(is.na(is_living_stable), 99, is_living_stable),\n    Living_stable = factor(\n      Living_stable, levels = c(1, 2, 99), labels = c(\"No\", \"Yes\", \"Missing\")\n    )\n  ) %&gt;% \n  mutate(\n    schizophrenia = ifelse(is.na(has_schizophrenia), 99, has_schizophrenia),\n    schizophrenia = factor(\n      schizophrenia, levels = c(1, 2, 99), labels = c(\"No\", \"Yes\", \"Missing\")\n    )\n  ) \n\nmodel_df \n\n# A tibble: 3,560 × 9\n     age race  education    is_male marital   is_living_stable has_schizophrenia\n   &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;        &lt;fct&gt;   &lt;fct&gt;     &lt;fct&gt;            &lt;fct&gt;            \n 1    43 White More than HS Yes     Married … Yes              No               \n 2    30 White More than HS No      Never ma… Yes              No               \n 3    23 Black More than HS No      Never ma… Yes              No               \n 4    19 White More than HS Yes     Never ma… Yes              No               \n 5    31 White &lt;NA&gt;         No      &lt;NA&gt;      &lt;NA&gt;             &lt;NA&gt;             \n 6    43 White HS/GED       Yes     Married … Yes              No               \n 7    33 White More than HS No      Never ma… Yes              No               \n 8    44 White &lt;NA&gt;         Yes     &lt;NA&gt;      &lt;NA&gt;             &lt;NA&gt;             \n 9    25 Black HS/GED       No      Never ma… Yes              No               \n10    29 Other More than HS No      Never ma… Yes              Yes              \n# ℹ 3,550 more rows\n# ℹ 2 more variables: Living_stable &lt;fct&gt;, schizophrenia &lt;fct&gt;",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Fisher's Exact Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_fisher_exact_test.html#contingency-table-with-tbl_summary-function",
    "href": "lessons/02_fisher_exact_test.html#contingency-table-with-tbl_summary-function",
    "title": "\n14  Fisher’s Exact Test\n",
    "section": "\n14.7 Contingency Table with tbl_summary Function",
    "text": "14.7 Contingency Table with tbl_summary Function\n\nThis is a table that shows the distribution of a variable in the rows and columns. Sometimes referred to as a 2x2 table. They are useful in summarizing categorical variables.\nWe want to create a contingency table of the demographic variable by living_stable to Check the distribution of the frequency count of variables (is_living_stable, Yes = stable and No = unstable has_schizophrenia, Yes = schizophrenia diagnosed and No = no schizophrenia).\n\n\n# creating new data frame keeping only the categorical variable of interest\n#   for our contingency table in the next section\n\nfinalModel_df &lt;- select(model_df, schizophrenia, Living_stable)\n\n# Adding label and overall number \n\nfinalModel_df %&gt;% \n  tbl_summary(by = Living_stable) %&gt;%\n  #add_n() %&gt;%\n  add_overall() %&gt;%\n  modify_spanning_header(c(\"stat_1\", \"stat_2\") ~ \"**Living_stable**\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nOverall, N = 3,5601\n\nLiving_stable\n\nMissing, N = 1,9621\n\n\n\n\nNo, N = 631\n\n\nYes, N = 1,5351\n\n\n\n\n\nschizophrenia\n\n\n\n\n\n\n    No\n3,018 (85%)\n59 (94%)\n1,485 (97%)\n1,474 (75%)\n\n\n    Yes\n73 (2.1%)\n2 (3.2%)\n21 (1.4%)\n50 (2.5%)\n\n\n    Missing\n469 (13%)\n2 (3.2%)\n29 (1.9%)\n438 (22%)\n\n\n\n\n1 n (%)\n\n\n\n\n\n\nFrom the table, it seems like the patients who were homeless (answered no to living_stable) were less likely to be diagnosed with schizophrenia. However, this is not conclusive as we cannot tell if this relationship was a true correlation or it was due to random sampling error. So we will perform the Fisher’s Exact test to confirm the relationship.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Fisher's Exact Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_fisher_exact_test.html#performing-the-fishers-exact-test-using-the-function-fisher.test",
    "href": "lessons/02_fisher_exact_test.html#performing-the-fishers-exact-test-using-the-function-fisher.test",
    "title": "\n14  Fisher’s Exact Test\n",
    "section": "\n14.8 Performing the Fisher’s exact test using the function fisher.test()",
    "text": "14.8 Performing the Fisher’s exact test using the function fisher.test()\n\nA priori, we hypothesized that people who are diagnosed as being schizophrenic are more likely to homeless (unstable living). So we conducted a one-tailed Fisher’s Exact test and specify the direction of the test as “greater”.\nFor a two-tailed test, the alternative argument has a default value of \"two.sided\".\n\n\n# running one-tailed fisher's exact test\nfModelGreater_ls &lt;- fisher.test(\n  x = model_df$is_living_stable, \n  y = model_df$has_schizophrenia, \n  alternative = \"greater\"\n)\nfModelGreater_ls\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  model_df$is_living_stable and model_df$has_schizophrenia\np-value = 0.9429\nalternative hypothesis: true odds ratio is greater than 1\n95 percent confidence interval:\n 0.1159862       Inf\nsample estimates:\nodds ratio \n 0.4175209 \n\n# running two-tailed Fisher's exact test\nfisher.test(\n  x = model_df$is_living_stable, \n  y = model_df$has_schizophrenia\n)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  model_df$is_living_stable and model_df$has_schizophrenia\np-value = 0.2246\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 0.09821534 3.75679596\nsample estimates:\nodds ratio \n 0.4175209",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Fisher's Exact Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_fisher_exact_test.html#interpretation-of-results",
    "href": "lessons/02_fisher_exact_test.html#interpretation-of-results",
    "title": "\n14  Fisher’s Exact Test\n",
    "section": "\n14.9 Interpretation of results",
    "text": "14.9 Interpretation of results\n\n14.9.1 One-Tailed test\n\nNull Hypothesis, \\(H_0\\): people who reported unstable living (homeless) are not more often diagnosed with schizophrenia.\nAlternative Hypothesis, \\(H_A\\): people who reported unstable living(homeless) are more often diagnosed with schizophrenia.\n\nThe \\(p\\) value is greater than 0.05 (\\(p\\) = 0.9429), so we fail to reject the null hypothesis. We conclude that people who reported unstable living are not significantly more likely to be diagnosed with schizophrenia than those who reported stable living.\n\n14.9.2 Two-tailed analysis\n\nNull Hypothesis, \\(H_0\\): There is no association between living-stable and schizophrenia diagnosis variable\nAlternative Hypothesis, \\(H_A\\): There is an association between living condition and schizophrenia diagnosis.\n\nOur \\(p\\)-value = 0.2246, so we fail to reject the null hypothesis, indicating that there is no statistically significant association between living condition and schizophrenia diagnosis.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Fisher's Exact Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_fisher_exact_test.html#visualization-of-the-statistical-test-output",
    "href": "lessons/02_fisher_exact_test.html#visualization-of-the-statistical-test-output",
    "title": "\n14  Fisher’s Exact Test\n",
    "section": "\n14.10 Visualization of the statistical test output",
    "text": "14.10 Visualization of the statistical test output\nWe would like to have a visual representation of the distribution of the categories of our analysis. As shown in the Fisher’s Exact test, there was no statistical significant association between living-stability and schizophrenia diagnosis. Hence, the interpretation of the plots below is based on description of the charts.\n\n14.10.1 Barplots\nWe will first construct a barplot using ggstatsplot:: (for statistical details).\n\ndisplayP_char &lt;- ifelse(\n  test = fModelGreater_ls$p.value &lt; 0.001,\n  yes = \"&lt; 0.001\",\n  no = as.character(round(fModelGreater_ls$p.value, 3))\n)\n\n# combine plot and statistical test with ggbarstats\nggbarstats( \n  data = finalModel_df,\n  x = Living_stable,\n  y = schizophrenia,\n  results.subtitle = FALSE,\n  subtitle = paste0(\"Fisher's exact test, p-value = \", displayP_char)\n)\n\n\n\n\n\n\n\nPatients who indicated living_stable (not homeless) had the highest proportion of not having being diagnosed with schizophrenia. Patients who indicated not having stable living (homeless) had the highest proportion of being diagnosed with schizophrenia.\n\n14.10.2 Mosaic plots\n\n# Basic Mosaic Plot\nmosaic_basic &lt;- finalModel_df %&gt;% \n  ggplot() +\n  geom_mosaic(\n    aes(\n      x = product(schizophrenia),\n      fill = Living_stable\n    )\n  ) +\n  labs(\n    #y = \"Living_stable\",\n    #x = \"schizophrenia\",\n    title = \"Mosaic Plot of schizophrenia by living_stability\") +\n  # Specifies default `geom_mosaic` aesthetics, e.g white panel background, \n  # removes grid lines, adjusts widths and heights of rows and columns to \n  # reflect frequencies\n  theme_mosaic() +\n  theme(legend.position = \"None\") +\n  theme(axis.text.x = element_text(angle = 60, hjust = 1, vjust = 1))\n  \nmosaic_basic\n\n\n\n\n\n\n\nCompared to those who are not homeless, those who are homeless had the highest proportion of being diagnosed with schizophrenia.",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Fisher's Exact Test</span>"
    ]
  },
  {
    "objectID": "lessons/02_fisher_exact_test.html#references",
    "href": "lessons/02_fisher_exact_test.html#references",
    "title": "\n14  Fisher’s Exact Test\n",
    "section": "\n14.11 References",
    "text": "14.11 References\n\nBower, Keith M. 2003. “When to Use Fisher’s Exact Test.” In American Society for Quality, Six Sigma Forum Magazine, 2:35–37. 4.\nMcCrum-Gardner, Evie. 2008. “Which Is the Correct Statistical Test to Use?” British Journal of Oral and Maxillofacial Surgery 46 (1): 38–41.\nWong KC. Chi squared test versus Fisher’s exact test. Hong Kong Med J. 2011 Oct;17(5):427\nPatil, I. (2021). Visualizations with statistical details: The ‘ggstatsplot’ approach. Journal of Open Source Software, 6(61), 3167, doi:10.21105/joss.03167\nZach Bobbit. (2021). Fisher’s Exact Test: Definition, Formula, and Example",
    "crumbs": [
      "One-Sample Tests",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Fisher's Exact Test</span>"
    ]
  },
  {
    "objectID": "03_header_two-sample.html",
    "href": "03_header_two-sample.html",
    "title": "Two-Sample Tests",
    "section": "",
    "text": "Text Outline\nThis text covers the basics of:",
    "crumbs": [
      "Two-Sample Tests"
    ]
  },
  {
    "objectID": "03_header_two-sample.html#text-outline",
    "href": "03_header_two-sample.html#text-outline",
    "title": "Two-Sample Tests",
    "section": "",
    "text": "Exploring data\nOne-sample tests\nTwo-sample tests\nLinear regression and ANOVA\nGeneralized linear models\nSome special topics\nMaybe a bit about power calculations…",
    "crumbs": [
      "Two-Sample Tests"
    ]
  },
  {
    "objectID": "03_header_two-sample.html#part-outline",
    "href": "03_header_two-sample.html#part-outline",
    "title": "Two-Sample Tests",
    "section": "Part Outline",
    "text": "Part Outline\nThis part of the text contains the chapters on various two-sample statistical tests:\n\n\\(t\\)-test\nWelch’s \\(t\\)-test\nMann-Whitney \\(U\\) test\nCochran’s \\(Q\\) test\n\\(\\chi^2\\) Test for Independence",
    "crumbs": [
      "Two-Sample Tests"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_ttest.html",
    "href": "lessons/03_two_sample_ttest.html",
    "title": "\n15  Two sample \\(t\\)-test\n",
    "section": "",
    "text": "15.1 Two sample \\(t\\)-test\nThis is also called the independent sample t test. It is used to see whether the population means of two groups are equal or different. This test requires one variable which can be the exposure x and another variable which can be the outcome y. If there are more than two groups, Analysis of Variance (ANOVA) would be more suitable. If data is nonparametric then an alternative test to use would be the Mann Whitney U test. Cressie, N.A., 1986\nThere are two types of independent t tests: the first is the Student’s t test, which assumes the variance of the two groups is equal, and the second being the Welch’s t test (default in R), which assumes the variance in the two groups is different.\nIn this article we will be discussing the Student’s t test.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Two sample $t$-test</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_ttest.html#two-sample-t-test",
    "href": "lessons/03_two_sample_ttest.html#two-sample-t-test",
    "title": "\n15  Two sample \\(t\\)-test\n",
    "section": "",
    "text": "15.1.1 Assumptions\n\nMeasurements for one observation do not affect measurements for any other observation (assumes independence).\nData values in dependent variable are continuous.\nData in each group are normally distributed.\nThe variances for the two independent groups are equal in the Student’s t test.\nThere should be no significant outliers.\n\n15.1.2 Hypotheses\n\n\\((H_0)\\): the mean of group A \\((m_A)\\) is equal to the mean of group B \\((m_B)\\)- two tailed test.\n\\((H_0)\\): \\((m_A)\\ge (m_B)\\)- one tailed test.\n\\((H_0)\\): \\((m_A)\\le (m_B)\\)- one tailed test.\n\nThe corresponding alternative hypotheses would be as follows:\n\n\n\\((H_1)\\): \\((m_A)\\neq(m_B)\\)- two tailed test.\n\n\\((H_1)\\): \\((m_A)&lt;(m_B)\\)- one tailed test.\n\n\\((H_1)\\): \\((m_A)&gt; (m_B)\\)- one tailed test.\n\n15.1.3 Statistical hypotheses formula\nFor the Student’s t test which assumes equal variance, here is an example of how the |t| statistic may be calculated using groups A and B:\n\\(t ={ {m_{A} - m_{B}} \\over \\sqrt{ {S^2 \\over n_{A} } + {S^2 \\over n_{B}}   }}\\)\nThis can be described as the sample mean difference divided by the sample standard deviation of the sample mean difference where:\n\\(m_A\\) and \\(m_B\\) are the mean values of A and B,\n\\(n_A\\) and \\(n_B\\) are the size of group A and B,\n\\(S^2\\) is the estimator for the pooled variance, with the degrees of freedom (df) = \\(n_A + n_B - 2\\),\nand \\(S^2\\) is calculated as follows:\n\\(S^2 = { {\\sum{ (x_A-m_{A})^2} + \\sum{ (x_B-m_{B})^2}} \\over {n_{A} + n_{B} - 2 }}\\)\nWhat if the data is not independent?\nIf the data is not independent such as paired data in the form of matched pairs which are correlated, we use the paired t test. This test checks whether the means of two paired groups are different from each other. It is usually applied in clinical trial studies with a “before and after” or case control studies with matched pairs. For this test we only assume the difference of each pair to be normally distributed (the paired groups are the ones important for analysis) unlike the independent t test which assumes that data from both samples are independent and variances are equal. Xu, M., 2017\n\n15.1.4 Example\n\n15.1.4.1 Prerequisites\n\n\ntidyverse: data manipulation and visualization.\n\nrstatix: providing pipe friendly R functions for easy statistical analysis.\n\ncar: providing variance tests.\n\n\n#install.packages(\"ggstatplot\") \n#install.packages(\"car\")\n#install.packages(\"rstatix\")\n#install.packages(tidyVerse)\n\n\n15.1.4.2 Dataset\nThis example dataset sourced from kaggle was obtained from surveys of students in Math and Portuguese classes in secondary school. It contains demographic information on gender, social and study information.\n\n# load relevant libraries\nlibrary(rcompanion)\nlibrary(car)\nlibrary (gt)\nlibrary(gtsummary)\nlibrary(ggpubr)\nlibrary(rstatix)\nlibrary(tidyverse)\n\n\n# load the dataset\nstu_math &lt;- read_csv(\"../data/03_student-mat.csv\")\n\nRows: 395 Columns: 33\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (17): school, sex, address, famsize, Pstatus, Mjob, Fjob, reason, guardi...\ndbl (16): age, Medu, Fedu, traveltime, studytime, failures, famrel, freetime...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nChecking the data\n\n# check the data\nglimpse(stu_math)\n\nRows: 395\nColumns: 33\n$ school     &lt;chr&gt; \"GP\", \"GP\", \"GP\", \"GP\", \"GP\", \"GP\", \"GP\", \"GP\", \"GP\", \"GP\",…\n$ sex        &lt;chr&gt; \"F\", \"F\", \"F\", \"F\", \"F\", \"M\", \"M\", \"F\", \"M\", \"M\", \"F\", \"F\",…\n$ age        &lt;dbl&gt; 18, 17, 15, 15, 16, 16, 16, 17, 15, 15, 15, 15, 15, 15, 15,…\n$ address    &lt;chr&gt; \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\",…\n$ famsize    &lt;chr&gt; \"GT3\", \"GT3\", \"LE3\", \"GT3\", \"GT3\", \"LE3\", \"LE3\", \"GT3\", \"LE…\n$ Pstatus    &lt;chr&gt; \"A\", \"T\", \"T\", \"T\", \"T\", \"T\", \"T\", \"A\", \"A\", \"T\", \"T\", \"T\",…\n$ Medu       &lt;dbl&gt; 4, 1, 1, 4, 3, 4, 2, 4, 3, 3, 4, 2, 4, 4, 2, 4, 4, 3, 3, 4,…\n$ Fedu       &lt;dbl&gt; 4, 1, 1, 2, 3, 3, 2, 4, 2, 4, 4, 1, 4, 3, 2, 4, 4, 3, 2, 3,…\n$ Mjob       &lt;chr&gt; \"at_home\", \"at_home\", \"at_home\", \"health\", \"other\", \"servic…\n$ Fjob       &lt;chr&gt; \"teacher\", \"other\", \"other\", \"services\", \"other\", \"other\", …\n$ reason     &lt;chr&gt; \"course\", \"course\", \"other\", \"home\", \"home\", \"reputation\", …\n$ guardian   &lt;chr&gt; \"mother\", \"father\", \"mother\", \"mother\", \"father\", \"mother\",…\n$ traveltime &lt;dbl&gt; 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 3, 1, 2, 1, 1, 1, 3, 1, 1,…\n$ studytime  &lt;dbl&gt; 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 3, 1, 2, 3, 1, 3, 2, 1, 1,…\n$ failures   &lt;dbl&gt; 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0,…\n$ schoolsup  &lt;chr&gt; \"yes\", \"no\", \"yes\", \"no\", \"no\", \"no\", \"no\", \"yes\", \"no\", \"n…\n$ famsup     &lt;chr&gt; \"no\", \"yes\", \"no\", \"yes\", \"yes\", \"yes\", \"no\", \"yes\", \"yes\",…\n$ paid       &lt;chr&gt; \"no\", \"no\", \"yes\", \"yes\", \"yes\", \"yes\", \"no\", \"no\", \"yes\", …\n$ activities &lt;chr&gt; \"no\", \"no\", \"no\", \"yes\", \"no\", \"yes\", \"no\", \"no\", \"no\", \"ye…\n$ nursery    &lt;chr&gt; \"yes\", \"no\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes…\n$ higher     &lt;chr&gt; \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"ye…\n$ internet   &lt;chr&gt; \"no\", \"yes\", \"yes\", \"yes\", \"no\", \"yes\", \"yes\", \"no\", \"yes\",…\n$ romantic   &lt;chr&gt; \"no\", \"no\", \"no\", \"yes\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\"…\n$ famrel     &lt;dbl&gt; 4, 5, 4, 3, 4, 5, 4, 4, 4, 5, 3, 5, 4, 5, 4, 4, 3, 5, 5, 3,…\n$ freetime   &lt;dbl&gt; 3, 3, 3, 2, 3, 4, 4, 1, 2, 5, 3, 2, 3, 4, 5, 4, 2, 3, 5, 1,…\n$ goout      &lt;dbl&gt; 4, 3, 2, 2, 2, 2, 4, 4, 2, 1, 3, 2, 3, 3, 2, 4, 3, 2, 5, 3,…\n$ Dalc       &lt;dbl&gt; 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1,…\n$ Walc       &lt;dbl&gt; 1, 1, 3, 1, 2, 2, 1, 1, 1, 1, 2, 1, 3, 2, 1, 2, 2, 1, 4, 3,…\n$ health     &lt;dbl&gt; 3, 3, 3, 5, 5, 5, 3, 1, 1, 5, 2, 4, 5, 3, 3, 2, 2, 4, 5, 5,…\n$ absences   &lt;dbl&gt; 6, 4, 10, 2, 4, 10, 0, 6, 0, 0, 0, 4, 2, 2, 0, 4, 6, 4, 16,…\n$ G1         &lt;dbl&gt; 5, 5, 7, 15, 6, 15, 12, 6, 16, 14, 10, 10, 14, 10, 14, 14, …\n$ G2         &lt;dbl&gt; 6, 5, 8, 14, 10, 15, 12, 5, 18, 15, 8, 12, 14, 10, 16, 14, …\n$ G3         &lt;dbl&gt; 6, 6, 10, 15, 10, 15, 11, 6, 19, 15, 9, 12, 14, 11, 16, 14,…\n\n\nIn total there are 395 observations and 33 variables. We will drop the variables we do not need and keep the variables that will help us answer the following: Is there a difference between the finals grades of boys and girls in maths?\n\\(H_0\\): There is no statistical difference between the final grades between boys and girls.\n\\(H_1\\): There is a statistically significant differencehe in the final grades between the two groups.\n\n# creating a subset of the data \nmath &lt;- stu_math %&gt;% subset %&gt;% select (sex, G3)\nglimpse(math)\n\nRows: 395\nColumns: 2\n$ sex &lt;chr&gt; \"F\", \"F\", \"F\", \"F\", \"F\", \"M\", \"M\", \"F\", \"M\", \"M\", \"F\", \"F\", \"M\", \"…\n$ G3  &lt;dbl&gt; 6, 6, 10, 15, 10, 15, 11, 6, 19, 15, 9, 12, 14, 11, 16, 14, 14, 10…\n\n\nSummary statistics The dependent variable is continuous (grades=G3) and the independent variable is character but binary (sex).\n\n# summarizing our data\n summary(math)\n\n     sex                  G3       \n Length:395         Min.   : 0.00  \n Class :character   1st Qu.: 8.00  \n Mode  :character   Median :11.00  \n                    Mean   :10.42  \n                    3rd Qu.:14.00  \n                    Max.   :20.00  \n\n\nWe see that data ranges from 0-20 with 0 being people who were absent and could not take the test therefore missing data.\nIdentifying outliers - Outliers can also be identified through boxplot.\n\n# creating a boxplot to visualize the outliers (without removing score of zero)\nggplot(data = math) + \n  aes(\n    x = sex, \n    y = G3\n  ) +\n  labs(\n    title = \"Boxplot to visualize the outlier\",\n    x  = \"sex\", \n    y = \"Maths Grades\"\n  ) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n# creating a boxplot to visualize the data with no outliers\nmath2 &lt;- math %&gt;% filter(G3&gt;0)\nggplot(data = math2) + \n  aes(\n    x = sex, \n    y = G3\n  ) +\n  labs(\n    title = \"Boxplot without the outlier\",\n    x  = \"sex\", \n    y = \"Maths Grades\"\n  ) +\n  geom_boxplot()\n\n\n\n\n\n\n\nThe second box plot shows us that there are no outliers as students with a score of zero have been removed. This score is not truly reflective of the performance between boys and girls as a grade of 0 may represent absence or other reasons for the test not been taken.\nWe remove the outliers before running the t test. However, other models can be considered such as the zero inflated model to differentiate those who truly got a 0 and those who were not present to take test.\n\n# finding the mean for the groups with outliers (score of zero included)\nmean(math$G3[math$sex==\"F\"])\n\n[1] 9.966346\n\nmean(math$G3[math$sex==\"M\"])\n\n[1] 10.91444\n\n# finding the mean for the groups without outliers (score of zero not included)\nmean(math2$G3[math2$sex==\"F\"])\n\n[1] 11.20541\n\nmean(math2$G3[math2$sex==\"M\"])\n\n[1] 11.86628\n\n\nThe mean has increased slightly in both groups and the difference in mean has been decreased in both groups after removing the outliers.\nVisualizing the data - We can use barplots to look at the difference in sample sizes between the groups and histograms check distribution of the data for normality.\n\nsample_size &lt;- table(math2$sex)\nsample_size_df &lt;- as.data.frame(sample_size)\ncolnames(sample_size_df) &lt;- c(\"sex\", \"count\")\n\n# plotting bar plot to see the distribution in sample size\nggplot(data = sample_size_df) + \n  aes(\n    x = sex, \n    y = count,\n  ) +\n  labs(\n    title = \"Distribution of sample size by sex\",\n    x = \"Sex\",\n    y = \"Number of students\"\n  ) +\n  geom_bar(stat = \"identity\")\n\n\n\n\n\n\n\nThe bar graph shows that there are slightly more females in the sample than males.\n\n# Histograms for data by groups \n\nmale = math2$G3[math2$sex == \"M\"]\nfemale = math2$G3[math2$sex == \"F\"]\n\n# plotting distribution for males\nplotNormalHistogram(\n  male, \n  breaks= 20,\n  xlim=c(0,20),\n  main=\"Distribution of the grades for males \", \n  xlab= \"Math Grades\"\n  )\n\n\n\n\n\n\n\nThe final grades for males seem to be normally distributed. If it is difficult to be certain whether the data is normally distributed, the histogram of the square root transformed data can be plotted. It can help reveal underlying normality in skewed data by compressing the spread.\n\n# plotting square root transformed distribution for males\nplotNormalHistogram(\n  sqrt(male), \n  breaks= 20,\n  main=\"Distribution of the grades for males \", \n  xlab= \"Math Grades\"\n  )\n\n\n\n\n\n\n\nA highly skewed data might still not appear normal after square root transformation, so this is not a solution to a skewed data, but rather an approach to check normality more precisely.\n\n# plotting distribution for females\nplotNormalHistogram(\n  female, \n  breaks= 20,\n  xlim=c(0,20),\n  main=\"Distribution of the grades for females \", \n  xlab= \"Math Grades\"\n  )\n\n\n\n\n\n\n\nSimilar to males, looking at the square root transformed distribution for females\n\n# plotting square root transformed distribution for males\nplotNormalHistogram(\n  sqrt(female), \n  breaks= 20,\n  main=\"Distribution of the grades for females \", \n  xlab= \"Math Grades\"\n  )\n\n\n\n\n\n\n\nFinal grades for females also appear to be normally distributed. The final score across both is almost evenly distributed.\nCheck the equality of variances (homogeneity) - By looking at the two box plots above for two groups, it does not appear that the variances are different between the two groups.\nWe can use the Levene’s test or the Bartlett’s test to check for homogeneity of variances. The former is in the car library and the later in the rstatix library. If the variances are homogeneous, the p value will be greater than 0.05.\nOther tests include F test 2 sided, Brown-Forsythe and O’Brien but we shall not cover these.\n\n# running the Levene's test to check equal variance\nmath2 %&gt;% levene_test(G3~sex)\n\n# A tibble: 1 × 4\n    df1   df2 statistic     p\n  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1     1   355     0.614 0.434\n\n#don't do this unless worried about the data\n\n\n# running the Bartlett's test to check equal variance\nbartlett.test(G3~sex, data=math2)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  G3 by sex\nBartlett's K-squared = 0.12148, df = 1, p-value = 0.7274\n\n#don't do this unless worried about the data\n\nThe p value is greater than 0.05 from both the test suggesting there is no difference between the variances of the two groups.\n\n15.1.4.3 Assessment\n\nData is continuous(G3)\nData is independent (males and females are distinct and not the same individual)\nData is normally distributed. We might still want to do a square root transformation.\nNo significant outliers.\nThere are equal variances.\n\nAs the assumptions are met we go ahead to perform the Student’s \\(t\\)-test.\n\n15.1.4.4 Performing the two-sample \\(t\\)-test\nSince the default is the Welch t test we use the \\(\\color{blue}{\\text{var.eqaul = TRUE }}\\) code to signify a Student’s t test.\n\n# perfoming the two sample t test\nstat.test &lt;- math2 %&gt;% \n  #mutate(sqrt_G3 = sqrt(G3)) %&gt;%\n  t_test(G3~ sex, var.equal=TRUE) %&gt;%\n  add_significance()\nstat.test\n\n# A tibble: 1 × 9\n  .y.   group1 group2    n1    n2 statistic    df      p p.signif\n  &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   \n1 G3    F      M        185   172     -1.94   355 0.0531 ns      \n\n\n\nstat.test$statistic\n\n        t \n-1.940477 \n\n\nIf you decided to work with square root transformed data, you could use the mutate function to create the ‘sqrt_G3’ variable in the ‘stat.test’ dataset, which produces quite similar p-value and t test statistic.\nThe results are represented as follows;\n\ny - dependent variable\ngroup1, group 2 - compared groups (independent variables)\ndf - degrees of freedom\np - p value\n\ngtsummary table of results\n\n math2 |&gt; \n  tbl_summary(\n    by = sex,\n    statistic =\n      list(\n        all_continuous() ~ \"{mean} ({sd})\")\n    ) |&gt; \n   add_n() |&gt; \n  add_overall() |&gt; \n  add_difference()\n\n\n\n\n\n\nCharacteristic\nN\n\nOverall, N = 3571\n\n\nF, N = 1851\n\n\nM, N = 1721\n\n\nDifference2\n\n\n95% CI2,3\n\n\np-value2\n\n\n\nG3\n357\n11.5 (3.2)\n11.2 (3.2)\n11.9 (3.3)\n-0.66\n-1.3, 0.01\n0.053\n\n\n\n\n1 Mean (SD)\n\n\n\n2 Welch Two Sample t-test\n\n\n\n3 CI = Confidence Interval\n\n\n\n\n\n\n\nInterpretation of results\nFor the Student’s t test, the obtained t statistic of -1.940477 is greater than the critical value at 355 degree of freedom (n1+n2-2) i.e. -1.984, due to which it is not statistically significant. Also, the p-value is greater than alpha of 0.05, due to which we we fail to reject the null hypothesis and conclude that there is no statistical difference between the mean grades of boys and girls. (A significant |t| would be equal to -1.984 or smaller; or equal to 1.984 or greater).\nEffect size Note: Effect size should only be calculated if the null hypothesis is rejected. We are showing how to do this for pedagogical reasons. In practice, we would not calculate an effect size because we fail to reject the null hypothesis.\nCohen’s d can be an used as an effect size statistic for the two sample t test. It is the difference between the means of each group divided by the pooled standard deviation.\n\\(d= {m_A-m_B \\over SDpooled}\\)\nIt ranges from 0 to infinity, with 0 indicating no effect where the means are equal. 0.5 means that the means differ by half the standard deviation of the data and 1 means they differ by 1 standard deviation. It is divided into small, medium or large using the following cut off points.\n\nsmall 0.2-&lt;0.5\nmedium 0.5-&lt;0.8\nlarge &gt;=0.8\n\nFor the above test the following is how we can find the effect size;\n\n#perfoming cohen's d\nmath2 %&gt;% \n  cohens_d(G3~sex,var.equal = TRUE)\n\n# A tibble: 1 × 7\n  .y.   group1 group2 effsize    n1    n2 magnitude\n* &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;ord&gt;    \n1 G3    F      M       -0.206   185   172 small    \n\n\nThe effect size is small d= -0.20.\nIn conclusion, a two-samples t-test showed that the difference was not statistically significant, t(355) = -1.940477, p &lt; 0.0531, d = -0.20; where, t(355) is shorthand notation for a t-statistic that has 355 degrees of freedom and d is Cohen’s d. We can conclude that the females mean final grade is greater than males final grade (d= -0.20) but this result is not significant.\nWhat if it is one tailed t test?\nUse the \\(\\color{blue}{\\text{alternative =}}\\) option to determine if one group is \\(\\color{blue}{\\text{\"less\"}}\\) or \\(\\color{blue}{\\text{\"greater\"}}\\). For example if we want to check the null hypothesis whether the final grades for females are greater than or equal to males we can use the following code:\n\n# perfoming the one tailed two sample t test\nstat.test &lt;- math2 %&gt;% \n  t_test(G3~ sex, var.equal=TRUE, alternative = \"less\") %&gt;%\n  add_significance()\nstat.test\n\n# A tibble: 1 × 9\n  .y.   group1 group2    n1    n2 statistic    df      p p.signif\n  &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   \n1 G3    F      M        185   172     -1.94   355 0.0266 *       \n\n\nSince, the p value is smaller than 0.05 (p=0.027), we reject the null hypothesis. We conclude that the final grades for females are significantly lesser than that for males.\nWhat about running the paired sample t test?\nWe can simply add the syntax \\(\\color{blue}{\\text{paired= TRUE}}\\) to the t_test() function to run the analysis for matched pairs data.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Two sample $t$-test</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_ttest.html#conclusion",
    "href": "lessons/03_two_sample_ttest.html#conclusion",
    "title": "\n15  Two sample \\(t\\)-test\n",
    "section": "\n15.2 Conclusion",
    "text": "15.2 Conclusion\nThis article covers the Student’s t test and how we run it in R. It also shows how we find the effect size and how we can conclude the results.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Two sample $t$-test</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_ttest.html#references",
    "href": "lessons/03_two_sample_ttest.html#references",
    "title": "\n15  Two sample \\(t\\)-test\n",
    "section": "\n15.3 References",
    "text": "15.3 References\n\nCressie, N.A.C. and Whitford, H.J. (1986). How to Use the Two Sample t-Test. Biom. J., 28: 131-148. https://doi.org/10.1002/bimj.4710280202\nXu, M., Fralick, D., Zheng, J. Z., Wang, B., Tu, X. M., & Feng, C. (2017). The Differences and Similarities Between Two-Sample T-Test and Paired T-Test. Shanghai archives of psychiatry, 29(3), 184–188. https://doi.org/10.11919/j.issn.1002-0829.217070",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Two sample $t$-test</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_mann_whitney.html",
    "href": "lessons/03_two_sample_mann_whitney.html",
    "title": "\n16  Mann-Whitney-U Test Example\n",
    "section": "",
    "text": "16.1 Introduction\nOverall goal: Identify whether the distribution of two groups significantly differs.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Mann-Whitney-U Test Example</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_mann_whitney.html#introduction",
    "href": "lessons/03_two_sample_mann_whitney.html#introduction",
    "title": "\n16  Mann-Whitney-U Test Example\n",
    "section": "",
    "text": "Mann Whitney U test, also known as the Wilcoxon Rank-Sum test, is a nonparametric statistical test of the null hypothesis, which is commonly used to compare the means or medians of two independent groups with the assumption that the at least one group is not normally distributed and when the sample size is small.\n\nThe Welch U test should be used when signs of skewness and variance of heterogeneity.\n\n\n\n\nIt is useful for numerical/continuous variables.\n\nFor example, if researchers want to compare the age or height of two different groups (as continuous variables) in a study with non-normally distributed data.\n\n\n\nWhen conducting this test, aside from reporting the p-value, the spread and the shape of the data should be described.\n\n\n\n16.1.0.1 Assumptions\n\nSamples are independent: Each dependent variable must be related to only one independent variable.\nThe response variable is ordinal or continuous.\nAt least one variable is not normally distributed.\n\n16.1.1 Hypotheses\nNull Hypothesis (H0): Distribution1 = Distribution2\n\nMean/Median Ranks of two levels are equal.\n\n\nAlternate Hypothesis (H1): Distribution1 ≠ Distribution2\n\nMean/Median Ranks of two levels are significantly different.\n\n\n\n16.1.1.1 Mathematical Equation\n\\(U_1 = n_1n_2 + \\frac{n_1 \\cdot (n_1 + 1)}{2} - R_1\\)\n\\(U_2 = n_1n_2 + \\frac{n_2 \\cdot (n_2 + 1)}{2} - R_2\\)\nWhere:\n\n\n\\(U_1\\) and \\(U_2\\) represent the test statistics for two groups (Male & Female).\n\n\\(R_1\\) and \\(R_2\\) represent the sum of the ranks of the observations for two groups.\n\n\\(n_1\\) and \\(n_2\\) are the sample sizes for two groups.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Mann-Whitney-U Test Example</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_mann_whitney.html#performing-mann-whitney-u-test-in-r",
    "href": "lessons/03_two_sample_mann_whitney.html#performing-mann-whitney-u-test-in-r",
    "title": "\n16  Mann-Whitney-U Test Example\n",
    "section": "\n16.2 Performing Mann-Whitney U Test in R",
    "text": "16.2 Performing Mann-Whitney U Test in R\n\n16.2.1 Data Source\nIn this example, we will perform the Mann-Whitney U Test using wave 8 (2012-2013) data of a longitudinal epidemiological study titled Hispanic Established Populations For the Epidemiological Study of Elderly (HEPESE).\nThe HEPESE provides data on risk factors for mortality and morbidity in Mexican Americans in order to contrast how these factors operate differently in non-Hispanic White Americans, African Americans, and other major ethnic groups. The data is publicly available and can be obtained from the University of Michigan website. For the purposes of this report/chapter, the example in the analysis uses synthetic data. Using this data, we want to explore whether there are significant gender differences in age when Type 2 diabetes mellitus (T2DM) is diagnosed. Type 2 diabetes is a chronic disease condition that has affected 37 million people living in the United States. Type 2 diabetes is the eighth leading cause of death and disability in US. Type 2 diabetes generally occurs among adults aged 45 or older, but may also occur amongst young adults and children. Diabetes and its complications are often preventable by following lifestyle guidelines and taking medication in a timely manner. 1 in 5 of US people don’t know they have diabetes.\nResearch has shown that men are more likely to develop type 2 diabetes, while women are more likely to experience complications from type 2 diabetes, including heart and kidney disease.\nIn this report, we want to test whether there are significant differences in age at which diabetes is diagnosed among males and females.\nDependent Response Variable: ageAtDx = Age_Diagnosed = Age at which diabetes is diagnosed.\nIndependent Variable: isMale = Gender\nResearch Question: Does the age at which diabetes is diagnosed significantly differ among men and women?\nNull Hypothesis (H0): Mean rank of age at which diabetes is diagnosed is equal among men and women.\nAlternate Hypothesis (H1): Mean rank of age at which diabetes is diagnosed is not equal among men and women.\n\n16.2.2 Packages\n\ngmodels: Helps to compute and display confidence intervals (CI) for model estimates.\nDescTools: Provides tools for basic statistics e.g. to compute Median CI for an efficient data description.\nggplot2: Helps to create boxplots.\nqqplotr: Helps to create QQ plot.\ndplyr: Used to manipulate data and provide summary statistics.\nhaven: Helps to import SPSS data into r.\n\nDependencies = TRUE : Indicates that while installing packages, it must also install all dependencies of the specified package.\n\n# install.packages(\"gmodels\", dependencies = TRUE)\n# install.packages(\"car\", dependencies = TRUE)\n# install.packages(\"DescTools\", dependencies = TRUE)\n# install.packages(\"ggplot2\", dependencies = TRUE)\n# install.packages(\"qqplotr\", dependencies = TRUE)\n# install.packages(\"gtsummary\", dependencies = TRUE)\n\nLoading Library\n\nsuppressPackageStartupMessages(library(haven))\nsuppressPackageStartupMessages(library(ggpubr))\nsuppressPackageStartupMessages(library(gmodels))\nsuppressPackageStartupMessages(library(DescTools))\nsuppressPackageStartupMessages(library(ggplot2))\nsuppressPackageStartupMessages(library(qqplotr))\nsuppressPackageStartupMessages(library(gtsummary))\nsuppressPackageStartupMessages(library(tidyverse))\n\nData Importing\n\nHEPESE &lt;- read_csv(\"../data/03_HEPESE_synthetic_20240510.csv\")\n\nRows: 744 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (1): ageAtDx\nlgl (1): isMale\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n16.2.3 Data Exploration\n\n# str(HEPESE)\nstr(HEPESE$isMale)\n\n logi [1:744] FALSE FALSE FALSE TRUE FALSE TRUE ...\n\nstr(HEPESE$ageAtDx)\n\n num [1:744] 87 70 68 60 55 33 38 65 50 68 ...\n\n\nAfter inspecting the data, we found that values of our dependent and independent variable values are in character format. We want them to be numerical and categorical, respectively. First, we will convert the dependent variable into numerical form, and our independent variable into categorical. Then, we will recode the factors as male and female. For simplicity’s sake, we will also rename our dependent and independent variable.\n\n# convert to number and factor\nHEPESE$ageAtDx &lt;- as.numeric(HEPESE$ageAtDx)\nclass(HEPESE$ageAtDx)\n\n[1] \"numeric\"\n\nHEPESE$isMale &lt;- as_factor(HEPESE$isMale)\nclass(HEPESE$isMale)\n\n[1] \"factor\"\n\n\nThe next step is to calculate some of the descriptive data to give us a better idea of the data that we are dealing with. This can be done using the summarise function.\nDescriptive Data\n\nDes &lt;- \n HEPESE %&gt;% \n select(isMale, ageAtDx) %&gt;% \n group_by(isMale) %&gt;%\n summarise(\n   n = n(),\n   mean = mean(ageAtDx, na.rm = TRUE),\n   sd = sd(ageAtDx, na.rm = TRUE),\n   stderr = sd/sqrt(n),\n   LCL = mean - qt(1 - (0.05 / 2), n - 1) * stderr,\n   UCL = mean + qt(1 - (0.05 / 2), n - 1) * stderr,\n   median = median(ageAtDx, na.rm = TRUE),\n   min = min(ageAtDx, na.rm = TRUE), \n   max = max(ageAtDx, na.rm = TRUE),\n   IQR = IQR(ageAtDx, na.rm = TRUE),\n   LCLmed = MedianCI(ageAtDx, na.rm = TRUE)[2],\n   UCLmed = MedianCI(ageAtDx, na.rm = TRUE)[3]\n )\n\nDes\n\n# A tibble: 2 × 13\n  isMale     n  mean    sd stderr   LCL   UCL median   min   max   IQR LCLmed\n  &lt;fct&gt;  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 FALSE    455  67.6  14.1  0.661  66.3  68.9     70    18    93    19     68\n2 TRUE     289  67.1  15.1  0.886  65.3  68.8     70    20    94    18     68\n# ℹ 1 more variable: UCLmed &lt;dbl&gt;\n\n\n\nn: The number of observations for each gender.\nmean: The mean age when diabetes is diagnosed for each gender.\nsd: The standard deviation of each gender.\nstderr: The standard error of each gender level. That is the standard deviation / sqrt (n).\nLCL, UCL: The upper and lower confidence intervals of the mean. This values indicates the range at which we can be 95% certain that the true mean falls between the lower and upper values specified for each gender group assuming a normal distribution.\nmedian: The median value for each gender.\nmin, max: The minimum and maximum value for each gender.\nIQR: The interquartile range of each gender. That is the 75th percentile – 25th percentile.\nLCLmed, UCLmed: The 95% confidence interval for the median.\n\nChecking Assumptions and Visualizing the Data\nThe next step is to visualize the data. This can be done using different functions under the ggplot package.\n1) Box plot\n\nggplot(\n HEPESE, \n aes(\n   x = isMale, \n   y = ageAtDx, \n   fill = isMale\n )\n) +\n stat_boxplot(\n   geom = \"errorbar\", \n   width = 0.5\n ) +\n geom_boxplot(\n   fill = \"light blue\"\n ) + \n stat_summary(\n   fun = mean, \n   geom = \"point\", \n   shape = 10, \n   size = 3.5, \n   color = \"black\"\n ) + \n ggtitle(\n   \"Boxplot of Gender\"\n ) + \n theme_bw() + \n theme(\n   legend.position = \"none\"\n )\n\n\n\n\n\n\n\n2) QQ plot\n\nlibrary(conflicted)\nconflict_prefer(\"stat_qq_line\", \"qqplotr\", quiet = TRUE)\n\n\n# Perform QQ plots by group\nQQ_Plot &lt;- \nggplot(\n data = HEPESE, \n aes(\n   sample = ageAtDx, \n   color = isMale, \n   fill = isMale\n )\n) +\n stat_qq_band(\n   alpha = 0.5, \n   conf = 0.95, \n   qtype = 1, \n   bandType = \"boot\"\n ) +\n stat_qq_line(\n   identity = TRUE\n ) +\n stat_qq_point(\n   col = \"black\"\n ) +\n facet_wrap(\n   ~ isMale, scales = \"free\"\n ) +\n labs(\n   x = \"Theoretical Quantiles\", \n   y = \"Sample Quantiles\"\n ) + theme_bw()\n\nQQ_Plot\n\n\n\n\n\n\n\n\nstat_qq_line: Draws a reference line based on the data quantiles.\n\nStat_qq_band: Draws confidence bands based on three methods; “pointwise”/“boot”,“Ks” and “ts”.\n\n\"pointwise\" constructs simultaneous confidence bands based on the normal distribution;\n\"boot\" creates pointwise confidence bands based on a parametric boostrap;\n\"ks\" constructs simultaneous confidence bands based on an inversion of the Kolmogorov-Smirnov test;\n\"ts\" constructs tail-sensitive confidence bands\n\n\n\nStat_qq_Point: Is a modified version of ggplot: : stat_qq with some parameters adjustments and a new option to detrend the points.\n3) Histogram\nA histogram is the most commonly used graph to show frequency distributions.\n\n\n\nggplot(HEPESE) +\n  aes(x = ageAtDx, fill = isMale) +\n  geom_histogram() +\n  facet_wrap(~ isMale) \n\n\n\n\n\n\n\n**3b) Density curve in Histogram**\nA density curve gives us a good idea of the “shape” of a distribution, including whether or not a distribution has one or more “peaks” of frequently occurring values and whether or not the distribution is skewed to the left or the right.\n\nggplot(HEPESE) +\n  aes(\n    x = ageAtDx,\n    fill = isMale\n  ) +\n  labs(\n    x = \"Age When diabetes is diagnosed\",\n    y = \"Density\",\n    fill = \"Gender\",\n    title = \"A Density Plot of Age when diabetes is diagnosed\",\n    caption = \"Data Source: HEPESE Wave 8 (ICPSR 36578)\"\n  ) + \n  geom_density() +\n  facet_wrap(~isMale)\n\n\n\n\n\n\n\nThis density curve shows that our data does not have a bell shaped distribution and it is slightly skewed towards the left.\n4) Statistical test for normality\n\nHEPESE %&gt;%\n  group_by(isMale) %&gt;%\n  summarise(\n    `W Stat` = shapiro.test(ageAtDx)$statistic,\n    p.value = shapiro.test(ageAtDx)$p.value\n  )\n\n# A tibble: 2 × 3\n  isMale `W Stat`  p.value\n  &lt;fct&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 FALSE     0.959 6.50e-10\n2 TRUE      0.937 9.99e-10\n\n\nInterpretation\nFrom the above table, we see that the value of the Shapiro-Wilk Test is 0.0006 and 0.000002, which are both less than 0.05. Therefore we have enough evidence to reject the null hypothesis and confirm that the data significantly deviates from a normal distribution.\n\n16.2.4 Mann Whitney U Test\n\nresult &lt;- wilcox.test(\n  ageAtDx ~ isMale, \n  data = HEPESE, \n  na.rm = TRUE, \n  exact = FALSE, \n  conf.int = TRUE\n)\n\ntibble(\n  Test_Statistic = result$statistic,\n  P_Value = result$p.value,\n  Method = result$method\n)\n\n# A tibble: 1 × 3\n  Test_Statistic P_Value Method                                           \n           &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                                            \n1          66178   0.880 Wilcoxon rank sum test with continuity correction",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Mann-Whitney-U Test Example</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_mann_whitney.html#results",
    "href": "lessons/03_two_sample_mann_whitney.html#results",
    "title": "\n16  Mann-Whitney-U Test Example\n",
    "section": "\n16.3 Results",
    "text": "16.3 Results\nWhile the analysis above is for synthetic data, we see that the mean age at which diabetes is diagnosed is not significantly different in males (69 years old) and females (66 years old). Of note, the Mann-Whitney U-Test applied in the real data (not shown in this report) showed that this difference is not statistically significant at 0.05 level of significance because the statistical p value (p=.155) is greater than the critical value (p=0.05). For the real data, the test statistic is W = 5040.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Mann-Whitney-U Test Example</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_mann_whitney.html#conclusion",
    "href": "lessons/03_two_sample_mann_whitney.html#conclusion",
    "title": "\n16  Mann-Whitney-U Test Example\n",
    "section": "\n16.4 Conclusion",
    "text": "16.4 Conclusion\nFrom the above result, we can conclude that gender does not play a significant role in the age at which one is diagnosed with diabetes. Diabetes is the 8th leading cause of death and disability in the US, and 1 in 5 US adults are currently unaware of their diabetes condition. This urges the need for increased policy efforts towards timely diabetes testing and diagnosing. Although previous research has suggested that there are gender based differences in diabetes related severity of inquiries, our findings suggest that this difference is not due to age, and may be due to other gender based differences, such as willingness to seek medical care, underlying health issues, etc. There may not necessarily be a need for gender-based approaches to interventions aimed at increasing diabetes surveillance, and efforts should focus on targeting the population as a whole.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Mann-Whitney-U Test Example</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_bootstrap_conf_int.html",
    "href": "lessons/03_two_sample_bootstrap_conf_int.html",
    "title": "\n17  Bootstrap Tests\n",
    "section": "",
    "text": "17.1 Introduction to Bootstrapping\nBootstrapping, introduced by Brad Efron in 1979, is founded on a straightforward concept: when our data is a sample from a larger population, why not generate additional samples by resampling from our existing data? However, since we lack access to new data, we resort to repeatedly sampling from our dataset with replacement.\nThe primary objective of bootstrapping is to augment the sample size for analysis, particularly in scenarios where the provided sample size is limited.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Bootstrap Tests</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_bootstrap_conf_int.html#mathematical-definition",
    "href": "lessons/03_two_sample_bootstrap_conf_int.html#mathematical-definition",
    "title": "\n17  Bootstrap Tests\n",
    "section": "\n17.2 Mathematical definition",
    "text": "17.2 Mathematical definition\nBootstrapping involves randomly selecting n observations from a sample with replacement to create a bootstrap sample. The process of sampling with replacement allows each observation in the original dataset to be selected multiple times or not at all in the bootstrap sample.\nOnce a bootstrap sample is obtained, the statistic of interest (e.g., mean, median, standard deviation) is calculated from this sample. This process is repeated multiple times to generate a distribution of the statistic under the assumption that the original dataset is representative of the population.\nBootstrapping can be used to construct confidence intervals for a population parameter (e.g., mean, median) by calculating the desired quantiles (e.g., percentiles) of the bootstrap distribution of the statistic.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Bootstrap Tests</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_bootstrap_conf_int.html#penguins-data",
    "href": "lessons/03_two_sample_bootstrap_conf_int.html#penguins-data",
    "title": "\n17  Bootstrap Tests\n",
    "section": "\n17.3 penguins Data",
    "text": "17.3 penguins Data\nThe penguins dataset contains measurements collected in the Palmer Archipelago of Antarctica, made available by Dr. Kristen Gorman and the Palmer Station Long Term Ecological Research (LTER) Program. This dataset, included in the palmerpenguins package, comprises observations on various attributes of penguins, including species, island of origin, physical measurements (such as flipper length, body mass, and bill dimensions), sex, and year of observation. In total, the dataset consists of 344 rows and 8 variables.\nIn this tests we are going to obtain the 95% confidence interval for flipper length of the Adelie penguin from two different Islands.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Bootstrap Tests</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_bootstrap_conf_int.html#cleaning-the-data",
    "href": "lessons/03_two_sample_bootstrap_conf_int.html#cleaning-the-data",
    "title": "\n17  Bootstrap Tests\n",
    "section": "\n17.4 Cleaning the data",
    "text": "17.4 Cleaning the data\n\nCodenew_penguins_df &lt;- \n  filter(penguins, species == \"Adelie\", island != \"Dream\") %&gt;% \n  select(species, island, flipper_length_mm) %&gt;% \n  arrange(island, .by_group = TRUE) %&gt;% \n  drop_na()\n\nview(new_penguins_df)\n\n\nThe island of Dream penguin population was excluded because their population size was much larger compared to Torgersen and Biscoe populations.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Bootstrap Tests</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_bootstrap_conf_int.html#assumptions-of-bootstrap",
    "href": "lessons/03_two_sample_bootstrap_conf_int.html#assumptions-of-bootstrap",
    "title": "\n17  Bootstrap Tests\n",
    "section": "\n17.5 Assumptions of Bootstrap",
    "text": "17.5 Assumptions of Bootstrap\n\nThe dataset is a random sample drawn representative of the population of interest.\nResampling with replacement accurately simulates the sampling process.\nObservations in the dataset are independent and identically distributed",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Bootstrap Tests</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_bootstrap_conf_int.html#checking-distribution",
    "href": "lessons/03_two_sample_bootstrap_conf_int.html#checking-distribution",
    "title": "\n17  Bootstrap Tests\n",
    "section": "\n17.6 Checking Distribution",
    "text": "17.6 Checking Distribution\n\nCode# check the boxplot of the data\nboxplot(\n  new_penguins_df$flipper_length_mm ~ new_penguins_df$island, las = 1, \n  ylab = \"Flipper Length (mm)\",\n  xlab = \"Island\",\n  main = \"Flipper Length by Island\"\n)\n\n\n\n\n\n\nCode# check the histogram of the data\nhist(\n  x = new_penguins_df$flipper_length_mm,\n  main = \"Distribution of Flipper Length (mm)\",\n  xlab = \"Flipper Length\"\n)",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Bootstrap Tests</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_bootstrap_conf_int.html#code-to-run-bootstrap",
    "href": "lessons/03_two_sample_bootstrap_conf_int.html#code-to-run-bootstrap",
    "title": "\n17  Bootstrap Tests\n",
    "section": "\n17.7 Code to run Bootstrap",
    "text": "17.7 Code to run Bootstrap\n\nCode# set a seed so that our random results can be replicated by other people:\nset.seed(20150516)\n\n# take a random re-sample of the data that is the *same size*\nN &lt;- length(new_penguins_df$flipper_length_mm)\n\n# a random sample:\nsample(new_penguins_df$flipper_length_mm, size = N, replace = TRUE)\n\n [1] 184 192 198 195 195 176 188 183 184 193 199 198 184 190 198 195 195 199 193\n[20] 197 198 189 197 188 189 199 200 190 183 198 194 190 191 196 189 195 198 197\n[39] 191 184 198 180 195 186 193 193 191 195 190 198 189 181 197 196 182 200 188\n[58] 184 202 189 197 186 181 195 181 191 185 193 196 185 192 199 186 196 180 190\n[77] 190 195 197 193 191 181 195 190 186 189 192 187 190 195 195 182 172 194 181\n\nCode# number of bootstrap samples\nB_int &lt;- 10000\n\n# create a list of these thousands of samples \nbootstrapSamples_ls &lt;- map(\n  .x = 1:B_int,\n  .f = ~{\n    sample(new_penguins_df$flipper_length_mm, size = N, replace = TRUE)\n  }\n)\n\n# subset of the random samples\nbootstrapSamples_ls[1:3]\n\n[[1]]\n [1] 183 190 189 188 181 198 181 172 187 189 189 193 180 197 191 190 196 191 195\n[20] 181 193 190 190 186 188 195 190 197 198 190 180 198 194 188 195 191 203 199\n[39] 190 189 195 186 189 199 202 197 189 190 194 190 181 190 190 181 186 196 174\n[58] 185 174 202 191 184 181 184 193 190 190 190 191 196 189 195 195 198 193 190\n[77] 197 184 186 188 193 190 191 195 198 180 191 185 189 192 183 192 199 186 195\n\n[[2]]\n [1] 187 194 187 189 184 188 187 187 184 197 193 191 187 189 190 172 187 186 180\n[20] 193 191 195 195 180 184 189 197 191 187 186 186 187 184 188 190 193 198 190\n[39] 195 198 184 197 195 195 195 198 194 191 198 197 198 186 194 195 189 186 181\n[58] 180 191 180 191 193 196 191 202 191 187 181 199 172 181 191 195 195 194 198\n[77] 191 191 190 192 190 199 195 193 195 197 188 181 190 185 186 191 174 193 195\n\n[[3]]\n [1] 191 196 203 195 185 195 193 186 186 202 186 203 187 180 185 186 192 202 186\n[20] 192 200 195 184 185 195 193 199 190 189 185 181 181 188 197 181 190 188 185\n[39] 187 184 184 195 199 186 200 186 192 195 190 182 189 191 203 193 195 191 191\n[58] 199 195 198 187 191 195 190 190 187 189 192 186 199 193 190 187 181 190 191\n[77] 190 190 183 193 190 197 181 190 187 198 187 190 200 184 190 184 186 191 193\n\n\n\nCode# The Sample Mean\nbootMeans_num &lt;-\n  bootstrapSamples_ls %&gt;%\n  # the map_dbl() function takes in a list and returns an atomic vector of type\n  #   double (numeric)\n  map_dbl(mean)\n\n# a normally distributed histogram using the samples from bootstrapping\nhist(bootMeans_num)\n\n\n\n\n\n\nCode# 95% confidence interval?\nquantile(bootMeans_num, probs = c(0.025, 0.975))\n\n    2.5%    97.5% \n188.7682 191.3684 \n\n\n\n17.7.1 Code for spearman correlation\n\nCode# Custom function to find correlation between the bill length and depth \ncorr.fun &lt;- function(data, idx) {\n  \n# vector of indices that the boot function uses\n  df &lt;- data[idx, ]\n\n# Find the spearman correlation between\n# the 3rd (length) and 4th (depth) columns of dataset\n  cor(df[, 3], df[, 4], method = 'spearman')\n}\n\n# Setting the seed for reproducability of results\nset.seed(42)\n\n# Calling the boot function with the dataset\nbootstrap &lt;- boot(iris, corr.fun, R = 1000)\n\n# Display the result of boot function\nbootstrap\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = iris, statistic = corr.fun, R = 1000)\n\n\nBootstrap Statistics :\n     original       bias    std. error\nt1* 0.9376668 -0.002717295 0.009436212\n\nCode# Plot the bootstrap sampling distribution using ggplot\nplot(bootstrap)\n\n\n\n\n\n\nCode# Function to find the bootstrap CI\nboot.ci(\n  boot.out = bootstrap,\n    type = \"perc\"\n)\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 1000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = bootstrap, type = \"perc\")\n\nIntervals : \nLevel     Percentile     \n95%   ( 0.9142,  0.9519 )  \nCalculations and Intervals on Original Scale\n\n\nThis code utilizes bootstrapping to estimate the sampling distribution and confidence interval of the Spearman correlation coefficient between bill length and depth in the iris dataset. The boot() function generates bootstrap samples, while the boot.ci() function calculates the bootstrap confidence interval. Visualizations are provided to aid in understanding the sampling distribution.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Bootstrap Tests</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_bootstrap_conf_int.html#conclusion",
    "href": "lessons/03_two_sample_bootstrap_conf_int.html#conclusion",
    "title": "\n17  Bootstrap Tests\n",
    "section": "\n17.8 Conclusion",
    "text": "17.8 Conclusion\nIn conclusion, bootstrap testing emerges as a valuable tool, particularly when confronted with small sample sizes. By leveraging resampling techniques, it offers a robust method to estimate parameters, assess uncertainty, and make reliable inferences about population statistics.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Bootstrap Tests</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_chi_sq_independence.html",
    "href": "lessons/03_two_sample_chi_sq_independence.html",
    "title": "\n18  Chi-Square Test of Independence\n",
    "section": "",
    "text": "18.1 What is a Chi-Square Test of Independence?\nThe Chi-Square Test of Independence is used to determine if there is a significant association between two categorical variables.\nPurpose: The test assesses whether the observed frequencies in a contingency table differ from the frequencies expected under the assumption of independence.\nWhat Does This Mean!?!?\nObserved Frequencies: This refers to actual counts or numbers recorded in each cell of the contingency table.\nExample: In a survey, 20 males preferred Product A, 30 males preferred Product B, 25 females preferred Product A, and 25 females preferred Product B.\nThese counts would be expected in each cell if the two variables were completely independent of each other.\nContingency Table: A contingency table displays the frequency distribution of variables in a matrix format, showing the observed counts of occurrences across categories of two variables.\nExpected Frequencies: These are the counts we would expect to see in each cell if the two variables were completely independent of each other. Calculated using the formula:\n\\[\nE_{ij} = \\frac{\\text{RowTotal} * \\text{ColumnTotal}}{\\text{GrandTotal}}\n\\]\nExample: If gender and product preference are independent, the expected frequency for males preferring Product A is calculated based on the overall proportions of males and Product A preferences in the total sample.\nAssumption of Independence: Under the assumption of independence, the occurrence of one variable does not affect the occurrence of the other variable.\nExample: Gender has no influence on product preference, meaning the distribution of preferences should be the same for both males and females.\nComparing Observed and Expected Frequencies: The Chi-Square test compares the observed frequencies with the expected frequencies. If the observed frequencies match the expected frequencies closely, it suggests that there is independence between the variables. Significant differences between observed and expected frequencies suggest an association between the variables.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Chi-Square Test of Independence</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_chi_sq_independence.html#what-is-a-chi-square-test-of-independence",
    "href": "lessons/03_two_sample_chi_sq_independence.html#what-is-a-chi-square-test-of-independence",
    "title": "\n18  Chi-Square Test of Independence\n",
    "section": "",
    "text": "Preference A\nPreference B\nTotal\n\n\n\nMale\n20\n30\n50\n\n\nFemale\n25\n25\n50\n\n\nTotal\n45\n55\n100",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Chi-Square Test of Independence</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_chi_sq_independence.html#statistic-for-chi-square-test",
    "href": "lessons/03_two_sample_chi_sq_independence.html#statistic-for-chi-square-test",
    "title": "\n18  Chi-Square Test of Independence\n",
    "section": "\n18.2 Statistic for Chi-Square test",
    "text": "18.2 Statistic for Chi-Square test\n\n\n\nPreference A\nPreference B\nTotal\n\n\n\nMale\n20\n30\n50\n\n\nFemale\n25\n25\n50\n\n\nTotal\n45\n55\n100\n\n\n\n1. Calculate Expected Frequencies: For each cell, the expected frequency is calculated as below Example for cell (Male, Preference A): \\(E_{Male,A}\\) = (50×45) 100 = 22.5\n2. Compute the Chi-Square Statistic: calculated using the formula:\n\\[\nx^2 = \\sum\\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\n\\]\nwhere \\(O_{ij}\\) is the observed frequency and \\(E_{ij}\\) is the expected frequency.\nSum this calculation over all cells in the table.\nDetermine Degrees of Freedom (df): df=(Number of Rows−1)×(Number of Columns−1)\n3. Find the Critical Value and Compare:\nUsing the degrees of freedom and the significance level (e.g., 0.05), find the critical value from the Chi-Square distribution table.\nCompare the calculated \\(x^2\\) to the critical value to determine whether to reject the null hypothesis.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Chi-Square Test of Independence</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_chi_sq_independence.html#conducting-the-test",
    "href": "lessons/03_two_sample_chi_sq_independence.html#conducting-the-test",
    "title": "\n18  Chi-Square Test of Independence\n",
    "section": "\n18.3 Conducting the test",
    "text": "18.3 Conducting the test\nResearch Question and Hypotheses The general question for a Chi-Square Test of Independence is: Is there a statistically significant association between two categorical variables?\nExample questions include:\n\nIs there an association between gender and voting preference?\nIs there a relationship between education level and job satisfaction?\nDoes the use of a new teaching method depend on the grade level of students?\n\nHypotheses From the research question, you derive the hypotheses:\nNull Hypothesis (\\(H_{0}\\)): There is no association between the two categorical variables (they are independent). Alternative Hypothesis (\\(H_{1}\\)): There is an association between the two categorical variables (they are not independent).\nBefore performing the Chi-Square test, check these assumptions:\nCategorical Data: Both variables should be categorical.\nIndependence of Observations: Each observation should contribute to only one cell in the contingency table.\nExpected Frequency: Expected frequency in each cell should be at least 5 for the test to be valid.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Chi-Square Test of Independence</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_chi_sq_independence.html#data-format-contingency-tables",
    "href": "lessons/03_two_sample_chi_sq_independence.html#data-format-contingency-tables",
    "title": "\n18  Chi-Square Test of Independence\n",
    "section": "\n18.4 Data format: Contingency tables",
    "text": "18.4 Data format: Contingency tables\nWe’ll use housetasks data sets from STHDA:http://www.sthda.com/sthda/RDoc/data/housetasks.txt. link\n\n# Import the data\n# file_path &lt;- \"http://www.sthda.com/sthda/RDoc/data/housetasks.txt\"\nhousetasks &lt;- read.delim(\"../data/03_housetasks.txt\", row.names = 1)\n\n# Show top rows of data\nhead(housetasks)\n\n           X.Wife Alternating Husband Jointly\nLaundry       156          14       2       4\nMain_meal     124          20       5       4\nDinner         77          11       7      13\nBreakfeast     82          36      15       7\nTidying        53          11       1      57\nDishes         32          24       4      53\n\n\nThe data is a contingency table containing 13 house tasks and their distribution in the couple:  1. by the wife only  2. alternatively  3. by the husband only  4. jointly",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Chi-Square Test of Independence</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_chi_sq_independence.html#graphical-display-of-contengency-tables",
    "href": "lessons/03_two_sample_chi_sq_independence.html#graphical-display-of-contengency-tables",
    "title": "\n18  Chi-Square Test of Independence\n",
    "section": "\n18.5 Graphical display of contengency tables",
    "text": "18.5 Graphical display of contengency tables\nContingency table can be visualized using the function balloonplot() [in gplots package]. This function draws a graphical matrix where each cell contains a dot whose size reflects the relative magnitude of the corresponding component.\n\n# Convert the data as a table\n\ndt &lt;- as.table(as.matrix(housetasks))\n\n\n# Graph of contingency table\nballoonplot(\n  t(dt), \n  main =\"housetasks\", \n  xlab =\"\", \n  ylab=\"\",\n  label = FALSE\n) \n\n\n\n\n\n\n\n\nThe size of the blue bubbles in the balloon plot represents the frequency or count of how often a specific task is performed by a particular household member or group of members. Larger bubbles indicate higher frequencies, meaning that the task is performed more often by that household member or group.\n\nOverall Summary:  1. The Wife appears to handle the majority of routine daily tasks such as cooking, cleaning, and shopping.  2. The Husband is more involved in tasks like Repairs, Driving, and Finances.  3. Some tasks are done jointly, showing cooperation between household members for tasks like Shopping, Official duties, and Holidays.  4. Alternating responsibility is the least common method of task distribution.\n\nRow and column sums are printed by default in the bottom and right margins, respectively. These values can be hidden using the argument show.margins = FALSE.",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Chi-Square Test of Independence</span>"
    ]
  },
  {
    "objectID": "lessons/03_two_sample_chi_sq_independence.html#compute-chi-square-test-in-r",
    "href": "lessons/03_two_sample_chi_sq_independence.html#compute-chi-square-test-in-r",
    "title": "\n18  Chi-Square Test of Independence\n",
    "section": "\n18.6 Compute chi-square test in R",
    "text": "18.6 Compute chi-square test in R\nChi-square statistic can be easily computed using the function chisq.test() as follow:\n\nchisq &lt;- chisq.test(housetasks)\nchisq\n\n\n    Pearson's Chi-squared test\n\ndata:  housetasks\nX-squared = 1944.5, df = 36, p-value &lt; 2.2e-16\n\n\n\nAs the results shown above, p-value &lt; 2.2e-16, which means we can reject the null hypothesis, and saying that the row and the column variables are statistically significantly associated.\n\n\n18.6.1 Nature of the dependence between the row and the column variables\nIf you want to know the most contributing cells to the total Chi-square score, you just have to calculate the Pearson residuals (r) for each cell (or standardized residuals).Pearson residuals can be easily extracted from the output of the function chisq.test():\n\nCells with the highest absolute standardized residuals contribute the most to the total Chi-square score.\n\nVisualize Pearson residuals using the package corrplot:\n\n# Contibution in percentage (%)\n\ncontrib &lt;- 100*chisq$residuals^2/chisq$statistic\n\n\n# Visualize the contribution\n\ncorrplot(\n  contrib, \n  is.cor = FALSE\n)\n\n\n\n\n\n\n\nFor a given cell, the size of the circle is proportional to the amount of the cell contribution.\nIt can be seen that:  1. The column “Wife” is strongly associated with Laundry, Main_meal, Dinner  2. The column “Husband” is strongly associated with the row Repairs  3. The column jointly is frequently associated with the row Holidays",
    "crumbs": [
      "Two-Sample Tests",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Chi-Square Test of Independence</span>"
    ]
  },
  {
    "objectID": "04_header_anova-and-regression.html",
    "href": "04_header_anova-and-regression.html",
    "title": "ANOVA and Regression",
    "section": "",
    "text": "Text Outline\nThis text covers the basics of:",
    "crumbs": [
      "ANOVA and Regression"
    ]
  },
  {
    "objectID": "04_header_anova-and-regression.html#text-outline",
    "href": "04_header_anova-and-regression.html#text-outline",
    "title": "ANOVA and Regression",
    "section": "",
    "text": "Exploring data\nOne-sample tests\nTwo-sample tests\nANOVA and linear regression\nGeneralized linear models\nSome special topics\nMaybe a bit about power calculations…",
    "crumbs": [
      "ANOVA and Regression"
    ]
  },
  {
    "objectID": "04_header_anova-and-regression.html#part-outline",
    "href": "04_header_anova-and-regression.html#part-outline",
    "title": "ANOVA and Regression",
    "section": "Part Outline",
    "text": "Part Outline\nThis part of the text contains the chapters on various ways to perform ANOVA and linear regression:\n\nOne-Way ANOVA\nTwo-way ANOVA\nWelch’s ANOVA\nKruskal-Wallace Test\nTukey HSD Post-Hoc Test\nRepeated Measures ANOVA\nRandom Intercept Models\nCorrelation Matrices and Covariances\nMultiple Regression (linear)\nPolynomial regression",
    "crumbs": [
      "ANOVA and Regression"
    ]
  },
  {
    "objectID": "lessons/04_anova_one_way.html",
    "href": "lessons/04_anova_one_way.html",
    "title": "\n19  One-Way ANOVA\n",
    "section": "",
    "text": "19.1 Introduction to one-way ANOVA\nOne-way analysis of variance (ANOVA) is an extension of a two-samples t-test for comparing means between three or more independent groups. A single factor variable is employed to categorize the data into several groups.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>One-Way ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_one_way.html#introduction-to-one-way-anova",
    "href": "lessons/04_anova_one_way.html#introduction-to-one-way-anova",
    "title": "\n19  One-Way ANOVA\n",
    "section": "",
    "text": "Hypothesis Testing\n\n\n\nNull Hypothesis \\((H_0)\\) the means between groups are not statistically different (they ARE the same).  Alternative Hypothesis \\((H_a)\\) the means between groups are statistically different (they ARE NOT the same).",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>One-Way ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_one_way.html#mathematical-definition-of-one-way-anova",
    "href": "lessons/04_anova_one_way.html#mathematical-definition-of-one-way-anova",
    "title": "\n19  One-Way ANOVA\n",
    "section": "\n19.2 Mathematical definition of one-way ANOVA",
    "text": "19.2 Mathematical definition of one-way ANOVA\n\\[\nY_{ij} = \\mu + \\tau_{i} + \\epsilon_{ij}\n\\]\nWhere,\n\n\n\\(Y_{ij}\\) represents the j-th observation (j = 1, 2, …, \\(n_{i}\\)) on the i-th treatment (i = 1, 2, …, k levels).\n\nSo, \\(Y_{23}\\) represents the third observation for the second factor level.\n\n\n\n\\(\\mu\\) is the common effect.\n\n\\(\\tau_{i}\\) represents the i-th treatment effect.\n\n\\(\\epsilon_{ij}\\) represents the random error present in the j-th observation on the i-th treatment.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>One-Way ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_one_way.html#data-description",
    "href": "lessons/04_anova_one_way.html#data-description",
    "title": "\n19  One-Way ANOVA\n",
    "section": "\n19.3 Data Description",
    "text": "19.3 Data Description\nThe PlantGrowth data set is already included in {base R} within the {datasets} package. The PlantGrowth data set includes results from an experiment to compare yield obtained under a control condition and two different treatment conditions. The measurements were obtained through the dried weight of each plant.\n\n# Call the data set into the Global environment\ndata(\"PlantGrowth\")\n\n# Use the `glimpse()` or `head()` function to skim observations\n# glimpse(PlantGrowth)\nhead(PlantGrowth)\n\n  weight group\n1   4.17  ctrl\n2   5.58  ctrl\n3   5.18  ctrl\n4   6.11  ctrl\n5   4.50  ctrl\n6   4.61  ctrl\n\n# Optional: You can use the `summary()` function to confirm that all variables\n# are being read correctly. E.g. the `group` variable is displayed as counts\n# for each level\nsummary(PlantGrowth)\n\n     weight       group   \n Min.   :3.590   ctrl:10  \n 1st Qu.:4.550   trt1:10  \n Median :5.155   trt2:10  \n Mean   :5.073            \n 3rd Qu.:5.530            \n Max.   :6.310            \n\n# Optional: You can use the `str()` function to view the factor levels and\n# confirm the appropriate reference level for analysis\nstr(PlantGrowth)\n\n'data.frame':   30 obs. of  2 variables:\n $ weight: num  4.17 5.58 5.18 6.11 4.5 4.61 5.17 4.53 5.33 5.14 ...\n $ group : Factor w/ 3 levels \"ctrl\",\"trt1\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n# R assigns reference level in alphabetical order. E.g. From our factor levels \n# (ctrl, trt1, trt2) - ctrl will be assigned as the reference level \n# automatically by R.\n# Here, we see that `group` is a factor with 3 levels,\n# with 1 corresponding to 1 (the first level).\n\n# If you need to re-assign the reference level, you can `relevel()` your factor\n# The first variable after `Levels:` is your new reference group.\nrelevel(PlantGrowth$group, ref = \"ctrl\")\n\n [1] ctrl ctrl ctrl ctrl ctrl ctrl ctrl ctrl ctrl ctrl trt1 trt1 trt1 trt1 trt1\n[16] trt1 trt1 trt1 trt1 trt1 trt2 trt2 trt2 trt2 trt2 trt2 trt2 trt2 trt2 trt2\nLevels: ctrl trt1 trt2",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>One-Way ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_one_way.html#assumptions-of-one-way-anova",
    "href": "lessons/04_anova_one_way.html#assumptions-of-one-way-anova",
    "title": "\n19  One-Way ANOVA\n",
    "section": "\n19.4 Assumptions of One-Way ANOVA",
    "text": "19.4 Assumptions of One-Way ANOVA\nThe assumptions of one-way ANOVA are as follows:\n\n\nAssumption 1: All observations are independent and randomly selected from the population as defined by the factor variable.\n\nAssumption 2: The data within each factor level are approximately normally distributed.\n\nAssumption 3: The variance of the data of interest is similar across each factor level (aka: Homogeneity of variance).",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>One-Way ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_one_way.html#checking-the-assumptions-of-the-one-way-anova",
    "href": "lessons/04_anova_one_way.html#checking-the-assumptions-of-the-one-way-anova",
    "title": "\n19  One-Way ANOVA\n",
    "section": "\n19.5 Checking the Assumptions of the One-Way ANOVA",
    "text": "19.5 Checking the Assumptions of the One-Way ANOVA\nAssumption 2: The data within each factor level are approximately normally distributed.\nTo check this assumption, we can examine if the data are approximately normally distributed across groups with 2 plots, the Q-Q plot or histograms, or with the Shapiro-Wilk test.\nFirst, we will look at the Q-Q plot:\n\n# Check for missing values to ensure balanced data\ncolSums(is.na(PlantGrowth))\n\nweight  group \n     0      0 \n\n# No missing datapoints\n\n# Normality assumption: Q-Q plot\nqq_plot_plantgrowth &lt;- ggplot(PlantGrowth) +\n  aes(sample = weight) +\n  facet_wrap(~ group) +\n  stat_qq() +\n  stat_qq_line()\n\nqq_plot_plantgrowth\n\n\n\n\n\n\n\nThe weight variable seems to be approximately normally distributed across each of the three groups based on the Q-Q plots.\nNext, we will look at histograms:\n\n# Normality assumption: Histograms\nhistogram_plantgrowth &lt;- ggplot(PlantGrowth) +\n  aes(x = weight) +\n  geom_histogram(binwidth = 0.5) +\n  facet_wrap(~ group)\n\nhistogram_plantgrowth\n\n\n\n\n\n\n\nAgain, the weight variable appears to be approximately normally distributed across the groups.\nLastly, we can use the Shapiro-Wilk test to test if the data is approximately normally distributed. P-values greater than 0.05 indicate that the data is likely approximately normally distributed.\n\n# Normality: Shapiro Wilks\nshapiro_plantgrowth &lt;- PlantGrowth %&gt;%\n  group_by(group) %&gt;%\n  summarise(\n    statistic = shapiro.test(weight)$statistic,\n    p.value = shapiro.test(weight)$p.value\n  )\n\nshapiro_plantgrowth\n\n# A tibble: 3 × 3\n  group statistic p.value\n  &lt;fct&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 ctrl      0.957   0.747\n2 trt1      0.930   0.452\n3 trt2      0.941   0.564\n\n\nThe p-value for each of our groups is above 0.05, so we can assume that weight is approximately normally distributed within each group.\nAssumption 3: Homogeneity of variance.\nWe can check this assumption with Levene’s test. A p-value greater than 0.05 indicates that the variance is similar across groups, and we therefore meet the requirements of this assumption.\n\n# Homogeneity of Variance: Levene's test\nlevenes_plantgrowth &lt;- levene_test(weight ~ group, data = PlantGrowth)\nlevenes_plantgrowth\n\n# A tibble: 1 × 4\n    df1   df2 statistic     p\n  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1     2    27      1.12 0.341\n\n\nThe p-value is 0.341, so we can therefore assume that our data meets the assumption of homogeneity of variance.\nWe can also check assumption 3 by visually examining a box plot of the outcome variable across groups:\n\n# Variance assumption: Box plot for weight by group\nbox_plantgrowth &lt;- ggplot(PlantGrowth) +\n  aes(y = weight) +\n  geom_boxplot() +\n  facet_wrap(~ group)\n\nbox_plantgrowth\n\n\n\n\n\n\n\nTo compare the box plot visually, we take a look at the interquartile range between each three plots. All three have approximately similar size and length, which satisfies our assumption homogeneity of variance.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>One-Way ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_one_way.html#code-to-run-one-way-anova",
    "href": "lessons/04_anova_one_way.html#code-to-run-one-way-anova",
    "title": "\n19  One-Way ANOVA\n",
    "section": "\n19.6 Code to Run One-Way ANOVA",
    "text": "19.6 Code to Run One-Way ANOVA\n\n# Include an outcome variable, `weight`, and predictor(s), `group`. as well as \n# which data set to pull variables from with `data = ` argument\nanova_fit &lt;- aov(weight ~ group, data = PlantGrowth)\n\n# Call summary statistics from `anova_fit` object\nsummary(anova_fit)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)  \ngroup        2  3.766  1.8832   4.846 0.0159 *\nResiduals   27 10.492  0.3886                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe summary includes the independent variable group being tested within the model. All variation that is not explained by group is considered residual variance.\n\nThe Df column tabulates the degrees of freedom for the independent variable, which is the number of levels in the variable minus 1. For example, group has 3 levels (3 - 1 = 2 degrees of freedom).\nThe Sum Sq column tabulates the sum of squares (total variation between group means and overall mean).\nThe Mean Sq column tabulates the mean of the sum of squares, which is calculated by dividing the sum of squares by the degrees of freedom for each parameter.\nThe F value column tabulates the test statistic from the F test, which is the mean square of each independent variable (only one in this case) by the mean square of the residuals.\nThe Pr(&gt;F) column is the resulting p-value of the F statistic. Remember: the p-value shows how likely it is that the calculated F value would have occurred if the null hypothesis were true.\n\nThe p value of the fertilizer variable is low (p &lt; 0.01), so it appears that the type of fertilizer used (group) has a real impact on the final crop yield (weight).\n\n19.6.1 Post-Hoc Test for Pairwise Comparisons\nTo compare the group means directly, we can conduct a post-hoc test to see which group means are different from one another. One post-hoc test is Tukey’s Test.\n\n# perform Tukey's Test for multiple comparisons\nanova_post_hoc &lt;- TukeyHSD(anova_fit, conf.level=.95)\n\nanova_post_hoc\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = weight ~ group, data = PlantGrowth)\n\n$group\n            diff        lwr       upr     p adj\ntrt1-ctrl -0.371 -1.0622161 0.3202161 0.3908711\ntrt2-ctrl  0.494 -0.1972161 1.1852161 0.1979960\ntrt2-trt1  0.865  0.1737839 1.5562161 0.0120064\n\n\nIn the above output, we can see that:\n\n\ntrt1-ctrl: P-value is approximately 0.391 when comparing treatment 1 (trt1) and control (ctrl), which is greater than 0.05. This indicates there is not a statistically significant difference in mean between these groups. Their means are the SAME.\n\ntrt2-ctrl: P-value is approximately 0.198 when comparing treatment 2 (trt2) and control (ctrl), which is greater than 0.05. This indicates there is not a statistically significant difference in mean between these groups. Their means are the SAME.\n\ntrt2-trt1: P-value is 0.012 when comparing treatment 1 (trt1) and treatment 2 (trt2), which is less than 0.05. This indicates there is a statistically significant difference in mean between these groups. Their means are NOT the SAME.\n\n19.6.2 Regression for Comparison\nTo compare the ANOVA results with those of a simple linear regression, we can run a regression model to examine if there are differences between the control group and the two treatment groups.\n\n# Simple linear regression\nlm_plantgrowth &lt;- lm(weight ~ group, data = PlantGrowth)\n\nsummary(lm_plantgrowth)\n\n\nCall:\nlm(formula = weight ~ group, data = PlantGrowth)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.0710 -0.4180 -0.0060  0.2627  1.3690 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   5.0320     0.1971  25.527   &lt;2e-16 ***\ngrouptrt1    -0.3710     0.2788  -1.331   0.1944    \ngrouptrt2     0.4940     0.2788   1.772   0.0877 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6234 on 27 degrees of freedom\nMultiple R-squared:  0.2641,    Adjusted R-squared:  0.2096 \nF-statistic: 4.846 on 2 and 27 DF,  p-value: 0.01591\n\n\nFrom the above output, we can see that:\n\nThe F-statistic is 4.846, which is the same F-statistic produced by the ANOVA.\nThe p-value of the model (0.01591) is the same as the p-value produced by the ANOVA.\nThe degrees of freedom for the ANOVA and linear model are also the same (2 and 27).\nThe Multiple R-squared value from the regression (0.2641) is related to the Sum of Squares for our factor and the Sum of Squares for the residual in the following way: \\(R^{2} = \\frac{SS_{factor}}{(SS_{factor} + SS_{residuals})}\\).\nThe linear model confirms that there is no difference in the treatment groups compared to the control group since the p-values for the treatment groups are larger than 0.05 (0.194 and 0.088 for treatment 1 and treatment 2 versus control, respectively).",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>One-Way ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_one_way.html#brief-interpretation-of-the-output",
    "href": "lessons/04_anova_one_way.html#brief-interpretation-of-the-output",
    "title": "\n19  One-Way ANOVA\n",
    "section": "\n19.7 Brief Interpretation of the Output",
    "text": "19.7 Brief Interpretation of the Output\nThe resulting p-value of the one-way ANOVA is 0.0159, which is less than our significance level of 0.05. Therefore we must reject our null hypothesis as the data supports that the mean weight between groups is statistically different. In other words, the mean weight(s) are NOT the same between group (ctrl, trt1, trt2).\nThe Tukey test results illustrate a true difference that only exists between treatment groups 1 and 2 (trt1 and trt2), and not between the control group and either of the treatment groups. This is confirmed by the output from the simple linear regression model.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>One-Way ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_two_way.html",
    "href": "lessons/04_anova_two_way.html",
    "title": "\n20  Two-Way ANOVA\n",
    "section": "",
    "text": "20.1 Introduction\nThe following two-way ANOVA lesson is based on “Two-Way ANOVA | Examples & When to Use It”, a tutorial by Rebecca Bevans.\nA two-way ANOVA examines the influence of two or more independent categorical variables, also known as factors, on a continuous dependent variable. Each factor must have at least two levels, which are the groups within each factor being analyzed. In this lesson, we will see an example of how to test for these assumptions and how to conduct a type I two-way ANOVA once we know they have been met.\nAn interaction occurs when the effect of one factor depends on the level of another factor. The two-way ANOVA method allows us to measure:",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Two-Way ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_two_way.html#introduction",
    "href": "lessons/04_anova_two_way.html#introduction",
    "title": "\n20  Two-Way ANOVA\n",
    "section": "",
    "text": "Main Effects - the individual effect of one factor on the dependent variable, and\nInteraction Effects - the extent to which the effects of one factor change across different levels of another factor.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Two-Way ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_two_way.html#how-does-a-two-way-anova-work",
    "href": "lessons/04_anova_two_way.html#how-does-a-two-way-anova-work",
    "title": "\n20  Two-Way ANOVA\n",
    "section": "\n20.2 How Does a Two-Way ANOVA Work?",
    "text": "20.2 How Does a Two-Way ANOVA Work?\nThe F test, a group-wise comparison test, is used in an ANOVA to determine statistical significance. The outcome of an F test, the F statistic, measures how different a groups’ variances are to the overall variance of the dependent variable. When the variance is higher between groups than within the group, the F statistic will be greater. A large F statistic suggests there’s a low likelihood that the observed difference is due to chance, and thus, the factor likely has an effect on the outcome.\nFor comparison, a critical value is calculated based on the desired alpha (α) and degrees of freedom of all groups. If the F statistic is greater than the F critical value, the results are considered statistically significant. For example, if the F statistic is greater than the F critical value at α = 0.05, then the p-value will be less than 0.05. For a model with two factors, we can generate three F statistics and three associated p values to test three possible hypotheses:\n\n\n\n\n\n\nNull Hypotheses\n\n\n\n\nH01: The population means of the first factor are equal.\n\nH02: The population means of the second factor are equal.\n\nH03: There is no interaction effect, that is, the effect of one factor does not depend on the value of the other factor.\n\n\n\n\n\n\n\n\n\n\nAlternative Hypotheses\n\n\n\n\nHA1: The population means of the first factor are not equal.\n\nHA2: The population means of the second factor are not equal.\n\nHA3: There is an interaction effect, that is, the effect of one factor depends on the value of the other factor.\n\n\n\n\n\n20.2.1 Assumptions\nThree assumptions must be met before conducting a two-way ANOVA:\n\nHomogeneity of variance: The variances for each group should be roughly equal. If the groups do not have equal variances, a non-parametric test, such as the Kruskal-Wallis test, may be used.\nIndependence of observations: The observations in each group are independent of each other and the observations within groups were obtained by a random sample. This can be assumed to be true so long as the experiment is properly designed. If observations are grouped in categories, this effect should be accounted for with the use of a blocking variable or a repeated-measures ANOVA.\nNormally distributed dependent variable: The values of the dependent variable should follow a normal distribution.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Two-Way ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_two_way.html#two-way-anova-table",
    "href": "lessons/04_anova_two_way.html#two-way-anova-table",
    "title": "\n20  Two-Way ANOVA\n",
    "section": "\n20.3 Two-Way ANOVA Table",
    "text": "20.3 Two-Way ANOVA Table\nThe following table includes the calculations used in a two-way ANOVA. For more detailed information on these calculations, please read Models and Calculations for the Two-Way ANOVA, a lesson by the National Institute of Standards and Technology.\nIn the ANOVA table, the main effect A has k levels and the main effect B has l levels. N represents the total sample size.\n\n\n\n\n\n\n\n\n\nSource of Variation\nSum of Squares\nDegrees of freedom\nMean Squares\nF value\n\n\n\nFactor A\n\\(SS_A\\)\n\\(k-1\\)\n\\(MS_A\\)\n\\(F_A\\)\n\n\nFactor B\n\\(SS_B\\)\n\\(l-1\\)\n\\(MS_B\\)\n\\(F_B\\)\n\n\nInteraction AB\n\\(SS_{AB}\\)\n\\((k-1)(l-1)\\)\n\\(MS_{AB}\\)\n\\(F_{AB}\\)\n\n\nError\n\\(SS_E\\)\n\\(N - kl\\)\n\\(MS_E\\)\n\n\n\nTotal\n\\(SS_T\\)\n\\(N-1\\)\n\n\n\n\n\nWhere\n\n\\[ MS_E := \\frac{SS_E}{N-kl} \\]\n\\[ MS_A := \\frac{SS_A}{k-1} \\text{ and } F_A := \\frac{MS_A}{MS_E} \\]\n\\[ MS_B := \\frac{SS_B}{l-1} \\text{ and } F_B := \\frac{MS_B}{MS_E} \\]\n\\[ MS_{AB} := \\frac{SS_{AB}}{(k-1)(l-1)} \\text{ and } F_{AB} := \\frac{MS_{AB}}{MS_E} \\]\n\nWe explain these components as:\n\n\n\\(SS_A\\): Factor \\(A\\) main effect sums of squares, df = \\(k-1\\)\n\n\n\\(SS_B\\): Factor \\(B\\) main effect sums of squares, df = \\(l-1\\)\n\n\n\\(SS_{AB}\\): interaction sum of squares, df = \\((k-1)(l-1)\\)\n\n\n\\(SS_E\\): error sum of squares, df = \\(N-kl\\)\n\n\n\\(SS_T\\): Total sums of squares, df = \\(N-1\\)",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Two-Way ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_two_way.html#crop-yield-two-way-anova-example",
    "href": "lessons/04_anova_two_way.html#crop-yield-two-way-anova-example",
    "title": "\n20  Two-Way ANOVA\n",
    "section": "\n20.4 Crop Yield Two-Way ANOVA Example",
    "text": "20.4 Crop Yield Two-Way ANOVA Example\nWe will use the agricultural crop yield dataset from the Two-Way ANOVA lesson lesson in our example, which can be directly downloaded here.\nIn this example, corn was planted in one of four blocks within a field, in either high or low density, and fertilized with one of three types of fertilizer. Their yield in bushels per acre was then measured. The three factors of this experiment with their associated levels are:\n\nType of fertilizer (type 1, 2, or 3)\nPlanting density (1 = low, 2 = high)\nBlock number in the field (block 1, 2, 3, or 4)\n\n\n20.4.1 Hypotheses\nSuppose we want to use a two-way ANOVA to examine whether the type of fertilizer and planting density (factors) have an effect on the average crop yield (dependent variable). To answer this question, the following three hypotheses will be tested:\n\n\n\n\n\n\nNull Hypotheses\n\n\n\n\nH01: Fertilizer type has no effect on average crop yield\n\nH02: Planting density has no effect on average crop yield\n\nH03: The effects of fertilizer type and planting density on average yield are independent of each other (no interaction exists)\n\n\n\n\n\n\n\n\n\n\nAlternative Hypotheses\n\n\n\n\nH11: Fertilizer type has an effect on average crop yield\n\nH12: Planting density has an effect on average crop yield\n\nH13: The effects of fertilizer type and planting density on average yield are not independent of each other (no interaction occurs)\n\n\n\n\n\n20.4.2 Loading Libraries and Data\nFirst, we will load our required packages and read in the crop yield dataset. The tidyverse package will be used to transform the data. The tidymodels and gt packages are necessary for creating presentation-ready tables of results. Finally, the AICcmodavg package will be used to construct an AIC table with which to compare our models (more on this later).\n\n# Uncomment the following code to install required packages\n\n# gt and tidymodels used for presentation-ready table:\n# install.packages(\"gt\")\n# install.packages(\"tidymodels\")\n\n# AICcmodavg used for AIC table:\n# install.packages(\"AICcmodavg\")\n\n# tidyverse for data transformation:\n# install.packages(\"tidyverse\")\n\n# Load the required packages\nlibrary(gt)\nlibrary(tidymodels)\nlibrary(AICcmodavg)\nlibrary(tidyverse)\n\n# read in the crop data dataset downloaded from\n# https://www.scribbr.com/wp-content/uploads//2020/03/crop.data_.anova_.zip\ncrop_data_df &lt;- read_csv(\"../data/04_crop_data.csv\")\n\n\n20.4.3 Data Exploration\nThe first few datapoints of the crop yield dataset were inspected and summary statistics were performed on the entire dataset to examine the structure, center, and spread of the data.\n\n# show first six rows of the dataset\nhead(crop_data_df)\n\n# overview of summary statistics\nsummary(crop_data_df)\n\n# A tibble: 6 × 4\n  density block fertilizer yield\n    &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;\n1       1     1          1  177.\n2       2     2          1  178.\n3       1     3          1  176.\n4       2     4          1  178.\n5       1     1          1  177.\n6       2     2          1  177.\n    density        block        fertilizer     yield      \n Min.   :1.0   Min.   :1.00   Min.   :1    Min.   :175.4  \n 1st Qu.:1.0   1st Qu.:1.75   1st Qu.:1    1st Qu.:176.5  \n Median :1.5   Median :2.50   Median :2    Median :177.1  \n Mean   :1.5   Mean   :2.50   Mean   :2    Mean   :177.0  \n 3rd Qu.:2.0   3rd Qu.:3.25   3rd Qu.:3    3rd Qu.:177.4  \n Max.   :2.0   Max.   :4.00   Max.   :3    Max.   :179.1  \n\n\n\n20.4.4 Transform Treatment Factors Into R Factors\nNote that the fertilizer, density, and block vectors are read in as numeric class objects, so we must convert them to the factor class. We will also specify the reference groups for each factor: fertilizer 1, low density, and block 1. If not specified, reference groups are automatically assigned in alphabetical order.\n\n# transform numeric vectors to factors and assign reference groups\ncrop_data_df$fertilizer &lt;- as.factor(crop_data_df$fertilizer) %&gt;% \n  relevel(ref = 1)\ncrop_data_df$density &lt;- as.factor(crop_data_df$density) %&gt;% \n  relevel(ref = 1)\ncrop_data_df$block &lt;- as.factor(crop_data_df$block) %&gt;% \n  relevel(ref = 1)\n\n\n20.4.4.1 Boxplots of Factors\nBox plots for each factor were created to further examine the spread and center of the data using ggplot2.\n\n# create box plot of yield by fertilizer\nplot_fertilizer &lt;- \n  ggplot(data = crop_data_df) +\n    aes(\n      x = as.factor(fertilizer),\n      y = yield,\n      color = fertilizer \n    ) +\n  labs(\n    title = \"Crop Yield by Fertilizer Type\",\n    x = \"Fertilizer Type\",\n    y = \"Crop Yield\"\n  ) +\n  geom_boxplot()\n\nplot_fertilizer\n\n\n\n\n\n\n# yield by density\nplot_density &lt;- \n  ggplot(data = crop_data_df) +\n    aes(\n      x = as.factor(density),\n      y = yield,\n      color = density\n    ) +\n  labs(\n    title = \"Crop Yield by Planting Density\",\n    x = \"Planting Density\",\n    y = \"Crop Yield\"\n  ) +\n  geom_boxplot()\n\nplot_density\n\n\n\n\n\n\n# yield by block\nplot_block &lt;- \n  ggplot(data = crop_data_df) +\n    aes(\n      x = as.factor(block),\n      y = yield,\n      color = block \n    ) +\n  labs(\n    title = \"Crop Yield by Block\",\n    x = \"Block\",\n    y = \"Crop Yield\"\n  ) +\n  geom_boxplot()\n\nplot_block\n\n\n\n\n\n\n\nThe data for each level appear to be approximately normally distributed with roughly equal variance and few outliers.\n\n20.4.4.2 Combined Boxplot of Crop Yield by Fertilizer and Density\nTo help visualize the data, a combined box plot visualizing the effects of fertilizer and density on average crop yield was created. The y-axis indicates the average crop yield. The x-axis is divided by fertilizer type (labeled at the top of the graph) and indicates the planting density for each level of the fertilizer factor.\n\nanova_plot &lt;- \n  ggplot(crop_data_df) +\n  aes(\n    x = density, \n    y = yield, \n    group = fertilizer,\n    color = fertilizer\n  ) +\n  geom_point(\n    cex = 1.5, \n    pch = 1.0, \n# set point positions to randomly jitter so points don't overlap\n    position = position_jitter(w = 0.1, h = 0.1)\n  )\n\nanova_plot &lt;- anova_plot + \n# compute summary stats to create error bars\n  stat_summary(\n# set calculation for error bar parameters to mean_se\n    fun.data = 'mean_se',\n    geom = 'errorbar',\n    width = 0.2,\n    color = \"grey50\"\n  ) +\n# create points for level means\n  stat_summary(\n# set calculation for mean parameter to mean_se\n    fun.data = 'mean_se',\n    geom = 'pointrange'\n  )\n\nanova_plot &lt;- anova_plot +\n# create three boxplots side by side for each fertilizer level\n  facet_wrap(~ fertilizer)\n\nanova_plot &lt;- anova_plot +\n  theme_classic() +\n  labs(\n    title = \"Crop Yield Averages by Fertilizer Types and Planting Density\",\n    x = \"Planting Density (1 = low density, 2 = high density)\",\n    y = \"Average Yield\"\n  )\n\nanova_plot\n\n\n\n\n\n\n\nThe differing means for each level of fertilizer and density suggest that the factors have an effect on average yield. Before we can test whether these potential effects are statistically significant, we must first check that the data meet the ANOVA assumptions. To do this, we will generate a two-way ANOVA and run diagnostics on the model.\n\n20.4.5 Performing the Two-Way ANOVA with Interaction and Blocking Variables\nRecall that the two-way ANOVA can actually account for more than two factors. The crops were planted across various blocks whose conditions may differ in terms sunlight, moisture, etc. This could possibly lead to confounding, so it is important to control for the possible effect of these differences by adding this third factor to our model.\nA two-way ANOVA was created to model the effects of fertilizer, density, the interaction between fertilizer and density, and the blocking factor. In the following code, the argument fertilizer * density is equivalent to fertilizer + density + fertilizer : density, where fertilizer : density represents the interaction term.\n\n\n\n\n\n\nAre your data balanced?\n\n\n\nBecause the crop data are balanced (the sample sizes across the levels within each factor are equal), we will use the base-R function aov(), which uses Type I sums of squares. If your data are unbalanced, use a different function to conduct a Type II ANOVA (for data with no significant interaction) or Type III ANOVA (for data with significant interaction).\n\n\n\n# performing two-way anova with fertilizer, density, fertilizer:density \n#   interaction and blocking factor\nfull_model &lt;- \n# 'fertilizer * density' = fertilizer + density + fertilizer:density interaction\n  aov(yield ~ fertilizer * density + block, data = crop_data_df)\n\n\n20.4.6 Checking ANOVA Assumptions\nWe will now create diagnostic plots, which include a residuals vs fitted plot, scale-location plot, Q-Q plot, and constant leverage plot, to evaluate the homoscedasticity of our data. Please read Understanding Diagnostic Plots for Linear Regression Analysis for more information on these plots.\n\n# set plot parameter to display 2 x 2 plots in output\npar(mfrow=c(2,2))\n# plot default diagnostic plots of full_model\nplot(full_model)\n\n\n\n\n\n\n# set plot parameter back to 1 x 1 format\npar(mfrow=c(1,1))\n\nThe residual vs fitted plot shows the data are randomly spread about the “0” line with no large outliers, so we can assume the factors have equal variances. Similarly, the scale-location plot shows an approximately horizontal line with randomly spread points, indicating equal variances. Thus, the homoscedasticity assumption is met.\nThe points of the Q-Q residuals plot roughly follow the reference line, indicating the data are normally distributed. We can conclude the normality assumption is met.\nThe constant leverage plot displays a horizontal “0” value line with points randomly spread around it, indicating the spread of the points are the same at different levels. No points lie outside of the critical value lines (not visible in the graph due to scale), indicating no outliers exist that could skew the data.\n\n20.4.7 Two-Way ANOVA - Full Model Interpretation\nNow that we have verified that the data satisfy the ANOVA assumptions, we may conduct the two-way ANOVA to test the hypotheses. Let’s interpret the results of the ANOVA of the full model:\n\ntable_full &lt;- full_model %&gt;% \n# turn model into data.frame of parameters\n  tidy() %&gt;% \n# create gt table\n  gt()\n\n# customizing gt table header\ntable_full |&gt;\n   tab_header(\n      title = \"Two-Way ANOVA of Crop Yield - Full Model\",\n      subtitle = \"for main full model (y ~ fertilizer * density + block)\"\n    )\n\n\n\n\n\n\n\nTwo-Way ANOVA of Crop Yield - Full Model\n\n\nfor main full model (y ~ fertilizer * density + block)\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nfertilizer\n2\n6.0680466\n3.0340233\n8.9443603\n0.0002909717\n\n\ndensity\n1\n5.1216812\n5.1216812\n15.0988167\n0.0001972614\n\n\nblock\n2\n0.4861389\n0.2430695\n0.7165735\n0.4912504731\n\n\nfertilizer:density\n2\n0.4278183\n0.2139091\n0.6306083\n0.5346558069\n\n\nResiduals\n88\n29.8505477\n0.3392108\nNA\nNA\n\n\n\n\n\n\n\nThe block variable’s p value of p = 0.49 and the interaction term’s p value of p = 0.53 are not significant at the α = 0.05 level. Hence, neither have a statistically significant effect on the crop yield. Because they do not add information to the model, They may be removed from the final model.\n\n20.4.8 Two-Way ANOVA - Density + Fertilizer + Interaction\nWe will now run a two-way ANOVA on a new model using the density, fertilizer, and interaction factors to investigate whether these terms are significant.\n\n# performing the two-way ANOVA with interaction\ninteraction &lt;- \n# 'fertilizer * density' = fertilizer + density + fertilizer:density interaction\n  aov(yield ~ fertilizer * density, data = crop_data_df)\n\ntable_int &lt;- interaction %&gt;%\n# turn model into data.frame of parameters\n  tidy() %&gt;% \n# create gt table\n  gt()\n\n# customizing table header\ntable_int |&gt;\n   tab_header(\n      title = \"Two-way ANOVA of Crop Yield - Interaction Model\",\n      subtitle = \"for interaction model (y ~ fertilizer * density)\"\n    )\n\n\n\n\n\n\n\nTwo-way ANOVA of Crop Yield - Interaction Model\n\n\nfor interaction model (y ~ fertilizer * density)\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nfertilizer\n2\n6.0680466\n3.0340233\n9.0010522\n0.0002731890\n\n\ndensity\n1\n5.1216812\n5.1216812\n15.1945174\n0.0001864075\n\n\nfertilizer:density\n2\n0.4278183\n0.2139091\n0.6346053\n0.5325000914\n\n\nResiduals\n90\n30.3366866\n0.3370743\nNA\nNA\n\n\n\n\n\n\n\nThe p value for the interaction term is greater than 0.05, hence we fail to reject the null hypothesis (H03) and conclude that there is no statistically significant interaction effect between fertilizer type and crop density on average yield. The interaction term should also be removed from the model.\n\n20.4.9 Two-Way ANOVA - Density + Fertilizer\nBecause the interaction term was not significant, we will remove it from our model and perform a two-way ANOVA on a model with only the density and fertilizer factors.\n\n# performing the two-way ANOVA without the interaction term\nmain_effects &lt;- \n  aov(yield ~ fertilizer + density, data = crop_data_df)\n\ntable_main &lt;- main_effects %&gt;% \n# turn model into data.frame of parameters\n  tidy() %&gt;%\n# create gt table\n  gt()\n\n# customizing table header\ntable_main |&gt;\n   tab_header(\n      title = \"Two-Way ANOVA of Crop Yield - Main Effects Model\",\n      subtitle = \"for main effects model (y ~ fertilizer + density)\"\n   )\n\n\n\n\n\n\n\nTwo-Way ANOVA of Crop Yield - Main Effects Model\n\n\nfor main effects model (y ~ fertilizer + density)\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nfertilizer\n2\n6.068047\n3.0340233\n9.073123\n0.0002532992\n\n\ndensity\n1\n5.121681\n5.1216812\n15.316179\n0.0001741418\n\n\nResiduals\n92\n30.764505\n0.3343968\nNA\nNA\n\n\n\n\n\n\n\nWithout the interaction term, we see that the p values for both fertilizer type and planting density are significant at the α = 0.05 level. Thus, we reject null hypotheses H01 and H02 and conclude that both fertilizer and density have a statistically significant main effect on crop yield.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Two-Way ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_two_way.html#determining-the-best-fitting-model-using-aic",
    "href": "lessons/04_anova_two_way.html#determining-the-best-fitting-model-using-aic",
    "title": "\n20  Two-Way ANOVA\n",
    "section": "\n20.5 Determining the Best-Fitting Model Using AIC",
    "text": "20.5 Determining the Best-Fitting Model Using AIC\nThe Akaike information criterion (AIC) is another method that can be used to determine the model that best fits the data. The model that explains the greatest amount of the variation in the data using the fewest possible independent variables is considered the best-fitting model. The lower the AIC value, the more variation is explained by the model.\n\n# creating a list of models to compare and their respective names\nmodel_set &lt;- list(main_effects, interaction, full_model)\nmodel_names &lt;- c(\"main_effects\", \"interaction\", \"full_model\")\n\n# using AICtab to compare models \ngt_fmt &lt;-\n  aictab(model_set, modnames = model_names) \n\n# create gt table using AIC\ngt_print &lt;- \n  gt(gt_fmt)\n\ngt_print\n\n\n\n\n\n\nModnames\nK\nAICc\nDelta_AICc\nModelLik\nAICcWt\nLL\nCum.Wt\n\n\n\nmain_effects\n5\n173.8562\n0.000000\n1.00000000\n0.81041300\n-81.59474\n0.8104130\n\n\ninteraction\n7\n177.1178\n3.261693\n0.19576377\n0.15864950\n-80.92256\n0.9690625\n\n\nfull_model\n9\n180.3873\n6.531150\n0.03817497\n0.03093749\n-80.14714\n1.0000000\n\n\n\n\n\n\n\nAs shown in this table, the main_effects model, which only includes the fertilizer and density variables without their interaction term, has the lowest AIC and is therefore the best fit for our crop data analysis.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Two-Way ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_two_way.html#post-hoc-testing-tukey-hsd",
    "href": "lessons/04_anova_two_way.html#post-hoc-testing-tukey-hsd",
    "title": "\n20  Two-Way ANOVA\n",
    "section": "\n20.6 Post-Hoc Testing (Tukey HSD)",
    "text": "20.6 Post-Hoc Testing (Tukey HSD)\nWe now know which parameters are significant, however, we are also interested in learning how the levels of the factors differ from each other. To quantify these differences, the Tukey’s Honestly-Significant-Difference test can be used.\n\n# Performing Tukey HSD on the final model (yield ~ fertilizer + density)\ntukey_crop &lt;- TukeyHSD(main_effects)  \n  \ntukey_crop %&gt;% \n# turn model into data.frame of parameters\n  tidy %&gt;% \n# create gt table\n  gt() %&gt;% \n  tab_header(\n      title = \"Tukey Multiple Comparisons of Means\",\n      subtitle = \"for main effects model (y ~ fertilizer + density)\"\n  )\n\n\n\n\n\n\n\nTukey Multiple Comparisons of Means\n\n\nfor main effects model (y ~ fertilizer + density)\n\n\nterm\ncontrast\nnull.value\nestimate\nconf.low\nconf.high\nadj.p.value\n\n\n\n\nfertilizer\n2-1\n0\n0.1761687\n-0.16822506\n0.5205625\n0.4452958212\n\n\nfertilizer\n3-1\n0\n0.5991256\n0.25473179\n0.9435194\n0.0002218678\n\n\nfertilizer\n3-2\n0\n0.4229569\n0.07856306\n0.7673506\n0.0119381379\n\n\ndensity\n2-1\n0\n0.4619560\n0.22752045\n0.6963916\n0.0001741423\n\n\n\n\n\n\n\nThis table shows the pairwise differences between each level of the factors. Comparisons with p values less than 0.05 are considered significant:\n\nfertilizer type 1 vs 3\nfertilizer type 2 vs 3\nlow vs high density\n\nTo visualize the differences between the levels of each factor, the 95% family-wise confidence intervals of the pairs for fertilizer and density were plotted below. The x-axes of the plots display the difference in means between the paired levels. The y-axis denotes the pair being compared.\n\n# plot tukey confidence intervals and set tick marks to horizontal (las = 1)\n#   position\nplot(tukey_crop, las = 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the significant confidence intervals do not include zero. From this plot, we see that only the fertilizer comparison of type 1 and 2 confidence interval includes 0. Thus, there is no statistically significant difference in the average crop yield produced by fertilizer 1 vs fertilizer 2. 95% family-wise confidence intervals for all other comparisons are statistically significant at the α = 0.05 level.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Two-Way ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_two_way.html#anova-vs.-linear-regression",
    "href": "lessons/04_anova_two_way.html#anova-vs.-linear-regression",
    "title": "\n20  Two-Way ANOVA\n",
    "section": "\n20.7 ANOVA vs. Linear Regression",
    "text": "20.7 ANOVA vs. Linear Regression\nThe model underlying the ANOVA we just performed is actually a linear regression. Let’s see what results we would get if we used linear regression to model the same relationships:\n\n# create linear regression model of yield ~ fertilizer + density\ncrop_lm &lt;- lm(yield ~ fertilizer + density, data = crop_data_df)\n\nsummary(crop_lm)\n\n\nCall:\nlm(formula = yield ~ fertilizer + density, data = crop_data_df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.16523 -0.30208 -0.05802  0.42576  1.47375 \n\nCoefficients:\n            Estimate Std. Error  t value Pr(&gt;|t|)    \n(Intercept) 176.5261     0.1180 1495.490  &lt; 2e-16 ***\nfertilizer2   0.1762     0.1446    1.219 0.226115    \nfertilizer3   0.5991     0.1446    4.144 7.57e-05 ***\ndensity2      0.4620     0.1180    3.914 0.000174 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5783 on 92 degrees of freedom\nMultiple R-squared:  0.2667,    Adjusted R-squared:  0.2428 \nF-statistic: 11.15 on 3 and 92 DF,  p-value: 2.601e-06\n\n\nThe output from the linear regression model indicate a significant difference in average yield exists between crops fertilized with fertilizer 3 and 1 (p = 7.57 e-05) but not fertilizers 1 and 2 (p = 0.22). Furthermore, the average yield for high density acres is significantly different than low density acres (p &lt; 0.001). These findings are consistent with the results of the Tukey’s HSD tests.\nRecall that the two-way ANOVA compares the variance within all levels of a factor to the total variation of the model. Similarly, the linear regression quantifies the variation of each group (or “level”) from the mean using dummy variables. Both use the same computations and thus the sum of squares for the independent variables are the same in the outputs of both the two-way ANOVA and linear regression. Because the density variable consists of two groups, its p value (p = 0.00017) is also the same in both models. The p value(s) for fertilizer differs between the two, however, because the three fertilizer groups are split into three dummy variables in the linear regression and compared against the group mean individually. The two-way ANOVA, on the other hand, is only concerned with the total variance within the fertilizer factor. Read “Common statistical tests are linear models” to learn more about how ANOVAs and many other statistical tests are simply different flavors of linear regression.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Two-Way ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_two_way.html#conclusions",
    "href": "lessons/04_anova_two_way.html#conclusions",
    "title": "\n20  Two-Way ANOVA\n",
    "section": "\n20.8 Conclusions",
    "text": "20.8 Conclusions\nThere is a statistically significant difference in average crop yield by both the fertilizer type and planting density variables with F values of 9.018 (p &lt; 0.001) and 15.316 (p &lt; 0.001) respectively. The interaction between these two terms was not significant.\nThe Tukey post-hoc test showed significant pairwise differences in average yield between fertilizer types 1 and 3 and between type 2 and 3. It also depicted significant differences in average yield between low and high planting density. Therefore, corn fertilized with fertilizer type 3 produced significantly more bushels per acre than those with fertilizers type 1 and 2, and acres of corn planted at high density produced more bushels than those planted at low density.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Two-Way ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_anova_kruskal_wallis.html",
    "href": "lessons_original/04_anova_kruskal_wallis.html",
    "title": "21  Non-Parametric ANOVA: The Kruskal-Wallis Test",
    "section": "",
    "text": "21.1 Introduction\nThe Kruskal-Wallis test (H-test) is a hypothesis test for multiple independent samples, which is used when the assumptions for a one factor analysis of variance are violated. In other word, it is the non-parametric alternative to the One Way ANOVA. Non-parametric means that the data does not follow normal distribution. It is sometimes called the one-way ANOVA on ranks, as the ranks of the data values are used in the test rather than the actual data points.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Non-Parametric ANOVA: The Kruskal-Wallis Test</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_anova_kruskal_wallis.html#introduction",
    "href": "lessons_original/04_anova_kruskal_wallis.html#introduction",
    "title": "21  Non-Parametric ANOVA: The Kruskal-Wallis Test",
    "section": "",
    "text": "21.1.1 Mathematical Equation\n\\[H = \\frac{{n-1}}{{n}}\\sum^k_{i=1}\\frac{n_i({\\bar{R_i} - E})^2}{{σ^2_R}}\\]\nWhere:\n\n\\(H\\) is the Kruskal-Wallis test statistic,\n\\(n\\) is the total number of observations,\n\\(R_i\\) is the sum of ranks for each group,\n\\(E\\) is the expected value of the sum of ranks under the null hypothesis.\n\\(σ^2_R\\) is the square of standard deviation of Rank sum.\n\nEquation for Expected Rank\n\\[E = \\frac{{n+1}}{{2}}\\]\nWhere:\n-n represents total number of observations.\nEquation for Rank Mean for group i\n\\[R_i = \\frac{{\\sum{R}}}{{n_g}}\\]\nWhere:\n\nR_i represents mean rank for \\(i^{th}\\) group,\n\\(\\sum{R}\\) represents sum of ranks in \\(i^{th}\\) group,\n\\(n_g\\) represents number of observation in \\(i^{th}\\) group.\n\nExample\n\n\n\nAssigning ranks/ E and mean rank calculated/ ready for H calculation\n\n\n\n\n21.1.2 Assumptions\n1. Ordinal or Continuous Response Variable – the response variable should be an ordinal or continuous variable.\n2. Independence – the observations in each group need to be independent of each other.\n3. Sample Size and distribution – each group must have a sample size of 5 or more and the distributions in each group need to have a similar shape but groups does not follow normal distribution.\n\n\n21.1.3 Hypothesis\nThe test determines whether two or more independent groups have same central tendency.\n\nH0: population rank sum average are equal for independent group and therefore come from same population.\nH1: population rank sum average are significantly different for at-least two or more independent group and therefore come from different population.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Non-Parametric ANOVA: The Kruskal-Wallis Test</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_anova_kruskal_wallis.html#running-kruskal-wallis-in-r",
    "href": "lessons_original/04_anova_kruskal_wallis.html#running-kruskal-wallis-in-r",
    "title": "21  Non-Parametric ANOVA: The Kruskal-Wallis Test",
    "section": "21.2 Running Kruskal-Wallis in R",
    "text": "21.2 Running Kruskal-Wallis in R\n\n21.2.1 Packages\n\n# install.packages(\"FSA\") # Houses dunnTest for pair wise comparison\n# install.packages(\"ggpubr\")  # For density plot and for creating and customizing 'ggplot2'- based publication ready plots\n# install.packages(\"ggstatplot\") # Houses gbetweenstats() function that allows building a combination of box and violin plots along with                                        statistical details.\n# install.packages(\"tidyverse\") # For wrangling and tidying the data\n# install.packages(\"MultNonParam\")\n\nlibrary(MultNonParam)\n\nLoading required package: ICSNP\n\n\nLoading required package: mvtnorm\n\n\nLoading required package: ICS\n\nsuppressPackageStartupMessages(library(ggpubr))   \nsuppressPackageStartupMessages(library(ggstatsplot))\nsuppressPackageStartupMessages(library(FSA))       \nsuppressPackageStartupMessages(library(gtsummary))\nsuppressPackageStartupMessages(library(tidyr))     \nsuppressPackageStartupMessages(library(tidyverse)) \n\n\n\n21.2.2 Data\nAs an example we will manually create a data, details of which can be found Here.\nThe data represents antibody production after receiving a vaccine. A hospital administered one of three different vaccines - A, B, or C to 6 individuals per group and measured the antibody presence (\\(\\mu\\)g/mL) in their blood after a chosen time period. The data is as follows: The goal of this exercise will be to determine how the three vaccines performed compared to each other. Essentially, we are looking to determine if the antibody data for each vaccine originates from the same distribution. The sample size is small and normal distribution cannot be assumed. Therefore, we will be conducting the Kruskal-Wallis test.\nNull Hypothesis (H0): The vaccines induce equal amounts of antibody production. (all three groups originate from the same distribution and have the same median)\nAlternative Hypothesis (H1): At least one vaccine induces different amount of antibodies to be produced.(at least one group originates from a different distribution and has a different median)\n\n# Creating dataframe for antibodies produced (in $\\mu$g/mL$) by three different vaccines;\n\nA &lt;- c(1232, 751, 339, 848, 447, 542)\nB &lt;- c(302, 57, 521, 278, 176, 201)\nC &lt;- c(839, 342, 473, 1128, 242, 475)\n\ndf &lt;- data.frame(A, B, C)\n\ndf_tidy &lt;- pivot_longer(\n  data = df,\n  cols = c(\"A\", \"B\", \"C\"),\n  names_to = \"Vaccines\",\n  values_to = \"Antibody\"\n)\n\ndf_tidy_sorted &lt;- \n  df_tidy %&gt;% \n  arrange(Vaccines)\n\ndf_tidy_sorted\n\n# A tibble: 18 × 2\n   Vaccines Antibody\n   &lt;chr&gt;       &lt;dbl&gt;\n 1 A            1232\n 2 A             751\n 3 A             339\n 4 A             848\n 5 A             447\n 6 A             542\n 7 B             302\n 8 B              57\n 9 B             521\n10 B             278\n11 B             176\n12 B             201\n13 C             839\n14 C             342\n15 C             473\n16 C            1128\n17 C             242\n18 C             475\n\nvaccine_efficacy = df_tidy_sorted\n\n\nstr(vaccine_efficacy)\n\ntibble [18 × 2] (S3: tbl_df/tbl/data.frame)\n $ Vaccines: chr [1:18] \"A\" \"A\" \"A\" \"A\" ...\n $ Antibody: num [1:18] 1232 751 339 848 447 ...\n\n\n\nvaccine_efficacy$Vaccines &lt;- as_factor(vaccine_efficacy$Vaccines)\n\nstr(vaccine_efficacy)\n\ntibble [18 × 2] (S3: tbl_df/tbl/data.frame)\n $ Vaccines: Factor w/ 3 levels \"A\",\"B\",\"C\": 1 1 1 1 1 1 2 2 2 2 ...\n $ Antibody: num [1:18] 1232 751 339 848 447 ...\n\n\n\n\n21.2.3 Computing summary statistics by group\nThe first step is to inspect the data and calculate a summary of statistics. This can be done by using the summarise function.\n\ngroup_by(vaccine_efficacy, Vaccines) %&gt;%\n  summarise(\n    count = n(),\n    mean = mean(Antibody, na.rm = TRUE),\n    sd = sd(Antibody, na.rm = TRUE),\n    median = median(Antibody, na.rm = TRUE),\n    IQR = IQR(Antibody, na.rm = TRUE)\n  )\n\n# A tibble: 3 × 6\n  Vaccines count  mean    sd median   IQR\n  &lt;fct&gt;    &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 A            6  693.  325.   646.  353 \n2 B            6  256.  156.   240.  114.\n3 C            6  583.  335.   474   373.\n\n\n\n\n21.2.4 Box Plot\nThe next step will be to visualize the dataset using a box plot. This will allow us to estimate differences in distribution.\n\nvaccine_efficacy %&gt;% \n  ggplot(aes(Vaccines, Antibody)) + \n  geom_boxplot() +\n  ggtitle(\"Vaccine Efficacy\") +\n  xlab(\"Vaccines\") + ylab(\"Antibodies\")\n\n\n\n\n\n\n\n\nBased on the box plot, we see that there is similarity in distribution of A and C while B looks to be different. We can also add the individual data points and connect the boxes to visually see the density distribution and compare with normal distribution for each vaccines.\n\n21.2.4.1 Adding error bars: mean_se\n\nggline(vaccine_efficacy, x = \"Vaccines\", y = \"Antibody\",\n       add = c(\"mean_se\", \"jitter\"),\n       order = c(\"A\", \"B\", \"C\"),\n       ylab = \"Antibody\", xlab = \"Vaccines\")\n\n\n\n\n\n\n\n\n\n\n21.2.4.2 Density plot with overlaid normal plot\nNext, we want to create a density plot to further visualize the data and compare it to what a normal distribution of these data should look like. This can be done by using the ggdensity function as seen below.\n\n# Plot the distribution of the antibodies for vaccine \nggdensity(df$A, fill = \"lightgray\", title = \"Distribution plot of Antibody Production for Vaccine A\") +\n  scale_x_continuous() +\n  xlab(\"A\") +\n  stat_overlay_normal_density(color = \"red\", linetype = \"dashed\")\n\n\n\n\n\n\n\n\n\n# Plot the distribution of the antibodies for vaccine \nggdensity(df$B, fill = \"lightgray\", title = \"Distribution plot of Antibody Production for Vaccine B\") +\n  scale_x_continuous() +\n  xlab(\"B\") +\n  stat_overlay_normal_density(color = \"red\", linetype = \"dashed\")\n\n\n\n\n\n\n\n\n\n# Plot the distribution of the antibodies for vaccine \nggdensity(df$C, fill = \"lightgray\", title = \"Distribution plot of Antibody Production for Vaccine C\") +\n  scale_x_continuous() +\n  xlab(\"C\") +\n  stat_overlay_normal_density(color = \"red\", linetype = \"dashed\")\n\n\n\n\n\n\n\n\nFrom these density plots, we see that our data is not normally distributed and distribution shape for two vaccines looks similar while one vaccine deviates. As our data is not normally distributed and has small sample size, we will now perform Kruskal-Wallis test to find out whether there are any significant differences between the three vaccines in terms of their efficacy (antibodies production in the body).\n\n\n\n21.2.5 Kruskal-Wallis Test\nThe Kruskal-Wallis test can be done in R using the kruskal.test function as seen below.\n\nresult &lt;- kruskal.test(Antibody ~ Vaccines, data = vaccine_efficacy)\n\nprint(result)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  Antibody by Vaccines\nKruskal-Wallis chi-squared = 7.2982, df = 2, p-value = 0.02601\n\n\n\n\n21.2.6 Tabulating the result\n\ntable1 &lt;-   \n  tbl_summary(\n    vaccine_efficacy,\n    by = Vaccines,\n) %&gt;% \n  add_p() %&gt;%\n  modify_caption(\"Antibody Production of Different Vaccines\") %&gt;%\n  bold_labels()\n\n\ntable1\n\n\n\n\n\n\nAntibody Production of Different Vaccines\n\n\nCharacteristic\nA, N = 61\nB, N = 61\nC, N = 61\np-value2\n\n\n\n\nAntibody\n647 (471, 824)\n240 (182, 296)\n474 (375, 748)\n0.026\n\n\n\n1 Median (IQR)\n\n\n2 Kruskal-Wallis rank sum test\n\n\n\n\n\n\n\n\n\n\n\n21.2.7 Interpretation\nFrom the Kruskal-Wallis test, we get that our test statistic is 26.63 with p-value 0.026, which is smaller than our level of significance 0.05. This gives us enough evidence to reject the null hypothesis. Therefore, we conclude that there is a significant difference in the efficacy of at least two of the three vaccines.\n\n21.2.7.1 Post-hoc-Test\nThe Kruskal-Wallis test helps to determine whether at least two groups differ from each other but it does not specify where in which groups the significance lies. We need to conduct a post-hoc test for this. For this purpose, the Dunn test is the appropriate nonparametric test for the pairwise multiple comparison. We will use Holm adjustment method for multiple comparison. You can read about various adjustment methods for multiple comparison herechen2017?\n\npair_wise_compare &lt;- dunnTest(Antibody~Vaccines, \n                              data = vaccine_efficacy,\n                              method = \"holm\"\n  \n)\n\npair_wise_compare\n\nDunn (1964) Kruskal-Wallis multiple comparison\n\n\n  p-values adjusted with the Holm method.\n\n\n  Comparison          Z     P.unadj     P.adj\n1      A - B  2.5955427 0.009444166 0.0283325\n2      A - C  0.6488857 0.516412268 0.5164123\n3      B - C -1.9466571 0.051575864 0.1031517\n\n\nWhen looking at the adjusted p-values in the last column for each pairwise comparison, we can see that only the A-B vaccine comparison has a p-value that is less than our level of significance of 0.05. Therefore, we conclude that there is significant difference in vaccine A-B while there is no significant difference between vaccines A-C, and B-C.\n\n\n\n21.2.8 Alternative method\nA very good alternative for performing a Kruskal-Wallis and the post-hoc tests in R is with the ggbetweenstats() function from the {ggstatsplot} package: It provides a combination of box and violin plots along with jittered data points for between-subjects designs with statistical details included in the plot as a subtitle.\n\nggbetweenstats(\n  data = vaccine_efficacy,\n  x = Vaccines,\n  y = Antibody,\n  type = \"nonparametric\", # ANOVA or Kruskal-Wallis\n  plot.type = \"box\",\n  pairwise.comparisons = TRUE,\n  pairwise.display = \"significant\",\n  centrality.type = \"nonparametric\", # It displays median for non parametric data by default.\n  bf.message = FALSE # Logical that decides whether to display Bayes Factor in favor of the null hypothesis. This argument is relevant only for parametric test\n)\n\n\n\n\n\n\n\n\nThis method has the advantage that all necessary statistical results are displayed directly on the plot. It also provides a more efficient and concise code.\nThe results of the Kruskal-Wallis test are shown in the subtitle above the plot (the p-value is after p =). Moreover, the results of the post-hoc test are displayed between each group via accolades, and the boxplots allow to visualize the distribution for each species.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Non-Parametric ANOVA: The Kruskal-Wallis Test</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_anova_kruskal_wallis.html#conclusion",
    "href": "lessons_original/04_anova_kruskal_wallis.html#conclusion",
    "title": "21  Non-Parametric ANOVA: The Kruskal-Wallis Test",
    "section": "21.3 Conclusion",
    "text": "21.3 Conclusion\nIn conclusion, the Kruskal-Wallace test is a non-parametric hypothesis test that can be used to determine if there are significant differences between two or more groups using the ranks of the data values. The first step involves visualizing the data to confirm it violates the rules of normality. Next, you conduct the Kruskal-Wallis test to determine if there are significant differences. Finally, you run a post-hoc test to calculate pairwise comparisons and determine which specific groups are significantly different.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Non-Parametric ANOVA: The Kruskal-Wallis Test</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_anova_kruskal_wallis.html#references",
    "href": "lessons_original/04_anova_kruskal_wallis.html#references",
    "title": "21  Non-Parametric ANOVA: The Kruskal-Wallis Test",
    "section": "References",
    "text": "References",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Non-Parametric ANOVA: The Kruskal-Wallis Test</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_random_intercept.html",
    "href": "lessons/04_anova_random_intercept.html",
    "title": "\n22  Repeated Measures ANOVA\n",
    "section": "",
    "text": "22.1 Introduction\nRepeated measures ANOVA is used when you have the same measure that participants were rated on at more than two time points. With only two time points a paired \\(t\\)-test will be sufficient, but for more times a repeated measures ANOVA is required. (2013-) There are many complex designs that can make use of repeated measures, but throughout this guide, we will be referring to the most simple case, that of a one-way repeated measures ANOVA. This particular test requires one independent variable and one dependent variable. The dependent variable needs to be continuous (interval or ratio) and the independent variable categorical (either nominal or ordinal). (2018-)",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Repeated Measures ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_random_intercept.html#neccessary-packages",
    "href": "lessons/04_anova_random_intercept.html#neccessary-packages",
    "title": "\n22  Repeated Measures ANOVA\n",
    "section": "\n22.2 Neccessary packages",
    "text": "22.2 Neccessary packages\nMake sure that you have installed the following R packages:\n\n\ntidyverse for data manipulation and visualization.\n\nggpubr for creating easily publication ready plots.\n\nrstatix provides pipe-friendly R functions for easy statistical analyses.(2018-)\n\n\ndatarium contains required data sets for this chapter.\n\nStart by loading the following R packages\n\n# Install packages first and then load the libraries. \n# install.packages(\"datarium\")\n\nlibrary(tidyverse)\nlibrary(ggpubr)\nlibrary(rstatix)",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Repeated Measures ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_random_intercept.html#data-source-and-description",
    "href": "lessons/04_anova_random_intercept.html#data-source-and-description",
    "title": "\n22  Repeated Measures ANOVA\n",
    "section": "\n22.3 Data source and description",
    "text": "22.3 Data source and description\nFor this example we will be using this dataset from the datarium package that contains 10 individuals’ self-esteem score on three time points during a specific diet to determine whether their self-esteem improved.\nOne-way repeated measures ANOVA can be performed in order to determine the effect of time on the self-esteem score.\n\n# Data preparation; wide format\ndata(\"selfesteem\", package = \"datarium\")\nselfesteem\n\n# A tibble: 10 × 4\n      id    t1    t2    t3\n   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     1  4.01  5.18  7.11\n 2     2  2.56  6.91  6.31\n 3     3  3.24  4.44  9.78\n 4     4  3.42  4.71  8.35\n 5     5  2.87  3.91  6.46\n 6     6  2.05  5.34  6.65\n 7     7  3.53  5.58  6.84\n 8     8  3.18  4.37  7.82\n 9     9  3.51  4.40  8.47\n10    10  3.04  4.49  8.58\n\n\nNow we “gather” columns t1, t2, and t3 into “long” format, then convert id and time into factor variables.\n\nselfesteem_df &lt;- \n  selfesteem %&gt;%\n  gather(key = \"time\", value = \"score\", t1, t2, t3) %&gt;%\n  convert_as_factor(id, time)\n\nselfesteem_df\n\n# A tibble: 30 × 3\n   id    time  score\n   &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt;\n 1 1     t1     4.01\n 2 2     t1     2.56\n 3 3     t1     3.24\n 4 4     t1     3.42\n 5 5     t1     2.87\n 6 6     t1     2.05\n 7 7     t1     3.53\n 8 8     t1     3.18\n 9 9     t1     3.51\n10 10    t1     3.04\n# ℹ 20 more rows\n\n\nThe one-way repeated measures ANOVA can be used to determine whether the means self-esteem scores are significantly different between the three time points.\nNote: Whilst the repeated measures ANOVA is used when you have just “one” independent variable, if you have “two” independent variables (e.g., you measured time and condition), you will need to use a two-way repeated measures ANOVA. Two and Three-way Repeated Measures ANOVA examples with this data can be found here.\n\n22.3.1 Summary statistics\nCompute some summary statistics of the self-esteem score by groups (time): mean and sd (standard deviation)\n\n# Statistics-summary\nselfesteem_df %&gt;%\n  group_by(time) %&gt;%\n  get_summary_stats(score, type = \"mean_sd\")\n\n# A tibble: 3 × 5\n  time  variable     n  mean    sd\n  &lt;fct&gt; &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 t1    score       10  3.14 0.552\n2 t2    score       10  4.93 0.863\n3 t3    score       10  7.64 1.14 \n\n\n\n22.3.2 Visualization\nCreate a box plot and add points corresponding to individual values:\n\nbxp &lt;- ggboxplot(selfesteem_df, x = \"time\", y = \"score\", add = \"point\")\nbxp\n\n\n\nVisualization of DATA",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Repeated Measures ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_random_intercept.html#test-assumptions",
    "href": "lessons/04_anova_random_intercept.html#test-assumptions",
    "title": "\n22  Repeated Measures ANOVA\n",
    "section": "\n22.4 Test Assumptions",
    "text": "22.4 Test Assumptions\nBefore computing repeated measures ANOVA test, you need to perform some preliminary tests to check if the assumptions are met.\n\n22.4.1 Outiliers\nOutliers can be easily identified using box plot methods, implemented in the R function identify_outliers() inside the rstatix package.\n\nselfesteem_df %&gt;%\n  group_by(time) %&gt;%\n  identify_outliers(score)\n\n# A tibble: 2 × 5\n  time  id    score is.outlier is.extreme\n  &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;lgl&gt;      &lt;lgl&gt;     \n1 t1    6      2.05 TRUE       FALSE     \n2 t2    2      6.91 TRUE       FALSE     \n\n\nThere were no extreme outliers. In the situation where we have extreme outliers, we can include the outlier in the analysis anyway if we do not believe the result will be substantially affected. This can be evaluated by comparing the result of the ANOVA with and without the outlier. It’s also possible to keep the outliers in the data and perform robust ANOVA test using the WRS2 package. WRS2 Package\n\n22.4.2 Normality Assumption\nThe outcome (or dependent) variable should be approximately normally distributed in each cell of the design. This can be checked using the Shapiro-Wilk normality test (shapiro_test() in rstatix package) or by visual inspection using QQ plot (ggqqplot() in the ggpubr package). If the data is normally distributed, the \\(p\\)-value should be greater than 0.05.\n\nselfesteem_df %&gt;%\n  group_by(time) %&gt;%\n  shapiro_test(score)\n\n# A tibble: 3 × 4\n  time  variable statistic     p\n  &lt;fct&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;\n1 t1    score        0.967 0.859\n2 t2    score        0.876 0.117\n3 t3    score        0.923 0.380\n\n\nThe self-esteem score was normally distributed at each time point, as assessed by Shapiro-Wilk’s test (\\(p &gt; 0.05\\)).\nNote that, if your sample size is greater than 50, the normal QQ plot is preferred because at larger sample sizes the Shapiro-Wilk test becomes very sensitive even to a minor deviation from normality. QQ plot draws the correlation between a given data and the normal distribution. Create QQ plots for each time point:\n\nggqqplot(selfesteem_df, \"score\", facet.by = \"time\")\n\n\n\nQQ Plot\n\n\n\nFrom the plot above, as all the points fall approximately along the reference line, we can assume normality.\n\n22.4.3 Assumption of Sphericity\nThe variance of the differences between groups should be equal. This can be checked using the Mauchly’s test of sphericity. This assumption will be automatically checked during the computation of the ANOVA test using the R function anova_test() in rstatix package. The Mauchly’s test is internally used to assess the sphericity assumption. Click HERE to know more about the Assumption of Sphericity and the Mauchly’s Test and to understand why is important.\nBy using the function get_anova_table() to extract the ANOVA table, the Greenhouse-Geisser sphericity correction is automatically applied to factors violating the sphericity assumption.\n\nres.aov &lt;- anova_test(\n  data = selfesteem_df, \n  # Selfesteem variable\n  dv = score,\n  # Sample individuals\n  wid = id, \n  # Independent variable time \n  within = time\n)\n\n# Get table\nget_anova_table(res.aov)\n\nANOVA Table (type III tests)\n\n  Effect DFn DFd      F        p p&lt;.05   ges\n1   time   2  18 55.469 2.01e-08     * 0.829\n\n\nThe self-esteem score was statistically significantly different at the different time points during the diet, \\(F_{(2, 18)} = 55.5\\), \\(p &lt; 0.0001\\), \\(\\eta^2_g = 0.83\\). where,\n\n\nF Indicates that we are comparing to an \\(F\\)-distribution (\\(F\\)-test),\n\n(2, 18) indicates the degrees of freedom in the numerator (DFn) and the denominator (DFd), respectively,\n\n55.5 indicates the obtained \\(F\\)-statistic value;\n\np specifies the \\(p\\)-value, and\n\n\\(\\eta^2_g\\) is the generalized effect size (amount of variability due to the within-subjects factor).\n\n22.4.4 Post-hoc test\nYou can perform multiple pairwise paired \\(t\\)-tests between the levels of the within-subjects factor (here time). We adjust \\(p\\)-values using the Bonferroni multiple testing correction method.\n\n# pairwise comparisons\npwc &lt;- pairwise_t_test(\n  data = selfesteem_df,\n  formula = score ~ time,\n  paired = TRUE,\n  p.adjust.method = \"bonferroni\"\n)\n\npwc\n\n# A tibble: 3 × 10\n  .y.   group1 group2    n1    n2 statistic    df           p p.adj p.adj.signif\n* &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;       \n1 score t1     t2        10    10     -4.97     9 0.000772     2e-3 **          \n2 score t1     t3        10    10    -13.2      9 0.000000334  1e-6 ****        \n3 score t2     t3        10    10     -4.87     9 0.000886     3e-3 **          \n\n\nAll the pairwise differences are statistically significant.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Repeated Measures ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_random_intercept.html#results",
    "href": "lessons/04_anova_random_intercept.html#results",
    "title": "\n22  Repeated Measures ANOVA\n",
    "section": "\n22.5 Results",
    "text": "22.5 Results\nWe could report the results of the post-hoc test as follows: post-hoc analyses with a Bonferroni adjustment revealed that all the pairwise differences, between time points, were statistically significantly different (\\(p &lt; 0.05\\)).\n\npwc &lt;- pwc %&gt;% add_xy_position(x = \"time\")\n\nbxp + \n  stat_pvalue_manual(pwc) +\n  labs(\n    subtitle = get_test_label(res.aov, detailed = TRUE),\n    caption = get_pwc_label(pwc)\n  )\n\n\n\nVisualization With Results",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Repeated Measures ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_anova_random_intercept.html#conclusion",
    "href": "lessons/04_anova_random_intercept.html#conclusion",
    "title": "\n22  Repeated Measures ANOVA\n",
    "section": "\n22.6 Conclusion",
    "text": "22.6 Conclusion\nThis chapter describes how to compute, interpret and report repeated measures ANOVA in R, specifically one-way repeated measures ANOVA. We also explain the assumptions made by one-way repeated measures ANOVA tests and provide practical examples of R codes to check whether the test assumptions are met.\n\n\n\n\n“ANOVA for Repeated Measures Designs Related Designs (Rationale for One-Way Repeated Measures ANOVA); Between Subjects and Between Conditions Variation; Data Assumptions for Repeated Measures ANOVA; Two-Way Related Design; ANOVA Mixed Design  One Repeated Measure and One Unrelated Factor ; More Complex ANOVA Designs; Effect Size and Power; A Non- Parametric Equivalent  the Friedman Test for Correlated Samples; SPSS Procedures for Repeated Measures ANOVA and Friedman.” 2013. In, 528–49. Routledge. https://doi.org/10.4324/9780203769669-23.\n\n\n“ANOVA Repeated Measures.” 2018. In, 222–50. SAGE Publications, Inc. https://doi.org/10.4135/9781071802625.n9.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Repeated Measures ANOVA</span>"
    ]
  },
  {
    "objectID": "lessons/04_corr_cov.html",
    "href": "lessons/04_corr_cov.html",
    "title": "\n23  Correlation and Covariance Matrices\n",
    "section": "",
    "text": "23.1 Introduction to Correlation and Covariance Matrices\nA prominent theme in statistical analysis is evaluating whether or not a relationship exists between two or more variables, and the degree to which that relationship exists.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Correlation and Covariance Matrices</span>"
    ]
  },
  {
    "objectID": "lessons/04_corr_cov.html#introduction-to-correlation-and-covariance-matrices",
    "href": "lessons/04_corr_cov.html#introduction-to-correlation-and-covariance-matrices",
    "title": "\n23  Correlation and Covariance Matrices\n",
    "section": "",
    "text": "Covariance measures the degree to which the deviation of one variable (\\(X\\)) from its mean changes in relation to the deviation of another variable (\\(Y\\)) from its mean. In other words, covariance measures the joint variability of two random variables or how these increase or decrease in relation with each other.\n\nFor instance, if greater values of one variable tend to correspond with greater values of another variable, this suggests a positive covariance.\nCovariance can have both positive and negative values.\nA covariance of zero indicates that the variables are independent of each other, meaning that there is no linear relationship between them.\n\n\nA Covariance Matrix shows the covariance between different variables of a data set. Covariance matrices are helpful for:\n\nCharacterizing multivariate normal distributions.\nDimensionality reduction techniques, such as Principal Component Analysis, where the matrix is used to calculate the principal components (i.e., linear combinations of the original variables that capture the maximum variance in the data).\nMachine learning models, like Gaussian mixture models, where they are used to estimate the parameters of the model.\n\n\n\nCorrelation tells us both the strength and the direction of the relationship between two variables by listing the Correlation Coefficient or \\(r\\) (“Pearson”, “Spearman”, or “Kendall”) for the pair as measure of association.\n\nCorrelation differs from covariance in that it is standardized between -1 and 1, making it easier to interpret.\nA correlation of -1 indicates a perfect negative linear relationship between two variables, and a correlation of 1 indicates a perfect positive linear relationship between two variables.\nA correlation of 0 indicates no correlation.\nThe magnitude of the correlation coefficient indicates the strength of the association:\n\n.1 &lt;  \\(r\\) &lt; .3 (small / weak correlation).\n.3 &lt;  \\(r\\) &lt; .5 (medium / moderate correlation).\n.5 &lt;  \\(r\\) (large / strong correlation).\n\n\nCorrelation is helpful to test for multicollinarity in regression models.\n\n\nA Correlation matrix allows for the exploration of the correlation among the multiple variables in a data set simultaneously. It provides this information through a table listing the correlation coefficients (\\(r\\)) for each pair of variables in the data set.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Correlation and Covariance Matrices</span>"
    ]
  },
  {
    "objectID": "lessons/04_corr_cov.html#mathematical-definition",
    "href": "lessons/04_corr_cov.html#mathematical-definition",
    "title": "\n23  Correlation and Covariance Matrices\n",
    "section": "\n23.2 Mathematical Definition",
    "text": "23.2 Mathematical Definition\n\n23.2.1 Variance\nThe equation for the variance of a single variable is as follows: \\[\nvar(x) = \\frac{1}{n-1}{\\sum_{i = 1}^{n}{(x_i - \\bar{x})^2}}\n\\]\n\n23.2.2 Covariance\nFor a sample of \\(n\\) data points, the sample covariance is calculated as: \\[\nCov(x,y) = \\frac{1}{n-1}{\\sum_{i = 1}^{n}{(x_i - \\bar{x})(y_i-\\bar{y})}}\n\\] Where,\n\n\n\\(x_i\\) denotes all the possible values of x.\n\n\\(y_i\\) denotes all the possible values of y.\n\n\\(\\bar{x}\\) denotes the mean of variable x.\n\n\\(\\bar{y}\\) denotes the mean of variable y.\n\n\\(n\\) denotes the sample size.\n\n\n23.2.2.1 Covariance Matrix\nThe covariance matrix, \\(\\sum\\), can be represented as: \\[\n\\sum = \\begin{bmatrix} Var(x_{1}) & ... & Cov(x_{1},x_{n})\\\\ : &: & :\\\\ :& :& :\\\\ Cov(x_{n},x_{1}) & ... & Var(x_{n}) \\end{bmatrix}.\n\\] Where,\n\n\n\\(x_1\\) is the first variable of interest in the data set.\nUp to the last variable of interest in the data set, \\(x_n\\).\n\n23.2.3 Correlation\nHere is a simplified equation for the Pearson Correlation Coefficient, \\(r\\) that allows us to see the relationship between correlation and covariance: \\[\nr = \\frac {Cov(x,y)}{(\\sqrt{var(x)})(\\sqrt{var(y)})}\n\\] This video explains the standardization and mathematical properties of \\(r\\):\n\n\n23.2.3.1 Correlation Matrix\nHere is an example of a correlation matrix:\n\n\n\nVariable 1\nVariable 2\n…\nVariable n\n\n\n\nVariable 1\n1.00\n\\(r_{12}\\)\n…\n\\(r_{1n}\\)\n\n\nVariable 2\n\\(r_{21}\\)\n1.00\n…\n\\(r_{2n}\\)\n\n\n…\n…\n…\n…\n…\n\n\nVariable n\n\\(r_{n1}\\)\n\\(r_{n2}\\)\n…\n1.00\n\n\n\nWhere,\n\nThe diagonal elements are always 1, because it represents the correlation of each variable with itself.\nThe off-diagonal elements represent the correlation coefficients between pairs of variables. For example \\(r_{12}\\) represents the correlation between Variable 1 and Variable 2.\nThe correlation is symmetric, meaning that the correlation between Variable 1 and Variable 2 is the same as the correlation between Variable 2 and Variable 1.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Correlation and Covariance Matrices</span>"
    ]
  },
  {
    "objectID": "lessons/04_corr_cov.html#data-description",
    "href": "lessons/04_corr_cov.html#data-description",
    "title": "\n23  Correlation and Covariance Matrices\n",
    "section": "\n23.3 Data Description",
    "text": "23.3 Data Description\nWe utilized the R data set mtcars to provide examples for developing a Covariance Matrix and checking the assumptions for/development of a Correlation Matrix using the pearson correlation coefficient.\nmtcars is a data set that belongs to base R that contains data about the various models of cars. It contains measurement from 32 different automobiles (1,973,074 models). The variable in the mtcars data set are:\n\n\nmpg: Miles/(US) gallon.\n\ncyl: Number of cylinders.\n\ndisp: Displacement (cu.in.).\n\nhp: Gross horsepower.\n\ndrat: Rear axle ratio.\n\nwt: Weight (1000 lbs).\n\nqsec: 1/4 mile time.\n\nvs: V/S (0 = V-shaped engine, 1 = straight engine).\n\nam: Transmission (0 = automatic, 1 = manual).\n\ngear: Number of forward gears.\n\ncarb: Number of carburetors.\n\n\n# Load data\ndata(\"mtcars\")\n# Print sample\nhead(mtcars) %&gt;% \n  kable(\n    format = \"markdown\",\n    digits = 2,\n    caption = \"The `mtcars` data set\"\n  )\n\n\nThe mtcars data set\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\nMazda RX4\n21.0\n6\n160\n110\n3.90\n2.62\n16.46\n0\n1\n4\n4\n\n\nMazda RX4 Wag\n21.0\n6\n160\n110\n3.90\n2.88\n17.02\n0\n1\n4\n4\n\n\nDatsun 710\n22.8\n4\n108\n93\n3.85\n2.32\n18.61\n1\n1\n4\n1\n\n\nHornet 4 Drive\n21.4\n6\n258\n110\n3.08\n3.21\n19.44\n1\n0\n3\n1\n\n\nHornet Sportabout\n18.7\n8\n360\n175\n3.15\n3.44\n17.02\n0\n0\n3\n2\n\n\nValiant\n18.1\n6\n225\n105\n2.76\n3.46\n20.22\n1\n0\n3\n1",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Correlation and Covariance Matrices</span>"
    ]
  },
  {
    "objectID": "lessons/04_corr_cov.html#assumptions",
    "href": "lessons/04_corr_cov.html#assumptions",
    "title": "\n23  Correlation and Covariance Matrices\n",
    "section": "\n23.4 Assumptions",
    "text": "23.4 Assumptions\n\n23.4.1 Covariance Assumptions\nBecause the covariance measures the degree to which two variables change together, the following assumptions should be met:\n\n\nLinearity: The relationship between the two variables should be linear.\n\nScale: Covariance is sensitive to the units of measurement, so variables should be measured on interval or ratio scale if possible.\n\nAn interval scale means that there are equal intervals between points on the scale (e.g., temperature in Celsius and Fahrenheit).\nA ratio scale is an interval scale with a true/absolute zero point (e.g., time in minutes, height and weight).\n\n\n\nMean centered data: Covariance is based on deviations from the mean, so for accurate calculations the data should be mean centered.\n\nOutliers: There should be no outliers, or outliers should be handled prior to interpreting results.\n\n23.4.2 Correlation Assumptions\nCorrelation examines the strength and direction of a linear relationship between 2 variables. As such, the following assumptions should be met:\n\n\nLinearity: Correlation can underestimate the strength of a relationship if the relationship between variables is non-linear.\n\nScale: Like covariance, correlation assumes interval or ratio scale for valid results.\n\nHomoscedasticity: The range or spread of one variable should be consistent across the range of the second variable.\n\nThis assumption is only relevant for Pearson Correlation. Spearman Rank Correlation and Kendall’s Tau do not assume homoscedasticity.\n\n\n\nNormality: For hypothesis testing, the variables should be approximately normally distributed.\n\nThis assumption is only relevant for Pearson Correlation. Spearman Rank Correlation and Kendall’s Tau do not assume normality.\n\n\n\nIndependence: Each pair of observations/variables should be independent of other pairs (e.g., you should not have repeated measures, clustered data, or time series data).\n\nOutliers: There should be no outliers, or outliers should be handled prior to interpreting results.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Correlation and Covariance Matrices</span>"
    ]
  },
  {
    "objectID": "lessons/04_corr_cov.html#checking-the-assumptions",
    "href": "lessons/04_corr_cov.html#checking-the-assumptions",
    "title": "\n23  Correlation and Covariance Matrices\n",
    "section": "\n23.5 Checking the Assumptions",
    "text": "23.5 Checking the Assumptions\n\n23.5.1 Linearity\nWe can check this assumption with scatterplots of the continuous data.\n\n# The pairs function creates a scatterplot matrix for all continuous variables\npairs(\n  # Notice that all continuous variables come after the tilde (~)\n  ~ mpg + disp + hp + drat + wt + qsec,\n  data = mtcars,\n  main = \"Scatterplot Matrix\"\n)\n\n\n\n\n\n\n\nFrom this visualization, it appears that the continuous variables have a linear relationship (whether negative or positive).\n\n23.5.2 Scale\nUsing the skimr package along with the help (?) function, we can quickly get information about all of the variables in mtcars to understand what kind of scale the variables are on.\n\nskim(mtcars)\n\n\nData summary\n\n\nName\nmtcars\n\n\nNumber of rows\n32\n\n\nNumber of columns\n11\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n11\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nmpg\n0\n1\n20.09\n6.03\n10.40\n15.43\n19.20\n22.80\n33.90\n▃▇▅▁▂\n\n\ncyl\n0\n1\n6.19\n1.79\n4.00\n4.00\n6.00\n8.00\n8.00\n▆▁▃▁▇\n\n\ndisp\n0\n1\n230.72\n123.94\n71.10\n120.83\n196.30\n326.00\n472.00\n▇▃▃▃▂\n\n\nhp\n0\n1\n146.69\n68.56\n52.00\n96.50\n123.00\n180.00\n335.00\n▇▇▆▃▁\n\n\ndrat\n0\n1\n3.60\n0.53\n2.76\n3.08\n3.70\n3.92\n4.93\n▇▃▇▅▁\n\n\nwt\n0\n1\n3.22\n0.98\n1.51\n2.58\n3.33\n3.61\n5.42\n▃▃▇▁▂\n\n\nqsec\n0\n1\n17.85\n1.79\n14.50\n16.89\n17.71\n18.90\n22.90\n▃▇▇▂▁\n\n\nvs\n0\n1\n0.44\n0.50\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▆\n\n\nam\n0\n1\n0.41\n0.50\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▆\n\n\ngear\n0\n1\n3.69\n0.74\n3.00\n3.00\n4.00\n4.00\n5.00\n▇▁▆▁▂\n\n\ncarb\n0\n1\n2.81\n1.62\n1.00\n2.00\n2.00\n4.00\n8.00\n▇▂▅▁▁\n\n\n\n\n# # Checking the help file for mtcars\n# ?mtcars\n\nAs we mentioned previously in the description of the data, only 6 variables (mpg, disp, hp, drat, wt, and qsec) are interval or ratio values.\n\n23.5.3 Outliers\nTo check for outliers in our variables of interest, we can use a box plot to visually examine the data.\n\nbox_mpg &lt;- ggplot(mtcars) +\n  aes(y = mpg) +\n  geom_boxplot()\n\nbox_disp &lt;- ggplot(mtcars) +\n  aes(y = disp) +\n  geom_boxplot()\n\nbox_hp &lt;- ggplot(mtcars) +\n  aes(y = hp) +\n  geom_boxplot()\n\nbox_drat &lt;- ggplot(mtcars) +\n  aes(y = drat) +\n  geom_boxplot()\n\nbox_wt &lt;- ggplot(mtcars) +\n  aes(y = wt) +\n  geom_boxplot()\n\nbox_qsec &lt;- ggplot(mtcars) +\n  aes(y = qsec) +\n  geom_boxplot()\n\n\nbox_mpg + box_disp + box_hp + box_drat + box_wt + box_qsec\n\n\n\n\n\n\nFigure 23.1\n\n\n\n\nmpg, hp, wt, and qsec all appear to have some outliers.\n\n23.5.4 Homoscedasticity\nTo check the homoscedasticity (or homogeneity of variance) assumption for correlations, we can examine resdiual plots from linear models after conducting hypothesis testing. We can also look back at the box plots (Figure 23.1) to check if the interquartile range of the variables looks approximately the same. All of the variables of interest, except for disp appear to have similar variances.\n\n23.5.5 Normality\nTo check this assumption, we can examine if the data are approximately normally distributed using the Q-Q plot or histograms or with the Shapiro-Wilk test.\nFirst, we will look at the Q-Q plot:\n\n# Normality assumption: Q-Q plot\nqq_mpg &lt;- ggplot(mtcars) +\n  aes(sample = mpg) +\n  stat_qq() +\n  stat_qq_line() +\n  ggtitle(\"mpg\")\n\nqq_disp &lt;- ggplot(mtcars) +\n  aes(sample = disp) +\n  stat_qq() +\n  stat_qq_line() +\n  ggtitle(\"disp\")\n\nqq_hp &lt;- ggplot(mtcars) +\n  aes(sample = hp) +\n  stat_qq() +\n  stat_qq_line() +\n  ggtitle(\"hp\")\n\nqq_drat &lt;- ggplot(mtcars) +\n  aes(sample = drat) +\n  stat_qq() +\n  stat_qq_line() +\n  ggtitle(\"drat\")\n\nqq_wt &lt;- ggplot(mtcars) +\n  aes(sample = wt) +\n  stat_qq() +\n  stat_qq_line() +\n  ggtitle(\"wt\")\n\nqq_qsec &lt;- ggplot(mtcars) +\n  aes(sample = qsec) +\n  stat_qq() +\n  stat_qq_line() +\n  ggtitle(\"qsec\")\n\n\nqq_mpg + qq_disp + qq_hp + qq_drat + qq_wt + qq_qsec\n\n\n\n\n\n\nFigure 23.2\n\n\n\n\nAll of the variables seem to be approximately normally distributed.\nNext, we will look at histograms:\n\n# Normality assumption: Histograms\nhistogram_mpg &lt;- ggplot(mtcars) +\n  aes(x = mpg) +\n  geom_histogram(binwidth = 0.5)\n\nhistogram_disp &lt;- ggplot(mtcars) +\n  aes(x = disp) +\n  geom_histogram(binwidth = 1)\n\nhistogram_hp &lt;- ggplot(mtcars) +\n  aes(x = hp) +\n  geom_histogram(binwidth = 1)\n\nhistogram_drat &lt;- ggplot(mtcars) +\n  aes(x = drat) +\n  geom_histogram(binwidth = 0.5)\n\nhistogram_wt &lt;- ggplot(mtcars) +\n  aes(x = wt) +\n  geom_histogram(binwidth = 0.5)\n\nhistogram_qsec &lt;- ggplot(mtcars) +\n  aes(x = qsec) +\n  geom_histogram(binwidth = 0.5)\n\nhistogram_mpg + histogram_disp + histogram_hp + histogram_drat + \n  histogram_wt + histogram_qsec\n\n\n\n\n\n\nFigure 23.3\n\n\n\n\nHere, it appears that wt and possibly qsec, disp, and hp appear to be approximately normally distributed.\nLastly, we can use the Shapiro-Wilk test to test if the data is approximately normally distributed. P-values greater than 0.05 indicate that the data is likely approximately normally distributed.\n\n# Normality: Shapiro Wilks\nshapiro_mpg &lt;- mtcars %&gt;%\n  summarise(\n    statistic = shapiro.test(mpg)$statistic,\n    p.value = shapiro.test(mpg)$p.value\n  )\n\nshapiro_mpg\n\n  statistic   p.value\n1 0.9475647 0.1228814\n\nshapiro_disp &lt;- mtcars %&gt;%\n  summarise(\n    statistic = shapiro.test(disp)$statistic,\n    p.value = shapiro.test(disp)$p.value\n  )\n\nshapiro_disp\n\n  statistic    p.value\n1 0.9200127 0.02080657\n\nshapiro_hp &lt;- mtcars %&gt;%\n  summarise(\n    statistic = shapiro.test(hp)$statistic,\n    p.value = shapiro.test(hp)$p.value\n  )\n\nshapiro_hp \n\n  statistic    p.value\n1 0.9334193 0.04880824\n\nshapiro_drat &lt;- mtcars %&gt;%\n  summarise(\n    statistic = shapiro.test(drat)$statistic,\n    p.value = shapiro.test(drat)$p.value\n  )\n\nshapiro_drat \n\n  statistic   p.value\n1 0.9458839 0.1100608\n\nshapiro_wt &lt;- mtcars %&gt;%\n  summarise(\n    statistic = shapiro.test(wt)$statistic,\n    p.value = shapiro.test(wt)$p.value\n  )\n\nshapiro_wt \n\n  statistic    p.value\n1 0.9432577 0.09265499\n\nshapiro_qsec &lt;- mtcars %&gt;%\n  summarise(\n    statistic = shapiro.test(qsec)$statistic,\n    p.value = shapiro.test(qsec)$p.value\n  )\n\nshapiro_qsec\n\n  statistic   p.value\n1 0.9732509 0.5935176\n\n\nFrom these outputs, we can see that disp and hp are the only variables with a p-value less than 0.05, indicating that they are likely not approximately normally distributed.\n\n\n\n\n\n\nIf the normality assumption is not satisfied, it is recommended to use non-parametric correlation, including Spearman Rank Correlation and Kendall’s Tau tests, which will be discussed later.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Correlation and Covariance Matrices</span>"
    ]
  },
  {
    "objectID": "lessons/04_corr_cov.html#code-to-run",
    "href": "lessons/04_corr_cov.html#code-to-run",
    "title": "\n23  Correlation and Covariance Matrices\n",
    "section": "\n23.6 Code to Run",
    "text": "23.6 Code to Run\n\n23.6.1 Covariance\nAs with correlation, we can use the cov() to calculate covariance, where the three same methods are available dependent on the validity of assumptions.\n\n# cov(dataName$variable1, dataName$variable2)\n\ncov_result &lt;- cov(mtcars$mpg, mtcars$drat)\n\nprint(cov_result)\n\n[1] 2.195064\n\n\nThe resulting covariance from mpg and drat is 2.195064. We can also use cov() to create a matrix. However, the data must be manipulated to become a square matrix.\n\n# We select the six continuous variables within the `mtcars` data set and save \n# into our matrix object.\ndataMatrix &lt;- mtcars[, c(1, 3:7)]\n\n# We save the output to an object for later.\ncovOutput &lt;- cov(dataMatrix)\n\nprint(covOutput)\n\n             mpg        disp         hp         drat          wt         qsec\nmpg    36.324103  -633.09721 -320.73206   2.19506351  -5.1166847   4.50914919\ndisp -633.097208 15360.79983 6721.15867 -47.06401915 107.6842040 -96.05168145\nhp   -320.732056  6721.15867 4700.86694 -16.45110887  44.1926613 -86.77008065\ndrat    2.195064   -47.06402  -16.45111   0.28588135  -0.3727207   0.08714073\nwt     -5.116685   107.68420   44.19266  -0.37272073   0.9573790  -0.30548161\nqsec    4.509149   -96.05168  -86.77008   0.08714073  -0.3054816   3.19316613\n\n\nThe resulting output is the calculated covariances among all the variables specified. The Covariance can take any value from -\\(\\infty\\) to \\(\\infty\\).\nWe can also scale the covariance matrix into a corresponding correlation matrix.\n\ncov2cor(covOutput)\n\n            mpg       disp         hp        drat         wt        qsec\nmpg   1.0000000 -0.8475514 -0.7761684  0.68117191 -0.8676594  0.41868403\ndisp -0.8475514  1.0000000  0.7909486 -0.71021393  0.8879799 -0.43369788\nhp   -0.7761684  0.7909486  1.0000000 -0.44875912  0.6587479 -0.70822339\ndrat  0.6811719 -0.7102139 -0.4487591  1.00000000 -0.7124406  0.09120476\nwt   -0.8676594  0.8879799  0.6587479 -0.71244065  1.0000000 -0.17471588\nqsec  0.4186840 -0.4336979 -0.7082234  0.09120476 -0.1747159  1.00000000\n\n\nThis defaults to Pearson Correlation so should be interpreted with caution in the case of variables that do not meet the proper assumptions.\n\n23.6.2 Correlation\nCorrelation can be calculated using: cor(), which calculates the correlation coefficient or cor.test(), which tests for the association (or correlation) between paired samples.\nThree different methods are available when using cor(), either pearson (which is default if none is specified), kendall, or spearman. Let’s run cor() on the variables that satisfied our assumptions: mpg, drat, wt, and qsec.\n\n# cor(dataName$variable1, dataName$variable2, method = \"methodName\")\n\n# Pearson is the default, and does not need to be specified. However, for \n# completeness is specified below.\ncorrPearson &lt;- cor.test(mtcars$mpg, mtcars$drat, method = \"pearson\")\n\nprint(corrPearson)\n\n\n    Pearson's product-moment correlation\n\ndata:  mtcars$mpg and mtcars$drat\nt = 5.096, df = 30, p-value = 1.776e-05\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.4360484 0.8322010\nsample estimates:\n      cor \n0.6811719 \n\n\nIn the output, the following is included:\n\n\nCorrelation Method: Pearson's Product-Moment Correlation\n\n\nData: mtcars$mpg and mtcars$drat\n\n\nt, which represents the t-test statistic: 5.096.\n\ndf, which represents the degrees of freedom: 30 (n - 2).\n\nalternative hypothesis, where true correlation is not equal to 0. Therefore, the null hypothesis states true correlation is equal to 0.\n\np-value, which is the significance level of the t-test (1.776e-05) and the probability of this correlation if the null hypothesis were true.\n\n95% confidence interval or conf.int, where we are 95% confidence that the true correlation coefficient lies between [0.4360484, 0.8322010].\n\nsample estimates provides the calculated value of the correlation coefficient: 0.6811719.\n\nThe following will illustrate the output from a cor.test with kendall specified as the method, or known as the Kendall Rank Correlation Coefficient. This is typically used if the data does not satisfy the normality assumption, so we will utilize the variables: mpg and disp, where disp did not satisfy our normality assumption as indicated by the Shapiro-Wilks test.\n\ncorrKendall &lt;- cor.test(mtcars$mpg, mtcars$disp, method = \"kendall\")\n\nWarning in cor.test.default(mtcars$mpg, mtcars$disp, method = \"kendall\"):\nCannot compute exact p-value with ties\n\nprint(corrKendall)\n\n\n    Kendall's rank correlation tau\n\ndata:  mtcars$mpg and mtcars$disp\nz = -6.1083, p-value = 1.007e-09\nalternative hypothesis: true tau is not equal to 0\nsample estimates:\n       tau \n-0.7681311 \n\n\nSimilar information is provided as with the Pearson method, where the type of correlation test and alternative hypothesis is specified. However, in this non-parametric test, we are provided the z-statistic accompanied by the resulting p-value (1.007e-09). The sample estimates also provides us with the Kendall correlation coefficient (also known as tau): -0.7681311.\n\ncorrSpearman &lt;- cor.test(mtcars$mpg, mtcars$disp, method = \"spearman\")\n\nWarning in cor.test.default(mtcars$mpg, mtcars$disp, method = \"spearman\"):\nCannot compute exact p-value with ties\n\nprint(corrSpearman)\n\n\n    Spearman's rank correlation rho\n\ndata:  mtcars$mpg and mtcars$disp\nS = 10415, p-value = 6.37e-13\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n       rho \n-0.9088824 \n\n\nSimilar to both outputs above, the output from Spearman’s rank method displays the type of correlation test, the variables being tested, alternative hypothesis, as well as the S test statistic and associated p-value (6.37e-13). The sample estimate also provides us with the Spearman’s correlation coefficient (also known as rho): -0.9088824.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Correlation and Covariance Matrices</span>"
    ]
  },
  {
    "objectID": "lessons/04_corr_cov.html#brief-interpretation-of-the-output",
    "href": "lessons/04_corr_cov.html#brief-interpretation-of-the-output",
    "title": "\n23  Correlation and Covariance Matrices\n",
    "section": "\n23.7 Brief Interpretation of the Output",
    "text": "23.7 Brief Interpretation of the Output\n\n23.7.1 Interpreting the Covariance\nFrom our Covariance Matrix, the covariance between mpg and drat is 2.195064. A positive covariance indicates that when mpg is high, drat also tends to be high. The covariance between mpg and disp is -633.097208. A negative covariance indicates that when mpg is high, disp tends to be low (vice versa).\n\n23.7.2 Interpreting Correlation Coefficients\n\n\n\n\n\n\n\n-1 indicates a strong negative correlation: Each time x increases, y decreases.\n0 means that there is no association between the two variables (x and y).\n+1 indicates a strong positive correlation: Each time x increases, y increases.\n\n\n\n\nFor the Pearson correlation, the resulting p-value was 1.776e-05, which is less than our significance level alpha of 0.05. Therefore, the data provides sufficient evidence to suggest that mpg and drat are significantly correlated (0.6811719). Each time mpg increases, drat increases.\nFor the Kendall’s Tau test, the resulting p-value was 1.007e-09, which is less than our significance level alpha of 0.05. Therefore, the data provides sufficient evidence to suggest that mpg and disp are significantly correlated (-0.7681311). Each time mpg increases, disp decreases.\nFor the Spearman Rank correlation, the resulting p-value was 6.37e-13, which is less than our significance level alpha of 0.05. Therefore, the data provides sufficient evidence to suggest that mpg and disp are significantly correlated (-0.9088824). Each time mpg increases, disp decreases significantly (illustrates a strong relationship as it is close to -1).",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Correlation and Covariance Matrices</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_regression_ols.html",
    "href": "lessons_original/04_regression_ols.html",
    "title": "\n24  Ordinary Least Squares Regression\n",
    "section": "",
    "text": "24.1 Ordinary Least Squares (OLS) Regression\nOLS is a “method that allows to find a line that best describes the relationship between one or more predictor variables and a response variable”howtop?, with our end result being:\n\\[\n\\hat{y} = b_0 + b_1x\n\\]\nThe best fitting line is typically calculated utilizing the least squares, which can be visually described as the deviation in the vertical direction.\nNow, we will examine the relationship between a vehicle’s weight (WT) and its miles per gallon or fuel efficiency (MPG).\nIn a previous lesson, we found a Pearson’s correlation coefficient of r(30) = -0.868, p&lt;0.05. Based on this information, we concluded that there is a strong negative relationship between MPG and WT. Where, heavier vehicles are associated with lower miles per gallon. Essentially this means that heavier vehicles are less fuel efficient. We will use the interpretation of this correlation as the basis of building an OLS regression to predict the value of MPG for a vehicle based on its weight. An OLS regression could be described as a common method used in regression analysis due to its efficiency in fitting the best straight line through a set of points. Thus, an OLS regression model gives best approximate of true population regression line as it minimizes the total distance from all of the points to the line.\nThe OLS model could be expressed as: \\[\\hat{y}_i = \\beta_0 + \\beta_1x_i\\]\nThen the OLS regression model line for our example is:\n\\[\\widehat{MPG_i} = \\beta_0 +\\beta_1*WT_i\\]",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Ordinary Least Squares Regression</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_regression_ols.html#ordinary-least-squares-ols-regression",
    "href": "lessons_original/04_regression_ols.html#ordinary-least-squares-ols-regression",
    "title": "\n24  Ordinary Least Squares Regression\n",
    "section": "",
    "text": "The line has the following properties:\n\nThe intercept (\\(\\beta_0\\)), its measure is defined by the units in Y. In our case, the units used in MPG. It is the predicted value of Y (MPG) when X (WT) is zero\n\n\nThe slope (\\(\\beta_1\\)), is the predicted change in Y for a one-unit increase in X. Like the correlation coefficient, it provides information on the relationship between X and Y. But unlike the correlation coefficient (unitless), it highlights the relationship in real terms of units. In our example this would look at how miles per gallon increase or decrease for a one unit increase in a vehicle’s weight (according to the R docummentation for the mtcars data set, weight is provided as a measure per every 1,000 pounds and miles per gallon are provided as Miles/(US) gallon), therefore the units are defined by the Y (MPG) and the X (WT).\n\n\n\nFor example, the data for a vehicle that weighs 2,000 pounds the unit is given as “2”\nFor example, a vehicle that spends one gallon of fuel per every 19 miles is given as “19”\n\n\n\n\n\n24.1.1 How to develop the best fitting line?\n\n\nSample residual/error terms plot\n\nThe best fitting line is one that minimizes errors in prediction or one with the Minimum sum of squared residuals (SSR). For more details, you can watch this video:khanacademy2018?\n\\[\nSSR = \\sum_{i = 1}^{n}{(y_i - \\hat{y_i})^2}\n\\]\n\\(residual_i = y_i - \\hat{y_i}\\)\nIt is important to note that prior to calculating the residuals, we must visualize and examine the data, which was done in the previous example. Then, we must run the regression line. We can utilize lm() to perform the OLS regression which will provide us with the model summary, including the following:\n\nPr(&gt;|t|) Multiple R-Squared Adjusted R-Squared Residual Standard Error F-statistic P-value\n\nOnce the model summary is given, we can then move on to creating the residual plots. When performing this step, we have to check the assumptions of homoscedasticity and normality.\n* Residuals = error terms\n* $$ Residual = observed value - predicted value $$\n* The larger the error term in absolute value, the worse the prediction\n* Squaring residuals solve issues arising from some residuals being negative and some positive.\n\n\nAssumptions\n\nLinearity: Linear relationship between the dependent variable and the independent variables.\nIndependence: The observations must be independent of each other.\nHomoscedasticity: The variance of the residuals should be constant across all levels of the independent variables.\nNormality: The residuals / errors should be normally distributed.\nNo multicollinearity: In the case of multiple regression (2+ independent variables), the independent variables should not be highly correlated with each other.\n\n\n\n\n\n\n\n\n\nBe careful about outliers\n\n\n\nOutliers can influence the estimates of the relationship.\n\n\n\n24.1.2 Example of an OLS regression in R\nIn R, the lm function command allows us to develop an OLS regression.\n\n# model predicting mpg (fuel efficiency) using wt (weight)\nMPGReg &lt;- lm(mpg ~ wt, data = mtcars)\nsummary(MPGReg)\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n\nThe OLS regression model is then:\n\\(\\widehat{MPG_i} = 37.285 - 5.344*WT_i\\)\n\n\nInterpretation: \\(\\beta_0\\)\n\nThe model predicts that vehicles with no weight will have 37.285 miles per gallon, on average.\n\nThis is not a very meaningful intercept as vehicles with “0” weight do not exist. A meaningful intercept can be created by subtracting a constant from the x variable to move the intercept.In R as part of the lm command, this can be done by surrounding the independent variable with I() which applies the function inside and treats it as a new variable. For our example we used the rounded lowest weight of the data (1.5) to predict miles per gallon.\n\nThis procedure does not change the slope of the line\n\n\n\n\n\n\n# model predicting mpg (fuel efficiency) using wt (weight)\nMPGReg2 &lt;- lm(mpg ~ I(wt-1.5), data = mtcars)\nsummary(MPGReg2)\n\n\nCall:\nlm(formula = mpg ~ I(wt - 1.5), data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  29.2684     1.1008  26.589  &lt; 2e-16 ***\nI(wt - 1.5)  -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n\n\nThen, the meaningful intercept model predicts that vehicles with a weight of 1500 pounds have 29.268 miles per gallon, on average.\n\nInterpretation: \\(\\beta_1\\)\n\nThe model predicts that on average, an increase of 1,000 pounds in the weight of a vehicle is associated with a decrease of 5.344 miles per gallon.\n\n\n\n\n# define residuals \nres &lt;- resid(MPGReg)\n\n# produce residual vs. fitted plot \nplot(fitted(MPGReg), res)\n\n# add a horizontal line at 0\nabline(0,0)\n\n\n\n\n\n\n# create Q-Q- plot for residuals\nqqnorm(res)\n\n# add a straight diagonal line to the plot\nqqline(res)\n\n\n\n\n\n\n\nBased on the graph above, it is visually clear that normality may not be met due to some outliers. This means that we must explore our data even deeper as it is possible that transformation of our data utilizing one of the following methods must take place:\n\nLog transformation Square Root Transformation Cube Root Transformation\n\nOnce the data is transformed, we can run the residual plot over again in order to achieve normality. For the sake of this presentation, we are only using an example with known limitations such as non-normality.\n\n24.1.3 Hypothesis testing in OLS regression\nThe null hypothesis in this case would be that the slope is zero indicating no relationship between x and y. Or in our example, we can state that there is no relationship between a vehicle’s weight and its miles per gallon. The alternative hypothesis is then that the slope is not zero.\n\\[\nH_0: \\beta_1 = 0\n\\]\n\\[\nH_1: \\beta_1 ≠ 0\n\\]\nWe can test this hypothesis by using the lm summary printout which provides the p-value for the wt coefficient. This indicates that there is indeed a significant relationship between the weight of the car and its efficiency (miles per gallon used). R provides a t-value for the ‘wt’ coefficient which has a p-value of p &lt; 0.000 as seen below:\n                  Estimate        Std. Error       t value      Pr(&gt;|t|)   \n    wt           -5.3445             0.5591       -9.559       1.29e-10\n\n24.1.4 R-Squared Value\nR-squared is “a measure of how much of the variation in the dependent variable is explained by the independent variables in the model. It ranges from 0 to 1, with higher values indicating a better fit”ordinary?.\nAdjusted R-squared is “similar to R-squared, but it takes into account the number of independent variables in the model. It is a more conservative estimate of the model’s fit, as it penalizes the addition of variables that do not improve the model’s performance”ordinary?.\n\n24.1.5 F-Statistic\nThe F-statistic “tests the overall significance of the model by comparing the variation in the dependent variable explained by the model to the variation not explained by the model. A large F-statistic indicates that the model as a whole is significant”interpre?.\n\n24.1.6 Visual representation\nThe visual representation of this model using ggplot is the following:\n\nggplot(mtcars, aes(x = wt, y = mpg))+\n  geom_point()+\n  geom_smooth(method = \"lm\", se = FALSE)+ #se is option for coinfidence bar\n  labs(x= \"Weight (per 1,000 pounds)\",\n       y = \"Miles per gallon\")+\n  theme_bw()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nAs shown in the figure above, we see the summary statistics represented in a visual manner with the line of best fit. As indicated previously, we see a steep negative correlation between weight of the car and the miles per gallon (efficiency) utilized.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Ordinary Least Squares Regression</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_regression_ols.html#references",
    "href": "lessons_original/04_regression_ols.html#references",
    "title": "\n24  Ordinary Least Squares Regression\n",
    "section": "\n24.2 References",
    "text": "24.2 References",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Ordinary Least Squares Regression</span>"
    ]
  },
  {
    "objectID": "lessons/04_regression_mls.html",
    "href": "lessons/04_regression_mls.html",
    "title": "\n25  Multiple Linear Regression\n",
    "section": "",
    "text": "25.1 Multiple Linear Regression\nMultiple Linear Regression (MLR) is a statistical technique used to understand the relationship between one dependent variable and two or more independent variables. The relationship is explained by by modeling the observed relationship using a mathematical representation or approximation.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "lessons/04_regression_mls.html#multiple-linear-regression",
    "href": "lessons/04_regression_mls.html#multiple-linear-regression",
    "title": "\n25  Multiple Linear Regression\n",
    "section": "",
    "text": "25.1.1 Key Components in MLR:\n\nDependent Variable (Y): The outcome variable or the variable that is potentially going to change due to influencing factors.\nIndependent Variables (X1, X2, …, Xn): Predictor or influencing variables used to predict the dependent variable.\nRegression Coefficients (β0, β1, …, βn): Parameters that represent the relationship between each independent variable and the dependent variable where β0 is the intercept, β1 to βn are the slopes for each independent variable.\nError Term (ε): Represents the random variability in the dependent variable that might perhaps not be explained by the independent variables.\n\n25.1.2 Model Equation:\n\\[\nY = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\ldots + \\beta_pX_p + \\varepsilon\n\\]",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "lessons/04_regression_mls.html#assumptions",
    "href": "lessons/04_regression_mls.html#assumptions",
    "title": "\n25  Multiple Linear Regression\n",
    "section": "\n25.2 Assumptions:",
    "text": "25.2 Assumptions:\n\n\nLinearity: it is assumed that the relationship between the dependent and independent variables is linear.\n\nIndependence: Observations are independent of each other.\n\nHomoscedasticity: The variance of the residuals (errors) is constant across all levels of the independent variables.\n\nNormality: Residuals are normally distributed.\n\nNo Multicollinearity: Independent variables are not highly correlated with each other.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "lessons/04_regression_mls.html#purpose",
    "href": "lessons/04_regression_mls.html#purpose",
    "title": "\n25  Multiple Linear Regression\n",
    "section": "\n25.3 Purpose:",
    "text": "25.3 Purpose:\nTo predict the value of the dependent variable based on the values of the independent variables and to understand the strength and type of relationships between the dependent variable and multiple independent variables.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "lessons/04_regression_mls.html#steps-in-conducting-mlr",
    "href": "lessons/04_regression_mls.html#steps-in-conducting-mlr",
    "title": "\n25  Multiple Linear Regression\n",
    "section": "\n25.4 Steps in Conducting MLR:",
    "text": "25.4 Steps in Conducting MLR:\n\n\nData Collection: Gather data for the dependent and independent variables.\n\nModel Specification: Define the model equation with the dependent variable and chosen independent variables.\n\nEstimation of Coefficients: Use statistical software to estimate the regression coefficients.\n\nModel Evaluation: Assess the model’s goodness-of-fit using R-squared, adjusted R-squared, and other metrics.\n\nDiagnostic Checking: Check the assumptions of MLR (linearity, independence, homoscedasticity, normality, no multicollinearity).",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "lessons/04_regression_mls.html#model-evaluation-metrics",
    "href": "lessons/04_regression_mls.html#model-evaluation-metrics",
    "title": "\n25  Multiple Linear Regression\n",
    "section": "\n25.5 Model Evaluation Metrics:",
    "text": "25.5 Model Evaluation Metrics:\n\n\nR-squared (R²): Measures the proportion of variance in the dependent variable explained by the independent variables.\n\nAdjusted R-squared: Adjusted version of R² that accounts for the number of predictors in the model.\n\nF-statistic: Tests the overall significance of the model.\n\np-values: Test the significance of individual regression coefficients.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "lessons/04_regression_mls.html#demostration",
    "href": "lessons/04_regression_mls.html#demostration",
    "title": "\n25  Multiple Linear Regression\n",
    "section": "\n25.6 Demostration",
    "text": "25.6 Demostration\nWe perform a multiple linear regression analysis on a dataset of medical insurance costs. This dataset includes variables such as age, sex, BMI, number of children, smoker status, and region. Our goal is to understand the relationship between these variables and to predict insurance charges based on the other factors.\nThe data of insurance could be found from Kagglelink\n\n# Load the data\ndata &lt;- read_csv(\"../data/04_insurance.csv\")\n\nRows: 1338 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): sex, smoker, region\ndbl (4): age, bmi, children, charges\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Preview the data\nhead(data)\n\n# A tibble: 6 × 7\n    age sex      bmi children smoker region    charges\n  &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;       &lt;dbl&gt;\n1    19 female  27.9        0 yes    southwest  16885.\n2    18 male    33.8        1 no     southeast   1726.\n3    28 male    33          3 no     southeast   4449.\n4    33 male    22.7        0 no     northwest  21984.\n5    32 male    28.9        0 no     northwest   3867.\n6    31 female  25.7        0 no     southeast   3757.\n\n\n\n25.6.1 Summary Statistics\nWe begin by examining the summary statistics of the dataset to understand its structure and the distribution of variables.\n\nsummary(data)\n\n      age            sex                 bmi           children    \n Min.   :18.00   Length:1338        Min.   :15.96   Min.   :0.000  \n 1st Qu.:27.00   Class :character   1st Qu.:26.30   1st Qu.:0.000  \n Median :39.00   Mode  :character   Median :30.40   Median :1.000  \n Mean   :39.21                      Mean   :30.66   Mean   :1.095  \n 3rd Qu.:51.00                      3rd Qu.:34.69   3rd Qu.:2.000  \n Max.   :64.00                      Max.   :53.13   Max.   :5.000  \n    smoker             region             charges     \n Length:1338        Length:1338        Min.   : 1122  \n Class :character   Class :character   1st Qu.: 4740  \n Mode  :character   Mode  :character   Median : 9382  \n                                       Mean   :13270  \n                                       3rd Qu.:16640  \n                                       Max.   :63770  \n\n\nwe will first log-transform the charges variable. This is often done to stabilize variance and make the data more normally distributed, which can help improve the performance and interpretation of regression models.\n\n# Log-transform the charges variable\ndata$log_charges &lt;- log(data$charges)\n\nBefore fitting the model, we need to ensure that our categorical variables are correctly encoded.\n\n# Convert categorical variables to factors\ndata$sex &lt;- as.factor(data$sex)\ndata$smoker &lt;- as.factor(data$smoker)\ndata$region &lt;- as.factor(data$region)\n\n\nBefore converting to factors:  sex: Character values  “female”, “male”.  smoker: Character values  “no”, “yes”.  region: Character values  “northeast” “northwest” “southeast” “southwest”. \nAfter converting to factors:  sex:  Factor with levels  1 = “female”,  2 = “male”.  smoker:  Factor with levels  1 = “no”,  2 = “yes”.  region: Factor with levels  1 = “northeast”, 2 = “northwest”, 3 = “southeast”, 4 = “southwest”.\n\n\n25.6.2 Model Fitting\nWe fit a multiple linear regression model to predict insurance charges based on the other variables in the dataset.\n\n# Fit the multiple linear regression model\nmodel &lt;- lm(log_charges ~ age + sex + bmi + children + smoker + region,\n  data = data)\n\n# Summary of the model\nsummary(model)\n\n\nCall:\nlm(formula = log_charges ~ age + sex + bmi + children + smoker + \n    region, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.07186 -0.19835 -0.04917  0.06598  2.16636 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      7.0305581  0.0723960  97.112  &lt; 2e-16 ***\nage              0.0345816  0.0008721  39.655  &lt; 2e-16 ***\nsexmale         -0.0754164  0.0244012  -3.091 0.002038 ** \nbmi              0.0133748  0.0020960   6.381 2.42e-10 ***\nchildren         0.1018568  0.0100995  10.085  &lt; 2e-16 ***\nsmokeryes        1.5543228  0.0302795  51.333  &lt; 2e-16 ***\nregionnorthwest -0.0637876  0.0349057  -1.827 0.067860 .  \nregionsoutheast -0.1571967  0.0350828  -4.481 8.08e-06 ***\nregionsouthwest -0.1289522  0.0350271  -3.681 0.000241 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4443 on 1329 degrees of freedom\nMultiple R-squared:  0.7679,    Adjusted R-squared:  0.7666 \nF-statistic: 549.8 on 8 and 1329 DF,  p-value: &lt; 2.2e-16\n\n\nCoefficients and Interpretation: Intercept: The intercept of the model is 7.0305581, which represents the expected log of charges when all predictors are zero. This value is highly significant with a p-value less than 2e-16. Age: For each additional year of age, the log of charges increases by 0.0345816. This effect is highly significant with a p-value less than 2e-16. Sex (male): Being male decreases the log of charges by 0.0754164 compared to being female. This effect is significant with a p-value of 0.002038. BMI: Each unit increase in BMI results in an increase in the log of charges by 0.0133748. This effect is highly significant with a p-value of 2.42e-10. Number of children: Each additional child increases the log of charges by 0.1018568. This effect is highly significant with a p-value less than 2e-16. Smoking status (yes): Being a smoker increases the log of charges by 1.5543288. This effect is highly significant with a p-value less than 2e-16. Region (northwest):Living in the northwest region decreases the log of charges by 0.0637876 compared to the baseline region. This effect is marginally significant with a p-value of 0.067860. Region (southeast): Living in the southeast region decreases the log of charges by 0.1571967 compared to the baseline region. This effect is highly significant with a p-value of 8.08e-06. Region (southwest): Living in the southwest region decreases the log of charges by 0.1289522 compared to the baseline region. This effect is significant with a p-value of 0.000241. The model explains approximately 76.79% of the variance in the log of charges, as indicated by the multiple R-squared value of 0.7679 and the adjusted R-squared value of 0.7666.  The overall model is highly significant, as indicated by the F-statistic of 549.8 with a p-value less than 2.2e-16.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "lessons/04_regression_mls.html#conclusion",
    "href": "lessons/04_regression_mls.html#conclusion",
    "title": "\n25  Multiple Linear Regression\n",
    "section": "\n25.7 Conclusion",
    "text": "25.7 Conclusion\nOur multiple linear regression model suggests that age, sex, BMI, number of children, smoking status, and region are significant predictors of log-transformed insurance charges.  Specifically, older age, higher BMI, more children, and being a smoker are associated with higher log-transformed insurance charges. In contrast, being male and residing in the northwest, southeast, or southwest regions tends to be associated with lower log-transformed insurance charges compared to their respective reference categories.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "lessons/04_regression_polynomial.html",
    "href": "lessons/04_regression_polynomial.html",
    "title": "\n26  Introduction to Polynomial Regression\n",
    "section": "",
    "text": "26.1 What is a Polynomial Regression?\nPolynomial regression is a type of regression analysis that models the non-linear relationship between the predictor variable(s) and response variable. It is an extension of simple linear regression that allows for more complex relationships between predictor and response variables. Field A, 2013",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Introduction to Polynomial Regression</span>"
    ]
  },
  {
    "objectID": "lessons/04_regression_polynomial.html#what-is-a-polynomial-regression",
    "href": "lessons/04_regression_polynomial.html#what-is-a-polynomial-regression",
    "title": "\n26  Introduction to Polynomial Regression\n",
    "section": "",
    "text": "26.1.1 When is a Polynomial Regression Used?\nPolynomial regression is useful when the relationship between the independent and dependent variables is nonlinear. It can capture more complex relationships than linear regression, making it suitable for cases where the data exhibits curvature.\n\n26.1.2 Assumptions of Polynomial Regression\n\n\nLinearity: There is a curvilinear relationship between the independent variable(s) and the dependent variable.\n\nHomoscedasticity: The variance of the errors should be constant across all levels of the independent variable(s).\n\nNormality: The errors should be normally distributed with mean zero and a constant variance.\n\nIndependence:The predictor variables are independent of each other.\n\n26.1.3 Mathematical Equation\nConsider independent samples \\(i = 1, \\ldots, n\\). The general formula for a polynomial regression representing the relationship between the response variable (\\(y\\)) and the predictor variable (\\(x\\)) as a polynomial function of degree \\(d\\) is:\n\\[\ny_i = \\beta_0 + \\beta_1x_i + \\beta_2x_i^2 + \\beta_3x_i^3 + ... \\beta_dx_i^d + \\epsilon_i,\n\\]\nwhere:\n\n\n\\(y_i\\) represents the response variable,\n\n\\(x_i\\) represents the predictor variable,\n\n\\(\\beta_0,\\ \\beta_1,\\ \\ldots,\\ \\beta_d\\) are the coefficients to be estimated, and\n\n\\(\\epsilon_i\\) represents the errors.\n\nFor large degree \\(d\\), polynomial regression allows us to produce an extremely non-linear curve. Therefore, it is not common to use \\(d &gt; 4\\) because the larger value of \\(d\\), the more overly flexible polynomial curve becomes, which can lead to overfitting the model to the data. Jackson SE, 2024\nThe coefficients in polynomial function can be estimated using least square linear regression because it can be viewed as a standard linear model with predictors \\(x_i, \\,x_i^2, \\,x_i^3, ..., x_i^d\\). Hence, polynomial regression is also known as polynomial linear regression.\n\n26.1.4 Performing a Polynomial Regression in R\n\nStep 0: Load required packages\nStep 1: Load and inspect the data\nStep 2: Visualize the data\nStep 3: Fit the model\nStep 4: Assess Assumptions\nStep 5: Describe model output",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Introduction to Polynomial Regression</span>"
    ]
  },
  {
    "objectID": "lessons/04_regression_polynomial.html#lets-practice",
    "href": "lessons/04_regression_polynomial.html#lets-practice",
    "title": "\n26  Introduction to Polynomial Regression\n",
    "section": "\n26.2 Let’s Practice!",
    "text": "26.2 Let’s Practice!\nNow let’s go through the steps to perform a polynomial regression in R. We will be using the lm() function to fit the polynomial regression model. This function comes standard in base R.\nFor this example, we will use the built-in mtcars dataset (from the standard R package datasets) which is publicly available and contains information about various car models.\n\n26.2.1 Hypotheses\nFor this example, we are investigating the following:\n\n\nResearch Question: Is there a significant quadratic relationship between the weight of a car (wt) and its miles per gallon (mpg) in the mtcars dataset?\n\nNull hypothesis (\\(H_0\\)): There is no significant relationship between the weight of a car (wt) and its miles per gallon (mpg).\n\nAlternative hypothesis (\\(H_A\\)): There is a significant relationship between the weight of a car (wt) and its miles per gallon (mpg).\n\nIn this case, the null hypothesis assumes that the coefficients of the quadratic polynomial terms are zero, indicating no relationship between the weight of the car and miles per gallon. The alternative hypothesis, on the other hand, suggests that at least one of the quadratic polynomial terms is non-zero, indicating a significant relationship between the weight of the car and miles per gallon.\nBy performing the polynomial regression analysis and examining the model summary and coefficients, we can evaluate the statistical significance of the relationship and determine whether to reject or fail to reject the null hypothesis.\n\n26.2.2 Step 0: Install and load required package\nAs we want to visualize our data after fitting the model, we will be loading the ggplot2 package.\n\n# For data visualization purposes\n# install.packages(\"ggplot2\")\nlibrary(ggplot2)\n\n\n26.2.3 Step 1: Load and inspect the data\n\n# Load mtcars dataset\ndata(mtcars)\n\n\n# Print the first few rows\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\n\n26.2.4 Step 2: Visualize the data\nBefore fitting a polynomial regression model, it’s helpful to visualize the data to identify any non-linear patterns. For our example, we will use a scatter plot to visualize the relationship between the independent and dependent variables:\n\n# Scatter plot of mpg (dependent variable) vs. wt (independent variable)\nggplot(mtcars) +\n  theme_minimal() +\n  aes(x = wt, y = mpg) + \n  labs(x = \"Weight (lbs/1000)\", y = \"Miles per Gallon\") +\n  geom_point()\n\n\n\n\n\n\n\n\n26.2.5 Step 3: Fit Models\nLet us create a function so we can build multiple models. We will fit a standard linear (degree = 1) and a quadratic polynomial (degree = 2) to the mtcars dataset.\n\n# Function to fit and evaluate polynomial regression models\nfit_poly_regression &lt;- function(degree) { \n  #argument specifies the degree of the polynomial for the regression\n  formula &lt;- as.formula(paste(\"mpg ~ poly(wt, \", degree, \", raw = TRUE)\")) \n  #paste concatenates the components into a single string eg:`mpg ~ poly(wt, 2)`\n  #poly returns an orthogonal polynomials as default\n  #(If `raw = TRUE` calculates raw polynomial instead of orthogonal polynomial)  \n  #as.formula converts the constructed string into a formula object \n  lm(formula, data = mtcars) #fitting the model\n}\n\n# Fit polynomial regression models with degrees 1 to 2\nmodel_1 &lt;- fit_poly_regression(1)\nmodel_2 &lt;- fit_poly_regression(2)\n\n*Note 1: To learn more about orthogonal polynomial regression, follow: a. Orthogonal polynomial equation and explanation b. Stackoverflow - difference between raw vs orthogonal polynomial c. StackExchange - Interpreting coefficients from raw vs orthogonal polynomial\n*Note 2: Using orthogonal polynomial regression would have also resulted in same plots based on which the assumptions are assessed. Moreover, it produces exact same p-value for the quadratic term but the p-value for linear term and the estimates for intercept, linear and quadratic term would be different than using raw polynomial regression.\n\n26.2.6 Step 4: Assess Assumptions\nBefore we can interpret the model, we have to check the assumptions. We will check these assumptions via plots:\n\nResiduals vs. Fitted values (used to check the linearity assumption),\na Q-Q plot of the Residuals (used to check the normality of the residuals),\na Scale-Location plot (used to check for heteroskedasticity), and\nResiduals vs. Leverage values (identifies overly influential values, if any exist).\n\n\npar(mfrow = c(2, 2)) #sets graphical parameters\nplot(model_1, which = c(1, 2, 3, 5))\n\n\n\n\n\n\nplot(model_2, which = c(1, 2, 3, 5))\n\n\n\n\n\n\npar(mfrow = c(1, 1))\n\nThe residuals vs fitted graph in the linear model has a U-shape, which suggests that we need a quadratic polynomial component. The Q-Q plot shows that the residuals are not normally distributed, so we should take additional steps to transform the response feature (such as via a square root or log transformation, or something similar).\n\n26.2.7 Conducting cube root transformation\nWe will cube root transform the response variable to build the models again.\n\n#cube root transformation of response variable (mpg)\nlibrary(dplyr)\nmtcars &lt;- mtcars %&gt;% mutate(mpg_cuberoot = mpg^(1/3))\n\n#Running model after cube root transformation\nfit_poly_regression_2 &lt;- function(degree) { \n  #argument specifies the degree of the polynomial for the regression\n  formula_2 &lt;- as.formula(paste(\"mpg_cuberoot ~ poly(wt, \", degree, \", raw = TRUE)\")) \n  #paste concatenates the components into a single string eg:`mpg ~ poly(wt, 2)`\n  #poly returns an orthogonal polynomials as default\n  #(If `raw = TRUE` calculates raw polynomial instead of orthogonal polynomial)  \n  #as.formula converts the constructed string into a formula object \n  lm(formula_2, data = mtcars) #fitting the model\n}\n\n# Fit polynomial regression models with degrees 1 to 2\nmodel_1_b &lt;- fit_poly_regression_2(1)\nmodel_2_b &lt;- fit_poly_regression_2(2)\n\n\n26.2.8 Assessing assumptions on cube root transformed data\nWe will assess the assumption of the polynomial regression with the plots similar to non-transformed model.\n\npar(mfrow = c(2, 2)) #sets graphical parameters\nplot(model_1_b, which = c(1, 2, 3, 5))\n\n\n\n\n\n\nplot(model_2_b, which = c(1, 2, 3, 5))\n\n\n\n\n\n\npar(mfrow = c(1, 1))\n\nAfter cube root transformation of the response variable, the residuals are seem to approximately normally distributed. Zoom in to see the difference between the Q-Q plots in the first quadratic model (model_2) and cube root transformed quadratic model (model_2_b). You can also draw the Q-Q plot again to see the difference more distinctly.\n\nqqnorm(residuals(model_2))\nqqline(residuals(model_2))\n\n\n\n\n\n\nqqnorm(residuals(model_2_b))\nqqline(residuals(model_2_b))\n\n\n\n\n\n\n\n\n26.2.9 Step 5. Describe Model Output\nAs the residuals of the cube root transformed model seem to be approximately normal, here is an example of how to interpret this output.\n\nsummary(model_2)\n\n\nCall:\nlm(formula = formula, data = mtcars)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.483 -1.998 -0.773  1.462  6.238 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               49.9308     4.2113  11.856 1.21e-12 ***\npoly(wt, 2, raw = TRUE)1 -13.3803     2.5140  -5.322 1.04e-05 ***\npoly(wt, 2, raw = TRUE)2   1.1711     0.3594   3.258  0.00286 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.651 on 29 degrees of freedom\nMultiple R-squared:  0.8191,    Adjusted R-squared:  0.8066 \nF-statistic: 65.64 on 2 and 29 DF,  p-value: 1.715e-11\n\nsummary(model_2_b)\n\n\nCall:\nlm(formula = formula_2, data = mtcars)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.16361 -0.08607 -0.02612  0.07303  0.23272 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               3.84775    0.18784  20.484  &lt; 2e-16 ***\npoly(wt, 2, raw = TRUE)1 -0.48063    0.11214  -4.286 0.000183 ***\npoly(wt, 2, raw = TRUE)2  0.03471    0.01603   2.165 0.038777 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1182 on 29 degrees of freedom\nMultiple R-squared:  0.8171,    Adjusted R-squared:  0.8044 \nF-statistic: 64.76 on 2 and 29 DF,  p-value: 2.012e-11\n\n\nThe model could be said as being highly significant as indicated by the p-value. The significant negative linear term and significant positive quadratic term imply that mpg decreases with weight initially, but the rate of decrease slows as weight increases.\nThe purpose of checking the transformed response is to confirm that the confidence intervals are valid and the signs of the regression coefficients are going are correct (i.e. in the same direction) as the original model (non-transformed quadratic model).\n\n26.2.10 Bonus Step: Visualize the Final Model\nFinally, we will build the scatter plot with the polynomial regression line to visualize the fit:\n\n# Create a data frame with data points and predictions \nplot_data &lt;- data.frame(\n  wt = mtcars$wt,\n  mpg = mtcars$mpg, \n  mpg_predicted = predict(model_2, newdata = mtcars)\n)\n\n# Scatter plot with the polynomial regression line\nggplot(plot_data) +\n  theme_minimal() + \n  aes(x = wt, y = mpg) + \n  labs(\n    title = \"Scatter Plot with Polynomial Regression Line\",\n    x = \"Weight (wt)\",\n    y = \"Miles per Gallon (mpg)\"\n  ) +\n  geom_point() +\n  geom_line(aes(y = mpg_predicted), color = \"red\", size = 1)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\nplot_data_b &lt;- data.frame(\n  wt = mtcars$wt,\n  mpg_cuberoot = mtcars$mpg_cuberoot, \n  mpg_predicted = predict(model_2_b, newdata = mtcars)\n)\n\n# Scatter plot with the polynomial regression line\nggplot(plot_data_b) +\n  theme_minimal() + \n  aes(x = wt, y = mpg_cuberoot) + \n  labs(\n    title = \"Scatter Plot with Polynomial Regression Line (Cube transformed)\",\n    x = \"Weight (wt)\",\n    y = \"Cube transfomred Miles per Gallon (mpg)\"\n  ) +\n  geom_point() +\n  geom_line(aes(y = mpg_predicted), color = \"red\", size = 1)",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Introduction to Polynomial Regression</span>"
    ]
  },
  {
    "objectID": "lessons/04_regression_polynomial.html#references",
    "href": "lessons/04_regression_polynomial.html#references",
    "title": "\n26  Introduction to Polynomial Regression\n",
    "section": "\n26.3 References",
    "text": "26.3 References\n\nField, A. (2013). Discovering Statistics Using IBM SPSS Statistics. (4th ed.). Sage Publications.\nhttps://bookdown.org/ssjackson300/Machine-Learning-Lecture-Notes/#welcome\nhttps://stats.libretexts.org/Bookshelves/Advanced_Statistics/Analysis_of_Variance_and_Design_of_Experiments/10%3A_ANCOVA_Part_II/10.02%3A_Quantitative_Predictors_-_Orthogonal_Polynomials#",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Introduction to Polynomial Regression</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_regression_random_intercept.html",
    "href": "lessons_original/04_regression_random_intercept.html",
    "title": "27  Random Intercept Regression",
    "section": "",
    "text": "27.1 Libraries Used\nlibrary(gt)        # to make table outputs presentation/publication ready. \nlibrary(broom)     # clean output.\nlibrary(knitr)     # to make tables. \nlibrary(gtsummary) # extension to gt\nlibrary(lattice)   # It is a powerful data visualization for multivariate data\nlibrary(lme4)      # Fit linear and generalized linear mixed-effects models\nlibrary(arm)       # Data Analysis Using Regression and Multilevel models\nlibrary(tidyverse) # for cleaning wrangling data",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Random Intercept Regression</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_regression_random_intercept.html#what-is-a-random-intercept-model",
    "href": "lessons_original/04_regression_random_intercept.html#what-is-a-random-intercept-model",
    "title": "27  Random Intercept Regression",
    "section": "27.2 What is a Random Intercept model",
    "text": "27.2 What is a Random Intercept model\nBefore talking about a random intercept model, let’s understand why they are necessary and important in the real world by discussing a variance component model first. This will make sense as we go along in this lecture.\n\n27.2.1 Variance component model\nWe are familiar with a fixed level of a factor or variable. Which means that the factor level in an experiment is the only thing we are interested. For example, let’s say we are interested in measuring the difference in resistance resulting from putting identical resistors to three different temperatures for a period of 24 hours. Let’s say we have three different groups, and each of these three different groups have a sample size of 5. So each of the three treatment groups was replicated 5 times.\n\n\n\nLevel 1\nLevel 2\nLevel 3\n\n\n\n\n6.9\n8.3\n8.0\n\n\n5.4\n6.8\n10.5\n\n\n5.8\n7.8\n8.1\n\n\n4.6\n9.2\n6.9\n\n\n4.0\n6.5\n9.3\n\n\nmean\nmean\nmean\n\n\n5.34\n7.72\n8.56\n\n\n\nIn this example, the level of the temperature is considered fixed meaning, the three temperatures were the only ones that we are interested in. This is called a fixed effects model.\na fixed effect model is a statistical model in which the parameters are fixed or non-random. This can also be referred to a regression model, in which group mean are “fixed” (non-random) or in simpler, terms something that is “fixed” in analysis is constant like sex assigned at birth or ethnicity.\n\\[ y_i = \\beta_0 + X_i\\beta_i + \\alpha_u + \\epsilon_i \\]\nNow, let’s say we want to look at different levels of factors that were chosen because of random sampling, like number of operators working that day, lot batches, days etc. So in this case we are now regarding factors not related to themselves (variables) but we are now trying to represent all possible levels that these factors may take, the appropriate model is now a random effects model.\nfitting these random effects models are important because we want to obtain estimates of different contributions that experimental factors make to the variability of our data! (we can represent this as the variance) this is what is called variance component\n\n\n27.2.2 Why this is relevant\nWell, a variance component model helps us see how much variance in our response at the different levels. But what if you are interested in seeing the effects of the explanatory variables? Or, what if your observations are NOT randomly sampled from simple random sample but instead from a cluster or a multi-level sampling design? Random intercept models or random effects models are important.\n\n\n27.2.3 Example 1: School level data\nLet’s say we have some data on exam results of students within a school and we use a variance component model and see that 15% of the variance is at the school level. Like for example, differing school districts, differing school policies etc. However, is it fair to really say that 15% of the variance in example scores is caused by schools? you could also say that maybe that part of the variance could be cause by the students being different themselves as well before taking the exam.\nIn this case, it might be important to control for the previous exams the students took, so you can look at the variance that is due to the things that happened when the students were at that school.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Random Intercept Regression</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_regression_random_intercept.html#fitting-a-single-level-regression-model",
    "href": "lessons_original/04_regression_random_intercept.html#fitting-a-single-level-regression-model",
    "title": "27  Random Intercept Regression",
    "section": "27.3 Fitting a single-level regression model",
    "text": "27.3 Fitting a single-level regression model\nwhen we want to control for something (like previous exams students took) we can fit a single-level regression model that looks something like this:\n\\[y_1 = \\beta_0 + \\beta_1x_i + e_i\\] where\n\n\\(y_1\\) is your dependent variable\n\\(\\beta_0\\) is your intercept and\n\\(\\beta_1\\) is your slope parameter (which is also your slope treatment effect).\nand \\(e_i\\) is your random error\n\nWhen you have clustered data fitting this model causes problems. Clustered data is data where you observation or participants are related. Like exam results for students within a school, height of children within a family etc.\nif we try to fit this clustered data:\n\nour standard errors will be wrong.\nthis single level data model doesn’t show up how much variation is at the school level and how much much of the variation is at the student level.\n\nSo fitting this type of data in this regression we wont know how much of an effect the school level has on the exam score, after controlling for the previous score.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Random Intercept Regression</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_regression_random_intercept.html#solution-fitting-a-random-intercept-model",
    "href": "lessons_original/04_regression_random_intercept.html#solution-fitting-a-random-intercept-model",
    "title": "27  Random Intercept Regression",
    "section": "27.4 Solution: Fitting a Random Intercept model",
    "text": "27.4 Solution: Fitting a Random Intercept model\nSo what we can do is combine the variance component and single-level regression model to build a random intercept model. So this random intercept model has 2 random terms. the level one random term: \\(e_{ij} \\sim N(0, \\sigma_e^2)\\) and the \\(N(0, \\sigma_u^2)\\) and has two parts:\n\na fixed part\na random part\n\n\\[ y_{ij} = \\overbrace{\\beta_0 + \\beta_1X_{ij}}^{\\text{fixed part}} + \\underbrace{u_j + e_{ij}}_{\\text{random part}}\\] where the fixed parts includes our parameters that we estimate as our coefficients, and the random part is the parameter we estimate as the variance \\(e_{ij} \\sim N(0, \\sigma_e^2)\\) and the \\(N(0, \\sigma_u^2)\\) and these are allowed to vary and \\(u_j\\) and \\(e_{ij}\\) are normally distributed.\nwhere:\n\n\\(y_{ij}\\) is your dependent variable at \\(i\\) individual and \\(j\\) level\n\\(N(0, \\sigma_u^2)\\) is the measurement at the school level\nand \\(e_{ij} \\sim N(0, \\sigma_e^2)\\) is the measurement at the student level\nand \\(i\\) subscript is for the students\nand \\(j\\) is the school subscript\n\nwe can also write this equation like so:\n\\[ Y_{ij} = \\mu + b_i + \\varepsilon_{ij} \\]\nwhere\n\n\\(Y_{ij}\\) is your dependent outcome of intested for a subject \\(i\\) at school \\(j\\)\n\\(\\mu\\) is the population average mean\n\\(b_i\\) is the random students effects (you have a random effect for every student)\n\\(\\varepsilon_{ij}\\) is your random error.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Random Intercept Regression</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_regression_random_intercept.html#final-key-points",
    "href": "lessons_original/04_regression_random_intercept.html#final-key-points",
    "title": "27  Random Intercept Regression",
    "section": "27.5 Final key points",
    "text": "27.5 Final key points\n\nrandom intercept models are used for answering questions about clustered data, and at different levels. For example, what is the relationship between exam scores at 11 and at age 16? how much variation is there between students progress from 11 to 16 at the school level?\n\\(b_i\\) is the error associated with the students.\n\\(\\varepsilon_{ij}\\) is the random error.\nfor a random intercept model, each individual will have a random intercept, but the sample slope.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Random Intercept Regression</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_regression_random_intercept.html#assumptions-of-a-random-effects-model",
    "href": "lessons_original/04_regression_random_intercept.html#assumptions-of-a-random-effects-model",
    "title": "27  Random Intercept Regression",
    "section": "27.6 Assumptions of a random effects model:",
    "text": "27.6 Assumptions of a random effects model:\n\nunobserved cluster effects is not correlated with observed variables (all \\(u_{ij}\\) terms are not correlated with the your predictors.)\nthe within and between effects are the same.\nyour error term is independent with your constant term.\nyou have homoscedasticity\n\\(b_i\\) and \\(\\varepsilon\\) are independent of each other",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Random Intercept Regression</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_regression_random_intercept.html#hypothesis-of-a-random-effects-model",
    "href": "lessons_original/04_regression_random_intercept.html#hypothesis-of-a-random-effects-model",
    "title": "27  Random Intercept Regression",
    "section": "27.7 hypothesis of a random effects model:",
    "text": "27.7 hypothesis of a random effects model:\nhypothesis testing for a random effects model runs as follows:\n\\[ H_0: \\sigma^2_u = 0\\] \\[H_1: \\sigma^2_u \\not = 0\\] the null hypothesis states that if \\(\\sigma^2_u\\) is true, then the random component is not needed in this model. so you can fit a single level regression model. to do this, you would can do a likelihood ratio test comparing the two model to see if sigma is significant. In other words, seeing if there is no difference in intercepts. If there is NO difference in intercepts (or the slopes are similar), then a random intercept model or random component is not needed.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Random Intercept Regression</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_regression_random_intercept.html#example-2-planktonic-larval-duration-pld",
    "href": "lessons_original/04_regression_random_intercept.html#example-2-planktonic-larval-duration-pld",
    "title": "27  Random Intercept Regression",
    "section": "27.8 Example 2: Planktonic larval duration (PLD)",
    "text": "27.8 Example 2: Planktonic larval duration (PLD)\nthis is example is from O’Connor et al (2007). A brief intro on this study, temperature is important for the development of organisms. In marine species, temperature is very important and linked to growth. So the time spent as a planktonic larvae can have associations on mortality and regulation on the species. Previous research has looked at the association between species comparison but not within species comparisons. What if we are interest in within and between species variation?\n\n27.8.1 load PLD data\n\nPLD &lt;- read_table(\"../data/04_PLD.txt\")\n\nI am curious about the structure of this data and how it briefly looks.\n\n#strcuture \nstr(PLD)\n\nspc_tbl_ [214 × 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ phylum : chr [1:214] \"Annelida\" \"Annelida\" \"Annelida\" \"Annelida\" ...\n $ species: chr [1:214] \"Circeus.spirillum\" \"Circeus.spirillum\" \"Circeus.spirillum\" \"Hydroides.elegans\" ...\n $ D.mode : chr [1:214] \"L\" \"L\" \"L\" \"P\" ...\n $ temp   : num [1:214] 5 10 15 15 20 25 30 5 10 17 ...\n $ pld    : num [1:214] 16 16 4 8 6.5 5 3.2 15 9.5 4.5 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   phylum = col_character(),\n  ..   species = col_character(),\n  ..   D.mode = col_character(),\n  ..   temp = col_double(),\n  ..   pld = col_double()\n  .. )\n\n# just the top - seeing how it looks\nhead(PLD)\n\n# A tibble: 6 × 5\n  phylum   species           D.mode  temp   pld\n  &lt;chr&gt;    &lt;chr&gt;             &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 Annelida Circeus.spirillum L          5  16  \n2 Annelida Circeus.spirillum L         10  16  \n3 Annelida Circeus.spirillum L         15   4  \n4 Annelida Hydroides.elegans P         15   8  \n5 Annelida Hydroides.elegans P         20   6.5\n6 Annelida Hydroides.elegans P         25   5  \n\n# brief summary\nsummary(PLD)\n\n    phylum            species             D.mode               temp      \n Length:214         Length:214         Length:214         Min.   : 2.50  \n Class :character   Class :character   Class :character   1st Qu.:12.28  \n Mode  :character   Mode  :character   Mode  :character   Median :20.00  \n                                                          Mean   :18.59  \n                                                          3rd Qu.:25.00  \n                                                          Max.   :32.00  \n      pld        \n Min.   :  1.00  \n 1st Qu.: 11.00  \n Median : 19.30  \n Mean   : 26.28  \n 3rd Qu.: 33.45  \n Max.   :129.00  \n\n\nI am curious about how this would look just plotting the variable pld or planktonic larvae duration and the temperature. So i am interested in seeing how the temperature is associated with their their survival duration.\n\n# how to plot in base R \nplot(pld ~ temp, data = PLD,\n     xlab = \"temperature (C)\",\n     ylab = \"PLD survival (Days)\",\n     pch = 16)\n# add lm line in base R\nabline(lm(pld ~ temp, data = PLD))\n\n\n\n\n\n\n\n\nyou can also do this in ggplot plot like so:\n\nggplot(data = PLD) +\n  aes(y = pld, x = temp) +\n  stat_smooth(method = \"lm\") +\n  geom_point() +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n27.8.2 Fit first model: Linear model\nLet’s first fit a linear model and check any assumptions. Why are we fitting a linear model first? It might be important to check the standard error of this model and compare to the next model, that might be a better fit later on.\n\n# fitting linear model first \n\nLinearModel_1 &lt;- lm(pld ~ temp, data = PLD)\n\nsummary(LinearModel_1)\n\n\nCall:\nlm(formula = pld ~ temp, data = PLD)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-44.158 -11.351  -0.430   7.684  93.884 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  54.8390     3.7954  14.449  &lt; 2e-16 ***\ntemp         -1.5361     0.1899  -8.087 4.65e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 20.36 on 212 degrees of freedom\nMultiple R-squared:  0.2358,    Adjusted R-squared:  0.2322 \nF-statistic:  65.4 on 1 and 212 DF,  p-value: 4.652e-14\n\n\nWe can fit this output in a gtsummary to make it nicer looking:\n\nLinearModel_1 |&gt; \n  tbl_regression()\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI1\np-value\n\n\n\n\ntemp\n-1.5\n-1.9, -1.2\n&lt;0.001\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\n\nI am interested in checking out visually, the equal variance (homoscedasticity) and so i will plot a a base residual graph:\n\n# better to do a scatter plot \nLinearModel_res &lt;- resid(LinearModel_1)\n\n# plot residual \nplot(PLD$temp, LinearModel_res,\n     ylab = \"residuals\",\n     xlab = \"temp\",\n     main = \"Residual graph\"\n     )\nabline(0,0)\n\n\n\n\n\n\n\n\nJust upon visual observation, it seems like this assumption may be violated so i think it might be important to do some transformations.\n\n\n27.8.3 Log transformation\n\nLinearMode_2Log &lt;- lm(log(pld) ~ log(temp), data= PLD)\n\nsummary(LinearMode_2Log)\n\n\nCall:\nlm(formula = log(pld) ~ log(temp), data = PLD)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0768 -0.3956  0.1802  0.5461  1.9656 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   4.6946     0.3128  15.011  &lt; 2e-16 ***\nlog(temp)    -0.6308     0.1093  -5.771 2.77e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8236 on 212 degrees of freedom\nMultiple R-squared:  0.1358,    Adjusted R-squared:  0.1317 \nF-statistic: 33.31 on 1 and 212 DF,  p-value: 2.767e-08\n\n\n\n\n27.8.4 residual of new log transformed graph\n\n# better to do a scatter plot \nLinearModel2_res &lt;- resid(LinearMode_2Log)\n\n# plot residual \nplot(PLD$temp, LinearModel2_res,\n     ylab = \"residuals(log)\",\n     xlab = \"temp(log)\",\n     main = \"Residual graph (log transformation)\"\n     )\nabline(0,0)\n\n\n\n\n\n\n\n\nA bit better! Now i kinda want to see the original plot i plotted with PLD and temperature:\n\nplot(log(pld) ~ log(temp), data = PLD,\n     xlab = \"Temperature in C\",\n     ylab = \"PLD in days\")\nabline(LinearMode_2Log)\n\n\n\n\n\n\n\n\nin ggplot you can use the facet_wrap() function to separate by phylum:\n\nggplot(data = PLD) +\n  aes(x = log(temp), y = log(pld)) +\n  geom_point() +\n  stat_smooth(method = \"lm\") +\n  facet_wrap(~phylum) +\n  theme_classic()",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Random Intercept Regression</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_regression_random_intercept.html#fitting-a-random-intercept-model-random-intercept-same-slope",
    "href": "lessons_original/04_regression_random_intercept.html#fitting-a-random-intercept-model-random-intercept-same-slope",
    "title": "27  Random Intercept Regression",
    "section": "27.9 Fitting a random intercept model (random intercept, same slope)",
    "text": "27.9 Fitting a random intercept model (random intercept, same slope)\nI am interested in seeing if the overall temperature and the PLD relationship is similar among species, but not the same. We are interested in plotting a mixed effects model with a random intercept but fixed/same slope. I am only interested in the species-specific plot for now with the phylum Mollusca.\n\n# filter to only mollusca\n\nMollusca_subset &lt;- \n  PLD |&gt; \n  filter(phylum == \"Mollusca\")\n\nggplot(data = Mollusca_subset) +\n  aes(x = log(pld), y = log(temp)) +\n  geom_point() +\n  labs(x = \"Log(temperature)\",\n       y = \"Log(PLD)\") + \n  stat_smooth(method = \"lm\", formula = \"y ~ x\", se = FALSE, fullrange = TRUE) +\n  theme_classic() +\n  facet_wrap(~ species)\n\n\n\n\n\n\n\n\n\n27.9.1 fitting model\nWe can use the library lme4 to fit a model of a linear regression with a random effect\n\n# creating log -transformed variables \nMollusca_subset$log_pld &lt;- log(Mollusca_subset$pld)\nMollusca_subset$log_temp &lt;- log(Mollusca_subset$temp)\n\n# mixed model with random intercept only \nRandIntModel_Mollusca &lt;- lmer(log_pld ~ log_temp + (1 | species), data = Mollusca_subset)\n\nsummary(RandIntModel_Mollusca)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: log_pld ~ log_temp + (1 | species)\n   Data: Mollusca_subset\n\nREML criterion at convergence: 43.7\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.37222 -0.32686 -0.09382  0.43821  2.34871 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n species  (Intercept) 0.86457  0.9298  \n Residual             0.03309  0.1819  \nNumber of obs: 44, groups:  species, 16\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)   7.1728     0.5284  13.575\nlog_temp     -1.5175     0.1592  -9.531\n\nCorrelation of Fixed Effects:\n         (Intr)\nlog_temp -0.896\n\n\n\nthe fixed effects: section in the output is the estimate for the fixed slope, and the grand mean of the intercept. in the fixed effects section the intercept here is the random effect, and you can see this\nin the Random effects section in this output, in this case our random effect was specified in the the Names sections, which tells us the parameter is the intercept.\nthe Group section tells us we have a random intercept for each species. We also have the variance and standard deviation for the random effects (as well as the residuals)\n\nSince we have a random effect at the individual level, we can subset this section out so it is clear to see that these organism will have a random intercept and fixed slope.\n\n# subset of the coefficients for random intercept and fixed slop\n\ncoef(RandIntModel_Mollusca)$species\n\n                      (Intercept) log_temp\nChlamys.hastata          7.612748 -1.51751\nCrassostrea.virginica    7.592515 -1.51751\nCrepidula.fornicata.     8.045609 -1.51751\nCrepidula.plana          8.063220 -1.51751\nHaliotis.asinina         5.807247 -1.51751\nHaliotis.fulgens         6.083069 -1.51751\nHaliotis.sorenseni.      6.688210 -1.51751\nMactra.solidissima       7.589660 -1.51751\nMopalia.muscosa          7.061086 -1.51751\nMytilus.edulis           7.141801 -1.51751\nNassarius.obsoletus      7.370159 -1.51751\nOstrea.lurida            6.967626 -1.51751\nPerna.viridis            8.144314 -1.51751\nStrombus.gigas           8.048942 -1.51751\nTivela.mactroides        7.677603 -1.51751\nTonicella.lineata        4.871674 -1.51751\n\n\nSo now we can see that in the Mollusca subset, we have all random intercepts for individual specifies, but the same slope.\n\n\n27.9.2 Inter class correlation coefficient (ICC)\nfor a random intercept model, we can run a diagnostic called the inter-class correlation coefficient (ICC), which lets us know how much group specific information is available for the random effect. this is somewhat similar to the ANOVA, in which it looks at the variability within groups compared to the variability between groups. Low ICC means that observation within group don’t really cluster.\n\\[ ICC = {\\sigma^2_{\\alpha} \\over \\sigma^2 + \\sigma^2_\\alpha} \\]\n\n# creating data frame\nvar &lt;- as.data.frame(VarCorr(RandIntModel_Mollusca))\n\n#check our data frame\nvar\n\n       grp        var1 var2       vcov     sdcor\n1  species (Intercept) &lt;NA&gt; 0.86457141 0.9298233\n2 Residual        &lt;NA&gt; &lt;NA&gt; 0.03309183 0.1819116\n\n#ICC equation\nICC &lt;- var$vcov[1] / (var$vcov[1] + var$vcov[2])\n\n# ICC value \nICC\n\n[1] 0.9631356\n\n\nIn our model, the \\(\\sigma^2_{\\alpha}\\) is 0.8645 (also the vcov part) and the \\(\\sigma^2\\) is 0.033. so once we do the mathematics we get 0.9631. Which is the proportion of the total variance in Y that is accounted for by clustering. This is a high value and therefore, suggesting we have within-group variability, so it might be good we are running this random effects model.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Random Intercept Regression</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_regression_random_intercept.html#interpretation-of-results",
    "href": "lessons_original/04_regression_random_intercept.html#interpretation-of-results",
    "title": "27  Random Intercept Regression",
    "section": "27.10 Interpretation of results",
    "text": "27.10 Interpretation of results\n\nsummary(RandIntModel_Mollusca)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: log_pld ~ log_temp + (1 | species)\n   Data: Mollusca_subset\n\nREML criterion at convergence: 43.7\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.37222 -0.32686 -0.09382  0.43821  2.34871 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n species  (Intercept) 0.86457  0.9298  \n Residual             0.03309  0.1819  \nNumber of obs: 44, groups:  species, 16\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)   7.1728     0.5284  13.575\nlog_temp     -1.5175     0.1592  -9.531\n\nCorrelation of Fixed Effects:\n         (Intr)\nlog_temp -0.896\n\n\nInterpretation: Summary of this PLD data includes information about the random effects. Here we can see that the column groups shows the random effect variable. in the name section, you can see that the random effect is our intercept. so we have the variation due to the species. in the Residuals section, this is the variation that cannot be explained by the model (the error). As you will notice our Standard error is smaller compared to the ordinary regression we ran in the previous one. Standard error for this model is 0.15 and the previous standard error for the first model we ran was 0.18.\nSo 0.86 / 0.86 + 0.03 = 0.96 , so the difference between between species can explain 96% of the variance that is is left over after the variance is explained by our fixed effect. since the random effects of the species explain most.\nthere is a very long description on the why the lmer() function doesn’t include the p-value that can be found here.\ninterpretation of temp variable for the fixed part, we can interpret this parameter the same as a single-level regression model, so \\(\\beta_1\\) is the increase/decrease in response for 1 unit increase/decrease in \\(x.\\) In other words, for one unit increase in the degrees of temperature, there is a -1.5 decrease in Planktonic larval duration. (or, as the temperature increase, the plankton duration is lower.)",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Random Intercept Regression</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_regression_random_intercept.html#conclusion",
    "href": "lessons_original/04_regression_random_intercept.html#conclusion",
    "title": "27  Random Intercept Regression",
    "section": "27.11 Conclusion",
    "text": "27.11 Conclusion\nIn this lecture you learned about the importance of a random intercept model, when it is appropriate to use a random intercept model, the difference between an ordinary single-level model, and a random intercept model, the assumptions of the random intercept model, hypothesis testing for the variation, the Interclass correlation coefficient (ICC) and finally, how to interpret results from the fixed part and the random part of a random intercept model.",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Random Intercept Regression</span>"
    ]
  },
  {
    "objectID": "lessons_original/04_regression_random_intercept.html#references",
    "href": "lessons_original/04_regression_random_intercept.html#references",
    "title": "27  Random Intercept Regression",
    "section": "27.12 References",
    "text": "27.12 References\n\nAbedin, Jaynal, and Kishor Kumar Das. 2015. Data Manipulation with r. Packt Publishing Ltd.\nAnnesley, Thomas M. 2010. “Bars and Pies Make Better Desserts Than Figures.” Clinical Chemistry 56 (9): 1394–1400.\nAnscombe, Francis J. 1973. “Graphs in Statistical Analysis.” The American Statistician 27 (1): 17–21.\nBorer, Elizabeth T, Eric W Seabloom, Matthew B Jones, and Mark Schildhauer. 2009. “Some Simple Guidelines for Effective Data Management.” The Bulletin of the Ecological Society of America 90 (2): 205–14.\nBorghi, John, Stephen Abrams, Daniella Lowenberg, Stephanie Simms, and John Chodacki. 2018. “Support Your Data: A Research Data Management Guide for Researchers.” Research Ideas and Outcomes 4: e26439.\nBroman, Karl W, and Kara H Woo. 2018. “Data Organization in Spreadsheets.” The American Statistician 72 (1): 2–10.\nChamberlin, Thomas C. 1890. “The Method of Multiple Working Hypotheses.” Science 15 (366): 92–96.\nsupplementary material for Tom Snijders and Roel Bosker textbook - Shjders, T Bosker R, 1999. Multilevel analysis: an introduction to basic and advanced mltilevel modeling, London, Sage, including updates and corrections data set examples http://stat.gamma.rug.nl/multilevel.htm\nUniversity of Bristol, Random Intercept model, (2018). http://www.bristol.ac.uk/cmm/learning/videos/random-intercepts.html\nMidway, S. (2019). “Data Analysis in R.” https://bookdown.org/steve_midway/DAR/random-effects.html",
    "crumbs": [
      "ANOVA and Regression",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Random Intercept Regression</span>"
    ]
  },
  {
    "objectID": "05_header_generalized-linear-models.html",
    "href": "05_header_generalized-linear-models.html",
    "title": "Generalized Linear Models",
    "section": "",
    "text": "Text Outline\nThis text covers the basics of:",
    "crumbs": [
      "Generalized Linear Models"
    ]
  },
  {
    "objectID": "05_header_generalized-linear-models.html#text-outline",
    "href": "05_header_generalized-linear-models.html#text-outline",
    "title": "Generalized Linear Models",
    "section": "",
    "text": "Exploring data\nOne-sample tests\nTwo-sample tests\nANOVA and linear regression\nGeneralized linear models\nSome special topics\nMaybe a bit about power calculations…",
    "crumbs": [
      "Generalized Linear Models"
    ]
  },
  {
    "objectID": "05_header_generalized-linear-models.html#part-outline",
    "href": "05_header_generalized-linear-models.html#part-outline",
    "title": "Generalized Linear Models",
    "section": "Part Outline",
    "text": "Part Outline\nThis part of the text contains the chapters on various different models within the GLM family:\n\nGeneralized Linear Models: Binary\nGeneralized Linear Models: Ordered\nGeneralized Linear Models: Count (Poisson)\nGeneralized Linear Models: Count (Negative Binomial)",
    "crumbs": [
      "Generalized Linear Models"
    ]
  },
  {
    "objectID": "lessons/05_glm_logistic.html",
    "href": "lessons/05_glm_logistic.html",
    "title": "\n28  Generalized Linear Models for Binary Response\n",
    "section": "",
    "text": "28.1 Generalized Liner models(binary) or Logistic Regression\nA type of Generalized Linear Model (GLM) used to model binary outcomes.",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Generalized Linear Models for Binary Response</span>"
    ]
  },
  {
    "objectID": "lessons/05_glm_logistic.html#generalized-liner-modelsbinary-or-logistic-regression",
    "href": "lessons/05_glm_logistic.html#generalized-liner-modelsbinary-or-logistic-regression",
    "title": "\n28  Generalized Linear Models for Binary Response\n",
    "section": "",
    "text": "Dependent Variable: Binary (e.g., 0 or 1).\nIndependent Variables: One or more predictor variables that can be continuous or categorical.",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Generalized Linear Models for Binary Response</span>"
    ]
  },
  {
    "objectID": "lessons/05_glm_logistic.html#purpose",
    "href": "lessons/05_glm_logistic.html#purpose",
    "title": "\n28  Generalized Linear Models for Binary Response\n",
    "section": "\n28.2 Purpose:",
    "text": "28.2 Purpose:\nThis regression models the relationship between a set of predictor variables and a binary response variable. Commonly used for classification problems where the outcome is categorical with two possible values (e.g., yes/no, success/failure).",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Generalized Linear Models for Binary Response</span>"
    ]
  },
  {
    "objectID": "lessons/05_glm_logistic.html#understanding-the-function",
    "href": "lessons/05_glm_logistic.html#understanding-the-function",
    "title": "\n28  Generalized Linear Models for Binary Response\n",
    "section": "\n28.3 Understanding the function",
    "text": "28.3 Understanding the function\nLogit Function: Links the linear combination of predictors to the probability of the outcome. The logit function is defined as:\n\\[\n\\text{logit}(p) := \\log\\left(\\frac{p}{1 - p}\\right).\n\\]\nSo our regression equation is thus written as:\n\\[\n\\log\\left(\\frac{p}{1 - p}\\right) =\n  \\beta_0 + \\beta_1X_1 + \\ldots + \\beta_pX_p.\n\\]\nThe probability of the outcome being 1 (success) is given by:\n\\[\np = \\frac{1}{1 + \\exp\\left[-(\\beta_0 + \\beta_1X_1 + \\ldots + \\beta_pX_p)\\right]}.\n\\]\n\n28.3.1 Interpret and Coefficients:\nIntercept (\\(\\beta_0\\)): The log-odds of the outcome when all predictors are zero.\nCoefficients (\\(\\beta_i\\)): The change in log-odds of the outcome for a one-unit increase in the predictor\n\n28.3.2 Odds and Odds Ratio:\nOdds: The ratio of the probability of the event occurring to the probability of it not occurring.\nOdds Ratio: The ratio of the odds of the outcome occurring for different values of a predictor.",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Generalized Linear Models for Binary Response</span>"
    ]
  },
  {
    "objectID": "lessons/05_glm_logistic.html#assumptions",
    "href": "lessons/05_glm_logistic.html#assumptions",
    "title": "\n28  Generalized Linear Models for Binary Response\n",
    "section": "\n28.4 Assumptions:",
    "text": "28.4 Assumptions:\nParameters are estimated using Maximum Likelihood Estimation (MLE). The goal is to find the values of the coefficients that maximize the likelihood of observing the given data. We assume that:\n\n\nObservations are independent: There is a linear relationship between the logit of the outcome and the predictors.\n\nNo multicollinearity among the predictors. The sample size is sufficiently large.",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Generalized Linear Models for Binary Response</span>"
    ]
  },
  {
    "objectID": "lessons/05_glm_logistic.html#data-analysis",
    "href": "lessons/05_glm_logistic.html#data-analysis",
    "title": "\n28  Generalized Linear Models for Binary Response\n",
    "section": "\n28.5 Data Analysis",
    "text": "28.5 Data Analysis\nWe will use the train.csv dataset from Kaggle’s: Machine Learning from Disaster competition.link\n\n# Load the Titanic train dataset\ntitanic_train_data &lt;- read_csv(\"../data/05_titanic_train.csv\")\n\nRows: 891 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): Name, Sex, Ticket, Cabin, Embarked\ndbl (7): PassengerId, Survived, Pclass, Age, SibSp, Parch, Fare\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Display the first few rows of the dataset\nhead(titanic_train_data)\n\n# A tibble: 6 × 12\n  PassengerId Survived Pclass Name    Sex     Age SibSp Parch Ticket  Fare Cabin\n        &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;\n1           1        0      3 Braund… male     22     1     0 A/5 2…  7.25 &lt;NA&gt; \n2           2        1      1 Cuming… fema…    38     1     0 PC 17… 71.3  C85  \n3           3        1      3 Heikki… fema…    26     0     0 STON/…  7.92 &lt;NA&gt; \n4           4        1      1 Futrel… fema…    35     1     0 113803 53.1  C123 \n5           5        0      3 Allen,… male     35     0     0 373450  8.05 &lt;NA&gt; \n6           6        0      3 Moran,… male     NA     0     0 330877  8.46 &lt;NA&gt; \n# ℹ 1 more variable: Embarked &lt;chr&gt;\n\n\n\n# Handle missing values by removing rows with NA values in 'Age' and 'Embarked' columns\ntitanic_train_data &lt;- \n  titanic_train_data %&gt;%\n  filter(!is.na(Age) & !is.na(Embarked))\n\n# Convert necessary columns to factors\ntitanic_train_data$Pclass &lt;- factor(titanic_train_data$Pclass)\ntitanic_train_data$Sex &lt;- factor(titanic_train_data$Sex)\ntitanic_train_data$Embarked &lt;- factor(titanic_train_data$Embarked)\ntitanic_train_data$Survived &lt;- factor(titanic_train_data$Survived, levels = c(0, 1))\n\n\n28.5.1 Fit the Logistic Regression Model\nFit a logistic regression model using Survived as the response variable and Pclass, Age, Sex, and Embarked as predictors.\n\n# Fit the logistic regression model\nmodel &lt;- glm(\n  Survived ~ Pclass + Age + Sex + Embarked, \n  data = titanic_train_data, \n  family = binomial\n)\n\n# Display the summary of the model\nsummary(model)\n\n\nCall:\nglm(formula = Survived ~ Pclass + Age + Sex + Embarked, family = binomial, \n    data = titanic_train_data)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  4.036825   0.430758   9.371  &lt; 2e-16 ***\nPclass2     -1.144614   0.290678  -3.938 8.23e-05 ***\nPclass3     -2.409565   0.291179  -8.275  &lt; 2e-16 ***\nAge         -0.036082   0.007715  -4.677 2.92e-06 ***\nSexmale     -2.515793   0.209293 -12.020  &lt; 2e-16 ***\nEmbarkedQ   -0.814190   0.567903  -1.434   0.1517    \nEmbarkedS   -0.493651   0.266886  -1.850   0.0644 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 960.90  on 711  degrees of freedom\nResidual deviance: 642.68  on 705  degrees of freedom\nAIC: 656.68\n\nNumber of Fisher Scoring iterations: 5\n\n\nModel Summary Interpretation:\nIntercept: The estimated intercept is 4.036825 with a standard error of 0.430758. This represents the log-odds of survival for a baseline group (first-class female passengers embarked from port ‘C’ with age = 0).\nPclass2: The coefficient is -1.144614 with a standard error of 0.290678. This indicates that being in the second class decreases the log-odds of survival by 1.144614 compared to being in the first class. The p-value (8.23e-05) is less than 0.05, indicating statistical significance.\nPclass3: The coefficient is -2.409565 with a standard error of 0.291179. This indicates that being in the third class decreases the log-odds of survival by 2.409565 compared to being in the first class. The p-value (&lt; 2e-16) is very small, indicating strong statistical significance.\nAge: The coefficient is -0.036082 with a standard error of 0.007715. This indicates that each additional year of age decreases the log-odds of survival by 0.036082. The p-value (2.92e-06) is less than 0.05, indicating statistical significance.\nSexmale: The coefficient is -2.515793 with a standard error of 0.209293. This indicates that being male decreases the log-odds of survival by 2.515793 compared to being female. The p-value (&lt; 2e-16) is very small, indicating strong statistical significance.\nEmbarkedQ: The coefficient is -0.814190 with a standard error of 0.567903. This indicates that embarking from port ‘Q’ decreases the log-odds of survival by 0.814190 compared to embarking from port ‘C’. The p-value (0.1517) is greater than 0.05, indicating that this effect is not statistically significant.\nEmbarkedS: The coefficient is -0.493651 with a standard error of 0.266886. This indicates that embarking from port ‘S’ decreases the log-odds of survival by 0.493651 compared to embarking from port ‘C’. The p-value (0.0644) is slightly greater than 0.05, indicating marginal significance.\n*Pclass, Age, and Sex are statistically significant in predicting survival.\nWe perform the Paris Plot to shows the relationship between the predicted probabilities and the empirical probabilities.\n\n# Predict probabilities\npredicted_probs &lt;- predict(model, type = \"response\")\n\n# Create the Paris plot\nempirical_probs &lt;- ecdf(predicted_probs)\nsorted_probs &lt;- sort(predicted_probs)\nplot(sorted_probs, empirical_probs(sorted_probs), type = \"l\", \n     xlab = \"Predicted probability\", ylab = \"Empirical probability\", \n     main = \"Paris Plot for Logistic Regression\")\ngrid()\n\n\n\n\n\n\n\nThe Paris plot shows that the logistic regression model is generally effective in predicting survival on the Titanic dataset. The predicted probabilities closely follow the empirical probabilities, especially at the extremes (very low and very high predicted probabilities).  There may be some room for improvement in the middle range of predicted probabilities, where the line is not as steep. This could indicate that the model might benefit from additional predictors or different modeling techniques to improve accuracy.\n\n28.5.2 Evaluate the model performance\nWe can visualize the ROC curve and calculate the AUC to evaluate the model’s performance.\n\n# Compute the ROC curve\nroc_curve &lt;- roc(titanic_train_data$Survived, predict(model, type = \"response\"))\n\nSetting levels: control = 0, case = 1\n\n\nSetting direction: controls &lt; cases\n\n# Plot the ROC curve\nplot(roc_curve, main = \"ROC Curve for Logistic Regression Model\")\n\n\n\n\n\n\n# Calculate AUC\nauc(roc_curve)\n\nArea under the curve: 0.8544\n\n\nThe ROC curve is well above the diagonal line and closer to the top-left corner, indicating that the model has good discriminatory power. This means the model is effective in distinguishing between survivors and non-survivors.\nAn AUC of 0.8544 falls within the “excellent” range. This means this logistic regression model has a high ability to distinguish between survivors and non-survivors on the Titanic dataset. &gt;AUC Value Range:\n\nAUC value: AUC ranges from 0.5 to 1.0.  value of 0.5 indicating that the test is no better than chance at distinguishing between diseased and nondiseased individuals.  A value of 1.0 indicates perfect discrimination.  AUC values above 0.80 are generally consideredclinically useful.  AUC values below 0.80 are considered of limited clinical utility.  When interpreting AUC values, it is important to consider the 95% confidence interval. The confidence interval reflects the uncertainty around the AUC value.A narrow confidence interval indicates that the AUC value is likely accurate, while a wide confidence interval indicates that the AUC value is less reliable.Çorbacıoğlu ŞK,etc. 2023",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Generalized Linear Models for Binary Response</span>"
    ]
  },
  {
    "objectID": "lessons/05_glm_poisson.html",
    "href": "lessons/05_glm_poisson.html",
    "title": "29  Poisson Regression Model",
    "section": "",
    "text": "29.1 Libraries for this lesson\n# Installing Required Packages\n# intsall.packages(readxl)\n# install.packages(skimr)\n# intsall.packages(gtsummary)\n# intsall.packages(GGally)\n# install.packages(epiDisplay)\n# install.packages(broom)\n# install.packages(tidyverse)\n\n# Loading Required Packages\nlibrary(readxl)\nlibrary(skimr)\nlibrary(gtsummary)\nlibrary(GGally)\nlibrary(epiDisplay)\nlibrary(broom)\nlibrary(tidyverse)",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Poisson Regression Model</span>"
    ]
  },
  {
    "objectID": "lessons/05_glm_poisson.html#introduction-to-poisson-regression-model",
    "href": "lessons/05_glm_poisson.html#introduction-to-poisson-regression-model",
    "title": "29  Poisson Regression Model",
    "section": "29.2 Introduction to Poisson Regression Model",
    "text": "29.2 Introduction to Poisson Regression Model\nThe Poisson regression model (PRM) is an appropriate model for studying counts response variable, which follows the Poisson distribution. Thus, the values of the response variable are non-negative integers. It is a type of Generalized linear models (GLM) whenever the outcome is count. It also accommodates rate data as we will see shortly. Although count and rate data are very common in medical and health sciences. For instance, how the colony counts of bacteria are associated with different environmental conditions and dilutions. Another example related to vital statistics, which is related to infant mortality or cancer incidence among groups with different demographics. In such scenarios, the benchmark model PRM is more appropriate than the linear regression model (LRM).\nBasically, Poisson regression models the linear relationship between:\n\noutcome: count variable (e.g. the number of hospital admissions, parity, cancerous lesions, asthmatic attacks). This is transformed into the natural log scale.\npredictors/independent variables: numerical variables (e.g. age, blood pressure, income) and categorical variables (e.g. gender, race, education level).\n\nFor example, we might be interested in knowing the relationship between the number of asthmatic attacks in the past one year with sociodemographic factors. This relationship can be explored by a Poisson regression analysis.\nWe know that logistic regression allows us to obtain the odds ratio, which is approximately the relative risk given a predictor. For Poisson regression, by taking the exponent of the coefficient, we obtain the rate ratio RR (also known as incidence rate ratio IRR),\n\\[\nRR = \\exp(b_p)\n\\]\nfor the coefficient \\(b_p\\) of the p’s predictor. This is interpreted in similar way to the odds ratio for logistic regression, which is approximately the relative risk given a predictor.",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Poisson Regression Model</span>"
    ]
  },
  {
    "objectID": "lessons/05_glm_poisson.html#mathematical-formulation-of-the-model",
    "href": "lessons/05_glm_poisson.html#mathematical-formulation-of-the-model",
    "title": "29  Poisson Regression Model",
    "section": "29.3 Mathematical Formulation of the Model",
    "text": "29.3 Mathematical Formulation of the Model\nThe Poisson distribution for a random variable Y has the following probability mass function for a given value Y = y \\[\nP(Y|y=\\lambda) = \\frac{e^{-\\lambda}\\lambda^y}{y!},\n\\] for \\(y = 0, 1, 2, \\dots\\). The Poisson distribution is characterized by the single parameter \\(\\lambda\\), which is the mean rate of occurrence for the event being measured. For the Poisson distribution, it is assumed that large counts (with respect to the value of \\(\\lambda\\)) are rare. And the rate \\(\\lambda\\) is determined by a set of \\(k\\) predictors \\(X = (X_1,\\dots, X_k)\\). The expression relating these quantities is \\[\n\\lambda = \\exp(\\beta)\n\\]\nThus, for observation i the simple model model for Poisson rate parameter \\(\\lambda_i\\) is given by\n\\[\n\\log \\lambda_i = \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip}\n\\]\nor equivalently\n\\[\n\\lambda_i = e^{\\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip}}\n\\]\nTogether with the distributional assumption \\(Y_i ∼ Poisson(\\lambda_i)\\), this is also called the Poisson log-linear model,\nIn Generalized Linear Model, response variable usually originates in the form of pdf which generally fits to the exponential family (EF) of distribution. Since Poisson distribution is the member of EF. So, we define the density function of EF as\n\\[\nf(y_i, \\theta_i, \\lambda) =\n  \\exp\\left[\\frac{y_i\\theta_i - b(\\theta_i)}{\\alpha(\\lambda)} + c(y_i, \\lambda)\\right], i = 1, 2, \\dots,n\n\\] Here,\n\n\\(\\theta_i\\) represents the link function\n\\(b(\\theta_i)\\) is the cumulant\n\\(\\alpha(\\lambda)\\) is the dispersion parameter\n\\(c(y_i, \\lambda)\\) is the normalization term\n\nSince the value of dispersion parameter for the Poisson distribution is one. The PRM is generally applied in the situations, when the response \\(y_i\\) is in the form of counts, that is, \\(y_i = 0, 1, 2, \\dots\\) and distributed as \\(P(\\mu_i)\\), where \\(\\mu_i = exp ( x^T_i \\beta)\\) and \\(log (\\mu_i) = x^T_i \\beta\\), while \\(x_i\\) denotes the \\(i\\)th row of design matrix \\(X\\) having order \\(n \\times r\\) while \\(\\beta\\) is a coefficient vector of order \\(r \\times 1\\), where \\(r = p + 1\\) are the explanatory variables.",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Poisson Regression Model</span>"
    ]
  },
  {
    "objectID": "lessons/05_glm_poisson.html#example",
    "href": "lessons/05_glm_poisson.html#example",
    "title": "29  Poisson Regression Model",
    "section": "29.4 Example",
    "text": "29.4 Example\nTo demonstrate the method of PRM, we consider the asthma attack data set(https://github.com/drkamarul/multivar_data_analysis/tree/main/data). The data on the number of asthmatic attacks per year among a sample of 120 patients and the associated factors are given in 05_asthma.csv.\nThe dataset contains four variables:\n\ngender: Gender of the subjects (categorical) {male, female}.\nres_inf: Recurrent respiratory infection (categorical) {no, yes}.\nghq12: General Health Questionnare 12 (GHQ-12) score of psychological well being (numerical) {0 to 36}.\nattack: Number of athmatic attack per year (count).\n\n\n29.4.1 Exploring the data\nLet’s begin by looking at the data.\n\nasthma &lt;- \n  read_csv(\"../data/05_asthma.csv\")\n\nRows: 120 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): gender, res_inf\ndbl (2): ghq12, attack\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nasthma %&gt;%\n  head(n = 10)\n\n# A tibble: 10 × 4\n   gender res_inf ghq12 attack\n   &lt;chr&gt;  &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1 female yes        21      6\n 2 male   no         17      4\n 3 male   yes        30      8\n 4 female yes        22      5\n 5 male   yes        27      2\n 6 male   yes        33      3\n 7 female yes        24      2\n 8 female yes        23      1\n 9 female yes        25      2\n10 male   no         28      2\n\n\n\n29.4.1.1 Structure of the dataset\n\nstr(asthma)\n\nspc_tbl_ [120 × 4] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ gender : chr [1:120] \"female\" \"male\" \"male\" \"female\" ...\n $ res_inf: chr [1:120] \"yes\" \"no\" \"yes\" \"yes\" ...\n $ ghq12  : num [1:120] 21 17 30 22 27 33 24 23 25 28 ...\n $ attack : num [1:120] 6 4 8 5 2 3 2 1 2 2 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   gender = col_character(),\n  ..   res_inf = col_character(),\n  ..   ghq12 = col_double(),\n  ..   attack = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\n\n\n29.4.1.2 Summary\n\nskim(asthma)\n\n\nData summary\n\n\nName\nasthma\n\n\nNumber of rows\n120\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ngender\n0\n1\n4\n6\n0\n2\n0\n\n\nres_inf\n0\n1\n2\n3\n0\n2\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nghq12\n0\n1\n16.34\n9.81\n0\n7\n19\n25\n33\n▆▅▃▇▅\n\n\nattack\n0\n1\n2.46\n2.01\n0\n1\n2\n4\n9\n▇▇▅▁▁\n\n\n\n\n\n\n\n29.4.1.3 Descriptives\n\nasthma %&gt;%\n  tbl_summary()\n\n\n\nTable 29.1: Asthma Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nN = 1201\n\n\n\n\ngender\n\n\n\n\n    female\n67 (56%)\n\n\n    male\n53 (44%)\n\n\nres_inf\n69 (58%)\n\n\nghq12\n19 (7, 25)\n\n\nattack\n2.00 (1.00, 4.00)\n\n\n\n1 n (%); Median (IQR)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n29.4.1.4 Pairs\n\nggpairs(asthma)\n\n\n\n\n\n\n\n\n\n\n\n29.4.2 Fitting the Poisson Regression Model\nUsing the glm() function to fit a Poisson regression model. The model formula specified attack as the response variable and gender(gender), recurrent respiratory infection (res_inf) and, GHQ12 (ghq12) as predictor variables.\n\n29.4.2.1 Univariate Analysis\n\npois_attack1 &lt;-  glm(attack ~ gender, data = asthma, family = \"poisson\")\nsummary(pois_attack1)\n\n\nCall:\nglm(formula = attack ~ gender, family = \"poisson\", data = asthma)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  1.02105    0.07332  13.925   &lt;2e-16 ***\ngendermale  -0.30000    0.12063  -2.487   0.0129 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 229.56  on 119  degrees of freedom\nResidual deviance: 223.23  on 118  degrees of freedom\nAIC: 500.3\n\nNumber of Fisher Scoring iterations: 5\n\n\n\npois_attack2 &lt;- glm(attack ~ res_inf, data = asthma, family = \"poisson\")\nsummary(pois_attack2)\n\n\nCall:\nglm(formula = attack ~ res_inf, family = \"poisson\", data = asthma)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   0.2877     0.1213   2.372   0.0177 *  \nres_infyes    0.9032     0.1382   6.533 6.44e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 229.56  on 119  degrees of freedom\nResidual deviance: 180.49  on 118  degrees of freedom\nAIC: 457.56\n\nNumber of Fisher Scoring iterations: 5\n\n\n\npois_attack3 &lt;- glm(attack ~ ghq12, data = asthma, family = \"poisson\")\nsummary(pois_attack3)\n\n\nCall:\nglm(formula = attack ~ ghq12, family = \"poisson\", data = asthma)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -0.230923   0.159128  -1.451    0.147    \nghq12        0.059500   0.006919   8.599   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 229.56  on 119  degrees of freedom\nResidual deviance: 145.13  on 118  degrees of freedom\nAIC: 422.2\n\nNumber of Fisher Scoring iterations: 5\n\n\nFrom the outputs, all variables are important with p &lt; .05. These variables are the candidates for inclusion in the multivariable analysis. However, as a reminder, in the context of confirmatory research, the variables that we want to include must consider expert judgement.\n\n\n29.4.2.2 Multivariate Analysis\nFor the multivariable analysis, we included all variables as predictors of attack. Here we use dot . as a shortcut for all variables when specifying the right-hand side of the formula of the glm.\n\npois_attack_all &lt;- glm(attack ~ ., data = asthma, family = \"poisson\")\nsummary(pois_attack_all)\n\n\nCall:\nglm(formula = attack ~ ., family = \"poisson\", data = asthma)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -0.315387   0.183500  -1.719  0.08566 .  \ngendermale  -0.041905   0.122469  -0.342  0.73222    \nres_infyes   0.426431   0.152859   2.790  0.00528 ** \nghq12        0.049508   0.007878   6.285 3.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 229.56  on 119  degrees of freedom\nResidual deviance: 136.68  on 116  degrees of freedom\nAIC: 417.75\n\nNumber of Fisher Scoring iterations: 5\n\n\nFrom the output, we noted that gender is not significant with P &gt; 0.05, although it was significant at the univariable analysis.\nFrom the above pairs graph, we can see that there is a relation ship between varibale gender and ghq12. The left side of the graph shows that high density between 20 and 30 but when change the gender and go to the right side, we can see that there is high density between 0 and 10. So both variables are related here and we can keep only one and R choose the variable ghq12 and remove the gender variable.\nNow, we fit a model excluding gender,\n\npois_attack_reduced &lt;- glm(\n  attack ~ res_inf + ghq12, data = asthma, family = \"poisson\"\n)\nsummary(pois_attack_reduced)\n\n\nCall:\nglm(formula = attack ~ res_inf + ghq12, family = \"poisson\", data = asthma)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -0.34051    0.16823  -2.024  0.04296 *  \nres_infyes   0.42816    0.15282   2.802  0.00508 ** \nghq12        0.04989    0.00779   6.404 1.51e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 229.56  on 119  degrees of freedom\nResidual deviance: 136.80  on 117  degrees of freedom\nAIC: 415.86\n\nNumber of Fisher Scoring iterations: 5\n\n\nFrom the output, both variables are significant predictors of asthmatic attack (or more accurately the natural log of the count of asthmatic attack). This serves as our preliminary model.\n\n\n\n29.4.3 Model Fit Assessment\nFor Poisson regression, we assess the model fit by chi-square goodness-of-fit test, model-to-model AIC comparison and scaled Pearson chi-square statistic. We also assess the regression diagnostics using standardized residuals.\n\n29.4.3.1 Chi-square goodness-of-fit\nChi-square goodness-of-fit test can be performed using poisgof() function in epiDisplay package. Note that, instead of using Pearson chi-square statistic, it utilizes residual deviance with its respective degrees of freedom (df) (e.g. from the output of summary(pois_attack_reduced) above). A p-value &gt; 0.05 indicates good model fit.\n\npoisgof(pois_attack_reduced)\n\n$results\n[1] \"Goodness-of-fit test for Poisson assumption\"\n\n$chisq\n[1] 136.7964\n\n$df\n[1] 117\n\n$p.value\n[1] 0.101934\n\n\n\n\n29.4.3.2 Model-to-model AIC comparison\nWe may also compare the models that we fit so far by Akaike information criterion (AIC). Recall that R uses AIC for stepwise automatic variable selection, which was explained in Linear Regression chapter.\n\nAIC(\n  pois_attack1, pois_attack2, pois_attack3,\n  pois_attack_all, pois_attack_reduced\n)\n\n                    df      AIC\npois_attack1         2 500.3009\npois_attack2         2 457.5555\npois_attack3         2 422.1997\npois_attack_all      4 417.7474\npois_attack_reduced  3 415.8649\n\n\nThe best model is the one with the lowest AIC, which is the model model with the multivariate analysis without gender.\n\n\n\n29.4.4 Interpreting the Results\nAfter all these assumption check points, we decide on the final model and rename the model for easier reference.\n\npois_attack_final &lt;- pois_attack_reduced\n\nWe use tbl_regression() to come up with a table for the results. Here, for interpretation, we exponent the coefficients to obtain the incidence rate ratio, IRR.\n\ntbl_regression(pois_attack_final, exponentiate = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nIRR1\n95% CI1\np-value\n\n\n\n\nres_inf\n\n\n\n\n\n\n\n\n    no\n—\n—\n\n\n\n\n    yes\n1.53\n1.14, 2.08\n0.005\n\n\nghq12\n1.05\n1.04, 1.07\n&lt;0.001\n\n\n\n1 IRR = Incidence Rate Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n\n\nBased on this table, we may interpret the results as follows:\n\nThose with recurrent respiratory infection are at higher risk of having an asthmatic attack with an IRR of 1.53 (95% CI: 1.14, 2.08), while controlling for the effect of GHQ-12 score.\nAn increase in GHQ-12 score by one mark increases the risk of having an asthmatic attack by 1.05 (95% CI: 1.04, 1.07), while controlling for the effect of recurrent respiratory infection.\n\nWe can also view and save the output in a format suitable for exporting to the spreadsheet format for later use. We use tidy() function for the job,\n\ntib_pois_attack &lt;- tidy(\n  pois_attack_final, exponentiate = TRUE, conf.int = TRUE\n)\ntib_pois_attack\n\n# A tibble: 3 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    0.711   0.168       -2.02 4.30e- 2    0.505     0.978\n2 res_infyes     1.53    0.153        2.80 5.08e- 3    1.14      2.08 \n3 ghq12          1.05    0.00779      6.40 1.51e-10    1.04      1.07 \n\n\nThen, we display the coefficients (i.e. without the exponent) and transfer the values into an equation,\n\nround(summary(pois_attack_final)$coefficients, 2)\n\n            Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)    -0.34       0.17   -2.02     0.04\nres_infyes      0.43       0.15    2.80     0.01\nghq12           0.05       0.01    6.40     0.00\n\n\n\\[\n\\ln(\\text{attack}) = - 0.34 + 0.43 \\text{res} + 0.05 \\text{ghq12}\n\\]\n\nintercept: when all predictors are zero, the expected count of the response variable is \\(\\exp(-0.34) \\approx 0.711\\).\nres_inf: when variable changes (holding ghq12 constant), the expected count of the response variable is multiplied by \\(\\exp(0.43) \\approx 1.53\\). This means that the presence of res_inf when (res_inf = Yes) increases the expected count by approximately 53% compared to when res_inf is absent (when res_inf = No).\nghq12: For each one unit increase in ghq12 (holding res_inf constant), the expected count of the response variable is multiplied by \\(\\exp(0.05) \\approx 1.05\\). This indicates that each unit increase in ghq12 is associated with a 5% increase in the expected count.\n\n\n\n29.4.5 Prediction\nWe can use the final model above for prediction. Relevant to our data set, we may want to know the expected number of asthmatic attacks per year for a patient with recurrent respiratory infection and GHQ-12 score of 8,\n\npred &lt;- predict(\n  pois_attack_final, list(res_inf = \"yes\", ghq12 = 8), type = \"response\"\n)\nround(pred, 1)\n\n  1 \n1.6 \n\n\nNow, let’s say we want to know the expected number of asthmatic attacks per year for those with and without recurrent respiratory infection for each 12-mark increase in GHQ-12 score.\n\nnew_data &lt;- tibble(\n  res_inf = rep(c(\"yes\", \"no\"), each = 4),\n  ghq12   = rep(c(0, 12, 24, 36), 2)\n)\nnew_data$attack_pred &lt;- round(\n  predict(pois_attack_final, new_data, type = \"response\"),\n  digits = 1\n)\nnew_data\n\n# A tibble: 8 × 3\n  res_inf ghq12 attack_pred\n  &lt;chr&gt;   &lt;dbl&gt;       &lt;dbl&gt;\n1 yes         0         1.1\n2 yes        12         2  \n3 yes        24         3.6\n4 yes        36         6.6\n5 no          0         0.7\n6 no         12         1.3\n7 no         24         2.4\n8 no         36         4.3\n\n\nWe can also check using the histogram,\n\nlambda_fun &lt;- function(ghq12, currRespInf = c(0, 1)){\n  exp(-0.34 + 0.43 * currRespInf + 0.05 * ghq12)\n}\n\npar(mfrow = c(1, 2))\nhist(\n  rpois(n = 10000, lambda = lambda_fun(ghq12 = 6, currRespInf = 0)),\n  main = \"Dist of Pred Asthma Attacks for Healthy Participant\"\n)\n\nhist(\n  rpois(n = 10000, lambda = lambda_fun(ghq12 = 28, currRespInf = 1)),\n  main = \"Dist of Pred Asthma Attacks for Sick Participant\"\n)\n\n\n\n\n\n\n\npar(mfrow = c(1, 1))\n\nWe can see that the histogram supports the relationship we saw in the pairs grpah above.\nWe can also predict the probability of number of ashma attacks.\n\n# Pr(nAttacks &gt;= 3|healthy)\n1 - ppois(q = 2, lambda_fun(ghq12 = 6, currRespInf = 0))\n\n[1] 0.07323225\n\n\nA healthy person has approximately 7% chance of experiencing 3 or more asthma attacks in the specified time interval.\n\n# Pr(nAttacks &gt;= 3|sick)\n1 - ppois(q = 2, lambda_fun(ghq12 = 28, currRespInf = 1))\n\n[1] 0.8192219\n\n\nA sick person has approximately 82% chance of experiencing 3 or more asthma attacks in the specified time interval.",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Poisson Regression Model</span>"
    ]
  },
  {
    "objectID": "lessons/05_glm_poisson.html#summary-1",
    "href": "lessons/05_glm_poisson.html#summary-1",
    "title": "29  Poisson Regression Model",
    "section": "29.5 Summary",
    "text": "29.5 Summary\nIn this lecture, we went through the basics about Poisson regression for count data. We performed the analysis for each and learned how to assess the model fit for the regression models. We learned how to nicely present and interpret the results. In addition, we also learned how to utilize the model for prediction.",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Poisson Regression Model</span>"
    ]
  },
  {
    "objectID": "06_header_special-topics.html",
    "href": "06_header_special-topics.html",
    "title": "Special Topics in Statistical Modelling",
    "section": "",
    "text": "Text Outline\nThis text covers the basics of:",
    "crumbs": [
      "Special Topics in Statistical Modelling"
    ]
  },
  {
    "objectID": "06_header_special-topics.html#text-outline",
    "href": "06_header_special-topics.html#text-outline",
    "title": "Special Topics in Statistical Modelling",
    "section": "",
    "text": "Exploring data\nOne-sample tests\nTwo-sample tests\nANOVA and linear regression\nGeneralized linear models\nSome special topics\nMaybe a bit about power calculations…",
    "crumbs": [
      "Special Topics in Statistical Modelling"
    ]
  },
  {
    "objectID": "06_header_special-topics.html#part-outline",
    "href": "06_header_special-topics.html#part-outline",
    "title": "Special Topics in Statistical Modelling",
    "section": "Part Outline",
    "text": "Part Outline\nThis part of the text contains the chapters on various different special statistical models:\n\nLinear Mixed Effects Models\nStructural Equation Models\nCox Proportional Hazards Regression\n(TBD) Multivariate Methods for Genetics/Genomics\nRidge, LASSO, and Elastic Net Regression",
    "crumbs": [
      "Special Topics in Statistical Modelling"
    ]
  },
  {
    "objectID": "lessons/06_elastic_net_regression.html",
    "href": "lessons/06_elastic_net_regression.html",
    "title": "\n30  Elastic Net Regression Model\n",
    "section": "",
    "text": "30.1 Libraries for the lessons\nFor this chapter, we will be using the following packages\nThese are loaded as follows using the function library(),\n# Installing Required Packages\n# intsall.packages(\"foreign\")\n# intsall.packages(\"skimr\")\n# intsall.packages(\"gtsummary\")\n# intsall.packages(\"GGally\")\n# install.packages(\"glmnet\")\n# install.packages(\"car\")\n# intsall.packages(tidyverse)\n\n# Loading Required Packages\nlibrary(foreign)\nlibrary(skimr)\nlibrary(gtsummary)\nlibrary(GGally)\nlibrary(glmnet)\nlibrary(car)\nlibrary(tidyverse)",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Elastic Net Regression Model</span>"
    ]
  },
  {
    "objectID": "lessons/06_elastic_net_regression.html#libraries-for-the-lessons",
    "href": "lessons/06_elastic_net_regression.html#libraries-for-the-lessons",
    "title": "\n30  Elastic Net Regression Model\n",
    "section": "",
    "text": "foreign: for reading SPSS and STATA datasets\n\nskimr: for summaries the datasets\n\ngtsummary: for coming up with nice tables for results and plotting the graphs\n\nGGally: for plotting the pairs graphs\n\nglmnet: ridge regression model\n\ncar: for finding vif\n\ntidyverse: a general and powerful package for data transformation",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Elastic Net Regression Model</span>"
    ]
  },
  {
    "objectID": "lessons/06_elastic_net_regression.html#introduction-to-elastic-net-regression",
    "href": "lessons/06_elastic_net_regression.html#introduction-to-elastic-net-regression",
    "title": "\n30  Elastic Net Regression Model\n",
    "section": "\n30.2 Introduction to Elastic Net Regression",
    "text": "30.2 Introduction to Elastic Net Regression\nIn multiple linear regression analysis, it is not uncommon for specific problems to arise during the analysis. One of them is the problem of multicollinearity. Multicollinearity is a condition that appears in multiple regression analysis when one independent variable is correlated with another independent variable. Multicollinearity can create inaccurate estimates of the regression coefficients, inflate the standard errors of the regression coefficients, deflate the partial t-tests for theregression coefficients, give false, nonsignificant, p-values, and degrade the predictability of the model. Multicollinearity is a serious problem, where in cases of high multicollinearity, it results in making inaccurate decisions or increasing the chance of accepting the wrong hypothesis. Therefore it is very important to find the most suitable method to deal with multicollinearity. There are several ways to detect the presence of multicollinearity including looking at the correlation between independent variables and using the Variance Inflation Factor (VIF). As for the method to overcome the problem of multicollinearity, one way is by shrinking the estimated coefficients. The shrinkage method is often referred to as the regularization method. The regularization method can shrink the parameters to near zero relative to the least squares estimate. The regularization methods that are often used are Regression Ridge, Least Absolute Shrinkage and Selection Operator (LASSO), and Elastic-Net. Ridge Regression(RR) is a technique to stabilize the value of the regression coefficient due to multicollinearity problems. By adding a degree of bias to the regression estimate, RR reduces the standard error and obtains a more accurate estimate of the regression coefficient than the OLS. Meanwhile, LASSO and Elastic-Net overcome the problem of multicollinearity by reducing the regression coefficients of the independent variables that have a high correlation close to zero or exactly zero.",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Elastic Net Regression Model</span>"
    ]
  },
  {
    "objectID": "lessons/06_elastic_net_regression.html#mathematical-formulation-of-the-model",
    "href": "lessons/06_elastic_net_regression.html#mathematical-formulation-of-the-model",
    "title": "\n30  Elastic Net Regression Model\n",
    "section": "\n30.3 Mathematical Formulation of the Model",
    "text": "30.3 Mathematical Formulation of the Model\nOne method that can be used to estimate parameters is Ordinary Least Squares (OLS). This method requires the absence of multicollinearity between independent variables. If the independent variable has multicollinearity, the estimate of the regression coefficient may be imprecise. This method is used to estimate \\(\\beta\\) by minimizing the sum of squared errors. If the data consists of n observations \\([{y_i, x_i}]^n_{i=1}\\) and each observation i includes a scalar response \\(y_i\\) and a vector of p predictors (regressors) \\(x_{ij}\\) for j = 1, …, m, a multiple linear regression model can be written as n the matrix form the model as \\(Y = X \\beta + \\epsilon\\) where \\(Y_{n \\times 1}\\) is the vector dependent variable, \\(X_{n \\times m}\\) represents the explanatory variables, \\(\\beta_{m \\times 1}\\) is the regression coefficients to be estimated, and \\(\\epsilon_{m \\times 1}\\) represents the errors or residuals. \\(\\hat\\beta_{OLS} = (X^T X)^{-1} X^T Y\\) is estimated regression coefficients using OLS by minimizing the squared distances between the observed and the predicted dependent variable. To have unbiased OLS estimation of the model, some assumptions should be satisfied. Those assumptions are that the errors have an expected value of zero, that the independent variables are non-random, that the independent variables are linearly independent (non-multicollinearity), that the disturbance are homoscedastic and not autocorrelated. If the independent variables have multicollinearity the estimates of coefficient regression may be imprecise.\n\n30.3.1 Ridge Regression\nRidge regression introduced by Horel (1962) is one method for deal with multicollinearity problems. The ridge regression technique is based on addition the ridge parameter (\\(\\lambda\\)) to the diagonal of the \\(X^T X\\) matrix forms a new matrix \\((X^T X + \\lambda I)\\). is called ridge regression because diagonal one in the correlation matrix can be described as ridge Hoerl and Kennard (1970). The ridge regression coefficients estimator is \\[\n\\hat\\beta_R = (X^T X + \\lambda I)^{-1} X^T Y, \\lambda \\geq 0\n\\] when \\(\\lambda = 0\\), the ridge estimator become as the OLS. If \\(\\lambda &gt; 0\\) the ridge estimator will be biased against the \\(\\hat\\beta_{OLS}\\) but tends to be more accurate than the least squares estimator. Ridge regression can also be written in Lagrangian form: \\[\n\\hat\\beta_{\\text{RIDGE}} = \\arg \\min_{\\beta}||y - X\\beta||^2_2 + \\lambda||\\beta||^2_2\n\\] where \\(||y - X\\beta||^2_2 = \\sum^n_{i=1}(y_i - x^T_i\\beta)^2\\) is norm loss function (i.e. residual sum of squares), \\(x^T_i\\) is the i-th row of X, \\(||\\beta||^2_2 = \\sum^p_{j=1}\\beta^2_j\\) is the penalty on \\(\\beta\\), and \\(\\lambda \\geq 0\\) is the tuning parameter which regulates the strength of the penalty by determining the relative importance of the data-dependet empirical error and penalty term. Ridge regression has the ability to solve problems multicollinearity by limiting the estimated coefficients, hence, it reduces the estimator’s variance but introduces some bias.\n\n30.3.2 Least Absolute Shrinkage and Selection Operator\nLeast Absolute Shrinkage and Selection Operator (LASSO) introduced by Tibshirani (1996) is a method that aims to reduce the regression coefficients of independent variables that have a high correlation with errors to exactly zero or close to zero. LASSO regression can also be written in Lagrangian form: \\[\n\\hat\\beta_{\\text{LASSO}} = \\arg \\min_{\\beta}||y - X\\beta||^2_2 + \\lambda||\\beta||_1\n\\] where \\(||\\beta||_1 = \\sum^p_{j=1}|\\beta_j|\\) is the penalty on \\(\\beta\\). with the condition \\(||\\beta||_1 \\leq \\lambda\\), where \\(\\lambda\\) is a tuning parameter that controls the shrinkage of the LASSO coefficient with \\(\\lambda \\geq 0\\). If \\(\\lambda &lt; \\lambda_0\\) with \\(\\lambda_0 = ||\\hat\\beta_j||_1\\) it will cause the shrinkage coefficient to approach zero or exactly zero, so LASSO helps as a variable selection. Like ridge, the absolute value penalty of the LASSO coefficient introduces shrinkage towards zero. However, on ridge regression, some of the coefficients are not shrinks to exactly zero.\n\n30.3.3 Elastic Net\nAccording to Zou and Hastie (2005), the Elastic-Net method can shrink the regression coefficient exactly to zero, besides that this method can also perform variable selection simultaneously with Elastic-Net penalties which are written as follows: \\[\n\\sum^p_{j=1}[\\alpha |\\beta_j| + (1 - \\alpha)\\beta^2_j]\n\\] with \\(\\alpha = \\frac{\\lambda_1}{\\lambda_1 + \\lambda_2},0 \\leq \\alpha \\leq 1\\). The coefficient estimator on Elastic-Net can be written as follows: \\[\n\\hat\\beta_{\\text{Elastic-net}} = \\arg \\min_{\\beta}||y - X\\beta||^2_2 + \\lambda_2||\\beta||^2_2 + \\lambda_1||\\beta||_1\n\\] Elastic-Net can be used to solve problems from LASSO. Where the LASSO Regression has disadvantages include; when p &gt; n then LASSO only chooses n variables included in the model, if there is a set of variables with high correlation, then LASSO only tends to choose one variable from the group and doesn’t care which one is selected, and when p &lt; n, LASSO performance is dominated by Ridge Regression. Multicollinearity is the existence of a linear relationship between independent variables, where multicollinearity can occur in either some or all of the independent variables in the multiple linear regression model. One way to detect multicollinearity is to use the Variation Inflation Factor (VIF). VIF value can be calculated by the following formula: \\[\n\\text{VIF}_j = \\frac{1}{1 - R^2_j}\n\\] if the VIF value &gt; 10, it can be concluded significantly that there is multicollinearity between the independent variables and one way to overcome multicollinearity is using the Ridge Regression, LASSO and Elastic-Net.",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Elastic Net Regression Model</span>"
    ]
  },
  {
    "objectID": "lessons/06_elastic_net_regression.html#example",
    "href": "lessons/06_elastic_net_regression.html#example",
    "title": "\n30  Elastic Net Regression Model\n",
    "section": "\n30.4 Example",
    "text": "30.4 Example\nTo demostrate the Elastic net regression, we will use coronary.dta (https://github.com/drkamarul/multivar_data_analysis/tree/main/data) dataset in STATA format. The dataset contains the total cholesterol level, their individual characteristics and intervention groups in a hypothetical clinical trial. The dataset contains 200 observations for nine variables:\n\n\nid: Subjects’ ID.\n\ncad: Coronary artery disease status (categorical) {no cad, cad}.\n\nsbp : Systolic blood pressure in mmHg (numerical).\n\ndbp : Diastolic blood pressure in mmHg (numerical).\n\nchol: Total cholesterol level in mmol/L (numerical).\n\nage: Age in years (numerical).\n\nbmi: Body mass index (numerical).\n\nrace: Race of the subjects (categorical) {malay, chinese, indian}.\n\ngender: Gender of the subjects (categorical) {woman, man}.\n\n\n30.4.1 Exploring the data\nThe dataset is loaded as follows,\n\ncoronary &lt;- foreign::read.dta(\"../data/06_coronary.dta\")\nhead(coronary, n = 10)\n\n    id    cad sbp dbp   chol age      bmi    race gender\n1    1 no cad 106  68 6.5725  60 38.90000  indian  woman\n2   14 no cad 130  78 6.3250  34 37.80000   malay  woman\n3   56 no cad 136  84 5.9675  36 40.50000   malay  woman\n4   61 no cad 138 100 7.0400  45 37.60000   malay  woman\n5   62 no cad 115  85 6.6550  53 40.30000  indian    man\n6   64 no cad 124  72 5.9675  43 37.60000   malay    man\n7   69    cad 110  80 4.4825  44 34.28411   malay    man\n8  108 no cad 112  70 5.4725  50 40.90000 chinese  woman\n9  112 no cad 138  85 7.4525  43 41.20000 chinese  woman\n10 134 no cad 104  70 6.4350  48 41.00000 chinese    man\n\n\n\n30.4.1.1 Structure of the dataset\n\nstr(coronary)\n\n'data.frame':   200 obs. of  9 variables:\n $ id    : num  1 14 56 61 62 64 69 108 112 134 ...\n $ cad   : Factor w/ 2 levels \"no cad\",\"cad\": 1 1 1 1 1 1 2 1 1 1 ...\n $ sbp   : num  106 130 136 138 115 124 110 112 138 104 ...\n $ dbp   : num  68 78 84 100 85 72 80 70 85 70 ...\n $ chol  : num  6.57 6.33 5.97 7.04 6.66 ...\n $ age   : num  60 34 36 45 53 43 44 50 43 48 ...\n $ bmi   : num  38.9 37.8 40.5 37.6 40.3 ...\n $ race  : Factor w/ 3 levels \"malay\",\"chinese\",..: 3 1 1 1 3 1 1 2 2 2 ...\n $ gender: Factor w/ 2 levels \"woman\",\"man\": 1 1 1 1 2 2 2 1 1 2 ...\n - attr(*, \"datalabel\")= chr \"Written by R.              \"\n - attr(*, \"time.stamp\")= chr \"\"\n - attr(*, \"formats\")= chr [1:9] \"%9.0g\" \"%9.0g\" \"%9.0g\" \"%9.0g\" ...\n - attr(*, \"types\")= int [1:9] 100 108 100 100 100 100 100 108 108\n - attr(*, \"val.labels\")= chr [1:9] \"\" \"cad\" \"\" \"\" ...\n - attr(*, \"var.labels\")= chr [1:9] \"id\" \"cad\" \"sbp\" \"dbp\" ...\n - attr(*, \"version\")= int 7\n - attr(*, \"label.table\")=List of 3\n  ..$ cad   : Named int [1:2] 1 2\n  .. ..- attr(*, \"names\")= chr [1:2] \"no cad\" \"cad\"\n  ..$ race  : Named int [1:3] 1 2 3\n  .. ..- attr(*, \"names\")= chr [1:3] \"malay\" \"chinese\" \"indian\"\n  ..$ gender: Named int [1:2] 1 2\n  .. ..- attr(*, \"names\")= chr [1:2] \"woman\" \"man\"\n\n\n\n30.4.1.2 Summary\n\nskim(coronary)\n\n\nData summary\n\n\nName\ncoronary\n\n\nNumber of rows\n200\n\n\nNumber of columns\n9\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\ncad\n0\n1\nFALSE\n2\nno : 163, cad: 37\n\n\nrace\n0\n1\nFALSE\n3\nmal: 73, chi: 64, ind: 63\n\n\ngender\n0\n1\nFALSE\n2\nwom: 100, man: 100\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nid\n0\n1\n2218.28\n1411.69\n1.00\n901.50\n2243.50\n3346.75\n4696.00\n▇▅▇▆▅\n\n\nsbp\n0\n1\n130.18\n19.81\n88.00\n115.00\n126.00\n144.00\n187.00\n▂▇▅▃▁\n\n\ndbp\n0\n1\n82.31\n12.90\n56.00\n72.00\n80.00\n92.00\n120.00\n▂▇▆▃▁\n\n\nchol\n0\n1\n6.20\n1.18\n4.00\n5.39\n6.19\n6.89\n9.35\n▅▇▇▃▂\n\n\nage\n0\n1\n47.33\n7.34\n32.00\n42.00\n47.00\n53.00\n62.00\n▃▇▇▆▃\n\n\nbmi\n0\n1\n37.45\n2.68\n28.99\n36.10\n37.80\n39.20\n45.03\n▁▂▇▆▁\n\n\n\n\n\n\n30.4.1.3 Descriptives\n\ncoronary %&gt;%\n  tbl_summary()\n\n\nTable 30.1: Coronary Data\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nN = 2001\n\n\n\n\nid\n2,244 (902, 3,347)\n\n\ncad\n\n\n\n    no cad\n163 (82%)\n\n\n    cad\n37 (19%)\n\n\nsbp\n126 (115, 144)\n\n\ndbp\n80 (72, 92)\n\n\nchol\n6.19 (5.39, 6.89)\n\n\nage\n47 (42, 53)\n\n\nbmi\n37.80 (36.10, 39.20)\n\n\nrace\n\n\n\n    malay\n73 (37%)\n\n\n    chinese\n64 (32%)\n\n\n    indian\n63 (32%)\n\n\ngender\n\n\n\n    woman\n100 (50%)\n\n\n    man\n100 (50%)\n\n\n\n\n1 Median (IQR); n (%)\n\n\n\n\n\n\n\n\n\n\n30.4.1.4 Pairs\n\ncoronary %&gt;%\n  select(-id) %&gt;% \n  ggpairs()\n\n\n\n\n\n\n\n\n30.4.1.5 Variance Inflation Factor (VIF)\nThe most common way to detect multicollinearity is by using the variance inflation factor (VIF), which measures the correlation and strength of correlation between the predictor variables in a regression model.\nThe value for VIF starts at 1 and has no upper limit. A general rule of thumb for interpreting VIFs is as follows:\n\nA value of 1 indicates there is no correlation between a given predictor variable and any other predictor variables in the model.\nA value between 1 and 5 indicates moderate correlation between a given predictor variable and other predictor variables in the model, but this is often not severe enough to require attention.\nA value greater than 5 indicates potentially severe correlation between a given predictor variable and other predictor variables in the model. In this case, the coefficient estimates and p-values in the regression output are likely unreliable.\n\n\nmodel &lt;- lm(chol ~ sbp + dbp + age + bmi, data = coronary)\nvif(model)\n\n     sbp      dbp      age      bmi \n3.395821 3.222173 1.215033 1.036817",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Elastic Net Regression Model</span>"
    ]
  },
  {
    "objectID": "lessons/06_elastic_net_regression.html#fitting-the-regression-mode",
    "href": "lessons/06_elastic_net_regression.html#fitting-the-regression-mode",
    "title": "\n30  Elastic Net Regression Model\n",
    "section": "\n30.5 Fitting the regression mode",
    "text": "30.5 Fitting the regression mode\nWe are interested in knowing the relationship between blood pressure (SBP and DBP), age, and BMI as the predictors and the cholesterol level (outcome).\n\n30.5.1 Ridge regression model\nIn R, we can use ridge regression using several packages, with glmnet being one of the most popular.\n\n# X and y datasets\nX &lt;- coronary %&gt;%\n  select(-id, -cad, -chol, -race, -gender) %&gt;%\n  as.matrix()\nX_train &lt;- X[1:150,]\nX_test &lt;- X[151:200,]\n\ny &lt;- coronary %&gt;%\n  select(chol) %&gt;%\n  as.matrix()\ny_train &lt;- y[1:150,]\ny_test &lt;- y[151:200,]\n\n# Fit the model\nridge_model &lt;- glmnet(X_train, y_train, alpha = 0)\n\n# Cross-validation\ncv_ridge &lt;- cv.glmnet(X_train, y_train, alpha = 0)\n\n# Optimal lambda\nridge_best_lambda &lt;- cv_ridge$lambda.min\n\n# Refit the model with the best lambda\nridge_model_best &lt;- glmnet(X_test, y_test, alpha = 0, lambda = ridge_best_lambda)\n\n# Coefficients\ncoef(ridge_model_best)\n\n5 x 1 sparse Matrix of class \"dgCMatrix\"\n                      s0\n(Intercept)  4.843393672\nsbp          0.015787232\ndbp          0.010245924\nage         -0.001719317\nbmi         -0.042688888\n\n# Predict values using the fitted model\nridge_pred &lt;- predict(ridge_model_best, newx = X_test)\n\n# Calculate RMSE\nridge_rmse &lt;- sqrt(mean((ridge_pred - y_test)^2))\nprint(ridge_rmse)\n\n[1] 1.002847\n\n# Manually predict values using the fitted model and RMSE\nridge_pred_manual &lt;- as.matrix(cbind(1, X_test)) %*% coef(ridge_model_best)\nsqrt(mean((y_test - ridge_pred_manual)^2))\n\n[1] 1.002847\n\n\n\n30.5.2 LASSO Regression\n\n# Fit the lasso regression model\nlasso_model &lt;- glmnet(X_train, y_train, alpha = 1) # alpha = 1 for lasso regression\n\n# Cross-validation\ncv_lasso &lt;- cv.glmnet(X_train, y_train, alpha = 1)\n\n# Optimal lambda\nlasso_best_lambda &lt;- cv_lasso$lambda.min # Lambda value with miniumum cross-validated error\n\n# Refit the model with the best lambda\nlasso_model_best &lt;- glmnet(X_test, y_test, alpha = 1, lambda = lasso_best_lambda)\n\ncoef(lasso_model_best)\n\n5 x 1 sparse Matrix of class \"dgCMatrix\"\n                      s0\n(Intercept)  4.541831089\nsbp          0.019700866\ndbp          0.005918034\nage         -0.001589662\nbmi         -0.039151781\n\n# Predict using the best lambda\nlasso_pred &lt;- predict(lasso_model_best, newx = X_test)\n\n# Calculate RMSE\nlasso_rmse &lt;- sqrt(mean((lasso_pred - y_test)^2))\nprint(lasso_rmse)\n\n[1] 1.001118\n\n# Manually predict values using the fitted model and RMSE\nlasso_pred_manual &lt;- as.matrix(cbind(1, X_test)) %*% coef(lasso_model_best)\nsqrt(mean((y_test - lasso_pred_manual)^2))\n\n[1] 1.001118\n\n\n\n30.5.3 Elastic Net Regresssion\n\n# Fit the elastic net regression model\nelastic_net_model &lt;- glmnet(X_train, y_train, alpha = 0.5) # alpha = 0.5 for elastic net\n\ncv_elastic &lt;-  cv.glmnet(X_train, y_train, alpha = 0.5)\n\nelastic_best_lambda &lt;-  cv_elastic$lambda.min\n\nelastic_model_best &lt;- glmnet(X_test, y_test, alpha = 0.5, lambda = elastic_best_lambda)\n\ncoef(elastic_model_best)\n\n5 x 1 sparse Matrix of class \"dgCMatrix\"\n                      s0\n(Intercept)  4.575575172\nsbp          0.018992980\ndbp          0.006653255\nage         -0.001276755\nbmi         -0.039544432\n\n# Predict using the best lambda\nelastic_pred &lt;- predict(elastic_model_best, newx = X_test)\n\n# Calculate RMSE\nelastic_rmse &lt;- sqrt(mean((elastic_pred - y_test)^2))\nprint(elastic_rmse)\n\n[1] 1.001303\n\n# Manually predict values using the fitted model and RMSE\nelastic_pred_manual &lt;- as.matrix(cbind(1, X_test)) %*% coef(elastic_model_best)\nsqrt(mean((y_test - elastic_pred_manual)^2))\n\n[1] 1.001303",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Elastic Net Regression Model</span>"
    ]
  },
  {
    "objectID": "lessons/06_elastic_net_regression.html#results",
    "href": "lessons/06_elastic_net_regression.html#results",
    "title": "\n30  Elastic Net Regression Model\n",
    "section": "\n30.6 Results",
    "text": "30.6 Results\n\n30.6.1 Coefficients\n\nc1 &lt;- coef(ridge_model_best) %&gt;% as.matrix()\nc2 &lt;- coef(lasso_model_best) %&gt;% as.matrix()\nc3 &lt;- coef(elastic_model_best) %&gt;% as.matrix()\n\ncoef &lt;- cbind(c1, c2, c3)\ncolnames(coef) &lt;- c(\"Ridge\", \"LASSO\", \"Elastic Net\")\nprint(coef)\n\n                   Ridge        LASSO  Elastic Net\n(Intercept)  4.843393672  4.541831089  4.575575172\nsbp          0.015787232  0.019700866  0.018992980\ndbp          0.010245924  0.005918034  0.006653255\nage         -0.001719317 -0.001589662 -0.001276755\nbmi         -0.042688888 -0.039151781 -0.039544432\n\n\n\n30.6.2 RMSE\n\nrmse &lt;- cbind(\"RMSE\" = c(ridge_rmse, lasso_rmse, elastic_rmse)) \nrownames(rmse) &lt;- c(\"Ridge\", \"LASSO\", \"Elastic Net\")\nprint(rmse)\n\n                RMSE\nRidge       1.002847\nLASSO       1.001118\nElastic Net 1.001303",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Elastic Net Regression Model</span>"
    ]
  },
  {
    "objectID": "lessons/06_elastic_net_regression.html#summary-1",
    "href": "lessons/06_elastic_net_regression.html#summary-1",
    "title": "\n30  Elastic Net Regression Model\n",
    "section": "\n30.7 Summary",
    "text": "30.7 Summary\nIn this lecture, we went through the basic multicollinearity problem of liner regression model and trying yo solve it. We discussed different methods about this. We can solve the multicollinearity problem using ridge, LASSO and Elastic-net. We can also compare between them using both simulated data and real data based on different criteria. And, based on those we can say that which one can perform better than others.\n\n\n\n\nHoerl, Arthur E, and Robert W Kennard. 1970. “Ridge Regression: Biased Estimation for Nonorthogonal Problems.” Technometrics 12 (1): 55–67.\n\n\nHorel, AE. 1962. “Application of Ridge Analysis to Regression Problems.” Chemical Engineering Progress 58: 54–59.\n\n\nTibshirani, Robert. 1996. “Regression Shrinkage and Selection via the Lasso.” Journal of the Royal Statistical Society Series B: Statistical Methodology 58 (1): 267–88.\n\n\nZou, Hui, and Trevor Hastie. 2005. “Regularization and Variable Selection via the Elastic Net.” Journal of the Royal Statistical Society Series B: Statistical Methodology 67 (2): 301–20.",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Elastic Net Regression Model</span>"
    ]
  },
  {
    "objectID": "lessons/06_survival_analysis.html",
    "href": "lessons/06_survival_analysis.html",
    "title": "\n31  Survival Analysis in R\n",
    "section": "",
    "text": "31.1 What is survival analysis\nSurvival analysis is a statistical method for analyzing survival data (longitudinal time to event data). Now, before discussing the statistical terms, lets think about what survives, and what does it mean by survival. The answer that comes to our mind is, it’s the living being that survives, and the survival time means life span such that time between ones’s birth to death. That’s true, survival analysis was originally used solely for investigations of mortality and morbidity on vital registration statistics. Xian Lui (2012) noted the history of arithmetic analysis of survival process can be traced back to the 17th century when English statistician John Graunt first published the life table in 1662. Since the outcome occurs over the course of time, the data should be longitudinal data.Since then the survival analysis was widely used in clinical trials.\nOver the past 50 years, the literature notes the expanded applicability of survival analysis to the diverse fields like domain of biological science, biomedical science, engineering, and public health. Survival time therefore does not necessarily mean life span of living organisms, it can be start of drug to the first remission of the disease, diagnosis of the disease to the incidence of comorbidities, construction of a building to the collapse of the building, marriage to divorce, unemployment to the start of addiction drugs, occupational careers etc. In survival analysis the latter outcome is called an event, and the outcome of interest is the time to event.",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Survival Analysis in R</span>"
    ]
  },
  {
    "objectID": "lessons/06_survival_analysis.html#general-features-of-survival-data-structure",
    "href": "lessons/06_survival_analysis.html#general-features-of-survival-data-structure",
    "title": "\n31  Survival Analysis in R\n",
    "section": "\n31.2 General features of survival data structure",
    "text": "31.2 General features of survival data structure\n1.Survival process: The primary feature of the survival data is the description of a change in status from specified original status as the underlying outcome measure. For example, start of cancer drug to remission of the cancer. The survival probability is the probability that an individual survives from the time origin (e.g., diagnosis of diabetes) to a specified future time t.\n2.Time to event: It is calculated subtracting the specified starting time of the original status from the time of the occurrence of a particular event. It varies for different observations.\n3.Censoring: Censoring is defined as assigning an specific value to any observation whose information on specified event/outcome is missing. Any study has a specified time or survival data generally are collected for a particular interval in which the occurrence of a particular event is observed. Researchers therefore can only observe those events that occur within a surveillance window between two-time limits. Many observations may not encounter the event in the given time frame, or some observations may get lost before the specified time. Such observations whose information on specified event are missing are censored which indicates that event did not occur for those observations in the given time. Censoring may occur for various reasons. In clinical trials, patients may be lost to follow-up due to migration or health problems, in longitudinal survey, some baseline respondents may lose interest in participating etc. Since censoring frequently occurs, most of the survival analysis literally deals with incomplete survival data, and accordingly scientists have found ways to use such limited information for correctly analyzing the incomplete survival data based on some restrictive assumptions on the distribution of censored survival times. Given the importance of handling censoring in survival analysis, a variety of censoring types are possible as below:\na)Right censoring: The observations that are lost to follow-up or that do not encounter event during the specified study period, the actual event for such observations is placed somewhere to the right of the censored time along the time axis. This type of censoring is called right censoring. This type of censoring occurs most frequently in survival data. The basic assumption of this type of censoring is that the individual’s censored time is independent of the actual survival time, thereby making right censoring non-informative.\nb)Left censoring\nc)Interval censoring",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Survival Analysis in R</span>"
    ]
  },
  {
    "objectID": "lessons/06_survival_analysis.html#statistical-methods-used-in-survival-analysis",
    "href": "lessons/06_survival_analysis.html#statistical-methods-used-in-survival-analysis",
    "title": "\n31  Survival Analysis in R\n",
    "section": "\n31.3 Statistical methods used in survival analysis",
    "text": "31.3 Statistical methods used in survival analysis\n1) Kaplan-Meier (product limit)\nThe Kaplan–Meier (KM) method explores the survival of a population under investigation and/or tests differences in the crude cumulative survival between exposure groups, with a graphical representation of the endpoint occurrence as a function of time. It is a survival probability estimation method for non-parametric data. As events are assumed to occur independently of one another, the probabilities of surviving from one interval to the next may be multiplied together to give the cumulative survival probability. The KM survival curve, a plot of the KM survival probability against time, provides a useful summary of the data that can be used to estimate measures such as median survival time. The large skew encountered in the distribution of most survival data is the reason that the mean is not often used. It is called product limit approach because it estimates the survival probability each time an event occurs. (meaning it does not consider time as an interval/range (e.g., 5-10 years of age) but as a specific time (e.g., 5 years, 6 years, etc).\nImportant limitations of the KM method are\n\nIt does not provide an effect estimate (i.e., a relative risk) or the related confidence interval to compare the survival in different patient groups.\nIt does not permit the adjustment of confounders in etiological research or predictors in prognostic research.\nIt requires data categorization, so calculation of the incremental increase (or decrease) in the relative risk of a given event associated with one unit (or any number of units) increase in the candidate risk factor is not possible. These limitations can be approached by Cox regression analysis,\n\n2) Log rank test\nIt is a method of comparing survival function among groups (non-parametric test). It test the following hypothesis;\nHo: In terms of survivability, there is no difference between two groups.\nH1: There is a survival differential between the two groups.\nWe can reject the null hypothesis and infer that there is enough evidence to claim there is a difference in survival between the two groups if the p-value of the test is less than specified p-value which is generally 0.05 (95% confidence level).\n3) Cox proportional hazard regression\nThe Cox model is a regression technique for performing survival analyses. This model estimates the hazard ratio (HR) of a given endpoint associated with a specific risk factor which can be continous or categorical variable. The hazard is the probability that an individual who is under observation at a time t has an event at that time. It represents the instantaneous event rate for an individual who has already survived to time t. The hazard function for a particular time interval gives the probability that the subject will fail in that interval, given that the subject has not failed up to that point in time. In regression models for survival analysis, we attempt to estimate parameters which describe the relationship between our predictors and the hazard rate. It is called the proportional hazards model because it assumes that the effects of different variables on survival are constant over time and additive over a particular scale. When the risk factor is a continuous variable, the Cox model provides the HR of the study endpoint associated with a predefined unit of increase in the independent variable and when the risk factor is categorical variable, the Cox model provides HR of one group compared to another reference group.\nMathematical equation for Cox model\nHazard equation\nHazard = Probability of the event happening at time t given it hasn’t happened up until time t\n\\[\nH(t) = \\frac{p(\\text{Event} \\in [t, t + \\Delta t) | \\text{Event} &gt; t)}{\\Delta t}\n\\]\nCox Model Equation\n\\[\nH(t) = H_0(t)\\exp[B_1X_1 + B_2X_2 + ....B_kX_k]\n\\]\nWhere\n\n\n\\(X_1, \\ldots, X_k\\) represents the predictor variables and\n\n\\(H_0(t)\\) is the baseline hazard at time t, which is the hazard of an individual having the predictors set to zero.\n\n\\(B_1, \\ldots, B_k\\) represent regression coeffiecient\n\nBy computing the exponential of the regression coefficient \\(B_1, \\ldots, B_k\\) (directly provided by the software), we can calculate the HR of a given risk factor or predictor in the model. For example, if the risk factor \\(X_1\\) is dichotomous and it is codified “1” if present (exposed) and “0” if absent (unexposed), the expression \\(e^{B_i}\\) (where exp = 2.7183) can be interpreted as the estimated increase in the HR of the event in patients with the risk factor compared to those without the same risk factor; this is applied by assuming exposed and unexposed patients are similar for all the other covariates included in the model. If the risk factor is a continuous variable and it is directly related to the incidence rate of a given event (e.g., age in years as a risk factor for mortality), the HR will be interpreted as an increase in the hazard rate of death due to a 1-year increase in age.\nAssumption of Cox model\nHazard may fluctuate as a function of time, but the hazardous effects of different variables on survival are constant over time and additive over a particular scale.\n\\[\n\\frac{H(t)}{H_0} = \\text{constant}\n\\]\nWhere,\n\\(H(t)\\) = Increased hazard as a result of exposure.\n\\(H_0\\) = Baseline hazard in non-exposed",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Survival Analysis in R</span>"
    ]
  },
  {
    "objectID": "lessons/06_survival_analysis.html#packages",
    "href": "lessons/06_survival_analysis.html#packages",
    "title": "\n31  Survival Analysis in R\n",
    "section": "\n31.4 Packages",
    "text": "31.4 Packages\nThe packages required for conducting survival analysis can be installed automatically using the ctv packages. Following are the packages that will be functioning in this survival analysis project\nsurvival The survival package is the cornerstone of the entire R survival analysis. Not only is the package itself rich in features, but the object created by the Surv() function, which contains failure time and censoring information, is the basic survival analysis data structure in R.\nggfortify ggfortify enables producing handsome, one-line survival plots with ggplot2::autoplot",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Survival Analysis in R</span>"
    ]
  },
  {
    "objectID": "lessons/06_survival_analysis.html#load-the-data",
    "href": "lessons/06_survival_analysis.html#load-the-data",
    "title": "\n31  Survival Analysis in R\n",
    "section": "\n31.5 Load the data",
    "text": "31.5 Load the data\nThis project uses the veterans dataset contained in the survival package. Veteran dataset contains data from a two-treatment, randomized trial for lung cancer. We load the data with required packages into our library as follows:\n\nlibrary(survival)\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggfortify)",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Survival Analysis in R</span>"
    ]
  },
  {
    "objectID": "lessons/06_survival_analysis.html#checking-the-veteran-data",
    "href": "lessons/06_survival_analysis.html#checking-the-veteran-data",
    "title": "\n31  Survival Analysis in R\n",
    "section": "\n31.6 Checking the veteran data",
    "text": "31.6 Checking the veteran data\n\ndata(veteran)\n\nWarning in data(veteran): data set 'veteran' not found\n\nhead(veteran)\n\n  trt celltype time status karno diagtime age prior\n1   1 squamous   72      1    60        7  69     0\n2   1 squamous  411      1    70        5  64    10\n3   1 squamous  228      1    60        3  38     0\n4   1 squamous  126      1    60        9  63    10\n5   1 squamous  118      1    70       11  65    10\n6   1 squamous   10      1    20        5  49     0\n\n\nVeteran data contains following variables\ntrt: 1=standard 2=test\ncelltype: 1=squamous, 2=small cell, 3=adeno, 4=large\ntime: survival time in days after randomization\nstatus: censoring status. 1 = dead, 0 = censored\nkarno: Karnofsky performance score (100=good). For proper pronounciation and meaning of this score click here\ndiagtime: months from diagnosis to randomization\nage: in years\nprior: prior therapy 0 = no, 1 = yes",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Survival Analysis in R</span>"
    ]
  },
  {
    "objectID": "lessons/06_survival_analysis.html#kaplan-meier-analysis",
    "href": "lessons/06_survival_analysis.html#kaplan-meier-analysis",
    "title": "\n31  Survival Analysis in R\n",
    "section": "\n31.7 Kaplan Meier Analysis",
    "text": "31.7 Kaplan Meier Analysis\nWe first need to use Surv() to build the standard survival object. Note: a “+” after the time in the print out of the output indicates censored observation.\n\n# Kaplan Meier Survival curve\n\nkm &lt;- with(veteran, Surv(time, status))\nhead(km, 80)\n\n [1]  72  411  228  126  118   10   82  110  314  100+  42    8  144   25+  11 \n[16]  30  384    4   54   13  123+  97+ 153   59  117   16  151   22   56   21 \n[31]  18  139   20   31   52  287   18   51  122   27   54    7   63  392   10 \n[46]   8   92   35  117  132   12  162    3   95  177  162  216  553  278   12 \n[61] 260  200  156  182+ 143  105  103  250  100  999  112   87+ 231+ 242  991 \n[76] 111    1  587  389   33 \n\n\nNow to begin our analysis, we use the formula Surv() and the Surfit() function to produce the Kaplan-Meier estimates of the probability of survival over time. The times parameter of the summary() function gives some control over which times to print. Here, it is set to print the estimates for 1, 30, 60 and 90 days, and then every 90 days thereafter. This is the simplest possible model. It only takes three lines of R code to fit it, and produce numerical and graphical summaries.\n\nkm_fit &lt;- survfit(Surv(time, status) ~ 1, data=veteran)\nsummary(km_fit, times = c(1,30,60,90*(1:10)))\n\nCall: survfit(formula = Surv(time, status) ~ 1, data = veteran)\n\n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    1    137       2    0.985  0.0102      0.96552       1.0000\n   30     97      39    0.700  0.0392      0.62774       0.7816\n   60     73      22    0.538  0.0427      0.46070       0.6288\n   90     62      10    0.464  0.0428      0.38731       0.5560\n  180     27      30    0.222  0.0369      0.16066       0.3079\n  270     16       9    0.144  0.0319      0.09338       0.2223\n  360     10       6    0.090  0.0265      0.05061       0.1602\n  450      5       5    0.045  0.0194      0.01931       0.1049\n  540      4       1    0.036  0.0175      0.01389       0.0934\n  630      2       2    0.018  0.0126      0.00459       0.0707\n  720      2       0    0.018  0.0126      0.00459       0.0707\n  810      2       0    0.018  0.0126      0.00459       0.0707\n  900      2       0    0.018  0.0126      0.00459       0.0707\n\n#plot(km_fit, xlab=\"Days\", main = 'Kaplan Meyer Plot') #base graphics is always ready\nautoplot(km_fit)",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Survival Analysis in R</span>"
    ]
  },
  {
    "objectID": "lessons/06_survival_analysis.html#survival-curves-by-treatment",
    "href": "lessons/06_survival_analysis.html#survival-curves-by-treatment",
    "title": "\n31  Survival Analysis in R\n",
    "section": "\n31.8 Survival curves by treatment",
    "text": "31.8 Survival curves by treatment\n\nkm_trt_fit &lt;- survfit(Surv(time, status) ~ trt, data=veteran)\nautoplot(km_trt_fit)\n\n\n\n\n\n\n\nNow to show one more small exploratory plot, we will create a new data frame to look at survival by age. First, we will create a new data frame with a categorical variable AG that has values LT60 and GT60, which respectively describe veterans younger and older than sixty. And we make trt and prior into factor variables.\n\nvet &lt;- mutate(\n  veteran,\n  AG = ifelse((age &lt; 60), \"LT60\", \"OV60\"),\n  AG = factor(AG),\n  trt = factor(trt,labels=c(\"standard\",\"test\")),\n  prior = factor(prior,labels=c(\"N0\",\"Yes\"))\n)\n\nkm_AG_fit &lt;- survfit(Surv(time, status) ~ AG, data=vet)\nautoplot(km_AG_fit)\n\n\n\n\n\n\n\nAlthough the two curves appear to overlap in the first fifty days, younger patients clearly have a better chance of surviving more than a year.",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Survival Analysis in R</span>"
    ]
  },
  {
    "objectID": "lessons/06_survival_analysis.html#conducting-log-rank-test",
    "href": "lessons/06_survival_analysis.html#conducting-log-rank-test",
    "title": "\n31  Survival Analysis in R\n",
    "section": "\n31.9 Conducting log-rank test",
    "text": "31.9 Conducting log-rank test\n\nlibrary(\"survival\")\nlibrary(\"survminer\")\n\nLoading required package: ggpubr\n\n\n\nAttaching package: 'survminer'\n\n\nThe following object is masked from 'package:survival':\n\n    myeloma\n\nlibrary(\"Rcpp\")\n\nFirst lets look at the summary table of the survival curve\n\nsummary(km_trt_fit)$table\n\n      records n.max n.start events    rmean se(rmean) median 0.95LCL 0.95UCL\ntrt=1      69    69      69     64 123.9282  14.84352  103.0      59     132\ntrt=2      68    68      68     64 142.0613  26.81071   52.5      44      95\n\n\nNow for better visualization with p-value, we are using following plot\n\nggsurvplot(\n  km_trt_fit,\n  pval = TRUE, conf.int = TRUE,\n  risk.table = TRUE, # Add risk table\n  risk.table.col = \"strata\", # Change risk table color by groups\n  linetype = \"strata\", # Change line type by groups\n  surv.median.line = \"hv\", # Specify median survival\n  ggtheme = theme_bw(), # Change ggplot2 theme\n  palette = c(\"#E7B800\", \"#2E9FDF\")\n)\n\nWarning in geom_segment(aes(x = 0, y = max(y2), xend = max(x1), yend = max(y2)), : All aesthetics have length 1, but the data has 2 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 2 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 2 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 2 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\nP-value is 0.93 which indicates that there is no significant difference between treatment 1 and 2 for survivability outcome.\nThe survival chance is 1.0 at time zero (or 100 percent of the participants are alive).\nAt time 250, the chances of survival for both trt=1 and tr2=2 are about 0.13 (or 13 percent).\nThe median survival time for trt=2 is about 60 days and for trt=1 is about 100 days, indicating that trt=1 has a better survival rate than trt=2, however the difference is not statistically significant\nThe following code shows how to perform a log-rank test to determine if there is a difference in survival between trt groups who received different treatments:\n\nsurv_diff &lt;- survdiff(Surv(time, status) ~ trt, data = veteran)\nsurv_diff\n\nCall:\nsurvdiff(formula = Surv(time, status) ~ trt, data = veteran)\n\n       N Observed Expected (O-E)^2/E (O-E)^2/V\ntrt=1 69       64     64.5   0.00388   0.00823\ntrt=2 68       64     63.5   0.00394   0.00823\n\n Chisq= 0  on 1 degrees of freedom, p= 0.9 \n\n\nThe Chi-Squared test statistic is 0 with 1 degree of freedom and the corresponding p-value is 0.9. Since this p-value is greater than 0.05, we cannot reject the null hypothesis.\nIn other words, we do not have sufficient evidence to say that there is a statistically significant difference in survival between the two treatment groups.",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Survival Analysis in R</span>"
    ]
  },
  {
    "objectID": "lessons/06_survival_analysis.html#cox-proportional-hazard-models",
    "href": "lessons/06_survival_analysis.html#cox-proportional-hazard-models",
    "title": "\n31  Survival Analysis in R\n",
    "section": "\n31.10 Cox Proportional Hazard Models",
    "text": "31.10 Cox Proportional Hazard Models\nNow we will fit a Cox Proportional Hazards model that makes use of all of the covariates in the data set.\n\n# Fit the Cox Model\ncox &lt;- coxph(\n  Surv(time, status) ~ trt + celltype + karno + diagtime + age + prior,\n  data = vet\n)\nsummary(cox)\n\nCall:\ncoxph(formula = Surv(time, status) ~ trt + celltype + karno + \n    diagtime + age + prior, data = vet)\n\n  n= 137, number of events= 128 \n\n                        coef  exp(coef)   se(coef)      z Pr(&gt;|z|)    \ntrttest            2.946e-01  1.343e+00  2.075e-01  1.419  0.15577    \ncelltypesmallcell  8.616e-01  2.367e+00  2.753e-01  3.130  0.00175 ** \ncelltypeadeno      1.196e+00  3.307e+00  3.009e-01  3.975 7.05e-05 ***\ncelltypelarge      4.013e-01  1.494e+00  2.827e-01  1.420  0.15574    \nkarno             -3.282e-02  9.677e-01  5.508e-03 -5.958 2.55e-09 ***\ndiagtime           8.132e-05  1.000e+00  9.136e-03  0.009  0.99290    \nage               -8.706e-03  9.913e-01  9.300e-03 -0.936  0.34920    \npriorYes           7.159e-02  1.074e+00  2.323e-01  0.308  0.75794    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n                  exp(coef) exp(-coef) lower .95 upper .95\ntrttest              1.3426     0.7448    0.8939    2.0166\ncelltypesmallcell    2.3669     0.4225    1.3799    4.0597\ncelltypeadeno        3.3071     0.3024    1.8336    5.9647\ncelltypelarge        1.4938     0.6695    0.8583    2.5996\nkarno                0.9677     1.0334    0.9573    0.9782\ndiagtime             1.0001     0.9999    0.9823    1.0182\nage                  0.9913     1.0087    0.9734    1.0096\npriorYes             1.0742     0.9309    0.6813    1.6937\n\nConcordance= 0.736  (se = 0.021 )\nLikelihood ratio test= 62.1  on 8 df,   p=2e-10\nWald test            = 62.37  on 8 df,   p=2e-10\nScore (logrank) test = 66.74  on 8 df,   p=2e-11",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Survival Analysis in R</span>"
    ]
  },
  {
    "objectID": "lessons/06_survival_analysis.html#plot-the-cox-model",
    "href": "lessons/06_survival_analysis.html#plot-the-cox-model",
    "title": "\n31  Survival Analysis in R\n",
    "section": "\n31.11 Plot the Cox model",
    "text": "31.11 Plot the Cox model\n\ncox_fit &lt;- survfit(cox)\n#plot(cox_fit, main = \"cph model\", xlab=\"Days\")\nautoplot(cox_fit)\n\n\n\n\n\n\n\nNote that the model flags small cell type, adeno cell type and karno as significant. However, some caution needs to be exercised in interpreting these results. While the Cox Proportional Hazard’s model is thought to be “robust”, a careful analysis would check the assumptions underlying the model. For example, the Cox model assumes that the covariates do not vary with time. In a vignette that accompanies the Survival package Therneau, Crowson and Atkinson demonstrate that the Karnofsky score (karno) is, in fact, time-dependent so the assumptions for the Cox model are not met. The vignette authors have presented a strategy for dealing with time dependent covariates.\nData scientists who are accustomed to computing ROC curves to assess model performance should be interested in the Concordance statistic. The documentation for the survConcordance() function in the Survival package defines concordance as “the probability of agreement for any two randomly chosen observations, where in this case agreement means that the observation with the shorter survival time of the two also has the larger risk score. The predictor (or risk score) will often be the result of a Cox model or other regression” and notes that: “For continuous covariates concordance is equivalent to Kendall’s tau, and for logistic regression it is equivalent to the area under the ROC curve.”.\nTo demonstrate using the Survival package, along with ggplot2 and ggfortify, here we will fit Aalen’s additive regression model for censored data to the veteran data. To further understand AA regression please click here. The documentation states: The Aalen model assumes that the cumulative hazard H(t) for a subject can be expressed as\na(t) + X B(t), where\na(t) is a time-dependent intercept term, X is the vector of covariates for the subject (possibly time-dependent), and B(t) is a time-dependent matrix of coefficients.”\nThe plots show how the effects of the covariates change over time. We can see the steep slope and then abrupt change in slope of karno.\n\naa_fit &lt;- aareg(\n  Surv(time, status) ~ trt + celltype + karno + diagtime + age + prior, \n  data = vet\n)\naa_fit\n\nCall:\naareg(formula = Surv(time, status) ~ trt + celltype + karno + \n    diagtime + age + prior, data = vet)\n\n  n= 137 \n    75 out of 97 unique event times used\n\n                      slope      coef se(coef)      z        p\nIntercept          0.083400  3.81e-02 1.09e-02  3.490 4.79e-04\ntrttest            0.006730  2.49e-03 2.58e-03  0.967 3.34e-01\ncelltypesmallcell  0.015000  7.30e-03 3.38e-03  2.160 3.09e-02\ncelltypeadeno      0.018400  1.03e-02 4.20e-03  2.450 1.42e-02\ncelltypelarge     -0.001090 -6.21e-04 2.71e-03 -0.229 8.19e-01\nkarno             -0.001180 -4.37e-04 8.77e-05 -4.980 6.28e-07\ndiagtime          -0.000243 -4.92e-05 1.64e-04 -0.300 7.65e-01\nage               -0.000246 -6.27e-05 1.28e-04 -0.491 6.23e-01\npriorYes           0.003300  1.54e-03 2.86e-03  0.539 5.90e-01\n\nChisq=41.62 on 8 df, p=1.6e-06; test weights=aalen",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Survival Analysis in R</span>"
    ]
  },
  {
    "objectID": "lessons/06_survival_analysis.html#plot-the-aalens-addictive-regression-model",
    "href": "lessons/06_survival_analysis.html#plot-the-aalens-addictive-regression-model",
    "title": "\n31  Survival Analysis in R\n",
    "section": "\n31.12 Plot the Aalen’s Addictive Regression model",
    "text": "31.12 Plot the Aalen’s Addictive Regression model\n\n#summary(aa_fit)  # provides a more complete summary of results\nautoplot(aa_fit)\n\n\n\n\n\n\n\n\n31.12.1 Interpretation of AA model graph\nLooking at this plot we can see that only karno is the time dependent variable because the graph is steeply below 0. Now with this knowledge, we can re-fit our model.",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Survival Analysis in R</span>"
    ]
  },
  {
    "objectID": "lessons/06_survival_analysis.html#references",
    "href": "lessons/06_survival_analysis.html#references",
    "title": "\n31  Survival Analysis in R\n",
    "section": "\n31.13 References",
    "text": "31.13 References\nLiu, X. (Xian X. ). (2012). Survival analysis models and applications. Wiley/Higher Education Press.\nAbd ElHafeez, S., D’Arrigo, G., Leonardis, D., Fusaro, M., Tripepi, G., & Roumeliotis, S. (2021). Methods to Analyze Time-to-Event Data: The Cox Regression Analysis. Oxidative medicine and cellular longevity, 2021, 1302811. https://doi.org/10.1155/2021/1302811\nAzzato, E., Greenberg, D., Shah, M. et al. Prevalent cases in observational studies of cancer survival: do they bias hazard ratio estimates?. Br J Cancer 100, 1806–1811 (2009). https://doi.org/10.1038/sj.bjc.6605062",
    "crumbs": [
      "Special Topics in Statistical Modelling",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Survival Analysis in R</span>"
    ]
  },
  {
    "objectID": "07_header_power.html",
    "href": "07_header_power.html",
    "title": "Statistical Power and Sample Size Determination",
    "section": "",
    "text": "Text Outline\nThis text covers the basics of:",
    "crumbs": [
      "Statistical Power and Sample Size Determination"
    ]
  },
  {
    "objectID": "07_header_power.html#text-outline",
    "href": "07_header_power.html#text-outline",
    "title": "Statistical Power and Sample Size Determination",
    "section": "",
    "text": "Exploring data\nOne-sample tests\nTwo-sample tests\nANOVA and linear regression\nGeneralized linear models\nSome special topics\nMaybe a bit about power calculations…",
    "crumbs": [
      "Statistical Power and Sample Size Determination"
    ]
  },
  {
    "objectID": "07_header_power.html#part-outline",
    "href": "07_header_power.html#part-outline",
    "title": "Statistical Power and Sample Size Determination",
    "section": "Part Outline",
    "text": "Part Outline\nThis part of the text will eventually contain some examples on statistical power calculations and sample size determination methods for some of the techniques covered in this text.",
    "crumbs": [
      "Statistical Power and Sample Size Determination"
    ]
  },
  {
    "objectID": "lessons_original/07_power_ols.html",
    "href": "lessons_original/07_power_ols.html",
    "title": "\n32  Power Analysis for OLS Regression\n",
    "section": "",
    "text": "32.1 Introduction\nThe power of a hypothesis test is the probability of correctly rejecting the null hypothesis or the probability that the test will correctly support the alternative hypothesis (detecting an effect when there actually is one)1. Then,\n\\[\nPower = 1-\\beta\n\\]\nWhere, \\(\\beta\\) = probability of committing a Type II Error (the probability that we would accept the null hypothesis even if the alternative hypothesis is actually true). Then, by decreasing \\(\\beta\\) power increases [@(pdf)ef].\nPower is mainly influenced by sample size, effect size, and significance level.",
    "crumbs": [
      "Statistical Power and Sample Size Determination",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Power Analysis for OLS Regression</span>"
    ]
  },
  {
    "objectID": "lessons_original/07_power_ols.html#introduction",
    "href": "lessons_original/07_power_ols.html#introduction",
    "title": "\n32  Power Analysis for OLS Regression\n",
    "section": "",
    "text": "High power: large chance of a test detecting a true effect.\nLow power: test only has a small chance of detecting a true effect or that the results are likely to be distorted by random and systematic error.\n\n\n\n\n\nVisual view of beta",
    "crumbs": [
      "Statistical Power and Sample Size Determination",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Power Analysis for OLS Regression</span>"
    ]
  },
  {
    "objectID": "lessons_original/07_power_ols.html#power-analysis-ols-regression",
    "href": "lessons_original/07_power_ols.html#power-analysis-ols-regression",
    "title": "\n32  Power Analysis for OLS Regression\n",
    "section": "\n32.2 Power Analysis: OLS Regression",
    "text": "32.2 Power Analysis: OLS Regression\nFor this power analysis we will use the univariate (simple) OLS regression example of our last presentation examining the relationship between a vehicle’s weight (WT) and its miles per gallon or fuel efficiency (MPG).\nWhen performed, the paired correlation provided us with a pearson’s correlation coefficient of r(30) = -.868, p&lt;0.05, (n = 32). When we ran this regression we got an (\\(R^2\\) = .75) Therefore for the r2 value (effect size) for a power analysis we will begin with an r2 value of .75 and an n = 32 to account for the observations already collected. However, the power analysis should occur before collecting samples so that we can have an appropriate number of observations required for our hypothesized effect size. In our example, we are also assuming that the variables are normally distributed. Based on our correlation analysis, weight likely needs a cubic transformation, This would mean that our model would have three coefficients of interest.\nFormula for a univariate Ordinary Least Squares (OLS) Regression:\n\\[\n\\hat{y}_i = \\beta_0 + \\beta_1x_i\n\\]\nThe OLS regression model line for our example is:\n\\[\n\\widehat{MPG_i} = \\beta_0 +\\beta_1*WT_i\n\\]\nUsing an alpha value of \\(\\alpha\\) = .05 (The probability of a type I error/rejecting a correct \\(H_0\\), we will identify the number of observations or sample size (n) necessary to obtain statistical power (80% or \\(\\beta\\) = 0.20) given various effect sizes. Statistical power in our example identifies the likelihood that a univariate OLS will detect an effect of a certain size if there is one.\nA power analysis is made up of four main components. We will provide estimates for any three of these, as the following functions in r calculate the fourth component.\nWe found three functions in r to conduct power analyses for an OLS regression:\n\nThe pwrss.f.reg function in the pwrss package\nThe pwr.f2.test function in the pwr package\nThe wp.regression function in the WebPower package\n\n\n32.2.1 The pwrss.f.reg function\nWe will start our power analysis using the The pwrss.f.reg function for one predictor in an OLS regression, with our given observations of n = 32 and \\(R^2\\) = .75. Given these values, we are expecting that one variable (WT) explains 75% of the variance in the outcome or Miles per gallon (R2=0.75 or r2 = 0.75 in the code)2.\n\nRegOne_lm &lt;- pwrss.f.reg(\n  r2 = 0.75,\n  k = 1,\n  n = 32,\n  power = NULL,\n  alpha = 0.05\n)\n\n Linear Regression (F test) \n R-squared Deviation from 0 (zero) \n H0: r2 = 0 \n HA: r2 &gt; 0 \n ------------------------------ \n  Statistical power = 1 \n  n = 32 \n ------------------------------ \n Numerator degrees of freedom = 1 \n Denominator degrees of freedom = 30 \n Non-centrality parameter = 96 \n Type I error rate = 0.05 \n Type II error rate = 0 \n\nRegOne_lm$power\n\n[1] 1\n\n\nGiven the information provided, we get 100% power. Our effect size of r2 = 0.75 is considered a large effect provided the following guidelines by Cohen (1988)3\n\\(f^2\\) = 0.02 indicates a small effect;\n\\(f^2\\) = 0.15 indicates a medium effect;\n\\(f^2\\) = 0.35 indicates a large effect.\nWe will use these guidelines to continue our exploration. We will concentrate on a fixed medium effect size. Where, the paired correlation is approximately r = .40 for a medium correlation and for an \\(f^2\\) or effect size of 0.15. using this fixed effect, we will look at various sample sizes to obtain power of 80% or greater given a medium effect size. In our sequence of possible sample sizes, the minimum n = 1 as n &gt; p(p+1)/2 = 1(2)/2 = 1\n\nOLSReg_df &lt;- tibble(n = seq.int(from = 2, to = 99 + 2))\n\nOLSReg_df$power &lt;- map_dbl(\n  .x = OLSReg_df$n,\n  .f = ~{\n    out_ls &lt;- pwrss.f.reg(\n      # \"Effect size\" of a linear model\n      r2 = 0.15,\n      # number of predictors\n      k = 1,\n      # sample size\n      n = .x,\n      power = NULL,\n      alpha = 0.05,\n      # Stop printing messages\n      verbose = FALSE\n    )\n    out_ls$power\n  }\n)\n\nWarning in qf(alpha, df1 = u, df2 = v, lower.tail = FALSE): NaNs produced\n\n\nThe following is the power curve for a fixed effect of f2 = 0.15\n\nggplot(data = OLSReg_df) +\n  theme_bw() +\n  aes(x = n, y = power) +\n  labs(\n    title = \"Omnibus (F-test) Power for Linear Model\",\n    subtitle = \"Fixed Effect Size R2 = 0.15, 1 Predictor\"\n  ) +\n  scale_y_continuous(limits = c(0, 1)) +\n  geom_point() +\n  geom_abline(slope = 0, intercept = 0.8, colour = \"gold\")\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nGiven the graph, we notice that we need an approximate sample size or n of close to 50 to detect a medium effect size in an OLS Regression.\nThe following is a power analysis for a univariate OLS regression given a fixed sample size. We will create a sequence of effect sizes that capture Cohen’s guidelines as well as the effect size of 0.75 of our sample regression. Our fixed n will be n = 32 as the sample.\n\nOLSRegN_df &lt;- tibble(R2 = seq(0, 0.75, length.out = 100))\n\nOLSRegN_df$power &lt;- map_dbl(\n  .x = OLSRegN_df$R2,\n  .f = ~{\n    out2_ls &lt;- pwrss.f.reg(\n      # \"Effect size\" of a linear model\n      r2 = .x,\n      # number of predictors\n      k = 1,\n      # sample size\n      n = 32,\n      power = NULL,\n      alpha = 0.05,\n      # Stop printing messages\n      verbose = FALSE\n    )\n    out2_ls$power\n  }\n)\n\nThe following is the power curve for a fixed sample size of n = 32\n\nggplot(data = OLSRegN_df) +\n  theme_bw() +\n  aes(x = R2, y = power) +\n  labs(\n    title = \"Omnibus (F-test) Power for Linear Model\",\n    subtitle = \"Fixed Sample Size = 32, 1 Predictor\"\n  ) +\n  scale_y_continuous(limits = c(0, 1)) +\n  geom_point() +\n  geom_abline(slope = 0, intercept = 0.8, colour = \"red\")\n\n\n\n\n\n\n\nGiven the graph, we notice that given n = 32, a power of 80% and higher is achieved when the effect size is at least approximately r2 = 0.20.\n\n32.2.2 The pwr.f2.test function\nPower analysis using the pwr.f2.test: where, u = 1, The F numerator degrees of freedom (u=1) or the number of coefficients(independent variables) in the model\nand we will use Cohen’s criteria for effect sizes and first provide analyses for a medium effect size of 0.15 [3]45\n\n# Using Cohen 1988 criteria, where, \n#f2 = 0.02 small effect;\n#f2 = 0.15 medium effect;\n#f2 = 0.35 indicates a large effect\n\n### Fixed Effect size f2 = 0.15###\n# n = 50\npwr.f2.test(\n  u = 1, \n  v = 50 - 1 - 1,\n  f2 = .15, \n  sig.level = 0.05, \n  power = NULL\n  )\n\n\n     Multiple regression power calculation \n\n              u = 1\n              v = 48\n             f2 = 0.15\n      sig.level = 0.05\n          power = 0.7653128\n\n# n = 25\npwr.f2.test(\n  u = 1, \n  v = 25 - 1 - 1,\n  f2 = 0.15, \n  sig.level = 0.05, \n  power = NULL\n  )\n\n\n     Multiple regression power calculation \n\n              u = 1\n              v = 23\n             f2 = 0.15\n      sig.level = 0.05\n          power = 0.4584646\n\n# n = 12\npwr.f2.test(\n  u = 1, \n  v = 12 - 1 - 1,\n  f2 = 0.15, \n  sig.level = 0.05, \n  power = NULL\n  )\n\n\n     Multiple regression power calculation \n\n              u = 1\n              v = 10\n             f2 = 0.15\n      sig.level = 0.05\n          power = 0.2289402\n\n\nNow, we will explore a fixed n = 32\n\n# ES = .02, r = .14\npwr.f2.test(\n  u = 1, \n  v = 32 - 1 - 1,\n  f2 = .02, \n  sig.level = 0.05, \n  power = NULL\n  )\n\n\n     Multiple regression power calculation \n\n              u = 1\n              v = 30\n             f2 = 0.02\n      sig.level = 0.05\n          power = 0.1210661\n\n# ES = 0.15, r = .39\npwr.f2.test(\n  u = 1, \n  v = 32 - 1 - 1,\n  f2 = 0.15, \n  sig.level = 0.05, \n  power = NULL\n  ) \n\n\n     Multiple regression power calculation \n\n              u = 1\n              v = 30\n             f2 = 0.15\n      sig.level = 0.05\n          power = 0.5637733\n\n# ES = .35, r = .59\npwr.f2.test(\n  u = 1, \n  v = 32 - 1 - 1,\n  f2 = 0.35, \n  sig.level = 0.05, \n  power = NULL\n  )\n\n\n     Multiple regression power calculation \n\n              u = 1\n              v = 30\n             f2 = 0.35\n      sig.level = 0.05\n          power = 0.8993357\n\n\nWe will now look at the 3 types of effect sizes given various sample sizes\n\neffect_sizes &lt;- c(0.02, 0.15, 0.35) \nsample_sizes = seq(20, 100, 20)\n\ninput_df &lt;- crossing(effect_sizes,sample_sizes)\nglimpse(input_df)\n\nRows: 15\nColumns: 2\n$ effect_sizes &lt;dbl&gt; 0.02, 0.02, 0.02, 0.02, 0.02, 0.15, 0.15, 0.15, 0.15, 0.1…\n$ sample_sizes &lt;dbl&gt; 20, 40, 60, 80, 100, 20, 40, 60, 80, 100, 20, 40, 60, 80,…\n\nget_power &lt;- function(df){\n  power_result &lt;- pwr.f2.test(\n    u = 1,\n    v = df$sample_sizes - 1 - 1, \n    f2 = df$effect_sizes,\n    )\n  df$power=power_result$power\n  return(df)\n}\n\n# run get_power for each combination of effect size \n# and sample size\n\npower_curves &lt;- input_df %&gt;%\n  do(get_power(.)) %&gt;%\n  mutate(effect_sizes = as.factor(effect_sizes)) \n\n\nggplot(power_curves, \n       aes(x=sample_sizes,\n           y=power, \n           color=effect_sizes)\n       ) + \n  geom_line() + \n  geom_hline(yintercept = 0.8, \n             linetype='dotdash',\n             color = \"purple\")\n\n\n\n\n\n\n\nBased on the graph, if we have an effect size of 0.15, we need approximately 50 or more observations (recall n = v + 1 + 1)\n\n32.2.3 The wp.regression function\nLastly, we use the wp.regression function to examine the appropriate sample size given an effect size of 0.15 to achieve a power of 80% or higher [6]7\n\n# Using webpower \n#p1 = 1\n### Fixed ES = 0.15 ###\nres &lt;- wp.regression(n = seq(20,100,20), \n                     p1 = 1, \n                     f2 = 0.15, \n                     alpha = 0.05, \n                     power = NULL\n                    )\nres\n\nPower for multiple regression\n\n      n p1 p2   f2 alpha     power\n     20  1  0 0.15  0.05 0.3745851\n     40  1  0 0.15  0.05 0.6654126\n     60  1  0 0.15  0.05 0.8389166\n     80  1  0 0.15  0.05 0.9280168\n    100  1  0 0.15  0.05 0.9695895\n\nURL: http://psychstat.org/regression\n\nplot(res,  main = \"Fixed Effect Size = 0.15\")+\nabline(a = .80, b = 0, col = 'steelblue', lwd = 3, lty = 2)\n\n\n\n\n\n\n\ninteger(0)\n\n\nThe results are similar to the previous functions. However, in this function, given an effect size of 0.15, we need an n of close to 60 to achieve 80% power.\n\n# Using webpower \n#p1 = 1\n### Fixed n = 50 ###\nres &lt;- wp.regression(n = 50, \n                     p1 = 1, \n                     f2 = seq(0.00, 0.35, 0.05), \n                     alpha = 0.05, \n                     power = NULL\n                    )\nres\n\nPower for multiple regression\n\n     n p1 p2   f2 alpha     power\n    50  1  0 0.00  0.05 0.0500000\n    50  1  0 0.05  0.05 0.3409707\n    50  1  0 0.10  0.05 0.5914439\n    50  1  0 0.15  0.05 0.7653128\n    50  1  0 0.20  0.05 0.8725329\n    50  1  0 0.25  0.05 0.9337077\n    50  1  0 0.30  0.05 0.9667049\n    50  1  0 0.35  0.05 0.9837529\n\nURL: http://psychstat.org/regression",
    "crumbs": [
      "Statistical Power and Sample Size Determination",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Power Analysis for OLS Regression</span>"
    ]
  },
  {
    "objectID": "lessons_original/07_power_ols.html#references",
    "href": "lessons_original/07_power_ols.html#references",
    "title": "\n32  Power Analysis for OLS Regression\n",
    "section": "References",
    "text": "References\n\n\n\n\n1. \nCohen J. Statistical power analysis for the behavioral sciences. 2nd ed. Hillsdale, N.J: L. Erlbaum Associates; 1988. \n\n\n2. \nA practical guide to statistical power and sample size calculations in r [Internet]. Available from: https://cran.r-project.org/web/packages/pwrss/vignettes/examples.html#3_Linear_Regression_(F_and_t_Tests)\n\n\n\n3. \nCohen J. Statistical power analysis for the behavioral sciences. 2nd ed. Hillsdale, N.J: L. Erlbaum Associates; 1988. \n\n\n4. \nPower in r | [Internet]. Available from: https://blogs.uoregon.edu/rclub/2015/11/10/power-in-r/\n\n\n\n5. \nStatistical power analysis - jacob cohen, 1992 [Internet]. Available from: https://journals.sagepub.com/doi/10.1111/1467-8721.ep10768783\n\n\n\n6. \nZhang Z, Wang L. Advanced statistics using r [Internet]. ISDSA Press; 2017. Available from: https://advstats.psychstat.org/\n\n\n\n7. \nWp.regression: Statistical power analysis for linear regression in WebPower: Basic and advanced statistical power analysis [Internet]. Available from: https://rdrr.io/cran/WebPower/man/wp.regression.html",
    "crumbs": [
      "Statistical Power and Sample Size Determination",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Power Analysis for OLS Regression</span>"
    ]
  }
]