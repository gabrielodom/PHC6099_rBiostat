---
title: "Multiple Linear Regression"
author: "Shelly Sinclair and Alvonee Penn"
date: "`r Sys.Date()`"
format:
  html:
    self-contained: true
    toc: true
    number-sections: true
    code-fold: true 
knitr:
  opts_chunk:      ########## set global options ############
    collapse: true # keep code from blocks together (if shown)
    echo: true    # don't show code
    message: false  # show messages
    warning: false  # show warnings
    error: false    # show error messages
    comment: ""    # don't show ## with printed output
    R.options:    
      digits: 3    # round to three digits
editor: source
---



# Multiple Linear Regression



**Multiple regression** generally explains the relationship between multiple independent or predictor variables and one dependent or criterion variable. A dependent variable is modeled as a function of several independent variables with corresponding coefficients, along with the constant term. Multiple regression requires two or more predictor variables.

**Equation**

$$
\hat{Y}= b_0 +b_1x_1 +b_2x_2 + ... b_nx_n + \epsilon
$$

where

$\hat{Y}$ is the predicted or expected value of the dependent variable, and is always numeric and continuous. 

$b_0$ is the value of $\hat{Y}$ when all of the independent variables $x_1 -x_n$ are equal to zero,

$x_1 -x_n$ are distinct independent or predictor variables

$b_1 - b_n$ are the estimated regression coefficients. Each regression coefficient represents the change in $\hat{Y}$ relative to a one unit change in the respective independent variable.

In the multiple regression situation, $b_1$, for example, is the change in $\hat{Y}$ relative to a one unit change in $x_1$, holding all other independent variables constant (i.e., when the remaining independent variables are held at the same value or are fixed).

**Assumptions**

1. Independence: The Y-values are statistically independent of each other as well as the errors. As with simple linear regression, this assumption is violated if several Y observations are made on the same subject.

2. Linearity: The mean value of Y for each specific combination of values of the independent variables ($X_1, X_2...X_n$) is a linear function of the intercept and parameters ($\beta_0, \beta_1,...$)

3. Homoscedasticity: The variance of Y is the same for any fixed combination of independent variables. 

4. Normal Distribution: The residual values are normally distributed with mean zero.

5. Multicollinearity: Multicollinearity cannot exist among the predictors (the variables are not correlated)

## Packages



```{r}

library(ggplot2)
library(gt)
library(car)
library(tidymodels)
library(lmtest)
library(pwr)
library(pwrss)
library(tidyverse)

```


## Data


The miles/gallon a car takes (dependent) based on the gross horsepower and weight of the car. 

```{r}

data("mtcars")
head(mtcars)

```

## Testing the Assumptions

### Linearity 

Is there a linear relationship between the dependent variable and each of the independent variables?

```{r}

# is there a linear relationship between MPG and HP?
ggplot(data = mtcars) +
  aes(
    x = hp,
    y = mpg
  ) +
  geom_point()

# is there a linear relationship between MPG and WT?
ggplot(data = mtcars) +
  aes(
    x = wt,
    y = mpg
  ) +
  geom_point()

```


### Multicollinearity 


Are the variables correlated? If the VIF score > 10 then there is multicollinearity. 

```{r}
#| code-fold: show

mod <- lm(mpg ~ hp + wt, data = mtcars)

# checking the VIF of both predictors
vif(mod)

# checking the correlation between the predictors
cor(mtcars$hp, mtcars$wt)

```

### Homoscedasticity

Checking the residual plots for homoscedasticity. The line below is relatively horizontal. 

```{r}

mtcars_df <- mtcars %>% 
  mutate(res_sqrt = sqrt(abs(rstandard(mod))))

ggplot(mtcars_df, aes(fitted(mod), res_sqrt)) +
  geom_point() +
  geom_smooth()

```

### Normality

Checking normality via histogram and QQ plot of the residuals.

```{r}

# histogram
hist(mtcars_df$res_sqrt)

# qq plot
plot(mod, 2)

# cook's d -> are the outliers in the qq plot driving the relationship in the model? 
cooks.distance(mod)

# Plot Cook's Distance with a horizontal line at 4/n to see which observations
# exceed this thresdhold
n <- nrow(mtcars)
plot(cooks.distance(mod), main = "Cooks Distance for Influential Obs")
abline(h = 4/n, lty = 2, col = "steelblue")



```

Cookâ€™s distance refers to how far, on average, predicted y-values will move if the observation in question is dropped from the data set.

We can clearly see that four observation in the dataset exceed the 4/n threshold. Thus, we would identify these two observations as influential data points that have a negative impact on the regression model.

Since there are very few extreme values according to cook's d values, we can assume they are not the driving force between the relationship of the predictor and outcome. 

Tentative model: 

mpg = $b_0$ + $b_1\times$horsepower + $b_2\times$weight + $\epsilon$

## Hypothesis Testing

Does the horsepower and weight of a car contribute significantly to the miles per gallon of the car? 

- What is the relationship between horsepower and weight, with miles per gallon of the car?-

$$
H_0: \beta_1 = \beta_2 = 0
$$

$$
H_1: \beta_1 \not= \beta_2 \not= 0
$$


```{r}
#| code-fold: show

car_model <- lm(mpg ~ hp + wt, data = mtcars_df)

# get the F statistic and p-value
summary(car_model) 

# creating a table of the model
car_model %>%
  tidy() %>% 
  gt()

```

The first step in interpreting the multiple regression analysis is to examine the F-statistic and the associated p-value, at the bottom of model summary.

In our example, it can be seen that p-value of the F-statistic is < 9.109e-12, which is less than 0.05. This means that we can reject the null and accept that either horsepower and/or weight of a car predict the miles per gallon. 

To see which predictor variables are significant, we can examine the coefficients table, which shows the estimate and the associated t-statistic p-values:

```{r}
#| code-fold: show

summary(car_model)$coefficient 

summary(car_model)$coefficient %>% 
  as_tibble() %>% 
  add_column(Variable = c("Intercept", "Horsepower", "Weight"), .before = "Estimate") %>% 
  gt()

```

Full model: mpg = 37.23 + (-0.03)$\times$horsepower + (-3.88)$\times$weight

For a given predictor, the t-statistic value evaluates whether or not there is 
significant association between the predictor and the outcome variable, that is 
whether the beta coefficient of the predictor is significantly different from zero.

It can be seen that horsepower and weight are significantly associated to changes in mpg.

```{r}
#| code-fold: show

# The confidence interval of the model
confint(car_model)

confint(car_model) %>% 
  as_tibble() %>% 
  add_column(Variable = c("Intercept", "Horsepower", "Weight"), .before = "2.5 %") %>% 
  gt()

```

## Likelihood Ratio Test

$H_0$: The full model and the nested model fit the data equally well. Thus, you should use the nested model.

$H_1$: The full model fits the data significantly better than the nested model. Thus, you should use the full model.

A likelihood ratio test compares the goodness of fit of two nested regression models.

A nested model is simply one that contains a subset of the predictor variables in the overall regression model.

We could then carry out another likelihood ratio test to determine if a model with only one predictor variable is significantly different from a model with the two predictors:

```{r}

#fit full model
model_full <- lm(mpg ~ hp + wt, data = mtcars)

#fit reduced model
model_reduced <- lm(mpg ~ hp, data = mtcars)

#perform likelihood ratio test for differences in models
lrtest(model_full, model_reduced)

#fit reduced model
model_reduced <- lm(mpg ~ wt, data = mtcars)

#perform likelihood ratio test for differences in models
lrtest(model_full, model_reduced)

```

Our p-values are less than 0.05 for both Chi-squared values, suggesting that the full model offers significant improvement in fit over the model with just one of the predictors. 

## Model Accuracy

In multiple linear regression, the $R^2$ represents the correlation coefficient between the observed values of the outcome variable (y) and the fitted (i.e., predicted) values of y. For this reason, the value of R will always be positive and will range from zero to one.

An $R^2$ value close to 1 indicates that the model explains a large portion of the variance in the outcome variable.


```{r}
#| code-fold: show

summary(car_model)$adj.r.squared %>% 
  as_tibble() %>% 
  rename("Adjusted R Squared" = value) %>% 
  gt()

```

Our model explains 81% of the variation in mpg values caused by horsepower and weight values. 


# Conclusion


```{r}

avPlots(car_model)

```


For multiple linear regression, there are multiple independent variables affecting the outcome. This outcome must be continuous, however, the independent variables can be numeric or categorical. 

There is an inverse or negative relationship between our predictors and outcome variable. The mpg decreases with heavier weight. Higher horsepower also reduces miles per gallon performance (higher fuel consumption).



# Power Analysis



Power analysis allows us to detect an effect of a given size with a given degree of confidence. It allows us to determine the probability of detecting an effect of a given size with a given level of confidence.

The `pwrss` package has a power analysis function for multiple linear regression:

```{r}

pwr_ls <- pwrss.f.reg(
  # R-squared from the model
  r2 = 0.81,
  # number of predictors
  k = 2,
  # sample size
  n = 32,
  power = NULL,
  alpha = 0.05
)
pwr_ls$power

```

Based on this analysis, our statistical power is 1. To create a power curve for a 
fixed sample size, we will vary the R2 values and visualize the change in 
statistical power. 

```{r}

# tibble of 100 R-squared values from interval [0,0.8]
powerLM1_df <- tibble(R2 = seq(0, 0.8, length.out = 100))

# adding column for power values based on the R-squared values
powerLM1_df$power <- map_dbl(
  .x = powerLM1_df$R2,
  .f = ~{
    pwrss.f.reg(
      # the "Effect size" of a linear model
      r2 = .x,
      # number of predictors
      k = 2,
      # sample size
      n = 32,
      power = NULL,
      alpha = 0.05,
      # Stop printing messages
      verbose = FALSE
    )$power
  }
)

# power curve for fixed sample size
ggplot(data = powerLM1_df) +
  theme_bw() +
  aes(
    x = R2, 
    y = power
  ) + 
  labs(
    title = "Omnibus (F-test) Power for Linear Model",
    subtitle = "Fixed Sample Size = 32, 2 Predictors"
  ) + 
  scale_y_continuous(limits = c(0, 1)) +
  geom_point() + 
  geom_abline(slope = 0, intercept = 0.8, colour = "red")

```

Because we saw that an R2 value of between 0.25 and 0.3 yields 80% power for
n = 32 samples, we will fix our power there and vary the sample size. An R2 
value of 0.25 equates to a univariate correlation of 0.5, which is quite strong. 

Note that because we have 2 predictors, we need n > p(p+1)/2 = 3
samples.

```{r}

# tibble of sample size values
powerLM2_df <- tibble(sample_size = seq.int(from = 4, to = 99 + 4))

# adding column for power values based on the sample sizes
powerLM2_df$power <- map_dbl(
  .x = powerLM2_df$sample_size,
  .f = ~{
    pwrss.f.reg(
      # "Effect size" of a linear model
      r2 = 0.25,
      # number of predictors
      k = 2,
      # sample size
      n = .x,
      power = NULL,
      alpha = 0.05,
      # Stop printing messages
      verbose = FALSE
    )$power
  }
)


# power curve for fixed sample size
ggplot(data = powerLM2_df) +
  theme_bw() +
  aes(
    x = sample_size, 
    y = power
  ) + 
  labs(
    title = "Omnibus (F-test) Power for Linear Model",
    subtitle = "Fixed Effect Size = 0.25, 2 Predictors"
  ) + 
  scale_y_continuous(limits = c(0, 1)) +
  geom_point() + 
  geom_abline(slope = 0, intercept = 0.8, colour = "red")

```



